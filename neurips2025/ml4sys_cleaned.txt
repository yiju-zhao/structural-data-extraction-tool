Morning. Welcome to our workshop. I'm about to get started, so let's see. Welcome to New Year's twenty twenty five workshop on machine learning for systems. I'm one of the organizers. Mimi and the other organizers are here. We're gonna meet each one of us As an overview of the workshop, we are interdisciplinary workshop that brings together researchers and practitioners in computer systems and machine learning. Focusing on novel applications of ML techniques, towards computer system problems. This is the ninth iteration of the same workshop. So when we we just got started, if people remember, they were the learned index papers, It really gave people this wild range of imagination. On what machine learning could do to traditional systems. This year, we'll additionally focus on compared to last year's LMM focus, using LMs and agent agentic workflows for computer system challenges And we're also applying machine learning to address emerging systems challenges introduced by Agented Workflows. Large scale training, and serving of large models, including LMs and multimodal models. That is not the only thing. We can be thinking about in the present time. Can also think about what this particular approach and technique can bring to the larger environment, not just computer systems. And this may include applying machine learning towards computer systems. Sustainability, including power, energy, and carbon optimization. A few stats about the workshop. As I mentioned, this is the ninth iteration of the workshop, We have really grown into a large field. For this time, we accepted 41 papers. Out of 60 submissions, We'll be presenting today four oral spotlight papers. We'll have six invited speakers, from academia, frontier labs, and startups. We Additionally, we'll be including including a debate that's aquifer aquifer sal. With four panelists. The topic will be, well, agents replace systems developers? And I'm delighted to announce that we're going to be hosting our first ever ML for Systems happy hour. So finally, we're able to offer free food, stay tuned. As a short introduction for our organizers, my name is Mimi. And can we all stand up a bit? We have Yeah. I think Stevia here. Oh, okay. Yeah. Come meet us during the breaks. So just as a preview of of what's about to come, in the morning, we'll start with three invited talks Take a short break, have another talk, and spotlight presentations. We'll have a poster session that bleeds a little bit into lunch. In the afternoon, we'll be opening with another keynote before diving into this debate. I'm really excited about this format. We'll see how it goes. We'll take a short break, have one more speaker, and two more spotlight presentations. And then we'll have our afternoon poster sessions. Any instructions with authors? No. Okay. Yeah. And this is the information to register for happy hour. I'll keep the slide down for a minute. Yeah. I'm told that to tell you guys who's registered right now. This is the spots are limited. And we wanna prioritize the attendees. And we'll be approving at like this minute. If you apply now. Then that's how dancing should Yeah. Yeah. So with that, I'll pass the mic to my co organizer Dan who will be introducing our first speaker. Everyone, I'm, delayed to introduce our first our keynote speaker, Azalea Mir Hosseini. Who will be talking about self improving AI and the future of computing systems. Azalea is an inter assistant professor of computer science and founder of Scaling Intelligence Lab at Stanford University, She has spent many years in industry AI labs including Google Brain, Anthropic, and Google DeepMind. Her past work includes MOE, neuroarchitecture, leading generative AI models. AlphaChip, a pioneering work on deep RL for layout optimization. And research on inference time scaling laws. She's also now a cofounder and CTO of a new startup called Recursive Intelligence. Yeah. Please take it away. Thank you, Dan. Thank you all for, coming to my talk. It's a pleasure. This is was among the first organizers of the organizers of the first iteration of the workshop, and we were there for for I was involved for a few years. A couple years at least, and then so so great to see the workshop is living on. So so many years, and it's becoming more and more timely as the time passes. I'm going to talk about test time scaling and emergence of local models. Let's start with scaling loss for pretraining. So probably everyone of you in this room have heard of scaling laws and scaling laws which is this very interesting phenomenon that suggests that as we throw in more compute data and parameters, into the model, there's this predictable way that these additional parameters would translate into more capability. So For example, here, there's an equation There's an ex there's a parallel equation that is correlating the test loss of this of training this language model with compute and this line, this prediction is very, very closely following the two empirical analysis and findings of from the data. So what this means is that for the first time in the past five years, we had this recipe that would translate more compute to more capability, and this has been like going on and the guiding light for training larger and larger models across several frontier labs. And then every now and then, we still hear that, oh, maybe pretraining's, scaling laws have hit a ball, but then it turns out that it's more like scale issue. So something comes up, like, some lab comes up with a a few new tricks, and then the scaling laws continue to hold on. So at least up up until now, this this phenomena had has been continuing to to work out. Now, in the last year and a half or so, a new kind of here appeared for scaling and that is scaling at test time. If you look into the cycle of language model training, it starts from pretraining follows by fine tuning. These are both training stages where the parameters of the model get updated. At test time, it's when the model is ready to be used. And we just, run sampling and inference on the model. And we do not change the parameters of the model. So there is no further learning on the parameters of the model. And last year, last year, we published this work called Large Language Monkeys. The name was inspired from the infinite monkey theorem, which suggests that if you have a monkey and a type typewriter in a room, and the monkey is infinitely typing on the room, just leave the monkey there. Almost with a probability of almost one, we can be sure that the monkey generates the works of William's Shakespeare. So the name comes from that, but the method is a little different. So for instead of a monkey, we use a large language model. The framework works as follows. It's a very simple framework for any input prop that we pass to the model, we sample it many, many times instead of just once, and then we have access to a verifier, which I get more into it later, but that can tell us which of these samples are correct. And once the correct sample is selected, then it then it's outputted, and then we have the full flow. The the full framework from the single input problem to a single output response. It turns out that this repeated sampling technique is extremely efficient. So if we have what we are showing here is we have three models Llama three eighty b, Llama three seventy b, and GPT for four o. You can see that with only one sample, taking asking them all of these, for pretty much all of these cases, GPT four o is a clear winner. But, as we increase the number of samples for our smaller models and use the Wey Fire, and we increased that by a lot, like we go from one to 10 to a 100 to all the way to 10,000 samples. And we see continued continued improvement in the performance of our smaller models. In a sense, it's telling us, it's it seems like these tiny model, like the HP model, already knows the answer to some of these really hard problems across math and coding here, just doesn't output it in the first try but we if we ask the model, thousand or 10,000 times, it can generate a correct answer. So we are basically using test on compute to list correct answers. Now We looked into this a bit further, and what we observed was that interestingly, we can write down the correlation between the accuracy and the number of samples that we have. It's not exactly accuracy. By accuracy, I mean having access to a to an Oracle verifier that can select the correct answer. That's why we call it coverage. Basically, the fraction of problems that are solved by at least one of the sample. That are taken from the model. And k in this case is the number of samples. So this this was the equation that we could model our kind of projection of the translation of test on compute to to, coverage, and the blue line is the actual coverage. So you you can see how the blue and orange line, the predicted versus measured very very closely are following each other. So what's happening here is that we are basically observing a new form of scaling laws this time for test time compute. Now, in order to explain why this is happening, we'll we we looked a little bit into our data and what we saw is that we saw the problem backwards. Like, why is the power load relationship between k and coverage? And it turns out that for this property to to hold, it is sufficient and necessary to have a left tail of heart problems in our task. So this is the parallel relationship that we see across a suite of problems and as a number of number of samples, this is the per problem exponential scaling because for a single problem, if the probability of being solved is PI, then you can write this easily. How what's the probability of success if he takes k samples. And then for this district for this equation to hold, we do need to have a formation of that looks like this, like a long tail for the heart problems, in this case, with a very low PASAD one, in in our task. And it turns out that that that was the case as we looked into a variety of data sets across a variety of model sizes. Here, we are arranging the model size from 70,000,000 parameters all the way to 12,000,000,000 parameters and more. And we have seen that the left tail heart problems the ability of the model to solve, these problems in the in this left tail format. Persists across all of these. And what this means is that that we can kind of predict the power law exponent of how we correlate test time compute to capability and do so two to four orders of magnitude, two to order orders of magnitude less inference compute. So what what is what I'm describing here is a new recipe to translate compute to more capability but this time at scaling time without changing the parameters of the model. So what this means for the the training of LLMs ecosystem is the following. So, traditionally, a lot of capital spends on is spent on pretraining. And then significantly less capital spent on fine tuning and at test time, it has it's more like a chatbot scenario. There is just a single back and forth between the model and the user. And that means, like, very, very cheap test time. But what we are seeing here is that we can spend a lot more compute perhaps in the millions of dollar or more have the model online, go and spend explore what by that, we mean using a lot of test on compute, enabling the model to use this tool as well as other actual software tools. In order to get to more and more capability. And And the relationship between this and the reasoning models or thinking models is that now we can bring back this data and experiences generated by the model back to the model and fine tune it further to make it better and more capable and create some sort of a data flywheel for the model self improvement. Now I mentioned earlier that for this to work, we do need we do need access to, verifiers. Basically, in the simple repeated sampling setting, all we have is a bunch of samples, like 10,000 samples. How do we select which one is that how do we this this this select the correct one and still and and open question. So there to add to answer this, there are two kind of types of tasks that we deal with. One is one is that there are domains where we have automated verification, which I get to in a second, and then there are domains that we don't have automated verification, and there are different ways to go about it. For example, best of end and majority voting or majority voting and model based rankers is a way to kind of approximate best of that, but it turns out that there is actually a large gap between majority voting and reward based modeling and the actual true coverage that we can get from these models. So here in this case, we have the number of samples, k, again, varied between one to 10,000 then on the y axis, we have the success rate And while if we had access to an Oracle verifier, we could have a lot more gains from this test time scaling. It turns out that with using a method like majority voting or reward modeling, we can't really follow this and, like, distinguish the correct versus incorrect one, especially after 10 to 50 or so samples. So the this is what's happening This is, what's what we call the generation verification gap. This is this gap that we want to kind of close, and we want to be able to follow the model and select the correct responses. Now there's certain applications in which where we are lucky and we have access or we can create some sort of automated verifiers, and one of them is using AI to to generate GPU kernels. So again, some of you in this room might have have to deal with it might it might have dealt with writing kudos kernels and so on. And what and although, if you do a good job at kernel design, it's amazing. It feels really good. But doing so, is really really hard because kernels are generally painful to ride, information about best practices for current design, is very fragmented. Best practices are also fragmented. It's hard to learn It's labor intensive. There are also new languages and DSLs that come. Come out pretty often. And, things change from one hardware to to to another. The idea here was that can we use test time scaling and get LLMs to write kernels? And the advantage here is that then we will have automated verifiers because for a given kernel and given PyTorch source code in this case, all we need to do is to to ensure that for input pairs going to the PyTorch source code, we get the same and and the kernel that the l l m design we get the same exact outputs. From both sources of code. So it's kind of a code translation from one language, PyTorch, to another language, CUDA in this case, and we we can just match the input output pairs And because machine learning models are more or less they have these static data flows, this becomes a much better much more suitable for our automated verification task. So last year or earlier this year, seems like a long time ago, we introduced KernelBench. Which is an evaluation framework for Kernel generation. Kernel bench is a suite of 250 problems as well as an even and a framework that allows you to run these models and measure various performance correctness and also, other performance metrics. In terms of hardware efficiency. So there are three levels to kernel bench. Level one, is the simpler kernels and those are the single operations in PyTorch, such as convolution, matrix multiple, These are simpler in the sense that the input is shorter, smaller, but they're actually pretty hard to optimize because over baseline because baselines such as PyTorch, these are heavily ops because these are used a lot. Level two, we had effusion of, a collection of ops, so these were more like layers in a deep learning model. And this would kind of test the LLM's ability to, write to be ability in writing fused ops which is a very important skill set and for writing good CUDA kernels. And level three are end to end models, such as mobile net, mini, GPT, and so on. And in this case, this is like a much harder task. We are basically giving them the entire an entire model and asking it write the CUDA kernel for the entire model. So I the framework works as, follows. So we have a problem, and input problem in PyTorch. We have the AI looks into the problem, generates the kernels, and then it gets various source of feedback, like it compiles it gets feedback about correctness, whether the out there's an output mismatch with the source code, It also gets the execution time from the PyTorch Profiler. And all these sort of feedback goes back to the model. And the model can then generate a new version of the of the CUDA kernel and so on. And when we released the paper, we did this set of analysis where we looked into how much, this iterative loop of test time scaling together with using feedback from, the execution results or for correctness or for from the profiler, for the performance, helps the model generate better and better kernels. And what we see here is that it does the model actually can leverage this feedback to an extent both from the accuracy and performance side and when we reach to 10 or so iterations, the models can do pretty well I believe these are our op level one results, and these are, in this case, we were seeing whether we can match PyTorch performance. And what we are showing here is in the best case, we get to 70% of level one results, matching with PyTorch. At the same this is great, and things are helping, the time scaling is helping, We have automated verifiers, but we are still a long time a long way, to go from having AI to generate kernels This benchmark generated a lot of interest, many companies started reporting results on it. And along the way, we have found many different ways that the model can do reward hacking or a reward can not capture the correct automate the correct kind of validation. So it has been a long process of about a year that we have worked on this, benchmark we are very happy to see, like, many major frontier labs and also other companies, application companies, building on top of, KernelBench. So going back in KernelBench, we had auto automated rewards but there are other many other domains where automated verification is harder to, to to to to have or it's nonexistent. So now, in this case, I'm going to talk about a project where we use test time scaling this time to directly kind of close the generation verification gap. So, So the idea that we had was, let's say, we have a a large set of samples and we want to create to collect the correct to, identify the correct sample, and let's do that by using a technique that is more or less familiar in the in classic machine learning, and that's weak, too, strong, super and and the way we approached this was that we throw in a bunch of different verifiers that were all not strong as we saw in the previous slides, In this case, we had an LLM judge, we had reward models, a suite of them, that each independently would score each of the generations, and our goal was that, can we use these weak verifiers and come up with a single score for the, quality of a sample and basically bootstrap from these big tech players. So the way it works is that it's there are three kind of stages. First is the score. In the scoring part, again, we go through every candidate solution and then we have our verifier whether it's an LLM judge or a model add a, like, a sinus score to these samples, then we have the waiting, and that's the core, like, optimization that goes here in this fork that we use week to, soup weak supervision. To, to kind of, estimate the quality of each verifier and then bring them together. Then the selection becomes easy an easier problem because that's, like, combining all these scores, weighted scores, and, choosing the one with the highest that we are most confident about. Now a little bit of, kind of underlying math behind this week's supervision. So first of all, again, weak supervision is a technique in traditional machine learning. Where we use weak verifiers, noisy labeling, sources, and use stronger results by combining them. The setup that we had was that we had n problems k generated solutions per problem, and then we had m verifiers. And the goal was that we wanted to predict the probability of why the correct being correct or the response being correct given all these m different verifier scores. And the assumption that we made was that each of these verifiers capture independent aspects of the correctness. Across these problems. And given this assumption, we could write this probability equation that relates the the the individual responses to, individual verifier. Scores. And given the this equation and the other probability conditional equation that holds for any scenario, then could go ahead and back solve for for these probabilities The the the probability of why or the response being correct, given all of the, other responses that we got from the verifiers. And it turns out that this actually does work very well. So in this case, we are looking into harder problems from GPQA diamond, math and MLU Pro. And for our baseline, again, we have repeated sampling in our general setup. So we are increasing the number of repeated samples from one to two to the power 10. And we are looking into majority voting and these other methods called called multi agent verification, Again, we see similar familiar results where the model, these approaches failed to follow the true coverage that we could get from this setup. And then in this case, we are using we are showing naive ensemble where we just average this scores by the models in our ensemble of vyfers, as a baseline, which turns out to be a very strong baseline. Then, you're showing the weaver or the speak to strong collect average rating of the scores where we get additional gains over that. And we show that these gains naturally go up as we have more labels for our task and the verifier scores for our task. So what this trend is showing that we are again seeing that we are using test time scaling this time on the not just on the generation side, but on the very we and that also boosts the overall performance of the model. And this turns out to be, again, a very effective technique in bringing up the capability of models to kind of a next generation or next model size. For example, when we apply this approach to lama eight b, across these problems, we are kind of increasing the capability the the average correctness of the model from 57% to 70%. And this is very close to the performance of LaMDA 70 b, which is 72% across these tasks. So we are kind of bringing up the 70 b mod seven b model all the way to the performance of a 70 b model, using this test time scaling on the generation and verification. And the same is true for the lama 70 b and the o three, mini. Performance. Again, we are going from the 70 in the 70 range performance to 86%. Which matches the o t o three mini. Performance. And and what is important here is that for we where we are using a very, very small number of label label samples across across the training set. And you're showing results on our on a on a variety of dev dev sets all at the same time. So so that suggests that there is more to be kind of explored here and elicited from the model. By applying these scaling methods. Now in terms of research, before I get to the next part of my talk, terms of research directions for this test time scaling, it feels like we are still kind of early in our understanding of the core principles that drive test time scaling. And this property is kind of opening a new era for us which is the era of synthetic data. Because we are kind of creating this We have now these agents and models that can create these new experiences indefinitely and those are great sources for us to create training data or maybe have better ways like continual learning where the models keep improving without this kind of rather unnatural kind of a way that we approach modeling right now where the models generate these, synthetic data and then offline maybe every once in a while, we bring them back to the model. It seems like there should be a more ways to do this more smoothly and continually. And the other research in this direction would be the infra that we would need for for high throughput and low low latency test time scaling because these kind of frameworks look very, very different from the mainstream chatbot frameworks. Where we have single back and forth with the model. So they they require their own custom ways that we deal with the hardware and the systems that we build around them. Now, I want to, for the last part of my talk want to talk about the recent work that we did at Stanford that is called Intelligence per watt. I get to that. But the idea that motivated this work was that we wanted to see how local models small models, how their capability is improving over the years, and what that means for the future of workload distribution or traffic distribution for AI. So first of all, the demand for compute is exploding because of the AI specifically because of AI serving demand. Of So Google Cloud in the last twenty months had a 1,200 increase in their compute needs, and NVIDIA had a ten year 10 x year over year increase in their compute like, demands, basically. And the interest infra response for that, that was that we have we need something in order of 250 gigawatt power data centers. Like, data centers that can serve this much power. And the graph on the right shows that this really is coming from these additional like, this explosion in the demand in AI serving going from something like in order of 160,000,000,000,000 to 1.3 tokens in in in a year and a half. And so now we looked into this data set of chat GPT, like a million chat GPT, queries. The dataset was released, and we were looking into the distribution of this dataset. What kind of questions people ask? And it turns out that 70 per 77% of requests of, chat GPT are practical guidance, information seeking, and writing, And what this means is that I we don't need necessarily the best frontier models to answer these And instead, local small local models, which in this work we define as as models with 20,000,000,000 parameter then, active parameters, or less. They can they can answer these type of questions. So that's one side of, one side of the story where small models are capable of addressing a lot of the queries, On the other side, we are also seeing that more we are we are having more and more powerful local accelerators Since 2012, there there was a 126 x improvement in the memory size of the of the local accelerators And what that means is that we can we can run larger and larger models locally, maybe on our laptop and so on. So the question to that we wanted to answer here is that what role can local inference play in redistributing the AI traffic and the inference demand? And to answer this, we define this new metric first, that kind of captures both the capability, but also the efficiency and performance from the hardware and system system side of the models, and we call this the intelligence per watt. So the way this works is that on the capability side, we are measuring the accuracy, and on the efficiency side, we are measuring the mean power used to, kind of address or solve a task by the model. So intelligence per watt is basically accuracy of the task, per watt. Per wattage of the model. And we to really see how the trends are changing and put everything together, we looked into a large kind of suite of models, 20 plus state of the art local models different types of hardware, both local and cloud, accelerators, workflows from Charge GPT queries, general reasoning, and general reasoning and match problem solving task and on the evaluation side, we looked into a variety of metrics from accuracy, throughput, energy, and so on. And we have released all of this data, which is like a lot very I think we can do a whole lot more with this dataset. That had all these kind of, pieces in it. But our analysis showed the following. Which I find way very, very interesting. So it turns out that local models their capabilities improving, significantly just in the past two years, the accuracy on this suite of tasks that we we looked into the accuracy went up by 3.1 x. In just two years. So, meaning, these like local small models are becoming really, really good. At the same time, right now, the way things work is that local accelerators in terms of their if efficiency, in IPW metrics or intelligence per watt, they're still lower. Than cloud, accelerators like an m four Apple m four Max is has a lower performance, than a b 200 in how in in its normalized efficiency metric that we we define. And that is not very surprising because these cloud accelerators are heavily optimized for running these LLMs versus versus the m four might not be as optimized or customized to serve these models. But that can but that can change. And the other piece of information that we found was that the intelligence efficiency is gaining is actually improving significantly as well. So in the past two years, the improvement here was something in the order of 5.3 x and 3.1 x of it was from better models, but another 1.7 x was from better hardware. What this trend is suggesting that our models, our small models are becoming better, Our small local hardware is also becoming better. And that means a lot of the traffic of AI inference and AI workflows can be actually addressed locally without the need for us to kind of offload them to cloud, to proprietary, or other bigger large models. And what this means is that maybe we need newer inference serving engines that are much more hybrid than what we have today. That can route traffic between our local accelerators and cloud accelerators and and that could be an interesting research topic but also very, very, very helpful in terms of how we get we can save our energy. We can reduce costs and so on. And, of course, this also needs new models and kernels that should go on to this local accelerators and better ways for us to measure intelligence per watt and similar metrics. So with that, I can conclude the talk, and here is the summary we talked about test time scaling. We talked about different ways that test time scaling make models at each size better. And that includes smaller models, matching the performance of larger models in many cases, And we also talked about the trend in hardware and model capability and how a lot of the AI inference traffic can be, kind of routed locally in the future. Thank you. Okay. We have this a few minutes for audience questions. It turns out we don't have the mic for the audience, but if you okay. We do have one, actually. Okay. So we have one question over there. Hi. Hi. Here. Thanks for the talk. I have a question from your earlier discussion where you talked of the test time scale and you did auto generation optimization of the kernel, GPU kernels. So my question is, are you able to extend it to the level where we can have just in time generation of optimized kernel based on the target hardware and the kind of workload is coming up. So is that the direction it can move to, or how does it look like? Eventually, yes. I think first, we need to kind of hone this down and make sure that the model actually can generate useful kernels. That are faster than the baselines that have you tried apart from GPU, that gen kernel generation for other hardwares, other than GPUs? Yes. It can do it for anything. I mean, of course, GPU, there's a lot more data on CUDA and the pre in pre training potentially fine tuning of the models, so that makes it easier. There is a lot of recent work on how to apply this to even new DSLs that are just created in a lab and how we bring other techniques such as, retrieval augmented methods to enable that. Okay. Thanks. I think I'll discuss offline more. Thank you. Sounds good. Any other questions? Yeah. This is Rajesh from Akranil Lutz. You had mentioned in your future research directions that, you are looking for changes to the infrastructure, both hardware and software, to better enable test time compute and relate it. Can you elaborate a little more on what changes you foresee or what little more on that, please? So there are system works that could that could be done, like my lab did if you're interested, take a look at the works like hydrogen and tochisar is from my lab, but there's a lot of other amazing work outside. So example, a property that makes it kind of makes it clear we need a custom way of looking into that is for the repeated sampling setting, we have the same problem, same prefix being used over and over again. Right? We are doing this attention over the same KB cache. And, of course, KB reusing the KB cache is one way. To leverage this property, but there are other ways that, that we have developed to kind of enable reuse in this specific setup to enable a really, really large batch. Like, if when we are sampling 10,000 times, that creates a lot of complexity. On how to efficiently you use our hardware. Using this specific property of this setup, makes a lot of sense. So yeah. Alright. Thank you. Does does this light work? Oh, okay. Okay. Well, while we setting up, I will do the the meantime. Okay. Alright. Awesome. Alright. So I'm excited to introduce our next speaker, Hansen Wong from OpenAI, my colleague, Hanson is a research engineer working on codecs, and I bet like many of you have already using codecs in your day to day life. And Hanson worked on training the first codecs one model launched in May and has been continuously iterating on the model since then. Prior to joining OpenAI, he cofounded a startup building AI analysis agents and worked on infrastructure at Meta. And without further ado, please take it away. Thanks so much, Meng Po, and thanks thank you all for being here. It's the room is a lot larger than it was, the last time I was here six years ago, so I'm very grateful for that. So, yeah, as Mongkeo mentioned, I work on Codecs, which is OpenAI's coding agent. And I think what just like one of the things to set the context that, like, really stands out to me is, like, looking at last year's ML for Systems conference. I colleague, Ahmed, was here talking about how OpenAI models were know, like, starting to get capable enough to really, like, get gold at the IY or the Olympiad level competitions, but I I think it's only really, this year in 2025 that we really started to reach the point where these models could actually start to do real world software in work in a way that was actually, like, very meaningful for users. Which, you know, like many of you have probably already experienced. So this chart shows benchmark called SuiteBench verified. So this is from a bunch of fine people at Princeton, many of whom are at the conference. We working on much better and newer benchmarks actually. But, but SweeVenture is still a great example and so if you're not familiar, basically, SweeVenge it's a it's a it's a benchmark of 500 tasks from open source issues in GitHub repos, and the models need to produce a pull request that resolves the issue. And so even last year, around June when g p GPT four o came out, the models were still, like, not very good at this task, starting around, I think, between 3040%. But just in the last year, the progress has been pretty incredible and a lot of it's been driven by the developments in this, like, test time compute as we heard. In the previous presentation, and so now with these reasoning models, the progress has really taken off. It's the models are almost, like, twice can resolve almost twice as many issues as they can before, where really, like, nearing the 80% to 90% mark, which is probably, like, pretty close to the limits as far as this benchmark goes. And then also, like, looking at, this METR time horizon graph, which I think is has been shared around quite a a lot recently. But basically, like, it's not just about completion, but also about, like, the length of the tasks and the difficulty of the tasks that the models are capable of achieving. And so it's almost like this exponential curve. Where the where the rate of progress in terms of how difficult these tasks that the models can solve, it's it's actually like increasing over time and I think the the codecs models are actually, like, pushing the frontier of that right now. Cool. And yeah. Like, I think going back to the, I I think, like, you know, this year, the model's achieved some some very impressive feats on achieving not just like gold at the ILY, but also the IMO. And achieving, a perfect score at the ACMI ICPC finals. So we're and this is actually, like, the same underlying reasoning model that powers codecs. So when you use codecs, and I hope you try it if you haven't already, you're getting, like, this level of intelligence and reasoning at your fingertips whether that's in the computer, or in the cloud. So, yeah, so so codecs was released The CLI was released earlier this year, in April. So you can just install it right away. It's just a simple NPM install. And then it's also available in the IDE and also, as the cloud product in chat JBT, which was released in May and and what's cool is that the same underlying open source agent, which we've actually released on GitHub as OpenAI slash codex, It's open source. Like, all that core logic is the same kind of, like, underlying agent infrastructure, that powers all of the different product services that we have. So it's really cool that, like, we're releasing that to the public. Anyone can achieve kinda like the same level performance that we do. The models are also available, in the API. I think the 5.1 codecs max model just became available the API yesterday. And, we're we're really starting to see, adoption take off both internally and externally. No better evaluation than just real world usage. So like 95% of of, technical stuff at OpenAI make use of Codecs. Codecs reviews a 100% of pull requests. Created an open AI. And externally, usage as measured in daily messages to codecs have increased about 20 x since, GPD five was released in August. Which was kind of like the first open AI model that was trained to be good at these kind of, like, coding agent scenarios. So cool. Yeah. So like what's under the hood of the codex agent? So we've intentionally kept the core like age infrastructure or, like, the loop that powers the agent extremely simple. So it's actually like, and it the nice part about this is, you know, as I said, it's it's open source. There are some links at the bottom. Like, you can actually go to the repo and check these out. We've open sourced, like, the prompts that tool specs, all of that. So, you know, like, when you start a task, like, you have basically, like, in the in the prompt, you have the developer instructions, the a set of function calling tools that we provide to the agent, the environment the user's environment context, and, of course, like, the prompt that the user gave us to perform the task. And then we basically just have this what is ultimately quite simple, this loop where the model receives that prompt, it reasons about what to do next. It always makes a tool call usually, so that would be either, like, running a command potentially, like, using an MCP if you've provided a a model context provider. Tool. And then afterwards, we provide the agent the results of that tool call. And this whole process repeats until until completion, essentially. And so, like, basically, like, this, like, single what is essentially like a single four loop, and and then we use the responses API, or you can also run the GPT OSS, models locally as well. But basically, like, you just repeat this in a loop until the task is complete, and that's kinda like ultimately, that's all you need to be able to achieve, both like these incredible performances at these competitions and also like solving real world software engineering tasks. But yeah. So, like, I think underneath that kind of, like, simple loop, there's a lot going on. And so I wanna kinda, like, provide a high level overview of some of the challenges that we've solved, which is kind of like a really interesting place. It's at the intersection of, ML and systems, of course. So the the first thing is, like, having you know, like, quality environments to use, not just for training the models, but also to serve them as part of the Codex Cloud product. There there is a lot of thought that goes into the tools that the agent gets to use. And then lastly, like, there's a lot of constraints around, model behaviors, durability, and, like, what does it mean to have, like, a you know, like, a coaching agent that users actually want to use and not just know, these things that are that are incredibly capable but, otherwise, like, very hard to work with. So so, environments. So if you go to chatgbt.com/codex, you can actually, like, create your own environment. And I think the really cool thing is, like, basically, what we've is we've taken the same infrastructure that powers our training environments and we've basically shipped it as Codex Cloud product. And so like a lot of the knobs that you see here, you know, when you create a codex environment as a user, those are, like, the exact same types of things that we use during training. So, you know, you start with a with a container image. You usually like a dockerized container. It might contain a of pre installed dependencies, but, you know, like, users can also provide their own then things like, one important thing is security. So, basically, by default, most containers, have Internet access disabled, but users can select access it's very important for things like, you know, like many enterprises have concerns around code exfiltration, or, you know, like accidentally installing bad software. The rest of Internet or, like, bad dependencies. So it's important to have those knobs, also, like, dedicated training to make sure that the model is, like, less vulnerable to that type of injection? So and then yeah. So, like, then then the tools in agent loop So we actually have, like, a fairly, opinionated take on on, like, kind of, like, what tools you should give the agent. So in general, like, our philosophy is that the, the the the terminal is like the the bread and butter of what codecs uses to accomplish most tasks. And in general, like, we do think that for a lot of things, like, the terminal is kind of like all you need. So that's not true for all tasks, of course. But, like, I think do find it to be true for the majority of tasks. So, like, you can read files, write files, run tests, and review and commit changes. And that's kind of like the the fundamental, like, software the inner loop of the software development cycle. And then so yeah. So, like, I think below is the roughly, like, the inter interface that we use for, that's for the exact or the terminal tool. Used in the agent So a couple of things to call out here. So one is like we do expose like knobs that let the model control, basically, like, how much token usage any any tool call takes and and how long that tool call is allowed to take. So the so the model has pretty fine grain control over this, like, cost and latency trade off, which which then, you know, like, the model is actually able to almost like learn by itself how to best optimize that through the reinforcement learning process. And then yeah. So I one philosophy we've taken also is to enable sandboxing by default. So on macOS and and the Linux, there exists, like, these sandboxing primitives where you can execute processes in, like, a a pseudo sandbox where file and network access is is tightly restricted by default. My my colleague, Fuad, has a excellent talk about some of the considerations that when in this design. But, basically, like, know, on macOS, can define these, like, sandboxing policies and similarly on on Linux. And based basically, we we have, like, a a fairly, well thought out set of, sandboxing policies that make it make it so that the agent can't just, like, you know, delete your home directory without explicit you know, unless you and then yeah. So, like, basically, just with, the terminal tool alone, a benchmark called terminal bench. Where the agent has to do various like, not just coding tasks purely, but things like, you know, like, plays orc, like, start a server. Or, like, you know, like download videos from the Internet. So without too much so basically, yeah, like, with that simple agent loop and a you know, like, relatively simple interface to execute commands, we're already able to achieve these kind of like state of the art performances on on these real world, tasks that require interaction with a peer. So, look, one interesting anecdote is that so you would think that models can just use Git to apply patches or or edit codes. It's actually like a pretty interesting design space of like how do you provide models tools to edit files. And so, like, git diffs are you know, well, they're one of the big advantages that they're well represented in the pretraining data. But, one of the downsides is that and contain a lot of references to things like line numbers, which make it very difficult to so that like, that that's basically impossible to predict upfront in a in a, like, auto aggressive model, manner. And so we actually have a custom patch format. So we've showed that with our developers as well. But basically, know, like, it's it's it's basically a simplified version of the GitDIP format No line numbers, but it's still kind of like the same approximate format where you can you can provide these, like, like, headings, and then context, and then the the patch format itself is at the core, like, still the same. And then, basically, during serving time, we also have these, like, basically, the the the grammar of the patch syntax is extremely simple. So that means, you can use things like constraint sampling, to ensure that, you know, the model always generates a syntact syntactically valid patch. So So the last thing I want to touch on is, like, concerns around, you know, models behavior and its durability. So, it's a big and important piece of what makes coding agents actually useful. In the real world So, you know, one thing that we've worked with other companies to try and standardize is the agent's MD. Basically, a a readme for agents in the same way that we have, like, contributor guides for humans. And then the idea here is that we we provide the the model with guidelines around things like code structure, build and test commands, style style guidelines, how to make a PR in the repo, and then we actually do enforce that the model adheres tightly to these instructions. One interesting thing is the models are actually very good at creating these files example, like, a model like OpenAI deep research is actually quite good at doing that of the box. So one trick that we've done is actually, like, for for many, source repositories, it's actually, like, possible to get a pretty good agent centri out of the box just by deploying our own agents to to do that. So and then I think I touched on the benchmarks earlier, but one limitation of a lot of the current set of benchmarks is that if they only capture the concept of correctness, as as specifically measured by ability to pass the reference unit tests? But in the real world, like, there's so much more to to software engineering and coding than than just correctness. Concerns like coding style, engineering best practices, you know, like, when to not repeat yourself versus, you know, like, maybe it's okay to to duplicate some code. It's a very nuanced decision. How to structure your code to be modular even things like usage of comments, overly defensive programming, patterns. So I think, like, you know, when it doesn't go right, I think this this picture on the right is actually from a Twitter Twitter comment, but basically, sometimes the models are prone to doing things in in things that might look a bit AI. And so know, if we don't do our jobs right, then like that kind of leak out. Now, we try our best to to mitigate these types of things. And then another interesting thing is that codex is actually a great code reviewer by itself. So on on GitHub, if you are connected to to codex, you can just, like, add codex and ask it to review your PRs. But like I mentioned earlier, like, all PRs at OpenAI are reviewed this way and there's been a surprising number of like actually, like, quite serious issues that have been flagged early as a result. And then we've we we published a blog post recently on the new, OpenAI alignment blog, which I recommend checking out. But, basically, like, there's been a lot of work that's gone into making the models very good at code review. And I think, like, one concern is specifically for code review, if you've used, like, other code review tools especially is, like, there's a very delicate trade off here between precision and recall or, like, the signal to noise ratio. So you flag things that are, like, kind of not true issues, it actually, like, often waste more time, than actually save the user. It's kinda like suggested by this formula here. So you you have this, like, very delicate trade off you can achieve between, you know, precision of the of the, accuracy of the the flagged issues versus you know, being able to flag the the actual issues that occur in practice. So this is something we've worked a lot to, really tune and then not just like find the right spot on the curve, but also push the curve upward as you can see in the as you can see in this plot There's a lot of safety training that goes into our models, not just codex, but also, like, all OpenAI models specifically. But, you know, like, some of the things that are specific to coding agents in particular, prompt injections, not just through user input, but also files in the sys file system or even dependencies where developers might try to sneak in attacks against LLM agents. One area that you know, like, adversaries in the in the wild are are are actually using these like, things like trying to exploit cybersecurity risks. That's something we actively work to mitigate. Also things like jail breaks and and general model safety training. One interesting thing to call out from from our system card, which is linked above is that, there's there's these, cool techniques we can use to make the models more collaborative. So one thing is that previously, we saw that models would be prone to kind of like stepping over user changes if they were if you're, like, editing a file at the same time as the coding agents editing the file sometimes. You end up with these conflicts that end up in you kinda, like, stepping over each other's work. And so we actually, like, simulated during training, like, users, like another model would be inserted in the loop, and it would kind of like make changes at the same time as the as the original models trying to make the changes. And then, basically, by doing so, you can kind of simulate the effect of, you know, cooperating with the user on on your laptop and then making sure that the model behaves responsibly when there's when there's conflicting changes. So kind of like what's next in terms of coding agents. Right? So one thing that we, we saw in the previous presentation that's very relevant for coding agents, the idea of, like, using parallel test time compute to improve performance So, we published this number with the Codex one model, so it's a bit old. But can still see the trend still holds true in practice where, you know, the more attempts you give these models, you actually get a pretty smooth scaling curve in terms of the improvement in accuracy. The models can often, like, it might not solve the problem the first try but if you give it four tries or eight tries, the probability that you get a correct solution increases quite a lot. This is something we've actually exposed in the Codex Cloud product. So basically, you can ask when you provided a task, you can you can ask for multiple responses Another interesting, technique is to extend the context of these models via this technique called compaction so the model summarize its work often compressing the tokens used in the context by up to a 100 times. So And then I think Yeah. To close off, I think the the best part about working on codex is that as we get a better codecs, it makes the future of codex models better. It makes it faster to develop these models, whether that's through better tooling for for the research side, faster development on the codecs product, Codecs kind of can start to review its own code. Agents can even help us make more tasks in training environments. And ultimately, our purpose as a company to to towards our goal of accelerating progress towards AGI. I Cool. So that's all I had have. You for listening. We have some time for questions. Yeah. Hi. This is Mart from BrowserUse. Thanks for the talk because it was nice to hear. All the aspects. Just wanted to ask about you said there are many components to training, like the safety training and then aligning with, like, the real world usage and also the reinforcement learning on on your typical tasks. And I just wanted to ask how do you reconcile all of these to do to do it at scale? Because I guess you can generate a lot of these test cases with, that you can run and verify, but same is not true for, I guess, like, safety. Or or the alignment alignment for collaborating with humans. And I just wanted to know, how do you how do you actually reconcile those two do do all of that scale? Is it just like all combined into one pipeline or do you try to do these stages separately? And try to make it work? Thank you. Yeah. I think, you know, I think we're all about things that scale well. So we as much as possible, we try to do it like try to address, like, all these things at the at the same time, if that makes sense. And then, ultimately, like, are are North Star is, like, aligning these things with how the tools are used in the real world? So much as possible, like capturing you know, like, how do people actually use the product in practice and making sure that the training reflects that that real usage I I have a, also, I'm Mylene. I'm affiliated with Zurich. I wanted to ask, do you think that there's a problem with, the compaction method? Do you think that we need new solutions for that? Because I'm not super familiar how it works, but I would assume that it's some kind of summary. The question is, like, what do we put into the summary? Like, do we miss relevant information? Do you think that, so that like, there's some interesting work and that you're, like, rethinking the strategies for this. I think there's definitely room for more sophisticated strategies, but find that this approach works well. And I think, like, one of the powerful things about this approach is that it's basically the model that decides what to put in the summary. And so you can imagine, like, over the course of of training, like, this the model like gradually learns how to do this on its own. Which is pretty interesting to see. So I'm Shamus Patel. I'm affiliated with Best Buy. I wanted to understand when you are talking about the parallel test compute scaling, how how do you plan to do the verifiers here, which can also scale, at the same time? I'm sorry. Could you clarify what you mean? So The panel that you Oh, you mean like as a as a user, how can you scale the verification process? Yeah. I think that's an area that we're actively looking into. One last question. Hi. Hey. I'm a big fan of Kodak's. I use it on a daily basis. I'm Pranav. I'm from a start up. I wanna understand, you mentioned the token consumption is dynamic The model learns to know how to consume more tokens or less tokens. Is that applied at a global level or is it like personalized for every user basis their coding patterns? You you mean, like, specifically with the tool? The How many how many tokens does it consume? Is it personalized according to every user? Is it, like, RL? Applied on, like, per user level? Or I think the one I showed specifically, that's at the tool usage level. So, basically, like, when a model runs a command, it can kinda control, like, how many tokens like, the maximum number of tokens that it should see from the tool call. And that's personalized for every particular session? Or is it like I guess it depends on, like, the command. So, like, you know, if you're running, like, a test command and you might see a lot of output, then it might make sense for the model to, like, be a little more restrictive about how many tokens should be seen from the result. Got it. Cool. Thanks. Alright. So for our next speaker, it's my my pleasure to introduce Veena Grover from NVIDIA. I had a pleasure to actually work with V Note like a while back. I was interning at NVIDIA, and that was a really good experience. V Not is a senior distinguished engineer at NVIDIA, where he was has worked there since 2007, he led the team that created CUDA C plus plus language and compiled helped, make GPU computing faster and easier across many fields. Since 2017, he has applied language and compiler ideas to accelerate deep learning models, leading a smaller group focused on performance and developer productivity. And today, he would talk about essence of CUDA, C plus plus, and for GPUs. Thank thank you, Magpog. Thank you. And you for inviting me here and glad to be here. So I'm gonna, try to give you a feel for what CUDA C plus plus is or was, and where it's going. So this is a slide so it's basically by one of our ninjas. Created this thing about me and him. So this is, what I do. I have a lot of people in my team, very enthusiastic, and I hope helped them basically you know, develop new ideas. And fun job. Okay. So here's my agenda. I'll go through this as quickly as I can. And, leave enough time for questions. And I'll be here at the end. So the basic idea is about what is a GPU computation? So let's go into that. So early on in early two thousands, NVIDIA and other people, ATI at the time were talking about GPU. GPU was general purpose GPU it is a nice, catchy term, and what they did was they used the shaded libraries pixel shaders and all those things to do so something like DirectX, OpenGL, and they were trying to do matrix multiply for simulation. And it was kind of a nice idea and then around 2025, Mark Harris he was at NVIDIA. He and he came with the term GPU and created some organization. And Nvidia published, a nice book. Called GPU Gems. And it's not very convenient way to program GPUs if you're not doing graphics. Right? And people used to struggle, they tried tens of thousands of lines of code, to do something simple And then around 2006, 2007, there was this one guy who came to NVIDIA. His name is John Nichols. And he created CUDA the vision for CUDA. And then that call it SIMT, a single instruction multiple threads. And so what they did was basically you have this compute hierarchy in our GPUs, threads, warps, and so on. There's a memory hierarchy, register files, which is very huge. And they published NVIDIA CUDA programming guide. And it's not officially released at that time, and I came along at that time in the middle of it and I didn't know what the what the GPU was, so everybody used to laugh at me. And I learned And gonna try to tell you what it is. So there's basically this idea of a thread really a lane so you have a Buddha thread, the right hand side is something called CUDA core. Then these are organized in thread blocks, and then there's a streaming, processor, on the right, which is what the hardware looks like. And then then we have a grid of this thread box and you have the cuda cable or GPU, the entire thing. So essentially the GPU is consist of a large number of SMs, Each SM is a thread block and each thread block contains thousands of threads or you can configure them. And they're very fast. Or at least So memory hierarchy is essentially, cartoon picture for ampere, so you have the different SMs and then each SM has access to a very fast register file. And that keeps changing, how many register files we have. Right? And then there's l one cache and shared memory which is pretty close to register speeds. And then there's l two. And then global memory. So let's get back to the software side. So each thread in the GPU in CUDA has a logical coordinate. So basically, so it's a two pole internally So they're called thread ID, Thread ID is a structure with three d and then there's a block ID which is its position. So essentially these two to define coordinate of a thread. And each thread that at the hardware level, threads are organized in groups of 32, they run-in parallel and they usually have a single program counter. All 32 threads in a warp have a single program counter. So when you advance it, all threads move to the next instruction. So that's where you get the massive speed up at the hardware level, but you lose some flexibility, as a result of that. Then at the higher level, we have GPU kernels. It's like a function, and I've seen there's a picture below In this picture, basic idea is that a kernel is a C plus plus like function is a kind of qualifier called global and then inside of that, you can basically write the code for a single thread. When you launch it, it'll run on however many threads you want it to run on. So give you a quick cartoon picture of what a control flow looks like in a thread block. So here's a picture of a a function called kernels. It has two arguments. An integer x and a pointer y. So when you start this kernel, on a GPU, x and y have the same value for all the threads. And at the curly basis, conceptually, you can think of all the threads start together. Right? And then they do something, s one, and then there's synchronization statement called sync threads. So all the threads must arrive at that sync threads before they can go move to s two. Right? Before that, they can go in any order don't guarantee any order. But at the performance there is some order, but you can't assume it. To greatness. And then, at the close at the exit of a kernel, all threats must arrive. Some threats can't leave and some threats are still executing that cannot happen. So that's the basic model. Now I'm going to move to heterogeneous competition. So that was just a device computation so far? So So what is heterogeneous computation? So typically with a GPU, you have a CPU attached to it. Or maybe more than one CPU. So you have CPU and GPU working together. So a kernel is launched from CPU. So CPU thread x 86 or ARM or So what happens is the CPU does some work of its own, It can also do some work for the GPU. It can say, hey, allocate some memory asynchronously put some of this data in the memory, this data from the GPU, It can do so, essentially, it can do computations. Coordinating and communicating with the GPU. Synchronizing sending data back and forth, and and they can work together, right? So you can basically say, start a kernel, While the kernel is running, you can start preparing the data for the next kernel you want to launch. You can pipeline and a parallelized CPU and GPU computation. And that's the key concept. Of a heterogeneous competition. And that is very important especially these days. This kind of evolved kind of organically. But people were trying to do just get the last cycle of performance and they kept adding stuff. So this is what it is today. Okay. So what is programming could I mean here? So there's a language called CUDA C plus plus a lot of people have tried to implement it. And there's been some confusion what CUDA C plus plus is. So effectively, it's an extended C plus plus language. Where you can write host computations So host computations just look like normal C plus plus. You write c plus plus, inside of that, you write some device code with some qualified like host device thread ID, special things. You have a unified CUDA CUDA program, and then the compiler will essentially do the following. So this is really the true essence of CUDA. So what happens is that at the top, you have a CUDA file, There's CUDA front end. What the CUDA front end does is it splits your program into two parts. So all the the host code all the C plus plus code, it can contain Windows specific things, It can contain anything you want. That's Visual C plus plus or GCC, or Clang or whatever C plus plus compiler you want. It supports that dialect. So we don't even look at that. We just put it out. Then we have the device code, which goes to our compiler, through LLVM, we call it NVVM IR, We use LVM optimizer. We we then generate PTEX at the end. PTEX is our assembly, virtual assembly. And that produces a kind of a binary. What we do is we serialize that binary and then we put that into a variable inside the C plus plus code, and then we give it to the C plus plus compiler and we get this a dot out that you can run and then it will basically start managing the computation. So essentially it's it's a way to program GPUs in your favorite c plus plus environment. So it's not a special language. It's just an extended host language that's that's been how it is so far. And it creates some challenges but but that's what basically gives people a way to program very quickly. So if you know C plus plus, learning good is you know, not a big deal. I mean, it still requires an effort, Okay. So okay. So changing gears, So so now what's the future, of the present? So NVIDIA announced Ku Tile and Tile IR recently. At GTC. And yesterday, a few days ago, they released it. So tile programming is the future. So the the motivation behind this was that most CUDA C plus plus kernels for DL they really work on a tile level. So this just use very low level construct to do it. So, for example, if you want to try to jam or attention or any such thing, you basically organize your tensors into tiles, and then you do some crazy things. And so it gives you a lot of power, a lot of flexibility at the lowest level So what we did was we basically created a little higher level DSL or language called Kotair. It basically has two two front ends. The first one that is released Python another one will be c plus plus. So you have a concept of a tile and then there's no memory spaces. There's no threads. You just have tile blocks, so they're not to So here's an example on the left, you have a a Python program. It contains a function called matmul, and then it has a decorator called c t dot kernel and then inside of that, it looks pretty straightforward and it's performing a simple matrix multiplications. But under the covers, it gets translated to the GPU. And then the right hand side, is similar program by C plus So we added some c plus plus extensions, and so the only difference you can see that you see namespaces on the right hand side, then there's a tile global function, and you can see some cases, templates, which says zeros. Of, template intraniation of of float. And so it makes it much simpler to write high performance GPU programs. And in in the end, c plus plus, I think will give you more power, so a lot of people use Cutlass the learning curve hopefully will be less. Okay. So that's pretty much I think what I want to talk about for the tile programming model. The next thing is essentially how are you going to use AI for GPU programming? So these are some results of internal research and speculations from our group. And other people at NVIDIA. So this is not yet public, a sense that we have not committed to if they will ever be released or when, So so first thing I wanna talk about is AI for GPUs is do you really wanna do? So so the one of the challenges that programmers and developers have is writing correct deal and GP programs. So if you do that today, and you are new, there is a large learning curve And especially with the new languages like the tile programming it's all new. There's not much experience there. That's the first challenge. The second challenge is how do you construct high performance GP programs? And that's also kind of a profession, so to speak. Today. As you might see, people write custom kernels and you see GPU mode and a lot of of groups very excited about it because it's important. So so one thing we learned from writing high performance GP programs is that if you want to write, let's say, a thousand line CUDA program just to do a simple gem, right? It contains a lot of stuff. So and even humans have troubles know, coming up with that. So you need I think for agents, you need high level abstractions. To reason and think about it. You don't just generate thousand lines of you know, code from some auto aggressive model. I mean you could, but that will be not easy. Right? So so so you need high level of abstractions. Then for performance, you also need to have some kind of an idea about the model the analytical and empirical performance model, of what you're doing. People use auto tuning sometimes that's very ad hoc, And then restructuring pieces of code for high performance. So you can write a live, let's say, matrix multiply But to really take advantage of, let's say, volatile GPU versus Hopper versus black hole, we may need to restructure it differently. Right? So so an agent or or AI has to be able to do that and a lot of this knowledge is not explicitly, you know, written down by ninjas. So so here is the first example of what we are doing. So so we have a developer, and, this is actually the what we do is we basically ask the developer asks question, write a CuTile program for something and then it goes to an LLM and then we get some program Then we have a verifier. So the verifier, the way it works is it takes the program, and we have a service on the site where you try to run the program. If you get errors, you try again. With the error message attached to the context. If it works, then we try to make sure that it works on all the interesting test cases to give the correct answer. And then we omit that. So below is there's actual prompt that you can use it just says in English, or at least as semi formal English, say, please generate a kernel for ReLU of mat model of tanh of a comma b. And then you say, where is this, b is that. And then you can say Matt Mullis should be in the floor 32 precision, and the final result is cost to floor 16. Right? We can say that and you get back something like this. After a few iterations. And and we verified this is correct. So so this is this is just functionality and this is just very early stages, very speculative. But this is one one area where I think agents and AI can help people or write programs where there's limited general knowledge. Right? Among the practitioners or or programmers Right? So that, it's pretty useful. So let's see. Now, the VisionTek GP programming, you know, we can basically like to get more things done. So we have a new profiler that NVIDIA released, inside Python. It can do a lot of experimental stuff. But profiling things and doing experiments. And we can also automate that So so my last, part is essentially some abstractions. So we have three or four abstractions that we created. For writing work specialized code. And, so something is called a future asynchronous computations. We have a paper at CGO early next year So the basic idea is that you have a full empty container for a tile, it's full or empty. And then basically it's a wrapper around a synchronization barrier and a buffer. We arrange a lot of them in a pipeline state in a circular manner. You can put and get from it. And the last thing I wanna talk about here is how do you model the GP program? So this is kind of one vision we have, and we did this by hand. So there's an archive paper we published, So the we essentially took a gem, which is more specialized, and tried to tune it. So first we come up with a proper model, analytical model, instrument a program, and validate it, took a lot of time do all those experiments, and then we have different parameters, the load latency, versus, you know, compute latency and this and that. And then we keep checking until it fits then we can use that to predict. So this is really a workflow that could be automated by an agent or AI. So so I think this is very promising idea, because if you have to do all this, by hand, even for something simple, it's very time consuming and error prone. Is where I think AI could really help. With that, I can take some questions. Hello? Can you hear me? Thank you. Thank you. That's a very great talk. Actually, it is very inspiring for me because I have been working on the programming using general AI for a while. Of the challenge for me was, like, writing tests. Right? So, writing tests using AI is notoriously hard because AI need to read the source script understand it, and understand the environment it it runs. And then write a code that can actually result error on its own. Right? So you have, discussing how to in your team to develop solutions for GPU programming. I imagine that would be very hard to actually write a test under a parallel environment. Right? So I have been writing tests for parallel programming, and it's very hard because the is a lot of unexpected behavior that happening during the concurrent situation, not happening in the single thread situation? So what is, from your perspective, a better direction for Nvidia or in general GPU programming to develop abstraction to write tests that is more robust under the parallel. That's a great question. I think so. I'd try to give a short answer. So so I think chat panel programming even for experts, people who have know, ten years of experience, is very challenging. So so what what I think we can do with AI in the in the short medium term would be to all the chores and mechanical things are error prone. We can eliminate those and let the programmer focus on the essential task. Then eventually, I think once we have better understood abstractions, then I think agents and AI can do more and more of that. Coding but you I think there'll be like an assistant rather than autonomous completely autonomous. That's my issue. Concern. Hi. This is Apala from Microsoft. Mhmm. I have a question that when you are writing performant kernels, it becomes even more hard to verify that the logical equivalent you got logically equivalent program to what you actually wanted. And, and even if, automated agent did say, oh, this is logically equivalent, it is very hard to convince the human that, yes, it is the agent is correct. And, so for example, if all this was happening before flash attention was invented and the agent just went and wrote a flash attention. Right. Then, it'd be very hard to convince the human that, yeah, you got something there. So how how do you deal with this problem of verifying highly performant kernels and convincing the programmer that yes, this is performant as well as functionally correct? Right. So so I think there are two challenges here. I think there are two challenges. So so the first part is verification of the output. I think that itself is is very tricky. For example, you could you could basically take a handful of test cases and make sure that your program is correct, then they may have a lot of corner cases. And I think that's really don't think it's a problem specific to AI, it's a problem specific to writing parallel programs. Parallel programming is very hard. And then I think AI can make make certain things easier and so one of the things, for example, if you if you write custom kernels even the experienced people at the right more and more layers by hand that's where things go wrong. As the complexity of your program let's say number of layers or or some other dimension, So I think there's that that's something that needs to be solved and AI, I don't think will solve it by itself. Alright. I think we run out of time. If you have more questions, you can talk to Veena after this. Let's thank you, the speaker again. So next we're gonna take a short break for ten minutes and then we gonna come back at 10:25 sharp to start the next invited speaker, and then spotlight. And for people and then there will be poster session after that, so if you have poster, please come to the front row to pick up the tape. So you can start putting up your poster on the wall. And, if you are the spotlight speaker, you can also come find us to test your computer and AV. Thank you, everyone, for joining. We can resume the next part of our workshop. You I'll give you a minute or two Yeah. Anyway, and that's not on. Does it something have to happen there? Do they need to switch on? Did it last time. Do we have to turn this on? Somehow? Okay. Okay. We just stay so that we So that, like, So just to recap, a little bit of the schedule here, thank you for putting up the posters. The poster session formally will begin, after the, ninja and spotlight talks. In about forty five minutes. If you have not put your post up, that's okay. We'll get an opportunity again. So just to get started on the workshop, again, it's my great pleasure to introduce Nirja She's an assistant professor in the department of ECE at UT Austin. Her works travel the boundaries of system and l ML specifically advances in systems, machine learning, and hardware architectures. To launch a new era of entire cloud as a computer. Bridging these complementary fields, our research focusing is on using and developing ML techniques for systems, building systems for ML. Welcome, Nirja. Alright. Thank you. That was a long introduction. So I would I would do a short one again, just reminding my name. Hey, everyone. My name is Nirajay Adwad. I'm an assistant professor at UT Austin in the department of ECE. I lead the UT system research group, and I am very, very thankful, to all the organizers particularly Mimi, who has been, you know, inviting me for three years and, you know, dealing with that patiently. I really appreciate that. Very happy to be here. Today, I will talk to you about some of the recent work that my group has been doing in the context of identifying and then addressing systemic implications. That are raised by these AI or ML for systems. I would like to begin with a little bit of, personal journey here. You know, that sort of can be described as from ML for systems, to AI everywhere. Right? In the early days, I don't know how many of you know this, I might be dating myself by saying that, but ML four systems actually was unconventional. Right? Because systems relied. They were deeply rooted in heuristics that were and by, you know, having experience and expertise that were sort of built in. They were very simple, so when I said as a as an early, you know, master's student, at India or early grad student at Berkeley, when I said machine learning actually can help systems, I got puzzled looks. Right? Raise eyebrows. But obviously, that has changed now. I don't to convince this crowd. Obviously, the talks and everything we have been we are hearing, this has changed drastically. Over the last decade. Right? And now LLMs different kinds of models, machine learning, AI, small, large, everything, actually has been driving system design. Right? Let me dive a little bit deeper with examples on what I mean by that. Right? We we are starting to see machine learning models, LLMs, small models, agents everywhere across the stack. The systems. Right? So, you know, one good example is to begin with, the core of OS, the kernel layer. Where, you know, you have sched p, for instance. That's sort of is very good at engaging in policy search, sort of generating new strategies, and safeguarding the outcomes that come. Right? That this was something that was core basically done by human human experts. Right? That is now done by, an agent. Going a little bit abstract, a little bit about data center which is, you know, data center as one computer. The SREs particularly, the site reliability engineers, they had to deal with keeping the systems actually up and running. So they were the glue. They would find out issues and so on and so forth. LLM for SRE, this agent actually now is at least this paper shows that it is capable of, parsing the logs automatically, identifying problems, doing root cause analysis, and and even going forward and giving fixes to that. Right? I don't think I need to talk a lot about this. There is the provocative paper that I'm sure Eon would be talking about. If not, we will ask him questions about it. Which basically threatens to append all systems research. Right? By by coming up various, various not just, you know, getting one solution to run, but identifying what solution can actually work. Again, this is something that's a moot point at this point, because there has been a full fledged talks about how to use these LLM agents for, writing code, for optimizing the return code, and so on and so forth. Lastly, you know how systems researchers assistance researchers, we think optimize is kind of the cornerstone? Anything you want to optimize, we write that as an objective function, and we you know, sort of optimize it, find out what optimizes that. Right? So, you know, it n p hard, and they complete, then we add in, constraints. We find, you know, heuristics. Relax those and find heuristics of blah, so on and so forth. Right? That actually, formally solving an optimization problem is all now becoming under the purview of LMM agents. Right? That this is a starting step. So my point is that LLM agents actually aren't just Right? They are actually participating inside the core systems layers driving the design. But my point here is, once the agents or the LLMs are permitting every layer, they don't actually just solve the problems. Right? They are now creating new ones. Okay? And in my group, we have been focusing about focusing on such problems. And today's talk, I thought I would focus on four such very basic systemic, problems that are interlocked, with each other. Okay? So, you know, first and foremost, now that we have LLMs everywhere in different layers, hopefully, that I can convince you of, we are not talking about deploying and configuring one LMM agent. Right? We're talking about deploying so many of them. And turns out that every LLM actually can be deployed in hundreds of different configurations. Change compression technique you use, change con, the strategies you use, and so on and so forth. Right? So your choice actually, has to be from all that hundreds of different configurations for all those LLMs, and your choice matters. It can decide whether your service is going to run eight x low or whether it is going to you 25 x more. So you care about it. LLM deployment today is bleeding money quietly. We know that all these accelerators, and GPUs and such, are very expensive. Right? So we are forced to share them. Wherever we can. Right? So we have been packing have been running multiple LLMs on the same GPU, which basically gets us to a multi tenant scenario. And as you would imagine, that gives rise to resource contention. Memory pressure, also compute concurrency, and and, and interference. Performance interference. Lastly, while we have focused so much on optimizing utilization, actually, turns out that we are taking you know, we're taking one step forward with utilization and two steps back. It comes to sustainability. Right, of the planet that we live on. So in this talk, I decided that I'll actually focus mainly, given the time, I'll focus mainly on the first two, that two in differing, lens. And I'll leave you with some links and you can feel free to check out our group's website. That's more updated. Mine is not. So, look at that because the know, the groups when it's maintained by students. So, obviously, that's very well maintained. Okay. So let's start with the first one, the deployment complexity. Like I said, LLM deployment today, the configuration choice and everything has been quietly played money. I told you that there can be hundreds of different configurations for a single LLM. Right? Changing, changing various things like compression or, parallelism techniques. The problem is the implication of a choice, of a configuration on level metrics. You know, the performance in terms of latency, throughput, or cost, memory used, and so on and so forth, is not trivial to figure out. That is hard, and that's why today's systems be it industry driven systems or, anything research wise also, I actually has relied on going with default choices. They either come up with and stick to the default choices, or they, let users figure out what they want? Both actually actually end up being suboptimal, either in time or cost. Perspectives. Right? So our goal here was then we we want to meet in of these different applications. You know, when LLMs are in different layers, they actually end up exerting different requirements from them. So our aim was to meet them, but do that automatically and also at a fraction of a cost of today's operators. While doing so. So that was our goal. Right? And we believe that there are two main things that need to come together to actually meet that goal. First, we need to be cost efficient. When we are trying to figure out the relationship between deployment configuration and the performance metrics. That we care about. Right? And that has to be accurate also. Going forward, once we choose a deployment configuration that is accurately, you know, profiled and everything, that meets the intent you have to actually automate all the deployment. Because if you don't, you don't actually end up getting and seeing the improvements that you imagined. And I'll show you how. So let's begin with the first one. How do we actually achieve cost efficiency while being accurate in profiling so that I can predict the, performance of a deployment configuration. What makes this expensive? We could, you know, for accuracy, we could just brute force. Right? We could just deploy an LLM across all different configurations, which is ridiculous. Right? It would be trivial you know, accurate, most accurate. It is expensive. Nobody's going to do that. Right? What makes this expensive, this whole process the two factors. Right? And those we attack individually to bring the cost down significantly. The first thing is that LLMs by themselves are really, really giant heavy. Right? So if you deployed that, it's just gonna cost efficiency, cost is out of the window. So we refuse. Deploy the whole LLM. What can we do? And secondly, we have to understand the implication of every possible deployment configuration. So it's not just the one giant LLM that is a problem all these configurations. Right? So let's take one at a time. One problem at a time. I refuse to use the entire LLM. Right? So we were like, okay. What do we do? If we wanted to avoid that? We were studying the LLM architecture then. Right? All all our work is kind of trying to see how both these systems and and machine learning kind of come together very well. So we were studying this. And you see how the inputs, you know, get you get the encodings, then sort of you do a lot of there are these hidden layers that compute your attention and so on and so forth. Right? We noticed that every model, actually, that we were studying, particularly the transform base models, had these hidden layers which were multiple of such hidden layers. And we we basically thought that if we actually reduce that redundancy, replacing that whole block of hidden layers with just one hidden layer, Right? Then created a version, a compact proxy of that model, that was just 2% of the entire model. Almost 2%. Then the cost of actually having to deploy this model for profiling reduces significantly. Right? And that's what our first insight comes from. We also verified that to be able to actually use this insight, we verified that there is a linear relationship. Between one hidden hidden layer and multiple hidden layers. Right? And this was kind of made it easy for us to come up with what we call LLM fingerprint. And I hope, there there are a lot many more details in the paper that was just presented last month at SC this year. But that was our keen side. This compact proxy that we call LMM Finger actually gets us closer to the, to the aim of cost efficient profile while being accurate. Now, the the second thing there was, even if we had this LLM fingerprint, you still required to understand different configurations. Using those. Still too many, configuration options and still too much cost there. Again, we refuse to sort of have to do that. Then one thing that comes to mind is, wouldn't it be nice to have an understanding that sort of takes us from few measurements to entire. Space of complete, the deployment configurations. So then, again, we sat down and tried to understand where where are these metrics, spending, you know, their time and effort in. What is latency, built off? Right? And then we understood that from the model's architecture. And translated that to an analytical model. Of our understanding. Okay? With that, our hope was that we would just measure using a couple of our three, four kind of measurements from using the LLM fingerprints and predict using the analytical models. So we would have eschewed the cost behind all of this that existing systems have. So here's what we did. I don't have the time, so I'm just gonna give you, the idea how we built these models, but, and hopefully, this is good enough plug for you to go see our paper. So let's say this is an LLM. A bit of notation not too much. DTP and DPP respectively define or show the degree of parallelism, different types of parallelism, tensor and parallel. So if you are fortunate enough to have a model and big enough GPU, they can actually work together without parallelism, that's what you do. You take the model, you completely deploy it on that one GPU that you had fortunately, and you measure the, latency. Of those. Right? Let's say that latency was x. Now let's say I I I don't have that fortunate situation, and I have to distribute that model. Let's say I do a tensor parallelism. I distribute the model across these GPUs. By the very fact that now every GPU has to do much lesser amount of work, the latency gets reduced. Proportionately. The degree of tensor parallelism. But then we have to do this all reduce operation. Introduces some overhead. That we capture using you know, I'm grossly simplifying the formulation we had, but, we captured that into, this other term, overhead of tipi, and lot of constants are kind of in b. Let's say we wanted to we needed to divide the model further. By using a combination of, you know, tensor and parallel and pipeline parallelism. That would basically reduce the work of every GPU once again, and so you will see the difference in, the latency. Once again, we are dividing by proportionately dividing Proportional value of that latency comes from that division of the original latency with the PP. Then there is an overhead added because these GPUs have to communicate to achieve that task. That ends up being an overhead term yet again there. And lastly, you because this is a combination of 10 and pipeline parallelism, there is going to be an all reduce, which, you know, again, for simplification is kind of represented in that one, constant there. So let's focus on this. Right? If I asked you to focus on this, and you would note that there are three unknowns. The latency x, the overhead of parallelism, overhead of pipeline parallelism. So we have a linear equation in these three unknowns. Let's invoke our high school math. Very simple. Right? Three unknowns, we need three equations. So basically, we need measurements using fingerprints just three measurements. Plug it in, solve, and you have these values of these unknowns. Now, obviously, in reality, there are many more, Greek symbols in our formulation that I have completely glossed over. But please take a look at that. This actually significantly improved, you know, we we had to use fingerprint, which was two, 3% of the whole model. Just a couple two to three actually, three, literally. Runs with those, and we knew everything we had to know. We wanted to know about all configurations and their implications. On user intent or metrics that applications would care about. Right? Similarly, there is, you know, how do we predict memory? And similar analysis. Where does the memory footprint come from for models? Have the time to go in, but, you know, I would really, point you to our people for that. So with that, hopefully, I have shown you how to achieve accurate and cost efficient profiling for LMS. To choose to go from all these deployment configurations to you know, choice of that configuration that so that you can meet user intent or application level requirements in terms of latency, memory, footprint, number of GPUs that need overall cost in other terms. But we are not done. Right? And I'll tell you why. Even if you had the configuration of choice that meets, unless you did close the loop, by looking at what available resources you have in your GPU cluster, you don't see the benefits. You do not see the cost going down. You see a lot of GPUs being added. So the first thing is you need to avoid idling, and there is a lot of work, so we borrowed load of waste strategies. Like you know, if you are from systems world, you know that you should you must pack at low load and distribute your work. At high load. We did that. Right? But even then, the number of GPUs we expected it to be to be required too much. Right? Definitely above what we had expected. And we realized that here is what was going on. Again, very small, very simple insight in here. When we chose a configuration that, let's say, for example, said pipeline parallelism degree of two, We divided the LLM into equal chunks. Right? And what happens then is if you have a class cluster that does not have two such GPUs that can accommodate, those equal chunks of the model, you had to add a new GPU. So we decided that, actually, this conventional wisdom, we let go of that. We are gonna break free from that. And we are gonna divide the LLM and an unbalanced way. Okay? And if I do that, knowing what availability of resources I have, now I'm able to find fragmented resources in the same cluster I without having to add a new GPU. So, this actually, the simple thing, just letting go of having to have a even chunks of model, actually goes a long way. Then you're gonna say, you know what, Neeraj? We knew that, but there is going to be. A latency impact. That's why we included that's why we said that you have to be, balanced Right? From, what about the pipeline bubbles that you introduced? What about those? Right? So let me tell you that actually and again, don't have enough time, but, see, in training, you have forward as well as backward pass. And backward pass, is much more work, not just equal, but much more work than the forward pass. In inference, a you just have forward pass. And so even though there are those bubbles get introduced, the end to end latency impact is actually negligible. Enough that you can actually utilize this very simple idea of you know, intentionally splitting the model in an imbalanced pipeline manner. Okay? So we built a system, you know, that sort of has these three personas. The we get the model from, model you know, providers. We we have this fingerprint controller as part of our system called Maverick. That generates the fingerprint profiles and estimates and creates this map of, you know, for that LLM, for a given configuration, what would be the impact on different metrics that we care about. The deployment control application developer, their intent, and makes deployment decisions looking at you know, the available cluster going ahead with our load aware and fragmentation aware deployment strategies. And that is then served in our scheduler. This was first and first served, but, you know, this opens up, like, our code is very modular. You can have more sophisticated scheduling strategies. You can plug in. I I don't want to go over this, but you know, please take a look at this paper. We do deliver on promise. We do meet various intents, and we do it at fraction of a cost that existing systems had. Like, promised. Right? I want to really go over, you know, the the second problem here. Right? I I told you that when we are forcing ourselves, these all these different LLMs, they're going getting deployed, and we have them sharing the underlying resources. Ends up creating resource contention in terms of memory, and the third point here is compute. Right? I won't actually be going into that It's eighteen minutes right now. Okay. Okay. Right? Okay. Thank you. Terms of resource contention over memory. Right? So where does the memory footprint actually of an LLM comes from? Right? It not just parameters. It's the working memory, working size, that we call context or kvCash. Actually, it's very, very important Every time you turn it a new token, you need to know what what has happened, and you have to have that. And that up being a sizable chunk of memory. Right? And so you see that impact, actually. If you see as request size or the in the workload keeps growing, your your kvCatch available kvCash sort of is not sufficient, and that shows up in increased latency. Right? Significant impact on the latency because of kvCash, and there is so much work from systems community on optimizing kvCash. One such thing that we have done. So basically, what we are saying is kvCash matters. Give me more of it. Right? You don't have it. That's decided by the of the world, whatever GPU, HBM that you have. For instance. Right? So what we can do is we can play around and move stuff around. And we know how to do that as systems, developers or architects. So I'm gonna, like, you know, what what literature has suggested that, let's take some part of kvCash. Let's use the CPU memory DRAM, as an extension of that. Effectively expanding the k v cache. Right? And you would say there is going to be overhead of transferring this kvCash back and forth between CPU and GPU memory. People do tricks like you overlap the trans the transfer time with computation time. Right? And that actually helps in improving latency. You know, huge improvement. But then when we were looking at this, we realized another thing, that this was actually good, but it was doing a lot of extra work that really was not needed. So why don't you use peer to peer memory can I take the question, you know, little bit? Let me finish this. Right. So we realized that there is an extra work overhead that was added. Why? Because you know that the you know, the cache that is part of it that was on CPU would basically, for every token, you would have synchronization. Right? So this second arrow that I just added there was extra. Was not required. That simple insight, that simple observation, basically, then pushed us to think about what else we can do. And we realized that if you see the GPU memory is taken by the parameters and kvCash. Parameters are static. They are not changing. So if we actually made our system work with just that, you know, basically take a few a few parameters, repurpose that memory as kvCash, You don't need to copy that because you can maintain a copy on CPU memory. Right? And you got the exact same result. Effective kvCash was increased without that burden of synchronizing. Right? And just with that, obviously, this is not as easy as I'm making it sound. We developed ONEEROS, which basically stands for a dream. Which sort of you know, expands kvCASH, depending on, you know, models we have available and so on and so forth. So our crux of the system, basically, is this remapping engine that has to answer these questions When to trigger this remapping, what models, how many layers, and which of those layers? It matters. And we utilize that these LLMs have different layers and they go back for every token. Right? So we utilize that pattern to decide which layers show be thrown out without actually seeing the impact on latency. We do really well compared to, state of the art baselines like VLLM. We improve not just, latency, but also throughput. Because we are just by not doing the extra amount of work. Whole insight about this work was identifying the key inefficiency that we were unnecessarily doing some work. Alright. So because we are running out of time, and I promise you that I will only talk about these two, I will leave you with you know, these two links that I'm not gonna talk about right now. But, in compute concurrency sense, if you think about what is actually placed together on one GPU, the kind of compute sharing that you do depends on the concurrency mechanism. Right? You can time multiplex, you can special multiplex. In special multiplexing, you have reservation based or packing based, and everything ends up depending on what models are sharing it. What load you have. And so that requires automated mechanisms to improve utilization. And and so on and so forth. Lastly, like I said, this whole focus on improving utilization is taking us two steps back in terms of sustainability metrics. We realize that won't make the same decisions if you care about power, if you care about carbon or care about energy. So power or such sustainability metrics need to be, promoted to the first class citizen And we need to solve those optimization problems, in harmony with performance compatibility. Of just focusing on performance compatibility. Okay. I'll stop here. For that. I think we have time for one question. From the audience. So you mentioned, you're using kvC cache and you're using system DRAM. You're using system DRAM There's a limitation on system DRAM. It's So Why you're not using peer to peer Drake DMA transfer from GPU to PCI devices, potentially CXL devices. Which which will give you a way more scalability and way more IOPS and performance. The the basic point I was trying to make was not that. Yes. We can we can go ahead and actually literally do GPU direct or these and we link between GPUs. We can share, we can we can do all of that. And going to CPU for kvCash expansion was not something we did, actually. That is what literature does. Are actually not doing that at all. We are basically letting go of parameters. We're deleting them, repurposing that part of HBM, as kvCash memory. That's all actually we did. Right? But but I agree with you. There are so many different ways another configuration navigation problem, like, what should be used for and and there are cost implications. I agree with you. Okay. Thank you, Nirja. So now moving to the next part of the work workshop, we have two spotlight talks right now. The first one is from Yousheng Zhang, PhD student from University of California, Santa Cruz, who'll talk about towards AGENTEC OS and LLM agent framework for Linux schedulers. Hello, everyone. My name is Yoo Seong Jeong, and I'm mainly, like, working on EVPS and operating some domain and also maintaining some open open source organization like, and show in this talk, I will talk about, like, we want to let LRM agents to fully automatically optimize operating system schedulers Well, actually, we want to make it fully optimize the entire OS. But currently, the system only has, like, extensible interface, which is the scariest can have significant performance. Impact. On most of the applications. So we are starting from the schederers. Why we want to do that? For example, there's two major problems. One is the semantic gaps. OS schedulers, the default English version now is e EVG. Is failing to understand application needs. So it's a long history problems. Application has different requirements. Like, some application require latency, Some application requires throughput. Some like, some application is batch processing, while some application is, like, interactive. With, like, a QI. So they have only have different SLOs. Also, we have a lot of knobs, and such is possible interface, which allow us to ingest some, like, algorithm and policies and code into code into OS kernel. But, typically, it requires like, deep kernel as best and workload as best. However, the workload developer who is developing the applications does not understand kernel in turn. Like, system admin with deploying the application to the kernel may and it lacks workload insights limit not a lot of the kernel and environments. Like, typically, like, left left or something like smartphone users. They they don't they don't know how to write code. Like, both kernel and application expertise. We have multiple solutions to software's problems now. For example, we have traditional IRR base or like, machine learning based models. Late heavy query require, like, human descending, like, say, action reward specific. So it's, like, typically, turning some parameters and only predefined workspace. Problem space. And it may require some, like, per workload return and per environment returns. And may may not be. May not be good to transfer to new problems. And, also, like, if you want to inference or involve ML model in the OS scheduler, it may cause inference overhang. Your whole scheduling pass because you need to make decisions in microseconds. And you also like semantic understanding to our application. Naive or Asian Solutions has been proposed many times, for example, people use it to write a GPU kernels. It can also be used to improve OS kernel But, technically, it's like we also have previous working list domain, People can specify some goals in English, which, like, the LLLM will transfer the config English into some configuration or algorithms, but it this means you still need human defined hellhole goals and specific context. Human still need to understand the workloads and OS. And, also, if you just apply agents to the system is unsafe, and my performance instead of improve it. We've test some basic like, cloud code and just ask you to write a basic scheduler for eBPF. It costs, like, thirty minutes, $6, hundreds of cost to do that, and spell a lot of language, narcissist. Successful. Our our team says, like, we want error and agents to serve as a OS level components, which so we can bridge the semantic gaps and not gap problems we described before. But also Kibla OS app abstractions, which means application developer does not need to understand kernel and short because they have standard interface. They, kernel kernel developer doesn't need to have too much knowledge about what a specific workload is doing. And so we have two parts here, and the first part is the AI age AI engine design, which we treat the problem space as two stages. The first thing is, like, we want to let agents use tools to analyze its workload intents and structures. Enhance its system environment, AI agent can generate some, like, workflow profile or contain the information about our workloads. So and, like, SO organization goal for our class, Then there are RMM agent can start to do the actual actually policy synthesis. Which is like configuration or generate new schedulers based on the lenses. So it's, like, fully automatically. Processed. So we also want to separate the system part, which is allow us to do a safe and efficient pre railing code with with the AI agents. So system can still remain safe and useful as AI agents gets better. So we need to separate AI's rule of reasoning from system rule of execution. You want to manage Yeah. So our goal is, like, we're to make AI agents manage operations from, like, human society to deploy a application and reduce overhead. The system architecture include MCP server and system demo. So it's, like, part of, like, agent and powerful interface. Or preliminary evaluations include two paths to One pi is less configuration, one pi is the synthesis. You can make, like, two times better performance and, like, reduce a lot of, like, cost and latency. And, well, we can let like RM agent can do actually generate new schedulers based on the answers. Well, let's kind of tool is mainly like a proof concept. It still needs standardized agent take bench marks with clear defined test and, like, go inference or SOS. And sometimes, MCP server is not a best interface. Maybe best is better, or we need better tools for providing Community also has proposing many other interface beyond schedulers. Like cache and CPU frequency. We are also working on, like, GPU memory management and scheduling. As our next step. To summarize, we propose SCLC P, a control plan framework can OS levels interface to enable fully automatic optimized OS. And we separate Go inference from parallelization synthesis into like, as a two stage problem, so we can achieve a lot of performance improvement. In the interest of time, we'll do the other spotlight talk and then do questions together. If you could have wait here for the other spotlight talk and take the questions later. Okay. Yeah. Thank you. So the next spotlight talk is by Janani Ramamurti, who's a PhD student at UT Austin, and will talk about automated multi agent workflows for RDL. Design. Hi everyone. I'm Janani Romo Muthi and I just wanted to clarify really quickly. I'm actually an undergraduate student at UT Austin studying elective electrical and computer engineering. I'm I'm a junior. And today, I wanted to present Veramass, which is a multi agent architecture search being done to generate their log code. Okay. So, nowadays, large language models are starting to be widely used for RTO design tasks, including Verilog generation. And for those of you who are unfamiliar with Verilog, that's basically a hardware description language. But fine tuning these LLMs is very expensive and requires large GPU memory, big token budgets, and often a lot of manual iteration. And frontier models can outperform fine tune models using prompt based reasoning a alone, but they come with high compute requirements as well as very long inference times and they essentially overthink every problem. So, existing methods such as Verit Thoughts already embed formal checks into prompting but they still assume one fixed structure, such as only using, like, chain of thought reasoning. But this is very inefficient. Because some dialogue tasks, such as implementing a multiplexer, don't need debate or multistep reasoning, while tougher modules like complex finite state machines might require deeper refinement. So we want an adaptive reasoning strategy where the model uses simple strategies when possible and escalates only when failures indicate complexity. So this is where mass or multi agent architecture search comes in. So in this context, an agentic operator is just a reasoning primitive or a prompting strategy such as chain of thought, react, or debates. And instead of picking one strategy manually, mass lets a controller compose operators into workflows like performing chain of thought and then moving on to something called self refine or doing a one shot prompting method depending on the task. So, here's the pipeline of how our method works. We start by taking an RTO hardware design problem statement and then we sample a set of candidate reasoning operators. And then each operator a against through EOSYS, which is an open source tool that performs synthesis and formal verification checks. And then we also run it through Open SDA for timing and power analysis. And the critical step here is that Veramas uses the error logs generated by all of these synthesis checks to decide whether to continue processing with more complex operators or stop early because we're satisfied with the design that it already generated. So, the set of operators that we use spans spectrum from simple to complex. We start with something called zero shot IO prompting, which has minimal reasoning You're simply putting in an input and getting an an output from the LLM. And then we have chain of thought, which has structured intermediate reasoning. And then we have React which alternates reasoning steps with external tool interaction. Lastly, we have self refine, where the model critiques and improves its own output, iteratively. And these are what we call agent operators or the modular reasoning tools that the controller assembles into workflows. So our controller is the brain of our algorithm, where and it runs in a cascade. So, we start with using a very simple operator, and then we move on to more complex operators. And at each stage, it evaluates the batch of generated designs using a metric that we compute from the EOSIS and OpenSTA results. We're checking for things like how many didn't have the correct syntax or how many failed formal verification checks. And this produces a confidence score. So if most designs succeed, then we stop, but, if many of the designs are failing, then we're gonna escalate to the next reasoning operator. So formally, we're optimizing for two things. The first being utility, and that's measured with PASET k, which is a standard cogener metric, and it's essentially the probability that at least one of k generated samples is correct. And the cost is measured as the token usage per query. And we use a small trade off coefficient, lambda, to balance these out, and this is just a metric from the original paper detailing multi agent search. Training our controller is extremely lightweight. All we do is sample 500 data points from the VerithaTuts training set, and for each data point, we generate about 20 candidates and run them through YOSIS and OpenSTA and count the failures. So if we look at some results, we see that on OpenAI models like o four Mini and GPT four o Mini, we get improvements in pass at one and pass at 10. But because these models are already strong, gains are smaller but still very consistent. And then on open source instruct models, we see larger improvements because these models are good on their own, and we see up to seven to 12% gains in PASET one when without any sort of fine tuning, just multi agent architecture search. And then, lastly, one thing I wanna cover real quick is that controller is very flexible. We can optimize for different goals without retraining the underlying LLM. So we can optimize for things such as power performance, and area. We evaluate this on a subset of data called PPA Tiny, So, if we look at that here, we see that we get improvements in area and delays sometimes by over 20%. We see some increases in power, but overall, we get improvements in area and delay. And this is just a general summary of our work. It's a multi agent prompting framework that integrates formal verification feedback into the operator selection loop. And in the interest of time, I'll stop here. But, feel free to check out. Have we have maybe time for one question each for both the Spotlight talks? Anybody? Could you please stay at the beam and if Hello. Thank you for the great talks. This is Utku from Harvard. So my question is for first talk Do you think there is any security implications of using LLMs within the OS? Yes. Laredo. There's significantly security problems based on the eBPF security mode because eBPF can, like, break a lot of your users based application and like, cause a lot of unfair in scheduler. So we need to have designer verifier or some, like, steady check or dynamic roll back to BigLab. Any other questions? Okay. Are you planning to present this, your work at LBC this year? I will not present this the other person also asked? That's fine. That's fine. Oh, maybe we can try later. Right? Because Awesome. Thank you, everyone. We have a wonderful set of posters, and please go and talk to the authors. They might have some great insights to share. After that, there'll be lunch, and then we'll have the post lunch session which will start with a keynote from, professor Ian Steiker from UC Berkeley. Thank you, everyone. So Hello? Okay. Now it works. Okay. Well, we'll come back to our afternoon session. And just a quick recap, we are going to have, an afternoon keynote from Yon Soiga, fall followed by a debate panel. So please stick around. They are very relevant. They are similar topics, and I I really hope that you will enjoy it as much as we are very excited about both the talk and the debate. And then we can have a short break. We're gonna have more invited talks, spotlight and, post their session. So but first thing first, for the next talk, I am very honored to introduce Ion Staiger, professor from UC Berkeley. He is the director of the Sky COMP lab and the executive chairman of Databricks and Annie Scale His current research focus on AI system and cloud computing, and his work includes numerous open source projects, such as VLLM, SGLAND, Shadbot Arena, SkyPilot, and many more. He's a member of National Cutting Up Engineering, honorary member of Romanian Academy, ASACM Fellow, and he's cofounded many companies that you probably heard of. And, I would like to welcome Eon for his very excited talk today. Thank you so much for having me. And, it's again this is, like they say, hot on the press. It's like just getting the results. Few minutes ago, the latest one. So, so we'll we'll excuse a little bit, maybe not being the most Polish talk, I ever given. First of all, I just want to recognize the fact that this is actually a very, you know, a group effort. I am only talking about it. Many of the students are here in the audience. So please feel free to contact them and ask the hard questions. So this is about, you know, using AI to improve, to accelerate, system research, in particular the performance, system the system performance research. Now using machine learning, and AI to improve the system, it's obviously not a new topic. It's actually one of the first talk here, a very influential talks given by Jeff Dean was in 2018 at New York, actually, that time was called NIPS. And since then, there are a lot of papers which are written published using AI in different domains, like databases, networking, and systems operating systems and more. So what is kind of different now, and I think why at least personally, I'm quite excited. I think we are seeing a phase change. So before when we use AI for improving systems, we are looking at the you know, AI a little bit like, a black box. Right? It's like, basically, you have a system, we have an workload. And then we either optimize a bunch of knobs, parameter, or you try to replace some of the components of the system like query optimize with with neural networks, which hopefully is our packet classification in working with a neural network, which are doing things better. However, what we see right now, at least what I'm going to talk today, is that the the the system and so forth is also more like a white box. We expose the system to the to the to the neural network, and the neural network is going to operate on the system code and improve that code. That's why how you are going to improve the systems. Okay? And a lot what I'm going to talk today is mostly focused on performance improvements It's a easier problem in general, but it's still very widely prevalent in early system conferences. When I'm talking systems and a system here, I'm referring to sync systems broadly. It's databases, networking, operating systems, even programming languages and things like that. So what are the disruptors here? So one, I think, is one of the, influential what first at least, you know, was, this fund surge. Which was using, from coming from the mind using, live language models for discovering new solution for mass problems and algorithms problems. Then it was late earlier this year was Alpha Evolve, which was published showing that you can extend that the same idea to other problems is pretty general, and the are giving some impressive results in that paper. Actually, it was June 2025, so it was even yeah. So very recent. And then there was open evolve. This is open source version of the Alpha Evolve Alpha Evolve again. You know, given the name you you know that it came from DMind. And this is what has happened. So we all at this paper early on then, and then over the summer, and, you know, JEPI is, you know, our own paper in this space, and many others. And, of course, you have this code assistance. So what we decided then in the summer, this summer, is like, well, you know, let's see how well these techniques are working. And, we are asking the students here, why don't apply your technique these techniques on your problems? Not benchmarks, on your own problems, your own research problems. And then you report back, and you have presented you know, try to understand where you are. And I think the results were were better than expected, and that that's why I am here. So you look about, I think, around ten eleven problems, and these are the problems. You see this from different domains. And, there are different level of maturity in this project. Some of them were already published. I think here, eight of them, were published in conferences you know, top conferences more or less in the in their fields. And, then what we got is that in large majority of the cases you see here, you know, eight or nine cases, people got better results than the SOTA, either the published in the published papers or the current best results you're getting in their particular research. Okay? Not only that, but it was quite cheap. They spend at least for this original getting early results, a few tens of bucks for each of this problem. So that's kind of, you know, what's made us exciting. Excited. Right? It's like you apply these tools to the your problems, research problems, you get pretty good results pretty fast. And pretty cheap. Okay? What not to like. So, so now let me take a step back and think about what is kind of the impact? This is what you know, you always ask. And if if you think about how do we do resell, at least system research, so how do we do it? We we are going to come up with a problem to add to solve. Right? Improving, you know, you know, transaction like throughput or improving latency for networking. Traffic, things like that. Right? And you have the program formulation. You know? And then what you do, you you try to figure out what is the evaluation framework, what is this is it going to be the system? And in many cases, you have a simulator. You know, which you are going to develop your algorithms. And then you are going to start developing the algorithm algorithms using this kind of evaluation framework. Right? It's not only about the simulator and so forth. You also have to decide which are the benchmarks, like your databases, TPCDS, or whatever, DPC, and things like that. You are going to decide know, what the benchmarks you are going to want to you know, to target. And then you start to design solutions and implement them and you evaluate them, and you are going to go this repeatedly until you get, in general, good enough results. And when you get good enough results, you write a paper. Right? That's what we do. Right? And, and, basically, here, what we are trying to focus on, on this kind of middle in this pipeline, basically, the solution generating the solution and evaluating it. Okay? And, this is what we try to make it automatic. And if you think about this, what all these kind of systems do, basically, they have a prompt generator, now the solution generator is a language model, which takes a prompt and generate a solution. Then you have the evaluator using you're taking that solution and evaluating it. And then you store the results in a database, in a storage system. And then you have a way to pick previous solutions and feedback to the next to the prompt so then you iterate Right? Because every new iteration is you are going to be based on some of the results you get in the previous iterations. And you do that until, you get hopefully good results or until your budget you you, you know, you are done, you know, with the budget, compute budget. Okay? Now you can have also an outer loop in which you basically you can go take us from time to time and look as there is results, the logs, and so forth. And may you may want to change the prompt or things like that or even the algorithm. But that's kind of the main idea. Right? So here is an example, and it's an example. It's, it's expert parallelism load balancing. Everyone kind of knows that. And, you know, this may model of experts, you have a lot of experts. And now when the model is larger, you have a lot of traffic, now the question is about you want to assign these experts on GPUs, and you want to ex to assign the experts on the GPU such that to have very good load balancing because that will maximize the throughput and will reduce the cost. Right? And it's not only you the problem is not only to assign this kind of of experts because some experts will be very popular and you also want to replicate the experts, and then, you know, assign the replicas. Right? So that's basically what what is the problem. And there are two like, I'll I already alluded. There are two metrics you care about, balancing factor about how well is this x the GPUs are load balanced. Balanced, the load. The experts are load balanced on GPUs. And it's you can think about average GPU load over the maximum GPU load. And the running time the algorithm because it's a very dynamic system. As you know, a token the experts are token it's using, you forward that expert to. Can, you know, depend from token to token, and also from layer to layer can do it. So you need to be pretty quick if you need to reassign these experts to the GPUs. To improve the load balancing. Right? So how fast you can compute a new assignment is also very important. Okay? And this is, again, the loop, the open evolve loop. And let's see what happens. So the prompt, you know, this simple example, you just describe the problem. Right? You say what is the problem, and then you say what are the goals. As you see, you improve the algorithms to achieve better load balancing and improve the algorithms to be more efficient. Right, assignment. Okay? This is what you described. Right? It's very natural. Then you give the code. In this case, you have a simulator, so you give the entire simulator. And but you are going to define a block block in this in this simulator This is where you want to I want you to change because this is where the load balancer code is, right, in this evolve block start and evolve block end. Right? That's what you do. Right? And then, you ask you you know, z solution generator is going to take this prompt and is going to hopefully provide, better algorithms in this between in this block which you asked you know, to be a vault. Okay? Then are going to run now the simulator, which has a new algorithm which was proposed by the LLM. And you are going to get the results. And you are going to store this in the storage. You are going to run on multiple benchmarks. You are going to take the average, and, again, you are going to the new solutions and the results in this storage. And now you are going to iteratively build the new prompt. And the new prompt, you are going to use some of the solutions previous solutions. One way to do it is like again, there are many ways to do it, but think about one easy way. You can choose a two best solutions. Right? So you say, okay. Do better than this one. And then, also, you can you can also select a bunch of random solutions. And the reason for that is provide diversity. Right? Maybe there are good ideas in those solutions, which allow you to develop even a better algorithm. Okay? And you add this to the prompt. And, then you go to the next iteration. There are a few you know, is a little more sophisticated actually. This this, a frame have a lot of configuration parameters. But one thing is that they have this kind of term of island. You have different which is think about different group of solutions. Again, for diversity, you you are going to evolve in parallel, and from time to time, you are moving the best solutions from one this island to another. But, basically, this is what it is. Okay? So next, I'm going to tell you about a bunch of results and a about, you know, some lesson we learned and, you know, things. So these are the results. So we try to do this for deep seek r one for this is this is our problem. It's expert parallelism, APLB. And the baseline was to we have to is an OSS implement implementation, which was provided by DeepSig by DeepSig, the company. And we also have an internal access to their internal implementation. Okay? And what are the results are pretty good? The for wanna use this OpenInvolve, the balance factor of the same is point 66 in this case. And but the rebalancing algorithm run time, significant improvements. It was order of magnitude faster than the open source implementation. And even 30 times faster than the internal implementation, of this company. Okay? So implementation. So and it was a final round kind of is, like, five hours, 300 iteration. And it was less than $220, and it was kind of a combination. Gemini, 2.5 Flash and Gemini 2.5 light. So not the most powerful models. And this is kind of the evolution you know, so to speak, what how happened. You have the iteration here. It's, you have only whatever. 100 probably 70 iteration. And you can see each of this improvement is a combined score. So you have here two You you care about balance factor and balancing algorithm. Runtime. You combine them because open evolve ask you to provide only one combined score. And here, I think is the weight is half half between the two. Okay? And you can see so the higher the better. And you can see here different points where you kind of make significant progress when you discover different techniques. And you are going to, use those in in in the load balancing algorithms. Okay? So, vectorized PyTorch operations, do more vectorization, and then zigzag pattern, which is by one technique to assign this expert replica to the GPS. Okay? We also try from then since then, we tried other two other revolutionary systems. One is JEPA, which is we developed at Berkeley, and this is, one difference is that it handles multiple results independently. So in general, if you run your, solution on multiple benchmarks, in the case of open evolve, average. Here, you report the results independently on each benchmark. And also, you have you select the solution on a parrot or frontier, And, also, the way is is not a fundamental limitation, but the way the JAPA is working today you rewrite the entire program because originally it was designed to generate multiple instances of the same program. Okay? And then, Shinkau, which had derived from OpenInvolve, and it adds richer feedback. What that means is, like, more or less, like, every 10, iteration, 10 solution, it summarizes its solutions, and then it on then it provides insights of all these 10 solution from the summaries and also provide some recommendations. This is supposed also to help the human you go back and to understand what kind of happened. I'm not sure how much we've done that, but just wanted to because Itauka is people from Shinkai, and I think they pointed to us that that's one of the when they tried, this is what, you know, to make it much easier for humans to interact. With this evolution algorithms by providing more explanations about what the evolution algorithms done has done. Now this one, it has more you need more inference steps because you need to do the summarizations, and you need to do the they regard the insights and provide recommendations. And here are the results. Okay. These are 10 cases. From the original problems. In all cases, we try to do 100 iterations, with but with, you have a little bit more LLM In general, one iteration is on LLM call. And, you know, there is no overwhelming winner. I am you don't need to look all these numbers. I'll tell you a little bit more. Open a World is a winner in six cases, Japan in three and the Shinkai in three. GPT five, it's winner in six KGs, and, Geminis three and four. And then you start to look and, obviously, the the main question It will be, well, you know, you have all these algorithms. You have all of these things, you know, if I want to use it, when should I use it On which I have this problem, which algorithms I should use it. Unfortunately, we started to analyze it. I, unfortunately, I will not be able to give you a definitive answer to that. I'm just giving you some very preliminary observations. So the program structure in terms of size. So it it turns out that open Evolve in general is going to generate shorter programs. Shorter than Java and shorter than Shinka. By a factor of two. Or more. Okay? Shorter. And also, Gemini, it's also in general, generating much more compact programs. Than GPD five by a factor of 1.8. At the same time, when you look at the structure, while Gemini three code is shorter, GPT five is more modular. Like, you look in these examples for Germany g p I four g p I three, you have two functions. While g p five, you have nine functions. Right? So it's more modular. That's also one reason it's more code. Okay? So it turns out that the shorter programs in general tend to perform better. So OpenInvolve, it's wins in six cases versus Japan and Sri And Sri Lanka and Sri. The modularity seems to help. Especially in the case of JEPA, where the majority of the winds are coming from GPT five instead of Gemini. And one our hypothesis is that modular pro in in jet, you you override the entire program. Right? You regenerate the entire program. In the case of Open EVOLVE and Shinkai, it's on the block you indicated that has to be changed. So the more you are going to generate it's like it seems to us is that, if his code is modular, it's easier to reuse. Right? So if you have to rewrite the entire code, it's much more prone to failures and so forth. That's kind of again. And, also GPT five seems to handle better reach feedback. So the the Open EVOLVE is the simplest feedback. It gives you if you have the solution and the results. For that solution. For JEPA, you have like I mentioned, if you have if you run your solution on multiple benchmarks, then you report instead of the average, the result for each benchmark. Okay? And Shinka is the most provides you the most feedback because provides you also, you know, every 10 solution. It provides you the insights over all these 10 solution or whatever. And also provides you some recommendations. Okay? So in Shinkai, JPT five wins, you know, the majority of cases, compared with the other ones. Okay. So now I am going to say a few lesson learned from this experience, which are communicated back to me by the student. So the first one I call this kind of it's a theme. Less is more and more is less. So you may expect that if you can start from the strongest baselines, you are going to get the best results. It turns out that's not the case. And sometimes, if you ask us starting from from a weaker baseline, you get a better result than you start from a strong baseline. We hypothesize this is because if you are in a strong you have a strong baseline, you are in kind of local minimum, and it's sometimes harder to get out from this local minimum. Okay? So here is one the strategy which is quite effective it's also, you know, is recommended by Open and World people. Is to use different baselines. It's a good strategy. Okay? The second one is about the hints. When you solve a problem, you know quite a bit about this problem. You have a hunch about what solution you should have and so forth. Right? Now it turns so and you know, the hints are good because you reduce a self space, because it's prevent directions. Right? Like, you advise a student and say, okay. This is a problem. Try z sync. Right? Now the fewer fees, hints, on the other hand, provides more freedom. Okay? And you can find better solutions. So you have to use the hints with care. Right? And for instance, one thing is don't use all the hints you know. And once, because you reduce the self space, and you the diversity. One thing you can do is you can have try different runs in each run, you have a different hint. Or different subset of hints. Or you can try this kind of multistage. You start with a few hints, if any, You get with some solutions, and maybe you're gonna get kind of stuck. You are going to now I'm going to give news other hints. It's like, again, it's a go and do this problem. And it's kinda, okay. I cannot make progress and so forth. Now you you start giving some hints. Right? So that's kind of the analogy. And the final one is restricting access to high level, say, library API. So it's a level of abstraction here can lead to better solutions. And, because a high level API, you can use this API and to generate the code faster, but you don't have this API, you can discover better optimization. Like, we we use libraries. Right? In many cases, we use libraries for our program. But in some cases, you know, this if we implement this kind of functionality from the libraries is not optimized for our use case. So if you don't use that and if you write yourself your functionality, you you are implementing yourself the functionality can be faster. So he see or he seems that with the PHP PAAP LB, you know, the providing PyTorch API, which If you provide the PyTorch API, it leads to reduce a custom operator with PyTorch. However, if you don't provide a PyTorch API, it leads to open evolve to direct directly optimize the custom operators and do a better job. Then is Cloudcast. This is another one. It's like, you know, it's like this is about streaming and moving the data between different clouds efficiently and, not spending too much money. Is like, providing the entire simulator to LLM at least to waste the LLM wasting times. You're looking at irrelevant code, like, for instance, saving generated pass. Has nothing to do with optimizing about how you are going to move the data between clouds or between cloud regions. Okay? So it's we do very careful about the level of abstraction you provide. The other thing is about the solution It's only as good as a evaluator. Okay? No question about that. Right? A float evaluator is a primary cause of flow solutions. Okay? And you need to prevent overfitting. Like, you know that. It's like, you know, so you need to divide use diverse workloads. And, you know, it's it's actually a lot of papers, system papers, are overfitted. Okay? Because what you do, you basically have a benchmark, and are going to beat that benchmark. That's it. Right? TPCC, you beat that benchmark. TPCS, you beat that benchmark. Okay? But if you think about that's yeah. That ultimate definition of overfitting. And in many system conferences, it's perfectly acceptable. But of course, in order to generalize, you try to have some holdout work workloads to do the testing. Prevent your hacking. Okay? Need to be very careful here. So here is like, for instance, in ePLB, is like, if you have an an expert, you know, is like you're safe to assign a fraction of an expert kind of to a GPU, which is less than one. It just is is like, do it is clump clump to zero, and then get rid of this experts, and then you have a you you allocate only a few experts and for which the load balancing looks very good. For the Cloudcast, if you are not careful, it reduces the data volume is transferring to get better results. Okay? You alleviate reward hacking? No secret. No no silver bullet. Comprehensive test and, it's also you another one, it's it it turns out it's pretty good, it's your constraints, the number of line changes. Right? It's similar with the k l divergence on penalty for RL. Right? You don't want to go too further from a solution which is kind of working. Right? Now another question is about you you run Actually, in this case, we assume that we have the solution, it runs, and you get the results. But in many cases, you can modify the code. You are going to have an error. Right? So what will happen in that case? One solution one one thinking about I I I this was my initial thing is that, hey. You just remove that solution. Right? Right? To ignore it. Right? It's like it's wrong. But it turns out that it's not always that clear because the wrong solution, which, you know, has an error, syntax error or something like that, may still have a good idea. Right? So maybe you want to have it part of the combined metric. Okay? With a very low small weight. Okay? So the next question is that, you know, you try to think then, okay, add this address, we call this address. It's like it's like, it's, and, you know, it works still quite well. So why do they work? You try to think start to think about that. And I think there are a few things. First of all, we are using it for performance problems. As a performance problems are kind of easier because when you write a simulator or when you are looking and you do the changes to improve the performance, in theory, you shouldn't impact the correctness of the system, the semantics. You shouldn't change the semantics of the system. Yes. In some cases, you do that, but in general, you shouldn't do it. Right? It's like and so it's a little bit easier. Right? The second one this is a funny one. It's like it's a a as a researcher, when you when you work on a problem, you are going to iterate until you get good enough solutions. The solutions are good enough. You are going to write a paper. But these things, you know, they don't know about it. They just keep going. Until they exhaust the budget. Right? So that's another one. The last, the the last one is that maybe it's, again, it's like, in some sense, these models, they do have an advantage. Okay. Maybe they have more advantage than than us. But one advantage is that they are trained on the entire literature from all the fields. And in many cases, some of the solutions reduces to apply some technique from one domain to your domain. But, of course, you need to know that technique. Okay? So LLMs probably are more better than most, to identify a technique from a different domain to apply to your domain. Okay? That's kind of what it is. While people, we specialize in one field. Right? And, here are some things we found, and you know, kind of evidence. Again, all the findings are very early. And you can see some of the techniques, which is the original domain it's in different, you know, political sciences, social social you know, social choice theory and electrical engineering. Okay. Electrical engineering is closer, but still. Okay. So next, I'm going to talk a little bit about, where we are and and try to I'm trying to be objective here. Okay? Maybe with our success. But, I think what we see is promising, but it's a lot of more work to be done. So this fall after the seminar in the summer, we we had this class in, at at Berkeley, and it was 33, mostly PhD students. And at the end of the class, actually, this Monday, we had a survey. And here are a few results. The first one, you know, did you get a gain here, right, for your problem? Everyone applying for their own problems. And, you can see, first is that significant gain over 5% improvement is 29%. Yes. Game, but more the marginal, it was 32%. So totally 61% pretty good. Now it might have performance. So in 61%, you can guide of better results. You know, as some definition of better. Are the primary bottlenecks? Well, here are the bottlenecks, and a lot of them, you can see context window limitations. Right? Because the context is growing if you add and more. But it's not only as that. The more is a the bigger the context, as you know, the less precise the LMM is. Right? It's not about 1,000,000 or 1,000,000,000 context window. You don't exhaust that. Right? And then, you have the round time failures. The incorrect evaluations, and evaluation latency. So the simulations becomes a bottleneck. Okay? And then inefficient search. Right? It takes the search, it takes too long. So So another question is that how much heavy lifting did you do? Ideally, you give the prompt, give the problem, go away, come back when you have a better result, good. So the fully auto autonomous was only in twelve percent of the cases. Occasional nudges, forty percent. And heavy supervision is 40%. And I think the students probably will tell you probably it's the heavy supervision. It's more than 40%. Right? But, again, this is and the the the the last one, which I think is a ultimate, you know, from my perspective, this is a real question. Okay. Are you going to use this in your work? Right? That's the ultimate question. Right? So 59 said yes. You know, and the rest maybe or no. Right? They were nice. The people you know, like, oh, maybe. You know, I'll come back in a few months if they're improved. Right? If they are improved, you know, maybe I'm going to use them. So, you know, these are the results. Of course, they are biased because these people are taking the class. So it's a little bit of some cost. You know? They know, they spend all this time in the class, you know, of all the pain. So has to work be worth something, so I'm going to use it. Right? Okay. So about research challenges, about the good evaluation, you evaluators are the key. They need to be fast, faithful, and correct. I think it's a lot of interesting research here. You may even build problem specific simulators. If especially if you have a big system. Right, like an operating system. What does it mean that? Right? Again, if you look only at the scheduling, at the disc and so forth, you can build simulators only for that, and you abstract away the rest of the system. Then, cascading evaluators. You can have different kind of evaluators. You can you can start from parametric models, which are very easy to check, but they are not very accurate. Although, to emulators and real systems. You can also leverage form a certification. Of course, one of the interesting direction is to go beyond system performance. Good evaluators will be the key. And also efficient search. Right? So I think it's so now we have, also announced it. Like, we have also a leaderboard for our problems here, which we published published today. And, we have a bunch of solution of of all these kind of frameworks I mentioned to you. We also developed as part of this class and effort, AutoEvolve, another framework, which is based on OpenEvolve, And, ideally, what it tries to do not ideally, is, like, at the high level, what it tries to do is to dynamically adjust always to have exploration versus exploitation. Right? So when you appear to get stuck, you want to increase exploration. You are going when you grow quick, when your performance grow quickly, you are good. You know, you want to exploit, to do more exploitation. One thing we do it, which seems to be very pretty good, is to have different prompts when you do exploration versus exploitation. And the other one is when the things when you generate code has an error, just ask to retry it. That's kind of pretty obvious thing. Right? Okay. And, it's working quite well, except one case. Is providing the best results. For all these. Nine out of 10 provides the best results. And, yeah, this is I got another one. Okay. So two other thing before finishing. So where where does it leave us? Right? Because it's a lot of question. Because the paper we had is very controversial. Title. On purpose. So, obviously, these are tools. Right? But you the way to think about them it's, again, so useful is an AI research assistant. It's exactly what it is. Which will elevate, hopefully, you So instead of spending time on developing solutions, spend time on selecting which problem to solve, formulating the problem carefully, and evaluation strategies. Right? And if this going is going to be very successful, the limitation will be our time to manage this AI assistance. Right? So it's a multiplier. Right? I think that's kind of exciting. So we can really accelerate the research. That's basically what it is. So one last thought, because this is what, at least personally, why I'm so excited about direction. Is because I today, building this software stack for AI it's very complex. More complex than ever. And why is that? For some very good reasons. First of all, is that, you know, we we build that very clearly distributed system since know, fifteen years ago. And then it was yes. Zero distributed system, but the world was much simpler. It was a homogeneous distributed system, have one server. The server has a CPU, hard disk drives, they're connected by Ethernet. That's it. Okay? And even the low workloads like we started with Spark and so forth, they are you know, bulk synchronous processing models, regular parallelism, you know, things like that. Today, you look at the infrastructure, it's far more complex. We have a myriad of accelerators, of course, Yeah. Oh, okay. Sorry. Yeah. Yeah. I'll I'll be I'll done. I'll say that. In the VLM, we follow how many accelerators type we have to we have. We see 400. Okay? And the networking, it's also very complicated now. And the application is very complicated now. And I don't know, like, and things are evolving much faster. You have a GPU every year. From NVIDIA. By the time we can actually our system to make it well, kind of well for the current generation, the new generation arrive. And you want to the performance is a is king, which means that you this modularity and so forth you know, that's kind of in theory. You kind of you do cross layer optimization. So for all these reasons, this stack is very complicated. You cannot keep up. Right? So now think about if you can't do it 10 times faster, 100 times faster? Now that will be a material change. Right? To advance, everyone. Okay? So, so in summary, again, system we believe that system is also changing the system research. And you cannot ignore it. There are some sort of solution. You can say that oh, for the majority of problems, it doesn't work. But for some problem, it does work. And you can expect that the set of problems is going to work. It's just going to grow. And there are many questions to remain. Remain to be addressed, so it's a lot of exciting exciting there are a of exciting research, problems. And I think at the end of the day, what you need to that might take away, it always, you know, it's like it's about you need to refocus. From really only solving the problem and iterating on the solutions to focusing which problems you are going to solve, and how you are going to evaluate it. You are going to demonstrate that is the you you solve the problem successfully. Okay? And fundamental, it's again, it's like the world is changing so fast. So we have to look at this as a way to help us to keep up with these changes because otherwise, there is no way we can keep up. Thank you. Alright. I think we have time for a few questions. And, kinda debaters come to us first too? Okay. Hi. Jan. Good to see you again. I'm Thomas Chirsky, one of the coauthors My question is, much of the systems research assumes sort of a fixed goal point. And then we optimize systems around maybe a benchmark or, you know, you mentioned, one particular benchmark we sort of overfit to that in the real world, we have more dynamic systems where that goal point is constantly evolving. How do you think, you know, methodologies like evolutionary methods will need to be improved or fixed in order to move with that goal point? For example, perhaps there's lessons from your your experience at Databricks or any scale where you have constantly evolving systems that need to be constantly tweaked. Yeah. I I I think that's exactly the problem The the problem is the the systems everywhere infrastructure as the application becoming more complex, they are changing continuously. So what you are going to do about the systems and the stack in between Because like you said, this was the assumption. The assumption is that know, it's pretty fixed. The hardware is pretty fixed. You know, it's x 86 servers and so forth. The application is pretty fixed. Web app web application, and you only have a LAMP stack. And that's pretty much it for the next fifteen years. This is gone. Right? So the the question is that, yeah, it's exactly how fast you can but fundamentally, you still have to assume when you develop something, you need to assume something about the boundaries. Right? There is fundamentally, you have to do it. Because otherwise, how you test it? How you make sure that you are going to work with that the other thing the other side. Right? So but if you are going so so say, you know, if you can if you can build a system, right, in one day, Right? As complex as, you know, original Mammoth or Spanish or something like that. Things are going to be very different. If your workload changes tomorrow, it's okay. I'm going to build another system, or I'm going to evolve the system. So that's kind of the thing. It's about the speed of the of the speed. Can you develop faster than the speed of changes? And now you cannot. It's very hard. Yeah. Hi. I'm Reika Singhal from Tata Consultancy Services, industry side side. So my question is, we look at system as a source code primarily here. What how about the deployment architecture, especially when we look at edge cloud deployment and then generating the source code for the whole system, and it becomes, you know, chicken and egg problem, but because first you have a code for the, architecture which you have in mind from a solution architect or you develop the code and then see how do you want to deploy it efficiently, like algorithm. So how do you solve it? Is it a joint optimization problem or so? Yeah. It has to be joint optimization problem. But what I want to say is that fundamentally, because you have to test this model, Actually, the testing will be even more and more important. Right? You need to have some specification specification testing for these problems. Right? So the testing is going to be harder and harder, and I don't think it's I I don't think this is I'm not advocating here for a more ad hoc world. What I'm advocating here, a more hopefully principled world. Right, which should enable to define what you want to do more precisely and there we need to spend a lot of time. And doing it, that should be fast. Right? But if you think about that, it has to be just testing and so forth, the faster you move, the more automated it has to be, the better it has to be. Okay. This is the last question. One more question. Hi, professor. Thank you very much for the talk. Do you think these tools actually can expand who can become a researcher? Because maybe you have people that are because of their domain knowledge or other skills, are good at formuling problems or evaluation. But may not be necessary good problem solvers. Right? I think that TELUS will expand incrementally. To get more and more but for researchers, I mean, it's like who is you still need probably at least a way I'm thinking about this today, still need to, at some level, you need to specify pretty clear what needs to be done. Right? I think that what will happen is probably you're going to go to to to do that at a higher level of obstruction and these things to do more together. Right? And, you know, you can do whatever. But eventually, you have to trust Right? This is this is in some sense is kind of boggles my mind. It's like because this is the biggest problem. The biggest problem is reliability. Right? It's like with AI. That's where you should spend the time. Right? Because if you think about and I'm going to end up with this with this rant, so to speak. You think about software engineering, how many of you here are software engineers? Okay. Where do you spend that time as a software engineer? Right? Do you spend time for building prototypes and feature, you know, demonstrating the feature? No. But you spend most of the time is taking the features and productizing them. Right? This is what you do. Right? Of course you might do observe, but this is what you do. Most of your time. At least based on what I know. So but that what it is, is making these features working reliable in all these use cases. So that's capital with the software engineering. Right? The same has to happen here, right, is the reliability, predictability. Are key. To trust these tools. To trust your application. That's where you spend the time. Yeah. Thank you. Is Dylan here? Oh, okay. Oh, wait. Maybe you switched sides. So you are together. Okay. Because it's like, yes and no. There. Who is no? You're slight. Yeah. Think you know. Right? Yeah. Are you not Yeah. He's He's no. And, know? Yeah. Right. We're going to be hello? We're going to be starting our debate It's a very exciting topic. Be asking this the panelist here the following question. Will AI agents replace systems developers? Now we'll have two teams. Whose positions are balanced. First, we'll have the proposition, meaning that they think that systems developers will be replaced by AI agents. So that's a replacement camp. First, I'll introduce Jason Jason's a research scientist at meta.ai. And a technical lead PyTorch Compilers. He started the Torch Dynamo and Torch in doctor projects. Which brings flexible graph capture and a high performance compiler to PyTorch two. He received his PhD from MIT and has over fifteen years of experience in machine learning, compilers, programming languages. Next, we have next, we have Dylan Patel. The founder, CEO, and chief analyst of SimeAnalysis. The preeminent authority on all things AI and semiconductors. So do ask questions. Do you want me to continue the introductions? You can check out his Wikipedia page. Next, we have the opposition. Meaning that they do not think that AI agents will replace system developers. First, we have our speaker, Niraja. She's an assistant professor in the department of ECE, IUT Austin. She's a cloud computing system researcher with a strong background in machine learning. Her work straddle the boundaries of systems and ML. Specifically advances in systems, machine learning, and hardware architecture that are about to launch a new era. In which we can use the entire cloud as a computer. On the other hand, new ML architectures are being developed for solving complex management problems in systems. Similarly, system research is being, is getting influenced by properties of an emerging ML. So we have seen from her talk that she's thinking a lot about how machine learning for systems research can really shape the system's ecosystem. Next, we have our other speaker, Hanssen Wang. Who's a research engineer at OpenAI. He focuses on the codex models. Integrated into Chatter BT. With codex, users can delegate coding tasks to parallel agents working autonomously in the cloud to analyze the codebase and generate poll requests Hanson worked on training the first Kotex one model launched in May and has been continuously continuously iterating on the model since then. Prior to joining OpenAI, he cofounded a start up building AI analyst agents and worked on ML infrastructure and meta. Where he actually was a workshop attendee, apparently, six years ago. So welcome back. So, before we dig begin the debate, we want to define the terms a little bit better. By systems developers, we mean that the people who build and operate the foundational layers of computing. That may include not just the general business logic programming, but also compilers, kernels, infrastructure, hardware, software, codesign, performance engineering, large scale ML system. Basically, all the things that you know, all of us do today. By replace, we also do not mean assist, accelerate, or collaborate. We mean that the primary design implementation, and debugging work will be done autonomously by agents with humans no longer in the critical path. So if that changes your position, it's time to Yeah. So each side will present two speakers, followed by some cross examination and audience q and a. You are if you already have questions, you could ask the questions here. By scanning the QR code, and this will be a lot easier for us. A quick poll here Who here already uses agents? Weekly? We need a photo for this. Okay. And who believes replacement is inevitable? That's a smaller number. So keep these prompts in mind, and we'll be soliciting questions from you shortly. First, I'll begin with opening statements. For each of you to explain your position. In generality. So about, like, two minutes each. Do you want this you want Test test. Oh, there there we go. Alright. Start with the opening statement. So agentic AI is a tidal wave that is crashing over the entire, our entire industry. Anyone who doesn't see this is in denial, obviously, or hasn't attended this conference so far, as we're already starting to see widespread adoption of AI in nearly every area. If you were to broaden this debate topic to will a gigantic AI systems replace developers, not just systems developers, I think you could make a strong argument in the affirmative. But this debate is about systems development. So the key question is, in this tidal wave that's coming, is systems development high ground or low ground? They say, are we gonna be affected more or less than than other areas in the in the system? Unfortunately, I think we're in low ground as system developers. Systems development is high complexity, don't think this is gonna save us. AI has proven very good at handling high complexity problems, because it has a longer context window than humans. But the real killer here is training data. Training data is the fuel that that helps AI improve and is currently the main bottleneck getting better models. And for training data, systems has more training data than most other areas. System software is widely open sourced. It's open sourced a lot more than product code. And, there's a lot of problems in systems such as performance, you can easily generate infinite training data to generate these recursive self improvement loops which can very easily cause AI to rapidly improve for specific sub problems. Moreover, I think there's evidence that AI is already replacing system developers. I'll remind you that this debate prompt doesn't say all developers. It just requires some developers to be replaced. And and as as a system developer myself, I've already been replaced. AI agents have completely changed my workflow in the past six months. About how I write code. I no longer write code directly. Often, I don't even open my IDE. I just have a codecs and claud prompt open, and I'm just prompting the models, to to write code on my behalf. And I'm reviewing and then giving feed feedback. So basically, I'm not a system developer anymore. I'm just a supervisor of AI agents who are now the systems developers. And in the coming years, I think AI is gonna improve dramatically. And more more importantly, we're we're gonna learn how to use it more effectively. I think that often I I think that the models have gone so good so fast that I think now I think one of the biggest bottlenecks is how effectively able to use these models. And I think we're gonna learn if the model stopped improving, I think we're we're gonna learn to use a much better and better over time. And, we're we're we're still in the really early days. So this this tidal wave sort of crashing across the industry just began to touch the shore. I think we have some really exciting times ahead. Thank you, Jason. Piggybacking on Jason, I don't I don't have a laptop to read from, but the the important thing about is that it is a lower dimensionality problem than general developers. Right? General developers can be creating anything. There's creativity around this. In in systems, there there's much less creativity. The define there's a much smaller defined space of possible you know, selections. For example, kernel generation. One of the things that is very close to being solved at seems, is is is a much smaller space to explore. Architecture search for sys for chips has been happening for years now. Right? These are things that even before LLMs were vogue, people were able to do architecture search for language, for for chips as well as, AI architectures. The the other aspect of this that's very important is in areas of reinforcement learning, we've we've we've had massive massive breakthroughs in using verifiable verifiable compute right. If anything is verifiable, then you can you can train on it. And chips, systems, these things operate at clock rates of gigahertz. Not not hertz like human systems. Right? And so the feedback loop here is so fast that you can generate significantly more data than any other you know, domain. And so system developers will be trained at a at a extreme speed. The the data to train these sis these models be generated at such high speeds that you you can't replace system developers quite rapidly. And and and when you talk about compilers or kernel generation, these these are areas where problem is actually simple enough. Right? The hardware has already been laid out. You're you're you've had it for a while. You you you can. Solve this problem. It is something solvable. And and as we've seen, AI has solved the games that are solvable. It it it can't maybe solve extremely high dimensionality problems like general developers, but but systems developers, it definitely can. Cool. Thank you. Now we're gonna move on to the nos. We we don't have to attack each other yet. Cool. So first of all, views are my own. Not my employers. But, I'm I'm I'm definitely on the pro acceleration camp. But I am I I think I disagree with the framing of the the term replacement, especially. I do believe that what will happen is that I think as Jan said in the previous talk, the level of abstraction kind of moves higher I think I think coding as kind of like a a sub portion of the loop that developers actually, kinda participate in practice of, like, taking requirements from the real world, encoding them into sort of a formalized spec. And then the last part, which coding, which is kind of like translating those specs into machine interpretable and machine executable code. I do believe that part of the loop will slowly become more and more automated, if not completely automated. But basically, like, somebody has to tell the machines what to do. And so rather than perhaps, like, maybe maybe the maybe the term system developer will evolve into more of something more of like a system architect. When I think of, like, designing distributed systems, every distributed system is built to accomplish a real world need or to in service of, like, a real task that needs to be automated in the real world. And so somebody needs to be gathering requirements for that, encoding that maybe in, like, a higher level natural language spec, but then you know, maybe the process of translating that into machine code is not is no longer something we have to do. Thank you. Okay. Yeah. I'm also on the no. Side. Thank you, Mimi, for clarifying the definition of what you meant, and that is that made it very, very clear. I stand my ground. I Even in my talk, I talked about there are implications of using all these agents and who's gonna solve that. Right? If agents are going to be everywhere, there are actually new problems they're introducing much as they're solving some of the existing problems. I think everything basically boils down to the fact that are trained models depending on training data. And so they are bound to the training data distribution. If you're telling me you can actually absolutely represent everything in training data, I might think about switching. Right? But as long as that is really infeasible, I do not believe that we can we can replace ourselves with these agents. I believe, what this is changing, it it's reshaping what we, as system developers, designers, are supposed to be doing. We are no longer required to sort of do the lower level task. And definitely, we can use these agents, but these agents are still assistants. We have to be acting as orchestrators. The important thing is asking the right question. After that, like what Ian talked about. Right? Then that loop perhaps can be replaced. But asking the right questions has always been, at least in systems, it has always been least 50% of the contribution. Right? Asking the right questions, and that has not changed. At all. So, yeah. I would stick to no. Interesting. So we are already hitting some disagreements. For instance, do we have enough data? Is this problem difficult or easy compared to general machine learning? Problems that we're solving today? So that's really interesting. Next, I wanna ask, what are the agents surprisingly at right now? Just so that we can build some common ground. So I so I I've been constantly surprised so so one sort of anecdote is I I work on PyTorch and that or the compiler for PyTorch. And as part of that, we have an an on call. Where, you know, basically, the on call is responsible for dealing with the issues that many of the people in this room and across the industry open on the on on GitHub, PyTorch, PyTorch. And normally what an on call would do sort of a year ago would they they sort of be play a router, right, type role. Like, some issues would be low priority, and so we so but other many other issues, we'd sort of hand off to other people. So I I might take you know, get 20 issues, and I might hand them to a bunch of humans. And my last on call, which was which was a month and a half ago, rather than sort of handing all the issues off, to humans, I, just copied and pasted the URL of the issue into codecs, and said, fix this issue with no guidance, no just just open ended, just fix it. And about 80% of the issues, there was, like, 20 or 30 issues. Kodak solved the solved the issue on on its own sort of without any help from me. It And now a lot of those those solutions were more complicated than they should have be and and and a a few of them were had some some bugs in them. But, you know, an 80% success rate is is really good. And and I was honestly, like, I saw that, and I'm like, oh, wow. Like, like you this is way more efficient because it would it actually like, the time it would take me to to hand these issues off to junior engineers, do code reviews, you'll Codecs was was it was easier to use and because it was so fast, because the turnaround time was so quickly, it actually, you know, saved me time versus sort of sort of routing the issues to other people. And I I I think, like like, you know, the other debaters mentioned, a few times about how we're basically just gonna be moving up the stack. And where, you know, we're gonna be rather than writing code directly, we're gonna be acting sort of as as architects and supervisor. And the but the the the thing I think that I completely agree with that. But I I think that the the key thing here is that we're gonna get this this dichotomy where senior engineers who are really good at supervising, really good at code reviewing, are all of a sudden gonna get way more productive. Like their productivity is gonna gonna gonna dramatically improve. But for junior engineers who are sort of just coming out of college, really the bottom few rungs of the ladder getting cut off. And it's it's it's basically, you know, if I if codecs is better at solving these types of issues, than a new college grad, why would I ever hire a new college grad? Like, there's there this is a huge huge problem. And but the key thing is, there's a lot more junior developers than there are senior developers. So, what this means is that for sort of the the the the skilled and talented know, they're gonna get way more productive. Their value that they're providing is gonna is gonna skyrocket. Then there's gonna be be sort of a much wider audience of people who are struggling to find work. And if you go talk to a senior in college, senior in college is probably having a pretty hard time finding a job right now. Because of this. And so I think on net, this means that a large number of systems developers are gonna be replaced, while a small fraction of them are going to, become a lot more productive. Adding to this, as you both astutely framed, right, system developers will be replaced by system architects or system orchestrators, so system developers will be replaced. Right? As we all agree. Definition. And and and so I think what's important to recognize here is no one writes assembly today. Right? When when was the last time anyone wrote assembly? Hey, for There's some of people here. For for for for, you know to be a bit more sensitive. For, you know, NVIDIA GPUs, I think there's only one man on earth who writes, directly in SAS that's Gray. Everyone else writes. At some higher level. And as we look at where AI has been extremely powerful, is on kernel generation. Right? Now a lot of times there is aural hacking and people's you know, or or or or reward hacking kernel bench currently. And and claiming that they're generating better kernels, but there's clear progress here. And it's not just on NVIDIA chips, it's on other other firms' chips where actually generating kernels for them is is much easier. And so just as people who wrote assembly were replaced in large droves, just like people who are printing out, and doing punch cards for programming IBMs and, like, the fifties were replaced. System developers are also being replaced. In the areas of kernel generation, kernel writing will not be a job in the in the near term in in the medium term future. Is being replaced, maybe followed by some orchestrator or some architect. But but those aren't system developers. Yeah. Well well, her definition remember Mimi's definition of system developers. Right? Just for me. But, what I want to say is, I'm not honestly, I'm not arguing against. Like, I'll take an, position of an academic. Right? In classes, for instance, you know, it's been a big problem to sort of tell what, use AI, not to use AI, how to use AI. Obviously, if you don't use, you're gonna be like living under a rock. You don't want to be doing that. The problem here is, like, how many of you actually think you can directly land a job where you can be the orchestrators? Right? You only do that after having done the actual job of you know, going through that experience and then landing somewhere where you actually get the understanding of okay, now I'm the sort of whatever principal engineer whatever, whoever is designing. Who is handing over. Here is a design document. To a junior. Right? I agree that juniors are getting replaced, but the problem with that is, and and this is like, I'm agreeing with that because all this dynamic shift that is happening is reflected in our our classrooms. So literally, we did a poll around, like, what classes are getting populated. And all of a sudden, the, you know, soft development engineering kind of classes or, data science classes. Like, you know, they're empty. Of a sudden. And everything else, like, hardware classes are getting more and more populated. So that is the the impact. But more and more, I see students using AI tools, and not using them in kind of responsible manner, I'm worried that they're actually, learning not to think. So when the court stops working, they cannot actually explain that. Right? So it it seems like we're heading towards if we replace, we're heading towards a place where we are getting biased. Like, the model feeding its its own output data to itself will create a sample bias because it you know, creates an illusion. That's what the world looks like. And it only being is able to predict that. What happens when things change? How do we you know, actually be ready for the real world? It feels like we are, as far a lot of time debating the definition of re replacement replacement rather. We only have two minutes. But I will say, it is hard to predict I think, how the roles will change. So, like, to the point about you know, assembly and lower level languages, yes, those went away. But as we got the higher level languages, we just could do so much more with software that amount of software developers 10 x, if not a 100 x. And it could be something similar here where, you know, like, the fact that I have, codecs and all these, like, different tools means that know, if I wanna create something, nothing stops me anymore from just being able to of, like, prompt it into existence. And that's tremendously powerful as as well. So, like, you know, maybe it's not, there there are probably right. I think, like, with with kernel engineers, like, kernel programming, like, I think that's probably something we just won't have to do anymore, but maybe that opens up the the the room to to do more creative things. I see. So so far, we're getting, the junior developers' everyday tasks of programming seems to be on the way of being replaced. Right? Is that the consensus we have here? Okay. And colonel Raiching, perhaps by being such a difficult but needed thing, to be done. Also, it's getting more and more automated. It doesn't seem like we're really touching on the entirety of the system stack, but I like that we're you know, beginning and yeah, beginning to touch on something. So, you know, if we're really talking about the definition of again, we do include the design component. That we think is an important part of system developers. And, you know, people can't disagree. Maybe, you know, I don't think about design anymore. I do think that's part of the And another part is to become a software architect, to be someone who's maybe prompting the model even, explaining what's wrong, or even labeling the data you need to have become someone with that experience. And if that experience is the way of thinking and learning, goes away, what does that mean for us? Just like a small addendum. To think about. And next for our question, what is the one concrete systems hack? That agents are fully owned within the the next twelve month that we probably don't think about much now? Sorry. Could you clarify what what's meant by hacks? So we talked about the kernel writing some compiler tests. Also, the junior developer. Type of task. But what is some other system developer work? You think will be replaceable within the next twelve month? I think almost everything. Like, I I know, when when I oh, if I need to do anything coding related, I'd no longer open my IDE. I I open, a my AI agent's console. And so I I can't think of a task where the first thing I'd open would be Versus Code anymore. And that wasn't true three months ago. That this is, like, or or or six months ago. I I think that there there's definitely, like like, there's a boundary here where, like, a lot of the design work is still sort of needs to be done by humans. But I think as AI models improve like, I look at the types of prompts that I'm giving AI I think the types of prompts that I'm giving AI are totally automatable. Like, I'm I I tell it basically every single time is too complicated. Please simplify it. I I I I I literally have, like, a a copy and paste like, simplify this prompt that I basically just just I I put in without even reading the output of the model. On many times. And and so, I think that that the boundary of sort of sort of how much supervision I need to provide it think it's gonna need less and less supervision over time. And I I don't see an odd like, roadblock that'll stop AI from advancing and stop sort of And and so I think I think it's it's it's gonna be pretty widespread. I think sticking to the point of a task that will be automated in the next twelve months, many many will you know, longer timelines will take take a bit more for system developer agents to replace, but but kernel generation one of those which which will be replaced you know, within the next year. Right? The the amount of, work out there to, you know, for lowering, any any any code. Right? It's not it's not an orchestrator. It's not a system orchestrator. Or system architect even. Right? It's researchers will literally just call Helion in PyTorch, and then PyTorch will lower through whatever, set of stacks, whether it's some know, NVIDIA closed source thing or some other open source kernel lowering library. And instead of having to write custom kernels in certain places to get peak performance, in fact, the performance will be roof lined right out of the gate. Because kernel generation will happen automatically, right, with with system agents. This this is an area where it's it's quite clear that this is exactly where you're going. And and you know, what what where where researchers won't need to interact with their system developer to get peak performance, to get, for with efficient kernels. Right? They they they just don't need to talk to you. Right? This is this is there's no, you know, senior system developer in the loop even. And so this is a a a wholesale case of system I think one area I'm looking forward to I think I think we're not quite there but, you know, one area that I'm looking forward to is like the act of cleaning up or refactoring code think it's, like, a very unglamorous part of a job. We kind of joke sometimes that, like, we're accumulating so much tech debt internally that you know, we're hoping that AGI arrives so that we can clean up the tech debt before we have to pay it off ourselves. But I think yeah. No. Real realistically, I think agents are getting to the point where they can be the, you know, like, the the payments against the tech debt perhaps and then think that's necessarily replacing anyone's job because they're nobody whose, like, full time job it is to hopefully. But, but it is something that we can hopefully offload to the machines. You know what I would love I would love to love an agent to just take our prototype and convert that into a production level code. Right? Be able to, reproduce results from all these research papers that you know, we feel it's hard and make that easy. I don't know how how feasible that is with what Ian talked about. But that is one area that at least research, academic research or conferences have struggled with. That's why there is a lot of effort on artifact evaluation, reproducibility, and so on and so forth, but we haven't made it. And I think that's one area I would love for agents to take over. Mhmm. Relating to that, can you name one production critical system for those of you working in the industry? Where humans are already optional. You mentioned the on call part, Jason. You want to add to that? Or any one of you? Yeah. I I mean, I I think there's definitely, lots of things we mentioned are areas where where humans are are optional. I mean, I I think it's I think it's the case where, you know, we have policies around around that that that even when even when tasks are written entirely by AI, and actually within meta, a significant fraction of of PRs are are a 100% authored by p by by AI. Mostly refactoring type things, sort of mechanical refactoring, changes are fully automated. But I I I think I think, like, one one trend that I'm I'm I'm looking forward to in the future is, you know, right now, like, and especially when I'm building systems, like, one thing I'm often thinking about is, like, okay. Like, how do I make this the system perfect? How do I make it beautiful? But if we drive the cost of building systems down, to close to zero because it's automated, we could have disposable systems. Like, rather than sort of build building the perfect system that needs to work for every problem, we can build a one off system that's that's that's independent of every single problem and just worry less about getting every detail right because if it's wrong, just like prompt the AI to create a new system, then you have a new system later that day. I think one area where we're actually like getting to a point where maybe like humans aren't needed is kinda like the review flow. So I think, like, internally, like, you know, all PRs are reviewed by by codex nowadays at OpenAI. It's getting to the point where it's, like, really almost superhuman, and, like, it can catch things that even, like, a human human reviewer wouldn't catch. So we've you know, joked around about, you know, like, what if Codecs could approve your PRs and also, like, reject your PRs. But I think it's, like, not that far far away to imagine a world where, like, you know, actually, you know, 90% of PRs just go through automated review, and maybe there's only five or 10% where they the humans actually need to be reviewing things. Yeah. Okay. So, with that in mind, we welcome a lot of audience questions, and we'll be looking through them from here. We'll get to the part where people may disagree. So throughout this, workshop, we've heard from Azilia, Yan, and many people that there's a lot of bottlenecks potentially. But they all take somewhat hopeful position. They are perhaps solvable. And that includes text time scaling for Azalea, and Yang talking about e vals, inefficient search, things that need human intervention. Which of your list of bottlenecks you think could be fundamentally would be fundamentally unsolvable coming from the opposition camp? By scale plus data plus maybe abandoning the system and starting a whole new ones. Yeah. I think I think it's to me, it's like this fundamental, like, outer loop of software development where you're kind of like, gathering requirements about the world, whether that's, like, coming from you know, people using products or know, systems breaking down due to, like, unexpected things happening in the world where, like, that's not necessarily predictable. If all you observe is, like, purely within the system, if if that makes sense. So there's this, like, whole outer loop of, like, gathering information from the world, potentially, like, working with other humans to, like, communicate, like, what needs to be done. Feel like that is still, like, a a long, long way from from being solved. Yeah. I I see many things, but I would just say, I would also don't get me wrong. I would love to have these agents just do all the things, and I can relax. Right? But, first, they need to be verified. That's my biggest thing. And I don't know how much it definitely reduces the work you need to do, but that verification of the of the result that you get you know, whether that is correct and whether it is correct longer term, is one of the bottlenecks I see as the major bottlenecks at least at this point. I have responses to that? Great. The the nice thing about, the current paths of scaling that we have ahead of us is that you can do functional proofs, functional verification both during training time to generate data, as well as during test time to verify your answer. And ensure that you're whether it's your kernel is accurate numerically, some startups haven't been doing that recently, but in general, this is something you can do. Or, you know, if you want to ensure that know, your model does know how to, say, search different types of architectures that you wanna implement, these are these are things that you can functionally verify both at training time and inference time. Right? So it is not something that is it needs you know, the the training data doesn't exist at all. Right? You can generate it, and you can prove it out functionally. At test time because most system developer tasks do not have a, you know, creativity aspect of it. There are there are multiple engineering trade offs, and you can weigh them. And and you have a solution. So so this is something that, you know, system developers aren't really needed for. Yeah. I I I mean, I I think AI is, often better at code review and finding bugs than a lot of a lot of people. Like, I I definitely feel like there's there's a there's a it's definitely a a case where where finding bugs is hard, but but, I don't think humans are particularly good at that either. What about solving bugs? Oh, I I mean like I I've had, like, really tricky memory leaks that I feel like would have taken me days to hunt down. I just throw out the AI and it it solves it for me really quickly. I it's amazing that type of problem. Yeah. But is it a skill issue? Is it like a AI issue? Maybe. Maybe. I I mean, it's it's, it's less work. I I feel like I need to explain my position. What I mean is like, what do you think is truly fundamental? Right? Mhmm. We can see some bottlenecks right now, but are they growing pains, or are there something truly fundamental? We mentioned something like interacting with human, gathering information, things breaking down that there might not be data about, how to resolve that kind of breakdown? Right? For these problems, maybe we won't have them anymore, or maybe we'll yeah, any any sort of, thoughts on that? Specifically, how do agents resolve bugs when the spec itself wrong? And when the person, you know, interacting with the agents, if there's a person right now, does not quite understand. Yeah. I do I do think we're reaching a point where basically it's like if you can describe what you want in natural language, we have these magical processing machines that basically turn one form of language into let's say, like, machine language. But, yeah, it's like, I think, yeah, to your point, it's like what if the spec itself is wrong or, even more challenging is, like, when the spec itself there, like, is no clear spec if it's, like, a you know, like, if it's a decision that has to it's like a trade off between multiple you know, parties with conflicting interests and then, you know, like, you you have one form of the spec, which you know, is advantage advantageous to some people, but not others. Then you have these very moral, like, maybe even, like, moral gray areas that kind of require humans at least for now. So all the things and correct me if I'm wrong, we have we have talked about our correctness kind of bugs. Right? There are also performance bugs, particularly talking about, like, cloud level programming. Let's say a microservice based application which has I don't know, thousands of nodes. When something goes wrong, it's very hard to actually pinpoint where it is. And we have a lot of machine learning based solutions. But they may or may not actually provide you with the right solution. Because of, you know, that stochasticity or nondeterminism doesn't actually bring it home. And that requires creativity because things go beyond what is seen and sort of somewhere a back pressure was built that sort of showed up here, at the end of the line. May or may not be easily feasible for an agent to track because it's hard for human beings too. Right? But creativity can take us there. Do you agree? So one minute on this. Yeah. So it's all I'll I'll throw a bone to the the other side. And give it sort of a a point, against, and so so so I was doing one example. Where I was asking AI to optimize a kernel, and the AI just removed all the memory fences. And removing all the memory fences, made it run faster, but it also introduced the data race that that just didn't manifest very often. So all the tests passed. And that was sort of a subtle correctness issue where, you know, I looked at that that that that change, and I'm like, this is this is clearly this is pretty sus. Now I I agree to it. I think it's still early. I think that there's there's, you know, technical solutions to that. But that's definitely the type of thing where, you know, you when you're using these types like, potential solutions, having, you know, adversarial AIs whose job it is to find the bugs in the optimization AIs is is is another approach. But but, you know, these are these are challenges that I think are are surmountable despite you know, that that being a very scary type of of optimization. Don't delete your memory fences. Yeah. Is really interesting, and I relate to what I was about to ask. Hasan hinted at moral questions. In deciding whether or not to take agent solutions. And Jason mentioned, what if the agent's too good at getting the fastest solution? Given an incorrect spec. So what are the, you know, risk, economics, accountability, issues that you see? Within the of the questions we're seeding is that if an agent is 10 x cheaper and two x slow, but correct, 98% of the time do I deploy it. Who is legally accountable when agents design unsafe infrastructure? And, like, is there gonna be a type of job where it's just debugging AI outputs when things go astray, and we just don't call them developers. Where do you see we go in the future? It's it's certainly true that the, catastrophic loss potential when you replace all system developers, which will happen, is is very high. You know, the these systems can have a catastrophic bug, which which may even be intentionally placed there, like a backdoor, or may not intentionally be placed there that that can be exploited or can cause, you know, the loss of a lot of performance or even worse, you know, say a system for, like, farming, you know, famine Right? These are these are certainly risks. And as junior system developers are replaced, which they are already being replaced, The the the skill sets to debug will go away, but that that creates a new field, a new job. Right? Maybe not a system orchestrator system architect, but maybe a system bug reviewer. Right? But not a system developer. And so so certainly these sorts of jobs will have to come. But they will be far fewer between when when system developer agents can develop and debug constantly, you know, in in in in Jason's specific example, Right? The the silly thing here is that system you know, these these current code agents are incredible at writing test cases. You just didn't put in your prompt. Right? So So, like, you know, that that's important to, record I mean, yeah. I do think there's an analogy to, like, more, like, traditional forms of engineering. I think software engineering is one of the types of engineering that's, like, almost fairly engineering and that there's, like, actually very few like, accountability kind of, like, guardrails for that field. But if you're if you think about, like, more traditional, you know, like, civil engineering or things like this where it's like sure, like, the design the process of, like, making the designs is, like, all done by software nowadays. Still, you know, like, the civil engineer is, like, accountable for, the safety of the the final building. So I think it it does kind of, like, feel like that appropriate. Analogy. Yeah. Yeah. Releasing to, like, risk and reward the audience questions. Largely, it just include a lot of different issues that we have yet to touch on. Will just do a brief summary. We have extensive extendability or maintainability of AI generated codebase. If so much of the code is actually AI generated? How do we even read and maintain that going forward? People are a little bit upset that GPU kernel generation is considered nearly solved. Human labor is a big part of r and d cost, and it might just be for profit that they're reducing it rather than being you know, meaningfully reduced. Are we confident on the accuracy, etcetera, and you know, current capability is not that good. Please stop exaggerating it. So just to give you some example on the very opinions that we have. Niraj, your turn. What what's ultimately the question? There's somebody on risk accountability. Well, I I want accountability. Right, if you take that's my personal take on it. And that's why I I kept saying agents should be really assistants. Whoever used it. I can go find that person, right, when things go wrong. Right? So there has to be accountability. Otherwise, yeah, it's like you said, we can actually, I feel, be led into this undebugable mess that only agents can understand, and it's now we have stopped thinking. We have stopped learning how to think and how to debug. So we are again at the mercy of these agents again to debug their own undiviable mess. That's how I honestly feel. So that was accountability. What because you've there are too many questions. Can you be sorry? I feel like the question we should be focusing on is, like, will the current bottleneck that is, extensibility and maintainability in this, like, risk landscape. You know, is this be a long term bottleneck to replacing system developers? For the sake of sticking to the question, I think the implication is it actually makes the system developers potentially more valuable. Right? People with experience, people who can go into the weeds. I somebody else wants to well, I don't even believe. Yeah. Any opposition to that? Yeah. I don't even believe that. Yeah. Like, so I so I think that definitely, like, if I look at what was say, what's the biggest weakness of AI coding tools today? They overcomplicate their solutions. They write x too much code than than they should. And so you know, what what do I do to for that? Like, I I ask the AI agents to simplify their own code. Some Sometimes, I'll ask Claude to simplify Codex's code and and Codex to simplify Claude's code. And I I I and there's been PRs that were so complicated. I went through, like, five to 10 iterations of just, like, simplify this, simplify this, simplify this. And then, like, by hand, the thing that I'm doing is I'm giving it, like, specific guidance of, like, simplify like like, this is unnecessary. Find a way not to do that. Like and and, like, I that that's, like, the main feedback I'm giving it. It's not like, get the thing right. It's it's it's do it in a simpler, more elegant, more maintainable way. And, you know, I I I think that this is something that that AI agents are gonna improve Like, I I don't think there there's some fundamental reason why you know, we can't train AI agents to write simpler, more more attainable code. Especially when people like me are generating so much training data for other folks on this panel to use. To to try to get do you use my data? Never mind. Never mind. The so so I I think I think that's something that will improve over time, and it is definitely one of the the biggest challenges. The other the other thing is just sort of like learning to let go. And that's one thing I've I've increasingly done more and more In that okay. If if the only one who's gonna need to read and maintain this code is other AI agents, How important is it that it that it actually be as simple and elegant as possible? Like, sometimes, like, I'm pushing to, like like, turn that 30 line solution and the 25 cell line solution But, you know, if if at at some point, like, if if it's if it's if maybe that's not even necessary. Maybe I'm wasting my time, and I should just say, the 30 line solution is fine. I'm I'm not totally sure I buy that argument, but I I could definitely see sort of see a world where, like, if we're entirely replaced, maybe the best solution to a little bit too much complexity is even more AI. Feeding to the point of the audience, which is, you know, current capabilities aren't that good. Like, calm down. You know, the the the important thing to recognize is today, AI agents are really, really good for MVPs. Not necessarily that great for going across your entire code base once it's too large. Right? Once it's spilling out of context windows, once extremely large, you you just it just can't make sense of it all. Right? It'll you can get it caught in loops doing the same thing over and over again. But the simple thing there is, like, look at how much research there is on long context here. Both on the data side as well as on the modeling side. Models are just limited in capabilities today. Know, over the last two years, we've had context sense go from, like, four k to a million. Or 2,000,000 and and and they gonna do multiple orders of magnitude increases again, probably. Right? And so a lot of these issues will be solved quite simply by just more scaling. Of of the models on context length on data on that that side. Right? So so the issues of hey. AI agents are only good for MVPs, but not for actually dealing with a lot large messy code base. When you wanna, say, refactor something. That that that's stuff that probably is just an artifact of current capabilities, not not indicative of future ones. Mhmm. So I have a minute to respond to that? And yeah, yeah. I just wanna add. I think, like, it's sometimes we take for granted, like, how quickly these things have become available. Like, just I think it's just really been this year that coding agents have actually gotten very good. And so there's, like, a there's, overhang between, like, raw capabilities that have become available to us versus, like, how good we are at using them and also, like, the tools that we have. And we're I think we're, like, even, at OpenAI, like, we're not quite sure how to do this. Like, you can today, like, all of us, we can, like, deploy an agent on, like, every file in the code base. And, you know, tell it to make an improvement. But, like, what are you gonna do with all those PRs? Right? So I think we're we're there there's still a lot of room to, like, figure out how to best make use of these. Agents. Thank you. And on that, we wanna move on to predictions, and it also relates to the audience question. Can you tie an economic value indicator to the replacement assistant developers? Now to make a concrete on where do we actually agree and disagree on, for example, what would be your adoption rate for coding agents prediction in the next year versus five year? How many roles will be impacted? What's being automated? And for the once against, the replacement theories, what will still exist? Resist, automation? What are the failure modes that we'll see that keep going? Are we specifically talking about code? We're talking about system developers, like predictions. If we don't think it's going to be replaced with agents. Yeah. If if we're talking about system developers, meaning the coders, programmers, a lot of it will be replaced. What I don't actually believe is that would be without any supervision. That I don't believe, or at least I don't want to us to get in that mess. And so this is actually pushing us in lot of other directions. How do I now get verifications? So, you know, formal verification is by far far away from actually doing this, but that would be something really fantastic to get there so that it can verify for us. And then perhaps I can think about replacement as such. But, until then, this this verification and actually, what we're doing is right, being really convinced about that is my biggest sort of bottleneck, call that, or roadblock. Towards this? I don't think this is a particular release. Novel prediction, but I do think the vast New York like, just even within a year, I think the masked the the vast majority of code will be AI ridden rather than human ridden, but But I think, like, that does bring like, to the one of the points raised in the audience about, you know, like, what do you do when all the code's written by AI? It's like it does actually, like, degrade your or humans humans' mental models of how the code operates, and it does make it much more challenging to maintain things. But I think prediction is that we will find ways to make this more manageable. I think like, one interesting phenomenon is, like, in in many production code bases, it's, like, inverted where like, you don't have a lot of docs, and you rely on the code as a source of truth. Like, if all the code's AI written, then I think it has to be the reverse where, like, now actually, like, the code or the spec is the source of truth, and then or sorry. The the docs were the specs were the source of truth and the code is Who's added I Yeah. When we talk about predictions, it's it's very easy to be caught up in know, artificial general intelligence nonsense. Right? This is this is over our specific intelligence in in in the areas of system developers. And and so that sense, you know, that there is you know, not not not giving, like, specific timelines, but there's there's it's clear that over the next, you know, decade, there will be many categories of system developers entirely replaced. And I think I think even the opposition seems to agree based on what they've said. So as far as system dev remind you as for next one year or five For next next one year. Categories will be replaced in the next ten years. Yes. With about one or five years. What are your prediction? We're gonna hold you a word to it? It's okay to be And then in the in the next one year, you know, I think think the vast majority of, researchers will not be passing things over to performance engineers. Right? That they'll be they'll be they'll at they'll continue to use higher and higher level libraries, that that do lowering. Right? There will be still some over the next one year, but in in in five years, I think that'll just be gone. Right? There will not be this this army of, you know, cracked CUDA colonel engineers and so that's that's my main prediction. So you think we'll have AI researchers but not you know? I think I think we'll have AI system developers. I don't think we'll have automated AI research. Think that's fairytale. Stuff. Interesting. Okay. Yeah. So so it sounds sounds like we all agree that sort of in the next year, like, basically, everyone's gonna be using AI agents to code. I think prob I think in five years, we'll increasingly have AI coming up with the ideas and sort of doing that more of that high level reasoning. Type work. You know, one one of the great predictions I think we're gonna see is I I think we're gonna see an explosion of companies that are just one or two people. Where, you know, rather than sort of hiring a team of a 100 or a thousand people, you'll you'll just have sort of a person with a brilliant idea and using AI tools, to, you know, not raise money and just sort of scale, sort of do more with less. And I think there's a lot of replace system developers in there. There's a lot of people you would have had to hire a bunch of system developers, and in that world, you you you would can skip that. But I think that there's still sort of a kernel of knowledge in that that that human is bringing And, I think that it's gonna allow humans to do way more. And I think it's actually an opportunity because, if sort of the only limitation is your imagination and sort of being able to come up with with with brilliant ideas. We can move away faster as a society. We can get more way way more done. We can generate more economic value. And I think it could be it would be be great for society. I think if you're a systems developer, you need to adapt, and you need to learn to use this technology to sort of do something that would have been impossible years before and not sort of hope that the industry is gonna stay the same forever. So you're saying that system developers today need to adapt. The next five years? Unsure there'll be. But there's probably gonna be fewer of them. I see. So we're gonna have one more audience poll before we move on to the closing statements. To remind the audience the poll is, who believes replacement is inevitable? Could you raise your hand now after the debate? If you still believe so or now you believe so? Somehow I can't tell. Okay. I think it's about like, 10%. Yeah. I mean, we'll keep this in mind and yeah. Let me ask the other girls because The other oh, okay. Just to be calibrated that everyone's paying attention. Who does not believe that replacement is inevitable? Who thinks that was still have system developers in the future? Okay. I'll say we have like a one, two, five, maybe not 10%. Right? And now we'll have one sentence closing statement. Actually, one sentence. How many commas Commas are fine, but no semicolons. I feel I feel like I I sort of did my closing statement in the previous one, but it's all handed off to you. System developers will be replaced. System architects and orchestrators is what these folks call the people who are no longer system developers. Oh, plus one Jason's statement on adaptation and I think of it more as evolution rather than replacement. I don't know what else to add. I think I've been, like, firmly rooting for no this is not feasible. And if at all, you need to be orchestrators. I combined them. With that aside, thank our debaters for today. Thank you very much. Next, we have our break, and I think another talk. Yeah. Yeah. No. I I better get posted. Excuse me, everyone. So, please, get back to the your seats. So we're gonna be starting soon. We have a really exciting talk for you. Coming up. Hello. Hi, everyone. We are going to have our next invited speaker talk very soon. Should we start? Hello, everyone. Welcome back. So we have another invited speaker talk. By Rahul. Rahul is a research engineer at Google DeepMind. Contributing to the training and inference performance of Gemini models. He previously worked on the SLA TPU compiler. Alright. Is this working? Can everyone hear me? Yep. Alright. Let's get started. So today, yeah, I'm gonna be chatting a little bit about, LMM serving, how we you know, some extent of how we do it at at Google, at Gemini, and, you know, in general, how TPUs help you serve, you know, really big models. So let's get started. Do you wanna do? You know, we use TPUs and accelerators to, what, train giant models. Then we kind of serve them a little bit and, you know, doesn't matter so much. The main challenge is know, training the big thing in the first place. But, you know, recently, you know, obviously, OpenAI's worked last year. Other inference time scaling work from from other labs, Google included. Right? Indicates that we care a lot more about inference time. The the flops we put in that inference time, both in, like, the RL post training setup as well as an actual serving end users. And as part of this, we have to start thinking about this new workflows throughout the the lifecycle of the model. So when we build a a model, rather than thinking about, okay, is the shape of the model. This is the accelerators I have. This is how I pretrain it. You ask yourselves, okay, How do I serve this as well? How do I post train it efficiently? And efficiently means a couple of things. Right? One means just cost per token. You can say, alright. You know, bigger models is less efficient because it costs more per op a token or per input token process. But there also is notion of token efficiency. Right? The the idea that you think longer, as you as you do more generation either in parallel or serially, you end up with better quality. So you you have this concept that I think anthropologists have called, you know, token efficiency. Right? Basically, how many tokens you need an average to achieve a certain quality benchmark. And this matters in the context of pre training, because now it's not so obvious, and a bigger model is better. Right? Make a model significantly bigger, and it gives you better quality, but it you know, better quality translates to needing somewhat fewer tokens to to generate the final output. It's not so obvious as a net win. Right? The the token efficiency reduction might be less than the the increased cost of of training of serving the thing. Thing. So ultimately, you really have you know, rather than just a single point, right, ultimate pre trained model quality, you have this sort of trade off. Between both the the the cost the the absolute throughput cost generating an output of the given quality, which is like the cost per token times the number of tokens you're generating, as well as the trade off in throughput and latency. Right? Especially in the in order to approximate sampling, which is how most LMs work today, you have sort of the, you know, this tension where generating more more tokens is not a particularly paralyzable problem. Right? You have things that speculative decoding, diffusion, that's that's sort of ideas. Which help you, you know, get something to do parallelism, but by and large, like all the rest of models, you know, you do one token at a time over and over again. So if you wanna get you know, if you want this in search, you want to, you know, solve the IMO, whatever, you have this problem that now if your model is huge and it takes forever to sample one token, then, you know, it doesn't matter if after, you know, a million tokens can give you a good answer. Because you're not gonna get there. Right? Users will be bored. Like, you wanted a time in your exam or your your workload or your coding or whatever to finish a job. So you need to think about now, not only, you know, how much does the model cost to train, and how much per token does it cost to serve, but also how is the latency of serving? So you have this, like, very multidimensional problem and your task is to find this, you know, this high dimensional surface of, you know, the the cost, throughput, the latency, and pick the the best point for your application. So before we get into the details, I want to give a very quick run through of, you know, what is the workload, you know, TPU and accelerator level. LIMMs basically have an MLP, which is a big MacMo, you know, traditionally sort of dense. Now it can be MOE or something more fun. And you have attention. Right? Which of of various kinds. And take in the big sequence of tokens at training time, and inference time, they can run sort of two different modes. You have the prefilled where you're taking a ton of tokens in parallel. You do four passes to all of them together. And you get out kvCaches. Then you take these KB caches and attend to them one token at a time or a few tokens at a time in order to guess the fashion in generate. And this fits at the actual answer. And as part of this autogressive process, you get either, you know, some some thinking tokens, some thought, maybe tool calls, whatever, like we saw earlier in the day. And eventually you get your answer. Right? And that sort of happens one token at a time. Time. And these workloads are very different in terms of their operational their operational intensity. Pre fill is super high intensity because you have all the tokens between all in parallel. The CPU loves this. You know, it's just very heavily flop bound. This is like a fantastic workload for most accelerators, TPUs and TPUs included. Generate though is a very different story. Because you have this big model, you have, you know, like, you know, billions or tens, hundreds, thousands, you know, trillion letters. Enormous number of parameters. And you're just doing, like, batch size one pretty and one token multiplying against these huge weights. It's like a really cruddy operational density. You can think of it as, you know, if you have, say, eight GPUs or whatever in your small system, you're kind of each beam bound at bound, loading the weights every single time, which is, like, really slow and hurts for, like, big models. Because doesn't matter how good or how big your model is. You made a model two x bigger, and it's two x longer to get a token. Then what's the point? Right? Like, I might as well take the small model and make it think longer. We also see that, you know, how do you give our batch size and generate? For the most part, you get it by running independent requests in parallel. So you have batch size of, you know, a couple 100, thousand, whatever from you know, the request you get in production or in your RL job. Run it together. And this gives you a great throughput win, because, obviously, your intensity is being brought up to the that of the machine. But it doesn't really give you a latency win. Right? Like, you're token for its getting given request, it gets even everything slower if you're doing all the other requests in parallel. We can look at how this matters to the accelerator. So Ironwood, which is Google's latest publicly announced GPU, has a ridiculous amount of of compute and Right? You know, four teraflops, eight terabytes of second machine bandwidth is is like a stupid amount. And also a pretty solid interconnect. Right? Like, egress balance on the chip is is you know, is it gonna gigabytes per second. Over a Taurus. You know? GPUs are pretty similar, like in terms of the amount of flops and the AFM bandwidth, you know, to within factor of two or three, so not, like, dramatically different. As well as you have the egress bandwidth. Like, I think, you know, black wall is like 900 egress from one chip goes, you know, to 600, whatever. They're about the same. So both these things, the problem is that they are very high intensity machines. And, yeah, that's a problem. Right? That you have a high concurrency. You know, I'm currently in my previous generation TPU. It is a 100 it's a thousand terahertz ups per per byte room load from HBM. On ironwood, it's a bit better, but still, like, 600. And this is crazy compared to, you know, Generate, where you have, like, one token per one token per batch size at per per independent request at any time. For prefilled, though, this is pretty good. Right? If all these sequences, like a prompt is maybe a thousand tokens, that's, like, perfectly matched with intensity here. You can do some model parallelism over a relatively small topology, you know, four, eight chips or, like, an NV link node. And get you a little more tensor of that. You also have self attention, which, you know, traditionally is, like, quadratic and, again, very heavily flop bound because you're doing all sequence in parallel. So it's kind of awesome. Right? You can use your data parallelism over as many shit as you want. You can do a little bit more parallelism if you really want to. And sort of prefill works very nicely in accelerated systems. And, you know, it works. Right? This is me, Gemini three point o. It's a giant model. You know, I can't tell you how big, but it's big. I stuck a book into it. It reads it in one in, like, ten seconds. Like, what the hell? This is pre fill, like, two first order approximation is to solve problem. Like, throw chips to the problem, you get back faster answers. You know? Easy. So as I sort of mentioned briefly, that's not true for generate. Can get throughput through adding more data parallelism. Right? You can add more batch, and you can get more requests done in parallel sort of amortizing the cost of learning the weights. This doesn't help you with any given request. Like, if I'm a user and I talk to, you know, Gemini Cloud or ChattyBT and wanna get an answer back, and it sits there and thinks for thirty minutes. I won't be a happy user. A lot of our our our our colleagues in various accelerator companies, you know, give big throughput numbers saying, oh, we do, you know, a thousand, 10,000 tokens per second per chip. But, like, as a user, I get one token. I get much less than this. Right? I'm I have this this huge throughput number, but it's across everybody else. I'm covered myself. And as an end user, these huge throughput numbers aren't directly beneficial. So we wanna figure out how can we can, you know, modify our system and our model to to fix this problem. Right? So let's see. Like, what exactly is the bottleneck? Right? If you look at, you know, Llama four zero five b, which is the biggest public model I found, you know, sure there are others that are more activated, but I couldn't find them. This is a big recent one. About half a half a trillion parameters activated. Right? The biggest thing I can find, look at one token, multiply the params, divide by the flops, on just one ironwood. Right? Can get this, like, norm this massive number of 6,000 tokens per second for a single request if you're flop bound. Right? Is insane. It's like, you know, centimeter of microseconds per token. But obviously, you know, don't get this. If you try using any chat app today, any any any LLM, the latency you observe is significantly worse. You are not getting 6,000 tokens for yours particular of less you know, in in serial. Problem, as I mentioned before, is the intensity mismatch, right, that you have all these flops but they're stranded behind loading parameters from HBM. So you know, what can we do about this? Right? Have, you know, all these parameters, how we're controlling parameters. If you look at one chip, they take, you know, hundred hundred milliseconds of load on on on on ironwood. Right? So going from the flop added number here of 6,000 if we're bottlenecked by the MXU, to only, you know, 10 tokens per second if we're bottlenecked by the Asian bandwidth. That sucks. Right? And the obvious answer is we do, you know, model parallelism, tensor parallels, megatron, whatever whatever you wanna it, right, and similar things for MOUs. Where you basically start these parameters across a large number of chips. So let's say you want to get to the flow bound point for a single request. Right? How much model parallelism do we need? Well, we just do the basic math. Right? We have three parameters. Let's say, before 16, divided by the latency you want, divided by the bandwidth. It tells you, okay. You need about, you know, 500 little over 500 ironwood. So Okay. Fine. You're saying that just take the the model, shard it 500 ways, One batch has one. We're done. So what is the constraint here? Right? We also have an, a collective constraint. We have to be able to do the model power collective, you know, for this one token. So pretty small. But under a very tight latency budget. You know, La La is about a 100 layers, so that means we have about two microseconds per collective to get this to to to keep us under this, you know, to keep us HPM bound, avoid bottleneck buyer interconnect. But this is, like, you know, nuts. Right? A GPU system is, like, chips. Like, where the hell do you get 600 chips altogether in this latency budget? There. So that's the basic motivation. Right? This is why we sell this thing as an inference You know, this is it's probably a surprise. You know, I said it's about, you know, l l m serving is really a marketing talk that you can buy TPUs. The motivation is, yeah, that TPU and, like, torus based, you know, accelerated systems have a very large number of chips compared to a switch based system. At a very low ICI latency. So latency for the 500 chip system, I measure it's about ten microseconds, and you can get it lower with some some tricks, which I'm not going to mention here. So the important part is that, yeah, giant pods of chips, you know, that, you know, like like like ironwood or the previous, you know, TP generations, let you drive latency way, way down, discharging the model so many different ways. And, you know, nightly, though this this collective is exposed, right, you go through this model collective, followed by the mat mill, and you can do various architecture things, well, things in the, you know, in how you bite your kernels and such forth to try and overlap things better. But the high level the the high level bit is still the same. That if you want to get low latency and benefit single requests, rather than just doing, you know, high throughput but really slow per token requests, you have to charge your model like a ton. On this you know, this is a very recent chip, and this is a model from several years ago. But even still, you just shred the model over, like, you know, hundreds of chips to try and get error codes and by number for a small batch, serving. And, yeah, for frontier models, I can tell you, I work on frontier models. You know, hundreds of thousands of laser centers chips. It's quite, you know, un unsurprising to you to for for us to use to serve our models as latency targets we want to hit. Now, you know, this is for TPUs. Do you do for GPUs? You've got, like, eight. You know, now you have a 144. I don't know. Goodbye by TPU, I guess. I don't know. So yeah. You know, these are all this, you know, me doing, like, spreadsheet numbers. Like, how do you actually go and program the system to to hit these numbers? The key idea behind the TV system and, like, our our serving second general is we want the hardware to behave in a very deterministic manner. If you have something like, you know, a fancy switch, which, you know, you know, which has, like, a tail latency that's really bad, or we have, you know, scheduling on the on the device side, which is unpredictable. This is very challenging. We're trying to get these latencies that low. But recall back then, we said our each layer should take only a few microseconds at a Right? Say even ten microseconds. That's kinda crazy. Like, your your your latency for a single, reads from each VM on, like, either GPUs or TVs are, like, multiple microseconds. So if you're just, like, being a bit sloppy about your work, you're doing the hardware handle, you know, pre fetching for you, you're dead because these magnitudes add up instantly. So what you really need is, like, complete hand control over, like, every last bit of the hardware. And that's basically how this works. Right? The only TPUs work is you have a big VLIW instruction stream. So basically every cycle, know exactly what the hardware is doing. You write the code. You can see this inspect this in the assembly. And you just do these operations. There's no notion of, you know, latency hiding automatically for you or thread switching or any of that. You just write. You want to start a transfer? Transfer start. Finish transfer? Transfer, done. And you have full control over this at the current languages, as well as even the compiler does a good job of this. But we can drop down to our various the current languages, either PALOS from Mosaic that is exposed from the, you know, JAX stack. And get these have the operations placed exactly where you want them. And because the hardware is so predictable and you have so much control over where they place, where you place instructions, you can basically hide latency perfectly. But you know, okay. Every single time I do this map mode, it'll take, you know, two hundred cycles to finish. Every time I do a DMA read of this buffer of this size, it'll take this much latency plus this much bandwidth time to load. And you can predict this exactly and place, like, the the consumption of the buffer right where you expect it to be. There's not much surprise here because the hardware just does what you say. You know, you can also ask, alright. Well, what about, you know, thread switching, async stuff, thread blocks? I don't know. That sounds hard. Don't do that. The the way the TPU system works is you are writing a single thread program. And this is not really a function of the accelerator system itself. Right? You still have multiple threads in the terms of having multiple chips in your system. The key idea is, again, the programming model. That you're writing a single thread at a time you write all the collect operations yourself, synchronization, DMA start, DMA whatever. All these operations, you do by hand. You sequence them at high latencies, you know, explicitly. So every single transaction, it takes more than, like, two cycles. Is expressed in the TPU I set, and expressed also at the the user level. As asynchronous operations. You can say asynchronous starters operation, and finish operation later when you know because of the deterministic behavior when the latency you know, will have been hidden. So basically, how do you write a serving program for this to hit these the very tight latency budgets? You basically write one big schedule sequencing all operations and exactly the latency you want to hide them and then you're done. Right? You know exactly how long the hardware takes to do these operations. Can either measure it empirically or just read the specification. The time is is completely predictable. And because you have you know, there's no race conditions here, there are no, like, you know, atomic operations that might not might not be deterministic every time, It's very easy to get, like, perfect synchronization, complete determinism across this you know, a very large system. And that's a good made them clear message here. Right? The hardware does certain things very well. It does. It does DMA trans HBM reads, whatever. And you get to tell to do what you want. Right? Just write code for each you know, the single scalar unit, which expresses a thread of control. You write code here saying what you wanna do. Do a map well now, start the DMA here, start the map whatever. Like, you write these operations. You sequence them in in the way you expect. And it does it. There's no magic here. Wanna do a kernel. You launch a kernel. You leave the DNA dangling. Okay. Up to you. You can finish the DNA later. That's totally fine. And, you know, you get to handle all these resources. You get full control over the the machine itself. There's no hidden, like there's no prefetching. There's no branch prediction. There's nothing in this hardware. If you want to preload the next part of your program, you write a function calling preload the next part of program. Right? The hardware has no magic there. You do exactly what you want, and that's how you avoid these, like, microseconds adding up and killing your latency. And then it lets you throw, like, a ridiculous number of chips at a problem. Right? 500 chips for a, you know, an old dense model. Trust me. Our models are bigger now. And just get away with it. Because, you know, full control over what the machine is doing. So, I mean, people will say this like, oh, no. I have to write a complicated kernel. That's too hard. I like skill. Like, whatever. But that doesn't make sense. Reason it doesn't make sense is because unlike, you know, other accelerated systems, there's no notion of host dependence in the TPU. Right? There's no notion at the when you hit the actual hardware, there's no notion of a kernel. The hardware is like a, you know, like a CPU. There's a big sequence of instructions. It runs them in order one at a time. And that's it. Right? You wanna fetch more programs, it is reads from an HTML and loads an SQL memory, whatever. Like, it's all completely deterministic and and controlled by the user. So if you wanna write small kernels, and then have them go one after other, that's fine. Because the way the compiler does it literally case the kernels and ducks them together at the end. In some sense, every TPU program is a giant kernel, just orchestrated by the compiler. And, again, like, you have control, but you also have, you know, a lot of help. Right? May not want to, like, manually bookkeep every single latency in the operation of the compiler. That's fine. Write the operation, so the compilers figure out schedules, it'll do it for you. You. You wanna say, oh, I wanna do a boring dense map mode. Compiler doesn't do this. Issue the map model. And they sort of play nicely together. Example, you say, oh, I wanna do a big dense map but I wanna prefetch the weight in a particular, you know, carefully overlap fashion to keep the atria band loop busy. That works too. You prefetch the weight. You feed the prefetch buffer into the compiler. It'll take that and keep You wanna do some async operations behind the map model the compiler runs. That's fine. Compiler knows, okay, you're doing an async operation. This buffer is reserved. The sync flag is occupied. And it's do it'll do its math more, you know, while keeping your state untouched. So sort of the key principle of XLA is not that it's, you know, clever, that there's a very opt optimization, so there's something like that. The key idea of XLA is that it lets you orchestrate things. Tracks all the resources you're doing, and basically lets you call the different kernels, the different, you know, built in h loads in XLA, without messing with messing with you. And it lets you offload decisions to compiler when you want but also you don't wanna do that, you can just take control of yourself. And so the key idea is you could overlap all the latencies You could compose, like, a ton of primitives both in the standard library. Which is quite huge, as well as from all your kernels that you write to yourself or you take from the Internet. And, you know, build your big program. And ultimately, it gets serialized and stitched into one giant kernel anyway. So sort of you get the the min latency performance without having to go through hell and run a big kernel every time you wanna do a new model. Now this is sort of the the we talked about the the high level idea. Use a ton of chips for for fast models. We've talked about, you know, how do you actually make the compiler, make the hardware do what you want. Now how do we get the model is the final part. So the key thing is as we touched upon at the beginning. Right? You want a certain latency throughput objective. And if you do this very, you know, cognizant of what chips you're using, So if you know, okay, I have ironwood, and I have, you know, this many chips I can use in one one latency domain based on the latencies that you measure or you see from the specification. Tells you, you know, how big is your model, how many parameters can you have per layer, what kind of quantitation that sort of thing. Thing. And you can ask yourself again. Okay. If I don't really care about latency, fine. I want throughput. So you crank up the batch size. You go. You get the marketing you throw a ton of flops at the problem. If you want low latency though, you think about, okay. I want lower latency. I want to use more chips. There's too many chips. The ICI collective itself becomes a bottleneck. So you wanna play with, okay, what kind of model can I build which minimizes communication? But still gets the quality I want? Or take a quality discount where pays but pay for pays for it by letting me generate each token more rapidly. And, you know, these are some ideas I pulled from, you know, from Claude because I didn't want to give anything I already know. Yeah. Make the models denser, make the model shallower, you know, whatever. These give you, you know, fewer layers, fewer collectives. But, you know, maybe give you throughput loss, but you can maybe pay back pay back by just getting tokens faster. And you can see here, there's a paper publication we made quite some time ago. Right? That empirically, once you have the system set up, can sort of sweep different things. Right? You can say, okay. I want a a more relaxed latency target, I can use a larger batch size. I can maybe use fewer chips and get less, you know, pay less for the the auto collectives and get a better throughput. You sorta get this trade off you want. The the point on this trade off you actually select depends on the the constraints you have from your workload. But that's a key idea that you should be aware of what system you're using, and and and build your model to fit that system exactly. And maybe, you know, that leads to pretty many compromises. Right? Shallower model, wider model, less sparse model, whatever. But that's okay if you end up with a better final artifact on that on that curve. Just think about it here. Let's say I take a model and, oh, no. I made the the the model worse. So I have to think longer. But the latency is better. Then great. I can just push along the curve, get the latency, regress it back to where it was before. And eat through eat enjoy the throughput win. So basically, even if from an accurate parameter point of view, the throughput is worse, because you're thinking about not just the point, you know, not just a certain point in this curve, but entire curve itself. You can end up with a better final operating point. And, yeah, ultimately, that's how we do it. Right? Take our model, jam, like, two point o. We said, okay. We'll we'll we'll train we'll service on trillion. We'll serve on iron word, 2.5. You know, I guess you'll find out. Thanks, guys. Testing? So we have some time for a few questions. Yeah. Really excellent talk. Increased my TPU FOMO. To incredible levels. Like, the question I have, Okay? That was not the question. So yeah. In response to your recent question about why DeepThink is is behind a $250 a month subscription Logan Kilpatrick said that that there are not enough TPUs in the world. To serve it. Like, openly to everybody. So I'm curious like, when do you think there will be enough TPUs in the world that everybody in this room can, conserve these models at this scale? Okay. So, you know, obviously, I can't give you numbers because, you know, you know, if I know them, I can't say them. But what I can say is that, obviously, you know, model quality at ISO cost has been improving dramatically in the last few years. I think we said publicly, Jeff, posted that we bought something like a 30 x throughput improvement. At ISO quality in the last year from, like, 1.5 to two point o or 2.5. And that translates to again, you know, better points in this curve. So there are two parts of Deepgram. Right? Deepgram has a throughput cost, which is less expensive, and also the latency cost. It takes, you know, like five minutes or ten minutes to give you an answer. And you know, they're they're correlated. Right? Like deep think is you know, it's ticking for a long time. We want to get the latency down to give you an answer fast. And that means we're running at a, you know, very extreme point on this throughput latency curve. Curve. So if you can get models to be either more token efficient, or just get the sampling latency down in the first place, then the translate not only to a win at that particular serving point, but let's select a different serving point with more throughput wins. And get the cost even more rapidly. So basically, not yet, but, you know, I expect pretty soon. But it may not be the deep link you see right now. Like, deep link, you know, things for, you know, how many tokens I in one generation length. Part of our work is to improve token efficiency. So maybe that actually what you end up with next year is, like, a model which doesn't think at all, thinks for, like, very little. And gives you the same quality answer. Like, I don't know exactly what the characteristic of the product will be, but what I can say is that, yeah, I I totally expected something that gives the same quality answers to be available, you know, to everybody very, very soon. What we've been doing, you know, so far. And then one, separate question. Also seeing I work working model deployment platform. We're seeing a lot of folks running smaller models targeted to more specific tasks that they've sort of POC ed it with frontier models. So I'm curious how you think these same lessons that we've learned from serving at this super large scale are going to apply at a smaller scale. Whether that means changing things about hardware and changing things about, like, system interconnect on the chip or, whether it means, like, different modeling approaches or something else. Sure. Yeah. So the the thing here is, like, we said that basically big model means use more chips. Small model means, you know, use fewer chips. Right? Because if you try to use a similar chips, it'll be bottlenecked by the collective. You still get a giant throughput win. Right? So I think I mentioned it very, very briefly here somewhere. Maybe I didn't. Somewhere? Okay. Yeah. Here. Like the first small models that are doing, like, relatively short generations, this looks more flop heavy. Right? Because you aren't really worried about collectives. You aren't necessarily even worried about pramper loading. You just run a very high batch size. So, yeah, like, we could you know, for an accelerator system, if you're thinking about using small models, mini care you know, frankly, you build a GPU. Right? To some extent, GPUs are fantastic for, like, the small special purpose models, like, you know, the classifier or you know, it checks the model output is safe, and TPUs are better for, like, you know, ATI. Right? Because of how the system is designed, that you have a relatively small system, you know, with a high high operation. Density. And that's great for for these kind of, you know, denser, you know, weaker models. If you have a really strong model, you want to think for a very long time, then you're forced to use a giant accelerator system, and then you wanna So it's kind of like you'd make a GPU like GPU if you want to serve like weaker models, and a TPU harder to serve stronger models. Any more questions? One last question. No? Next have our two more, like, paper print stations. The first one is, by Aya. Aya is a computer science PhD student. At Harvard. Advised by Vijay. And, he's supported by the NSF Graduate Research Fellowship, His research interests are broadly in machine learning for a system optimization, and his recent work has focused on hardware aware autonomous performance engineering. Safety stores. Cool. Thanks. Hey, everyone. I'm Aria. I'm excited to, share my work on, swizzle perf, which is hardware aware LLMs for, kernel performance optimization. This, work was done last summer during my internship at, AMD. So thanks to all my collaborators at Orbit at AMD for that. So here's a quick summary of, what I'm going to walk through in this talk, and, we'll start with a background on what chiplet swizzling on AMD architectures is, and, some prior work on ML for GPU code optimization. So, sure many of you are familiar with a lot of the really cool recent breakthroughs in GPU kernel optimization, autonomous GPU kernel optimization in the past maybe one or two years. And at AMD, we took it maybe one step further by decomposing this kernel optimization task into optimizing for specific bottlenecks. So think fixing bank conflicts or uncoalesced memory accesses, and we can validate this with profiler feedback. Now, unfortunately, a lot of this prior work is not what we call hardware aware. And, even though there's some really strong results that come out of it, we show how this fundamentally limits the hardware specific optimizations that you're able to make. Now, what is hardware awareness? So at a high level, it's understanding the hardware and scheduling to be able to actually design the software specifically for the hardware. And in this case study, we focus on memory locality, but but you could imagine there's many different things that we can focus on that are specific to the hardware. For, the purposes of this study, we care specifically about the number of triplets per GPU. And the default runtime block scheduling policy. But there's many other things that you can care about, such as the warp size or number of, SMs or CUs depending on the provider, And importantly, different GPUs have different specs. Thus the way you need to program for them varies significantly. So Now, on AMD GPUs, they introduced eight triplets. And, this means that each chiplet has its own, separate l two cache, and of course, we wanna hit in the cache as much as possible. Now, by default, if we, tile up a gem and schedule it onto the different giblets, you're gonna have really bad cache locality. Because adjacent tiles are gonna be on different caches. You're not gonna hit it all. So my mentor over there spent several days working on this you know, few lines of code and this is gonna swizzle your blocks to improve the, cache hit rate on these, triplets. You can imagine that it's not really scalable for a really great GPU programmer to spend a long time on one optimization. So the purpose of our stuff was to think about how we can actually use hardware awareness to make this optimization. Autonomously. So Now, when you're actually able to drop in this optimization, you remap the tiles such that now you hit a lot more in the cache, and you get, improved hit rate and also speed up. So I'll walk through how we get there autonomously. This is, at a high level, what I'm doing, and I'll walk through each, piece of it. But to start, like, the prior work, you give an And you also give some hardware awareness information coming from the profiler and, hip device attribute, And you give runtime scheduling information coming from the architecture guide. You construct the prompt with this hardware awareness and some history from prior iterations. You output a new kernel that you can compile and validate to make sure that it's correct. And you can also profile for the metric of interest within which in this case is l two hit rate, and the speed up, which, you know, we we care a lot about. And we can combine all of this into what we call a bottleneck report, add it to a buffer, and iterate. Keep going from there and and and see speedups down the line. So So I'll quickly walk through some of our main results. And as you can imagine, different kernels have different baseline hit rates. And the purpose of this work is to improve these hit rates. For Gem, the swizzling pattern gives us a 14%. L two hit rate. And for soft max, we see a completely different pattern that leads to a huge You actually get close to a 100% hit rate, so you can't get much better that. Now across this suite of of 10 ML and science kernels, we see that, our swizzle proof methodology is able to get us nine successful patterns compared to some baselines, which we don't put in hardware awareness or we just overload it with a bunch of documentation without actually kind trying to sift through what's important. We don't see anything close to as good. And we don't just see l two head rates. We we see speed ups. But what I'm also really excited to share is, our future work, where we're thinking about zooming out a little bit. Now, of course, SwizzlePerf is really specific to one optimization on one architecture. We wanna go to a lot of architectures and a lot of optimizations and see how hardware awareness can can push the needle. So a computer architect and, you know, the first day I stepped into class, we care about co design. Co design means we're designing the hardware to support emerging workloads, it means we're designing the software to really utilize the underlying hardware. Hardware awareness is not new. It didn't start with this paper. It didn't start in the few years with autonomous kernel generation. It's really old. And there's a been a lot of really great work in hardware awareness for know, across the the software stack. And what I'm really interested in is how we can bring hardware awareness into our new agentic code optimization work. Flows. Our current understanding is that LLMs can recall and reason about hardware where details. And I encourage you guys to check out some of our labs work on quark, where we, benchmark element reasoning on computer architecture, and see that compared to a lot of other systems tasks, LMs have really strong understandings of GPU architectures. LMs can also implement hardware agnostic GPU optimizations. And you know, our our awesome collaborators at Stanford have done some some great work that show that with some ML tricks, you can use elements to generate fast CUDA code. But most of these optimizations are limited to hardware agnostic things like tiling coloss memory accesses, reduced bank conflicts. All of these things work on a lot of different architectures, and you know, this is great, but we also wanna think about things that only work on the specific architecture that you're running on. Because that's how we really squeeze every last drop of perf. So What this tells us is that right now, LMs cannot implement hardware aware GPU kernel optimizations. You know, there's there's definitely some exceptions where they do a great job, But at a high level, we observed that they can't use hardware specific instructions like you know, the you know, GPU specific Tensor Core instruction, TMA, TMM. Etcetera. And there's, a lot of these things that change a lot with each generation of GPU. And they can't make hardware specific optimizations. Like, for example, the disaggregated memory optimization that I I talked about right now. And, why Unsurprisingly, we think, this is because there's not enough good there's not enough good data out there. So, So as I wrap up, our goal a high level is we wanna teach Ellen how and when to make hardware specific optimizations. And confident that if you can do this consistently, you'll get really great performance on a lot of different architectures, even as you go in the future, you'll you'll see great results. Yeah. Thank you. Thank the speaker. We'll do a combined q and a after, other spotlight. Speak. So this will be our last final talk, also spotlight presentation. By. Rokai is a final year PhD student in ECE at Yale University. Where he designs computer architecture, systems, and compression algorithms for energy efficient AI. His work has appeared in top venues like Micro, ISCA, NewRePS, and ICML. Including a best paper nomination, and, I triple e microtopics. Thank you, Horan, for the introduction. And thanks everyone for attending our presentation on learn to chart. So, this work was done during my internship at Microsoft. Just a quick advertisement. The team is hiring both interns and the full time for next, so, yeah, it's a great team. And my mentor, Paula, is here today. Feel free to reach out to her if you are interested. Thanks. Okay. So, in the era of IOM, ARM nowadays, the large scale distributed inference on a cloud level becomes really essential. Distributed inference, essentially is just put different parts of your models or data, across different devices, and doing the computation in parallel. Here are some examples of different parallelization types. You may be very familiar with all of those. So here are tensor parison, x perparison, which are a little delayed for the latest MOE models? And also there are pipeline Parism, and a lot of other parison types are there. And if we zoom into this paradigm types, right, how do we specifically describe a parallelization strategy? So let's use this well knowing example of the tensor paradigm on the Atlas and blocks. As an example? So one very well known perspective is the degrees. Right? So basically, how many number of devices are involved So in this example, the decrease is simply four. In perspective that we found very interesting but somehow got neglected before is the per operation, perism dimension. Basically, this means that on which dimensions does the each operator's output get shared across devices? Right. For example, in this, example, we have f f one being shorted on dimension one, FFN two got shorted on dimension zero. And this is often neglected before because this is usually determined by the handcrafted heuristic in different frameworks. But we find it interesting to include that in the search space as well. So in short, this design space for the parallelization strategy on the inferen distributed inference system is very large. Because we have different parallelization types and strategies which are complementary to each other. Thus creating a vast and complex design space. Especially when the number of total devices and the number of nodes on the computation graph got increased. For this vast design space, expert level exploration on the strategy remains very important, but we do want to leverage the help of an autonomous framework to help in exploring this space. So here is the standard framework of using the autonomous agent to search. The question is to how to how do we model the parallelization strategy search problem into this framework? Right? And we find that the key is just to integrate the strategy space into the action space. Specifically, we model all these sub strategies basically the different options for different degrees, and the per op dimensions. Into the discrete sub action spaces. So that the agent can then freely select a compound of subactions. And we want to explore whether reinforcement learning based agent is a promising candidate here. Below is the overview of our framework. We use a PPO based agent as our searching agent. On each iteration, it will select a compound of sub actions which determines the strategy. We then send this strategy to an in house simulator to provide the feedback. Sometimes, the output from the simulator can be very noisy, due to the violation of either the SLAs or the hardware's limitations. So that we have these dual rewards to encourage the high throughput strategies while avoiding, the violating ones. And this is our policy network design. It's pretty straightforward. It's an ELITE context based autoregressive transformer. If if you look at the figure from left to right, we have this, a light history queue, can store number T of the prior best actions. Then we send that through a lightweight embedding layers, and after embedding, we send that through transforming codel blocks, Right? And we get the logics, and we perform the action sampling. To get a strategy alpha. We send this alpha into our simulator to get the reward, which would be, the throughputs in this sense in this case. And then we use the reward to perform the gradient descent, And if we find the, new action is better than any of the ones that existed in our history queue, we will update our queue accordingly. We also have several optimizations that come up with our Opex network to improve the searching efficiency. The first optimization would be the sequential multi agents. Essentially, what we are doing is that we will launch several agents sequentially to search on the same design space. Every new agent will inherit the prior best results and highlight history cue from the previous agents. Right? And if the new results found by the new agent worse than the prior best results, it will receive a soft penalty. By doing this, it can hugely alleviate the issue of local maximum, And while more agents will have better chance of finding a good strategy we will have, right, However, having a fixed search budget for each agent will be very costly. So that comes, to our second optimization, the confidence early exit. This is also straightforward. Basically, once the agent starts to exploit the strategy it has found before, there's no meaning to keep the searching going on. And this is also back up from this loss curve. Right? We can see that the loss doesn't change much it starts to exploit. So we define this confidential confidence score which is essentially just a maximum of the softmax of the logics for each subactions. And as long as the confidence score of every subactions is greater than a predefined threshold, we will just early exit terminate the search we can allocate the remaining budgets for the current agent to the remaining agents for the searching efficiency. So here is our experimental setups. For the system, we choose using h 100 systems, and deploy in one point two trillion and one point six trillion MOE models. We allow 12 fused operations on our computation graph and up to 24 k GPUs. The total search space is roughly 10 to the power of nine. We said we allow 4,000 search budgets for our agents. That means 4,000 times of simulator calls. And confidence threshold setting to be point nine five. We first compare the search quality of our proposed method with the other meta heuristic search algorithms which are very well, widely, adopted. Basically, the simulated annealing and the random walk. So here, all the results are normalized to the random walk, so we can see that our methods provide a much better quality compared to those meta heuristic algorithms. We also measure the improvement of the strategy found by our agents over the MicroCharm Heuristic. So we can see that we can observe from 1% to 6% improvements over the Magatron LM heuristic from the new strategies found by our agents? Here is just an a simple example of the results found by our agent. Right? So on top, should be very familiar with this, is the what LM Megatron LM is doing. For the all reduce based on the FFM blocks. And on the bottom is the all gather based parallelization strategy found by our agent, So, essentially, we can see here the only difference is that we choose our agent choose to shard on the dimension one as well for the FFN one. The difference would be then we will have three altogether instead of two or reduced. Depending on your system implementation. If you have a faster altogether, you will have a better overall latency. So it all depends on, what's your system looks like. But it just I just wanna illustrate that our agency is able to find a better strategy compared to a heuristic based on the simulator. Okay. Great. Thank you. And I'm happy to take questions. Again, I want special thanks to my mentors, collaborators, and the team in micro give me a huge support. Thank you. And the paper link is here. We also provide code link. The code is not ready yet. We are cleaning up things for attended contest, but it will be there. Thank you. To either of these talks, any questions? I guess in the meantime, can ask a question to Suisopurf Maybe I'll just save the question. To that. Hi. Hi. This is a question for you. For you. So you mentioned hardware aware LLMs. Yes. What are some strategies you're using to explain the hardware to them? And get them to reason through that, I guess? So it changes based on the optimization we're making. Right? Like, obviously, the hardware details that you'd have to know for swizzling is different than the hardware details you have to know for I don't know, using asynchronous memory copies or something. So Right. For this paper, we just manually figured it out. Like, we got the information from the profiler, got it from architecture guides, we said, okay, this is the important information that you need. Now we're thinking about how do you scale this so that you don't have to do it by hand and haven't quite figured that out yet. But we're thinking about it. Yeah. Okay. Thanks. Any other questions? Oh, in the back. Hello. I have a question for, Du Kyi. Is currently is your environment like actual h one hundreds or are they more of an analytical modeling of the performance? Sorry. You say the environment is what? Like actual hardware's performance. Oh, got it. Yeah. It's based on the in house simulator. So the simulator will provide, the estimated performance of the real system. And is there a reason behind it's not the actual system that you're working on top of? Is it because of the latency? Yeah. I guess one of the reasons is that the speed to get reward right? So to deploy changing the parallelization strategy and get the real the real system speed up like to getting the reward, time is much longer by just run running through the simulators. Yeah. I think that's one of the reason. Makes sense. Thank you. Thank you. Another question for Rokai. Like, kinda following up on that question. Did you did the models discover any bugs in your simulator that they reward hacked and, like, otherwise, did you observe the sort of, like, reward hacking things that people have seen with Sukana, KernelBench, other places where the models you know, skip work, drop memory barriers, and other otherwise, like, exploit weaknesses in the reward function. So basically, sorry. Like, could you could you just repeat the question at the beginning? Yeah. Did you observe reward hacking like, and what did you do to detect it or avoid it? Oh, I didn't, see any reward hacking. But I I guess that will that is heavily affected like affected by the design of your simulator as well. Right? And how your design, basically at what level of details you have in your simulators, simulator. But I I guess that's also a great question. I I certainly get So think that's another benefit of our method. So basically, we don't need to, ask the user to impose any details of the implementation of the simulator. You just need to provide the reward to us. So I think that's another we can treat the simulator as a pure black box. I guess that's one of the benefits if we compare to other existing methods like internship programming and all those things. I don't know whether that answer your question. Partly. But I guess it's like if you treat the simulator as a black box, the model can optimize that black box and not optimize the thing you actually are trying to optimize, like, the hardware itself. Right? And, like, a user providing you the reward is frequently going to miss things about the the, like, how to design that evaluator appropriately Like, a prominent instance was I think, IntelliJ was only checking whether the default CUDA context had, like, completed its work. But not or the default CUDA stream had completed its work. But not whether all streams had completed work. So the agent learned to push all the work into the other streams so that it would appear the kernel had finished faster. Right? So there's, like, quite like, quite a large number of these projects have had these, like, very like, somewhat subtle, like, reward hacking by the agents. That, have yeah, undermined their claims to, like, improve I'm curious if you've seen anything like that. Yeah. Totally agree with that. So but, again, since we are treating the the the black box, so the more accurate of the simulator, then you can capture more details of the real system. The better result of our origins will provide. Yeah. Got it. Thank you. Cool. I think it's the time for the poster session. And we're going to have a closing remark at the end. Yeah. So, the closing remark is mostly for the online, livestream, which is ending now, I think. But yeah. So for the, people who are physically here, yeah, we're gonna do the poster session. And then afterwards, starting at 05:30, there's going to be the first ever NERPS, ML for systems, workshop happy hour. Really excited about that. And it looks like based on the number of people who have applied, it's going to be probably pretty crowded. So I recommend going earlier if you can. And if you're a, speaker panelist, or, have a poster or did the oral presentation, please to one of us. We'll give you a sticker so that you can get preferential treatment. To get yeah. In case in case that fully runs out. So we'll see. Yeah. But hopefully, it won't come to that. And, yeah, thanks everyone. And see you at the party.