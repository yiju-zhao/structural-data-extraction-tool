Hi. Hello, everyone. I think that, our workshop will be starting now. Yeah. First is our opening remark. Yeah. You, everyone, for attending our workshop today. I'm Xin Yuan, and this is Chunghwa. Yeah. We are the organizer and the moderators today for our workshops. Yeah. We are the first workshop on the efficient reasoning. Yeah. We know that, these days, that foundation models are everywhere, and we have so many large reasoning models. Yeah. But, when people apply them in real world applications and in many settings, that well, people will complain about their efficiency. So we want to talk about how to make this large reasoning models more efficient. In detail, we are talking about four questions in our workshop today. The first is how to design, create, and maintain high quality data for training and evaluating these large recent models on challenging recent task. And the resource constraints contribute to engines, of these directions, including maybe long context reasoning, simple planning, multi hop deduction, and real time decision making on diverse domains. And for the second question, we also want to develop maybe more advanced training algorithm for maybe supervised fine tuning or doing some reinforcement fine tuning to and some efficient type time inference methods such as pruning, compression, progressive generation, and search based strategies to reducing the time or space complexity without, diluting the accuracy of s phoneme. And the third question is about, Yeah. And and it's okay? He'll he'll he'll operate the I see. I see. Yeah. Yeah. And I think the first question is about we want to, implement of efficient audio training systems and the long term source inference engines to support contract reasoning models. We're unlocking new reasoning potentials on commit accommodated GPUs technicals may include dynamic QV cache placement, context graph exclusion, and on device two, topic about both the aerial and world. They are two very famous, RL training systems these days. And the last question is about if we also want to adapting this larger recent models to real world reasoning capabilities in real world results constrained scenarios. Such as clinical decision making, robotics, autonomous driving, developing world health, and maybe on orbit, augmented We will just want to use these models to be really useful in real world. And today that we will have nine speakers, from both academia research. And the industry. Yeah. They are professor Bedi Chen. Yeah. Professor Aman Kuhn. Professor and, Nicholas professor not professor. Yeah. Researcher. Yeah. Professor Wei, professor Yu, professor Zhang Hao, and Wang Zhang from Badang State. Yeah. We will also have four panelists from the industry to talking about some frontier research regarding to the reasonableness. And today and we are also very happy to see that this year, we have received submissions. We have received nearly 300 submissions. I think it's a very huge number for our workshops days, and then we have maybe 182 acceptance. Among them, we have select, maybe not 22, 32 spotless and four hours. And today, we will also, give the best paper award to one of the other paper. And our schedule is that, in the morning, we will have four invited talk and two hour talks and then maybe the post session one. And in the afternoon, we will have the five, another invited talks and one panel discussion and two oral talks, and also our panel section two. And here are other organizers. Yes. Thanks their contribution to this workshop and the thanks to our audience. Yeah. Let's start maybe our first, speaker. Yeah. This will come professor, Jun Escaping. Alright. Yeah. I think I'll just get started with you. Alright. Yeah. Thank you all for coming. Very efficient of you to come so early in the morning to the workshop. So, gonna talk a few about a few things a bit more towards research and a bit more towards maybe future architectures of reasoning and of efficient reasoning. Because, I'm pretty interested in recurrent depth architectures. And, I'll tell you what that means and why I think it's interesting for reasoning in this talk. Before we start, talk about a few key concepts And I think I can only do this wrong, but, these will be the concept of which I'm working with. So for the purpose of this talk, what's test dot compute? For the purpose of this talk, we'll say that test time compute is just the ability of machine learning model to use more computation at test time. We'll make no further, like, ideas of, like, what kind of computation that would be or what shape it takes or whether there's a chain of thought or not. And, but maybe the bigger question is like, what actually even is reasoning? And, it's quite good to have the slide at the beginning of this workshop because I really don't have a good answer Like, because I think like, many of us are familiar with this kind of definition of reasoning. Right? Where we say, maybe reasoning is just learning to pattern match abstract concepts to meta strategies to learn logic and structure and planning, It's not just memorizing facts, But if we investigate it for a second, what does it what does it actually mean? Right? It just means something more along the lines of, it's just everything where the model doesn't know the answer. Right. So but maybe that's actually not a crazy definition. Right? Because like, we see a lot of, works today where reasoning really is just logical inference. And, I think have a good answer here what really is reasoning. But I think, with these working definitions, this will be what the talk will be about. So So because what we are very familiar with over the last now, one and a half years, is we've seen a lot of verbalized reasoning. Right? We we have a model, the model gets a question. The question is very hard. Here's something about planning. Right? And the model runs for minutes. Now, for maybe from more modern models for hours. And the model does logical inference. It run it runs some sub components. It thinks about the question. It really looks like whether it has some set of premises, which starts out with. It runs a lot of computation to, like, work on those premises until it arrives a solution to the problem. And this is really how we have been able to scale these models to many domains that we have never seen them work before. Like, even two years ago. Right? Like, if you remember back to GPT three, it was terrible math. And now we have Math Olympiad models just based on verbalized reasoning. But why is also super important to investigate why would this even be necessary? Why do we need reasoning these models? Why do we need verbalized reasoning? Actually straightforward. We have these multi hop problems. For example, I like we like this this test sentence from this one paper, which is called John's father's mic. Mike's father is Tom. John's grandfather is Of course, the answer is Tom. If you think about this, in a transformer, then you need a certain number of layers to even encode these three relationships. And here, if you have a model that has fewer than four layers, you can't robustly do this sort of inference. You actually need a certain amount of depth in the model to do a certain amount of inference. That's why we've been so successful with verbalizing of thought as a community because we can use the verbalized gen of thought to sort of extend this graph that I'm showing you here more like horizontally, so more to the right. Right? Because we we put in more tokens in the middle, and we can use those tokens to have a longer multi hop. Now what I think this graph also implies is that, scaling computation only horizontally is just one way of extending this graph Right? We could also just make our models deeper. Also left in some fun models that trained over the last months where, like, none of the models are hundreds of layers deep or 200 layers deep. So there's a second access of compute scaling. We can't we can scale, of course, always the sequence dimension But these kind of problems suggest that we can also scale in the depth dimension. And how would we do that? So And, why would we do that? And how is it different? Right? Think it's often interesting to think about that. We scale in a depth dimension. As a as a form of continuous reasoning or a form of latent reasoning meaning that we put a part of this reasoning chain that we used to do in the SQL dimension, we used to verbalize this reasoning. We put a part of it into the depth of the model to make it more efficient. So how would we do that and what's what are approaches to that? So what I'll put I'll talk about today recurring in-depth. And what that means really is that it's a model that just has a recurrence where it has a repetition. In the depth of its layers. So it has some layers or maybe all of its layers and these layers are repeated means that you can make the model very deep if you repeat the layers a lot of times. Or you can make it very shallow if you repeat them only once. Right? In this way, you can sort like adaptively change the depth of the model and you can make very deep models that you couldn't otherwise really handle terms of number of parameters. Right? Because in a standard transformer, if we make it very deep, we have a lot of layers. So we actually have to either make it very thin which is very bad for optimization, or we have to make the models very, very large, it's also hard to handle for efficient reasoning for many inference domains. So just having a smaller subset of layers that we repeat seems to be very promising. But and what's also interesting about this is that this is really like such a foundational concept of machine learning. Right? Like, you may have seen this idea of a current in-depth universe transformers. Or you might have seen this idea, right, as like loop transformers or as representation recycling like an alpha fold or as an end to the computation time from the before transformers era as equilibrium models, as diffusion models, as the infinite refinement, implicit neural nets. Or maybe just even older Not sure who's in the room. I say, really it's a type of hot field model sometimes. But why why do we keep on going back to recurrence? Right? Why do we keep on making our neural networks to be recurrent? And I'm gonna bring up a few interesting motivations that I think may be clarified from different angles why we think this is interesting. So So the first motivation, the very classical one in our I'll show you a bit brief about it, and you don't have to suffer through a lot of neuroscience. But the classical the classical motivation for recurrence in neural networks is neuroscience. It's the idea that we have recurrent firing patterns in the brain it don't just use every neuron once in the brain. Right? And, there's even like from, like, if you look at this, like, from more more like a zoomed out pops up perspective, right, you can probably heard about brainwaves. So the idea that you have, like, different patterns in the brain. If someone uses this paper, this is a paper from a few years ago that shows these like metastable patterns. Here in the cortex. And this is interesting. Right? Because we have these different frequencies and this is like a classical, like, classical approach to motivating recurrence is from neuroscience. But, let's skip that. A more recent approach to motivating recurrence is the idea of universality. Especially from this paper, also aptly named universal transformers from about 200,018. Right. And this is just a classical transformer like the one that you probably still learn for interviews. And don't probably don't use anymore. It's just both blocks, the encoder blocks and the decoder blocks are both recurrent. And why did they build it like this? Right? If you look into the appendix of this paper, they actually have a very short proof section. That this actually is the only architecture that they think that will give you a form in their, like, like, there's like hand wavy, of proving this. This is the only architecture that will give you a universal computation. Because you can basically write as we come back to before, you can't represent all simply in this you can't represent all symbols. You have a fixed number of computational steps. If you have a model where you can just put in more computational steps, by which the human depth then you can represent some form of Turing machine that you couldn't do finite precision in a transformer. So here's sort of an argument that you need recurrence to implement unit facility. And, has been made a bit more precise in recent years. For example, there's a paper really like from the spring called A Little Depth Goes A Long Way, The Power of Love Depth Transforms. And what they show in this paper is that if you have certain forms of complexity classes, going to that too much in this talk, but basically transformers are in TC zero which is a lower complexity class. And if you want to solve harder things, you want to solve TC one, is for example, is regular languages. And, like, the can be a regular language is just always like a lot of bracket resolving. I think that's a reasonable languages are. Wanna do a lot of packet resolving, you can train a very wide model which I have here in so you have a very wide and very fixed step deep model. Which I have in blue here. Your scaling is pretty bad. You have to make a very deep model to have the model learn even a certain small subset of vocal languages. And you can also use chain verbalize general font And with verbalize chain of thought, you have a linear scaling But if you scale in-depth, you actually have an architecture where it's only logarithmic. So you only need a little amount of extra layers have a much broader complexity class that you can use with your model. So this is so these papers are both arguments towards, from the computation from universality to for why you would want to use recurrence. But maybe the part three that I like also a lot is part based on deep thinking literature. And what this means is, it's really a question of, like, if we so we often we wanna learn patterns. Like, a lot of custom machine learning is about learning patterns, learning associations. But what if we really wanna learn algorithms? And more explicitly, what if we really want to learn iterative algorithms? And we wanna make it really make it very hard for the model to memorize? And, there's been a lot of great work doing this on, like, smaller setups. Example, one that I've even illuminating for this is this example of maze finding. So these are all like tiny little incidents. They're recurrent. And these models are trained to just solve this maze here. Right? They Get the maze definition in the middle. And they have to produce a path in the bottom. It's a very toy task. Right? And then, people in this paper here, they train these maze models on mazes of any sizes between like nine and thirteen pixels, right, so like kind of the size that's here on the on the slides. Right? These are tiny mazes. And it's just a recurrent model. It trains a lot in these mazes. But why do they do that? Right? They do that because they want the model to not just pattern match to how a maze social looks like, but they want the model to learn the algorithm that solves MACE finding. So they find in these papers is that, if these models are recurrent, then the model actually can learn the algorithm. And then after the model has learned the algorithm, at test time, it can be deployed to solve harder problems that it saw during pretraining. Or I guess back then training. Right? Like, this is a much longer maze. But because the model just has learned the algorithm, here, because it has learned paralyzed back end filling, The model can just solve any maze of an arbitrary hardness here, where hardness here now we define this by size. Let's go even further. Right? Like, there's a an appendix in the paper where it's an massive maze of, like, a thousand by a thousand pixels. And I think there's a big promise here from depth occurrence that maybe this is a way that we can really learn algorithms And if we can get our models to learn algorithms, then we can get them to learn solutions and generalizable not just between different domains, also between different hardness levels. Because the model can run can learn the algorithm on easy problems and ideally just execute the same algorithm on a much harder problem. And like just in the same way that maybe all of us We've learned addition. And we haven't learned addition for, like, let's say, like, 2,000 digit numbers. But all of you could sit down with a pen and paper in enough time and I could motivate you to do a 2,000 digit addition. Because you know the algorithm for addition, you can just slowly execute it. And I think that's very promising that we're gonna want is to do something like that. So how do we do that? And what's one concrete approach towards that? I And, let me take a sip sip of a slide. So what's one approach towards that? So one approach that we talked about yesterday in the post session a bit over the year, so that's been a lot of work on this for many other great people, is to use these sort of recurrent transformers, these universal transformers, and to, like, bring them into modern setups train them with modern data, see what they would do. And there's a few things that you can build into these that like an upgrade from classical universal thermostats that make us more stable because there's been a lot of great work on the small scale models in the last five years and what you would need to make this recurrence more stable. There's a few interesting things here. So, in this particular architecture, have three kinds of blocks. And each of these blocks is made of a number of layers. And that's interesting because, so when you in the original Universal Thomas, you just had one layer that you've been recurring. But, we find that you often you want to learn a more complex nonlinear operator. And you really want to have depth in your model. So here, actually, these all of these recurrent blocks themselves are deeper. They are, like, here are four layers. They can learn them more complicated from the operator. And then what we also have here is that classically in something like universal, almost you have a sort of forgetting problem. Where, if the model recurs for a long amount of time, then input data is very far away from the end. Right? Because you injected the input data only at the beginning, of the recurrence. And you run the recurrence for a long time, And if you think about this like a dynamical system, then this is a bit complicated because you feed in the input data only at the beginning or so only at the boundary of them. Of the model. And what's much more stable actually it's not an accident, but this looks much more like gradient descent or it looks much more like a diffusion model, that you have a sort of import injection where in every step of this recurrence, so we have this recurrent block of green, right, which you repeat, every step of the recurrence, you condition on the input embeddings. And the second part of this thing here is that there's there's an intralation of the state here. Right, so there's a state variable. It's called s. And we keep on refining the state variable by running this recurrence. And it makes a lot of sense based on prior literature use actually a random initialization for the state variable. This means that we train a model that from arbitrary initial states, it just learns to refine these states. And it's a very desirable property because we want the model to learn iterative refinement. Right? We want to have a prior towards iterative aspects. And then there's a number of ways to train these. We found to be really scalable is to just train them with the simplest thing, which is just to have migration end to end And here, this is actually a truncated back propagation where you take your model doing training every time you see data, you just randomize the number of you'll do. And then just back up. In this way, the model sort of has to handle all recurrences between here and 200. And we train a model that does not that doesn't just work for if you repeat it four times or repeat it 16 times. But it works for any number of occurrences. Is important for efficiency. There's a number of different alternatives that you could do here where there's a lot of work in this field and there's a lot of different models and they're differ a lot on based on what objective they use. To train these models. And you can ask me all that later. And, does this actually work? Am I wasting your time with talking about all of this stuff? Interestingly, this does work. And, this leads you to models that have aside from their actual performance, have interesting properties. So what these models learn, if you just train them just at large scales of like a few billion parameters, maybe a trillion tokens just on web text. Is that they learn different behaviors on different kinds of data. But if you remember during training, the model just saw randomized occurrences. So the model never had any the model didn't we didn't do more computers during training on harder problems. We just did random compute everywhere. But the test server we see is that the model actually converges quicker on easier problems. Like, and especially on problems like, like knowledge based problems here, like ARC challenge as a knowledge base base benchmark. These benchmarks are models of, like, saturates quickly, either the model knows the effect or the model does not know the effect. And then more computation doesn't help you to not more affect more. But on other tasks like here on GSM at K, the model actually can use and can leverage an iterative algorithm to solve the task. And we see that it doesn't learn so just from data. Which I think is pretty interesting. Alright. Here's a few more examples where, like, all the interests in the top really are a bit more knowledge heavy where the model just needs a few recurrences because either it knows how to solve the benchmark question, like from SEQ to the science fact. Question, or it doesn't know the question. But on benchmarks where the model can leverage computation, it seems to have learned to do so. It's pretty interesting. And especially if you compare this to like a baseline trained with the same amount of parameters, the same set up, same data, then these two models are often similar. It's a knowledge heavy benchmark. But on task like g s m eight k, quorum model is five times better than the baseline. And what is it doing? If you look at this, we see that the model really is learning some sort of recurrence. That depends on the number of on how hard the token is. So here, what you see is that I put down a sentence. And I show you the convergence of this latent state. So I show you like how quickly these latent states stop moving because we have a recurrence. Right? So we have an initial latent state. Random, and then we run the recurrence. And we run the recurrence, And then some point, this model may converge. And this is interestingly very token dependent because the And this is surprising because we didn't train for this behavior. Right? We just training, we just took a batch of data and we said, okay. Do 17 recurrence on this whole batch. But now at test time, we see actually the model has learned just from scale and just from training it. That on easier tokens, it can converge quicker. And on harder tokens, it can converge slower. But here, for example, you see especially that for the colon, model converges much slower because often, like, colons or commas are occasions where you have to figure out really what comes next. You have to plan a bit ahead. Versus, if you write the first word of methodical, kind of obvious that you wanna finish the word methodical, so you don't need a lot of computer to finish that. Right. And these kind of like graphs to me like show love promise of recurrence and, especially, also, promise of activity and compute. Right? Because it's still kind of crazy that it's the year 2025. We have these super large models of, like, even the smallest ones that you probably are using is, like, are 7,000,000,000 parameters or something, 8,000,000,000 parameters. On every token, the model produces it runs all APLIM parameters. No matter if the token is the answer to the real hypothesis or not. Or the token is just you had for dinner today and how are you doing, and thank you for your answer. And yes, you're absolutely right. The model uses the same amount of compute all these tokens. And maybe graphs like this indicate that this maybe is not necessary. And you see more of a slide of how these trajectories work. We're doing this interesting because, this is sort of like a top down view. Because you have the you have PCA interactions of the latent state on the x and y dimension. And you have the sequence now on the z direction. So, we go from the bottom up This is the sequence. And what you see here is that these, all these latent states, because you do the recurrence by every token, all these latent states, they converge to different points but they because this is a still a transformer, right, and they attend to each other, They form these structures as you go along the sequence. And maybe we can zoom in in few of these. Because what is interesting here is that this model, is not trained to always be convergent. Right? There's some some earlier work on equilibrium models or on fixed point models where we really want our recurrent models to always converge to a fixed point. But because this model is just trained at scale with vectoration, this is not a necessary requirement. And we find that the model often still converges to a fixed point. That's a very same implementation that you can do in in have a dynamical system. Right? So here, we see that for some tokens, I'm showing you the first six BCAA directions. And you see how this model just converges to a fixed point. What I'm showing here really is that these are the different states that the model runs through. As it answers what should come next for this token. So the darkest color is the first state and the brightest color is the last state. And these all just converge to the middle. Interestingly, on a number of hard tokens, number of fin math and in reasoning, The model learns all kinds of dynamical behaviors. Where we also see that for example here on this token, really is number three. The model has learned to find this orbit, and it traces out this orbit in latent space. And, we're still not entirely sure how to mix in these orbits. But, there seems to be a way to implement, for example, like, some some modular arithmetic or some functions where you have a set of of these interlocking orbits. Way you can implement more complicated functions in latent space, right, in your general system. And I think the neuroscience people are pretty excited about this. Because we have a sort of you know, I have a frequency. But I think for us, it's just interesting that the model does the model just learns from data, and appropriate behavior for this dynamical system. Of the recurrence. Its state states. So also other tests to this stuff like slider, where the model is learned to just like slowly slide into one direction. Which we think often as relates to counting. Operations on the model. What I take with some of his pleasures It's interesting there's like so much complexity that we just can learn in these recurrent models. Just from pretraining. And we have these different terminal behaviors that emerge just from scale. It's it's hard to analyze these and we require more tools from representation analysis, more probing, really understand what's going on here. Because the model can collect more functions in one part of this recurrence. And it's a bit more challenging to analyze. But I also want to talk to you about here in this workshop is you aside from all this about recurrence, what are the benefits for inference? And why is recurrence very efficient? So I'm gonna give you a few examples for why I think it's actually a surprisingly efficient architecture. You can do interesting things once you have this recurrence. So one thing that's very straightforward is you can just do self parallel decoding. Classically, in speculative coding, you have a draft model. You have to load the draft model. It's a different model. You maybe have has a have to hope it has a SIM tokenizer. Have to do all this engineering to have the draft model make sense and speculate the next four eight sixteen tokens. Right? You have this recurrent depth model, you can just use itself as speculator. Because you can just you can just run the model with only a few number of iterations to speculate. And then you verify with many iterations. And you can even reuse the same computation that you did already just to continue running on them. Very simple. What's also interesting is that, if you think about how you do inference with these kind of recurrent depth transformers, and if you do inference naively, you'll find that the k v state you have a k v state every time you run the recurrence, That the k v state grows with number of recurrences. And that sounds kinda bad because then, that means if we do a lot of compute, we have to also allocate a lot of memory to store all our kv states. And what we find in the paper is actually that's just not necessary. What we found in the paper is actually that's just not necessary. Because all the recurrence are in the same space. Right? It's just the same recurrent block that's repeated. That means that all the k v states match to each other. And what we can just do is we can just throw away all the early k v states. And for every token in the sequence, we just store the last k v state. No matter whether this was recurrence four or recurrence 137, store the last k v state. And then once we run that for occurrence again on a on a new token, we still attend to all prior k v states no matter what their position was. So we always just default to we always default to, attending to the most computed state. And it's pretty interesting that it works. Even works a bit better if you have only a few occurrences. And I've shown you this graph before. Here's another one of them. Just repeating this. What this graph also implies is that we can just use, really, we can we can use per token exits. Right? We can just look at this and we can see when our recurrence converges. And once it converges, or it stops changing in some way that we have defined. We exit. And in this way, we can we can have different compute per token. Which leads us to more efficient inference. Now, some of you can see this here. This is some are MLM new categories. In this example, you see that on some MLM categories, like for example here on on easier math, you see that the model uses maybe more like a median of eight or nine occurrences. And if it's a harder problem, maybe it's like it's logical fallacies and the middle right. The model even uses a middle median of like about 20 recurrences. To hit the same convergence threshold. Also, what what was also interesting about been up to very recently with a bunch of collaborators, so also in the audience, is that, we classically, recurrence also is very the recurrence the model is, of course, very sequential, so it's also very slow. So what's pretty interesting is that, classically, you run inference. Right? So now I've I'm always gonna be drawing these two d graphs. You have the token positions, we have the sequence on the x axis. And you have the recurrence on the y axis. Right? And classically, pointer. Don't know a pointer. Like classically, if you just inference model, you would first be talking about this one. And then you would run the recurrence one here in this example. One, two, three, four, five, six, seven. Six a six times. And then you go to the next token, you're under recurrence. And this is where you're talking at with very flop efficient. But you can always see this is not very palisable. And it's very slow to run inference at this scale. Especially if we have very fast GPUs. Just very slow setups. So there's a lot of exciting literature in the fusion models. And there's some interesting connections between these recurrent models and diffusion models. That we can just take samples from diffusion and apply similar principles here And in diffusion, this is called a diffusion forcing sampler. But what it just means is just entirely encapsulated in this figure. Because what this means is that instead of normally, they're just turning the currents like a a like a u shape, you know, we go we have to, like, always run the whole recurrent before we do the next token. Here, we just immediately decode the next word. After we do one recurrent step, we decode the next word. And now we have two states that are uncapped data like. Semi finished So we run the records on both states. And then we get the third one. We run it again. So we get a fourth state. Right? And so we we run this wave over the computation. Also, I have another picture here. What this picture again shows you is that there's the sequence. On the x axis. And there's the compute steps on the y axis. And what you see here is there's almost like a wave of quotation running over the model like a diffusion model. Right? And you see that like at some point the tokens are frozen. That's why these are like just virtual lines and they stop changing. But there's a region where they are the model is still trying different different tokens and it's it's like it's generating from motor positions at once. So it's through the recurrence, we get a bit of a step we get a few steps away from the autorogas if need nature of the model. So we can run this sort of, like, parallelized inference. And this actually does make the models about four times faster. Even on domains like GSMED k or math where we really need to have the model be sequential sometimes. Then we can still use, this parallelization on parts where this is just fine. Like, we have a sampler where it just figured out based on convergence Can I advance the sample or not? Actually see this here in the in the middle of this crazy chart because there there's a part where it just it just goes down vertically. On the right hand side. Right. Well, that that that just means that there's like a a certain amount of uncertainty over the sequence. And the model is not the sampler is not advancing the sequence, not making this this uncertain region wider, before it has resolved this uncertain part. And after it has resolved this uncertain part, again, the the sequence can grow again. And the model keeps on running generating. So in summary, aside from all maybe our hopes and maybe interests in recurrent models just for reasoning, I think it's interesting that they also simplify a bunch of things that are kind of tricky in inference. Like speculative coding, like QVC sharing, adaptive commutation and parallelization. Let me do a few summary talk points. So why are we interested in recurrent and not just do genophage? We can of course always do chain of thought. As we do normally, as we do right now. It's a it's a pretty powerful paradigm and I don't expect it to go away at all. Recurrence is also a super interesting paradigm to maybe be complementary to that. And we just see that just scaling it and just applying it with based on all literature, it seems to scale surprisingly far. There's still a big question in this field of how we get from these kind of occurrences to something that you've seen more for the mazes. Where we can get some sort of arbitrary explanation, extrapolation compute. Where these aren't just just sigmoids, but we can actually run the recurrence for longer and longer and then continue to improve performance on some benchmark task. So the question of, like, how would you set up this objective? How would you set up the architecture of this model? To encourage the sort of extrapolation from easy to hard? So Also, some fun practical questions on how to post train these. And I think this is maybe a complementary part of scaling model performance and verbal constriction of thought. Right? That we don't have to just have two access to scale compute. So, model scale, model size and verbal chain of thought. Maybe a third axis in which we can also scale compute and just have these three axes together. And with this, I'm done and I think there is time for questions maybe. Thank you so much for your time. Hi. Thank you so much for this really super interesting and fascinating presentation. I really appreciated it. I have two rather short questions. The first question is, so you talked about these, recurrent algorithms. Being able to learn some sort of a linear repeatedly. And and and now I'm wondering, how how important is it that it was trained for that particular linear update algorithm Or or versus versus how could is it generalizing these sort of algorithms? So say for instance, you have your pre your your your training data, for this whole, I'm I'm I'm architecture. Does it then generalize across, like, very wide set, of of of of of of such linear recurrent algorithms. That you can see if, I don't know, in benchmarks or so without being explicitly trained for this. Like, how how how well that does generalize? The first question. Yeah. Let me just first because I'll forget the other one. Otherwise. So, seems pretty generalizable. Also, but technically this is not a linear this is not a linear recurrence. So the operation is right? Like, if you go back here, So, let me go back a bit. Right. Because because this all these blocks are in nonlinear operations, repeating these nonlinear blocks this is not a linear recurrence. Right? It's a nonlinear recurrence. But, this is just nitpicking. And, Vitu, your more harder question? Yep. No. No. Yep. No. No. But it's here. These benchmarks, it seems to generalize to And like like we For example, we have this like this mastermind benchmark, which is just lot it's a logic puzzle from the sixties or seventies. And the model has never seen that. It seems to still use recurrent in a good way for that master mind benchmark I find very surprising. Actually. Nice. Your second question. Nice. Yeah. And the second question is very pragmatic in the sense of of let's say, I have an amazing benchmark for for exactly this kind of problem. And I would want to try how good such a recurrent model is working. Are there any kind of pretrained models that I could just get and then and then have a good idea? Do you have to train everything here from scratch, basically, if I want to you could just download the model I talked about in this talk. Sorry? You can just download the model I talked about in this talk. Okay. Yeah. Yeah. It also has like like you can just use it for inference. It has like a bunch of code for these different like specific decoding or like kvCash sharing in a VLM plugin. Should work. Otherwise, just reach out. Nice. Thanks. Thanks so much. Hi. Yeah. I have one question. So about skill renewal, you are talking about deeps and impact of deeps, how how it impact the modeling model. I'm wondering, do you observe some of the the law of scaling for the deeps, like how much deep we should skill and the lore will this law keep going, if we can scale much much more deeper, all the recently performance can be also be improved with the Deepu test or it has the end that's killing the escalating are going to end. So I as the deep for the deep scaling for real time model also going to end. Yeah. That's my question. Yeah. Yeah. There there some papers where, the people treasurer drive adaptive skilling off of big steps and trauma. But it's pretty high rate because it's it's pretty domain specific. If you go back to the original Kaplan paper, they they said, okay, depth doesn't really matter. But, it's also because the back in the Kaplan paper, they really cared a lot about complexity. And more about generic model points about about memorization for these small transforms. And there, if you want to know a lot of facts, it just doesn't help you to be deep. Deep. And then maybe there's just been a few more more recent studies where people show that, okay. Depth could be good. If you really wanna do reasoning, if you wanna solve math problems. But I don't think there's been like a super comprehensive study what exactly the scaling curve would be. But we we have these like we have these theoretical results, but many real problems are not regular languages, unfortunately. And like, maybe real maybe many real problems are a bit of a mix of knowledge some reasoning. So it's not such to me as like comprehensively like what the scaling coefficient should be that we should rely on our practice. I think that's future work hopefully for someone in the Yeah. Thanks. Yeah. Peace. Thank you for the great talk. As far as you understand, you are doing recurrent computation for each and every token. Is it possible to internalize the token generation? Into the model representations? I think it will be interesting as well. Like here, we're really bound to something that looks like a causal and somber but if you squint at it, because we can scale that training up and train it by next token prediction. But if you figure out how to, like, have different training objectives, you can also have the recurrence over multiple tokens. Right? Towards a classical maybe like recurrent t five or something. I think it would be interesting as well. Okay. And one final question. You talked about, generalizing this to an objective, but can allow generalization to more much more infinite depth. Yeah. Can you talk about, like, what are the major bottle bottlenecks that you face if you simply come up with the objective like that? Right? So so for this model we always see this as sort of the sigmoid curves. Where it uses compute for a while. And then it does the performance at some point saturates. And I think it's a very big question like why it saturates. You could think about this as like, maybe this is just a model has learned to do addition perfectly. And between iterations one and three two, it runs the addition. And then here, because this is just a MKW, the multilayer has to, like, do some inference. Multilayer has to figure out the benchmark question. Maybe the model figured out the benchmark question wrong. And it just does the wrong addition for future steps. And then it gets that's why deployments is not peaked already, or not at at a ceiling yet. Or it could be that this model has learned this addition algorithm, but has learned the algorithms in exactly. And if it repeats the algorithm, repeats the algorithm, there's a certain error rate that the model can't get around yet. Because we we're here, so it's a bit funny because we have these discrete algorithms like addition, for integers, and we implement them in continuous models. And there can be a certain amount of error on how the model implements this algorithm, It's not so clear whether it's more of the first, whether model makes some sense premise and then just runs a lot of computation in the wrong direction. Or the model just runs competition but in exactly. Okay. Thank you. Sure. Great talk. Very interesting idea. My my question, the the kvCash, the recurrent And and you mentioned, performance is preserved if you sort of truncate the cache and only keep the last step. They're in here so states. I'm just wondering how do you avoid, like, a context rift in that scenario where you might have a you know, a complicated system prompt if you can only look so far back, how do you keep keep it on task? So, yeah. I think, yeah, I think I was I was a bit quick in that section. So it's important here that there is still one kV state Right? So, you don't really lose any it's a long system problem, you still have one k v state per token. But here's the question. You can sort of like I think about the recurrence as just having a large number of layers. Right? And you have one k v state per layer. So what we're what we're saying here is that we don't need to store the k v state for all these like fake layers. We just need to store the k v state for the last recurrence. So for the last layer in some sense. So we still have a k v state for every in the sequence. We don't lose any information. But we still like, we don't need to store these intermediate computations. Like we would in an autonomous one where we store the k v state for all layers. Thanks. Thank you. Hey. Thank you for the talk. I understand that according to my understanding, you're don't really train a model to be adaptive beyond just sampling it. Pretraining. From different lengths? And I think one interesting thing with these type of models is that you might want to kind of control this, that actually is adaptive. Have you thought about any reinforcement learning other type of things that could be done to encourage the model to train to, like, do more recurrences on more difficult tasks until you get some reward or Yeah. Things like that? So I think that that's a super that's a super interesting question. Right? Because if if you go back to the older papers like University of Thomas, this is something they specifically trained for and it's like all of this is about adaptive compute. Modules. Which are, like, parts of the model that really decide end to end, should I exit here or should I run more compute? In our experiments, we found this pretty hard to scale. But this could be like a skill issue on my part really. Right? That's like, it's pretty hard to fast to these very large models. And then because training, we really want the model to be better at hard on everything. And we have to train this ACT module, which really wants to, like, do as least amount of work as possible. So we found it really hard to do that during pre training. But I think maybe it's promising to this in a post training stage where we, like, now the model is trained, we can then train an exit condition later, maybe Or there is a better way to do it during pre training? It's it's a min max game somehow, There should be a very interesting solution to that. Yeah. I was also thinking more about post training. Thanks. Sort of think we can talk. I'm gonna read this because I wrote the sound. Do you think recurrent, depth health transformers approximate reasoning, for specialized algorithms. In a more compute efficient way. Have we have you guys tried some experiments in that area? Because that's very interesting because we started with a puzzle and the maze, That was really amazing to see that. What if we can do it for very advanced task and very advanced algorithms? Yep. Think we it's really like we really struggle with like yeah, probably can't give you like a concrete answer, Intuitively, the the this seems sensible and the model should have this capability, but then we find this in theory, in this, like, theory part, Oh, let me just call all the way there. Right? Yep. Here, this is true. Right? Like, here, it is more efficient to learn regular languages by recurring in-depth than making an order wider using verbal chain of thought, Well, this works in practice, I think this remains to be seen yet because it's a very often it's a very hard apples comparison. So we train a lot of these recurrent walls, and they're kinda small. And at a small scale, it's pretty hard to make good verbal Chernoffov models if you're used to, like, I don't know, five is in the clouds, then you go back to the open source models and get them to do real translation. It can be a bit dicey sometimes. I think there's a lot of interesting work to be done here to really figure out how these in practice, and I don't know yet. Okay. Thank you. Hi. Thanks for the great talk. This is super interesting. Would be interested to understand bit a little bit better, like, the differences between the different dimensions in which you can scale. So, like, one would be the recurrence, the other one would be the autoregressive. Thing. One difference that I can see is that, like, for the autoregressive dimension, you sample. So you actually, like, committing to one specific trace where here, the recurrence basically like keep converging to something. Yeah. Could compare the two? I think that that that's a very good assessment of these different trade offs. Right? I think, there's this common intuition in the community that running the recurrence in-depth, It's a con it's a very large whatever, had a major hidden spade. States. So, we can the model could learn to implement many algorithms in parallel. But since it makes sense to actually symmetry break all of this and to to pick a certain and then go down that route? So, that's maybe these two axes are complementary. Have you tried it out to use both? Like, for demo, this example here, like, this like these models still run run verbal chain of thought. Like for example, like here, Like Tesaski's GSM case Jyn'Thought. Which just means that this model actually is running like per token, depth. It still writes out the answer in like oh, j s k. I can like three ampersand triples. It does both here. Actually, this example And it's it's very natural for the model side because like, you can always be more better by still writing out the answer. And here, that is actually skilled in both dimensions. Thank you. Yeah. I think I'll get away from now, and we can see what comes next. And We we are saying, to us, professor for a really interesting talk, and we will have sharp morning break and next week. Plant center will come here at 10AM. It's Amin Kuhan, professor from Yale University Yes. And thanks everyone and let's restart at 10AM. Testing. Our next speaker will start at 10AM, and we are everyone to like, go back to the sea and waiting for professor Amir Khan's remarkable talk. Hi, everyone. Now we honored to invite professor Amin Ke Huang. Yeah. He's professor in the computer science department in the Yale University, and he did the natural language processing app. And today, he will give us a very remarkable talk about efficient listening. As welcome, professor Amy Kehan, Thank you for the introduction and for having me. So today, I'm excited to tell you a little bit about, frameworks for better understanding evaluation and scientific reasoning. It's kind of a a different flavor of a talk in in terms of, like, efficiency. I'm not gonna talk about new methods to improve efficiency. What I'm gonna tell you how we can efficiently evaluate reasoning and give you some insights on where more reasoning could help and where it might not. Help. So to motivate this a little bit, I I think I had this venue, everyone knows that reasoning models rely on long chain of thoughts or test on compute to do very well. And, we see that this type of inference really pushes the boundaries of, performance across many different tasks. However, the the issue is, like, this type of, inference time compute is also making evaluations much more expensive. So many times when we are developing new models, if you want to evaluate them, it becomes very costly as well. The other issue is about the alignment evaluation. And here by alignment, I mean, like, the general alignment that we want models in nonverifiable tasks to kind of follow human as in their output. And compared with verifiable tasks like math and, like, coding, evaluations of such systems is, challenge. So, in this, talk, I'm going to talk about two challenges. One is just this evaluation in nonverifiable tasks, and second is, about scientific problem solving. The first one, we want to know if models, can follow human preferences and if we can do this type of evaluations more efficiently, And in the second one, I'm going to tell you about more, efficient benchmarking methods and where actually inference time compute can help in this sub of complex tasks. So specifically, first, I'm going to tell you about, evaluating LLM alignment through evaluation of LLMs as judges. And the the insight that I wanna highlight is that we can reuse computation for better and more efficient, benchmarks. And second, I'm going to tell you how we can design frameworks to know where reasoning actually is limited and how we can actually improve. That. So the first word is, on evaluating the element alignment. By evaluating the elements. So as judges, actually, we presented it, two days ago here. At Nuurex. So basically, LMM evaluation in nonverifiable tests looks like this. You have a question, but it doesn't have, like, objective correct. Answer, but then you want to basically compare models and how good or bad they they follow user preferences. And usually, how people evaluate this is just, you collect user votes or human votes, and then you aggregate these votes into a leaderboard. The famous one is LM Arena, that every new model is evaluated. On. And currently, I just took this today, like, Gemini three Pro is top of this leaderboard. However, because this is very, expensive to do, like and then you cannot just always collect human feedback. What people do is just automating this process by replacing the human waters with, elements. And this is actually done in many benchmarks in alignment evaluations, such as El Pollock And then you can do the same process, have the LMM judge to just you know, specify the preferences. You aggregate the results and get to a leaderboard such as this one or in a hard that you see. Here. And so how does this work exactly in practice? It's like you have a bunch of models that you want to evaluate, I'm listing four of them here. A Cohen, Lamma, Alma, and Gemma. And then you have a, set of input instructions. You you run these input instructions through your models, get the outputs, and then ask an LMM based evaluator to give you this scores or do some sort of pairwise comparison. And then you aggregate these rankings and its scores and you get to, a final ranking of which model is better than. Other model. So, LLMs can be also evaluated as a judge And, this means that we want to know how they perform if they're used as an evaluator. And, this is typically done in meta evaluation as studies. The setup is like this. You have still a set of prompts, but then you also collect a set of models. These are just used to create the pool of instances that you want to evaluate and then you usually do a pairwise comparisons between samples of these models. And then, basically, you collect, ground truth votes for for which ones is preferred. Can do this by human evaluators or as is typically done, you use a strong model, or LLM to to use as a preference oracle or as a ground truth. Then let's say, you want to know if these models, like Quinellama or or more good as judges. What you can do is just using them as judges, to also collect their votes here, and then, just compare how they correlate with the ground truth, which, which could be a LMM evaluated or human. Evaluated. And then based on this, we can get rankings again. And then this ranking tells us which one of these models, Cohen Lam, are almost better than better at, judging, the outputs of other LMM. LMS. So the key question we're asking here is, like, if an LMM is good at generation, is it also good at evaluation? So we call this generator, evaluator consist short, which is a correlation of performance, which between evaluation and, generation. And the setup is exactly like I described before. I just put both of them in this figure below. And, it is a bit different than like, other types of consistencies people have studied. Like like, there there are some works that looked into if an LLM prefers its own output because it generates, the same output itself. But this is like looking in ranking evaluation and then seeing if rankings are correlated with between generation and evaluation. So and you might ask, okay. Why would I care about this? So the the you if we find that there's high then we can use LLMs, as judges and evaluate them as judges, and then that tells us how good they are in generation as well. So it helps us approximate general alignment events. Evaluation. So let let's see how we can do this. So basically, we started this by, like, using a control setting, using a strong LLM like GPT four o at the time, as a preference oracle for both the generation and evaluation oracle. And then we use, Alpaca eval and Arena Heart as input instructions. These are instructions in general tasks, that are instruction following and alignment, and people use that for eval. Then we use 15 LLMs to evaluate, and then we for the generator ranking, we basically gave the model, both pairs like we also repair reversed this to eliminate the position bias. For the evaluator ranking, we get the, evaluation task instances. By pairwise comparisons of LLMs that we want to evaluate, versus a baseline LLM. This could be something like a GPT. Four. And then for evaluation metric, we use a basically, a correlation, or inner agreement between the preference oracle and our LLMs judges. And we do a step of consistency filtering and this this, this is important. I will, I will touch on this in the next slide. So these are the results, On left, you can see alpaca eval. On right, you can see areina heart. So this is kind of dataset dependent, but in both cases, we see high correlation between generation and evaluation performance of models. And, especially for the RNA heart, we see very high correlation. I mentioned about consistency filtering, and this is actually pretty we the oracle is very confident and like which one is correct. To order to limit ourselves to those type of instructions, we basically reverse it outputs and then saw if the model flips its decision with got rid of that instance. And that is important. As you can see here, if we don't do this filtering, we will see that the performance or, like, the correlation between generation and evaluation is, lower and actually significantly lower. So again, like, you might say, why would I care about this? We call this, kind of framework, align eval. Which, helps us evaluate alignment by evaluating LLMs as judges. Which, helps us approximate the generator ranking. So bay basically, the setup, I I just described was but for more details, we use RNA hard at instructions. Set. And use GPT four o or CLOS 2.7 Sonnet as the preference oracle. Were the best models at the time that we were conducting the experiments. So, basically, recall that in generation evaluation, for every new model that you get, you need to generate a lot of outputs and then you need to send them to an LMM based evaluator like a GPT 4.5 or GPT or or clot point five or something to get the scores. And, because whenever you have a new model or new check point, you need to do this LMM based evaluation. And you can see that this this can be kind of expensive. But in our paradigm, this is different. You only run the LMM based evaluator once. This is like just to get the gold, labels here. And then you have that and you save that and then you don't need to for any new model that you want to evaluate, you don't need to send it through API to evaluate it. So as you can see here, like, the cost of align eval, the last row, is zero API cost. Whereas, like, for the other benchmarks, you can see that because they use LLM judges, you need to pay API costs to evaluate. It. And then, you you might ask, okay. $10 is not damaged. But but assume that you're, like, training a new RL model, like a new DPU or and you're evaluating your checkpoints every 100 steps or so. Then this adds up very quickly. So what you're arguing is that you can use a line eval and then you don't pay that cost. And then it tells you pretty well if your model is good or not. So, how do we know if this is reliable? We do correlation analysis. We LMR in our ranking, the self controlled version. This is similar to what other people do. We use 23 LMMs to rank, and then, the compared with, few different benchmarks as base. So here you can see the correlation with l m arina, which is like human votes. And you can see that, like, our, align eval achieves pretty good correlations. It's it's a not as high as the judge based bench. Like a GPT four o as a judge or, in a hard style control, but it is very competitive Compared with judge free benchmarks, we get higher correlation. So, for example, there's mixed eval, which is another benchmark that is created from objective task. And here we can see that, the output performing. That. There's this benchmark if you're familiar with it called If Eval. Which is basically, followed instruction following evaluation through verifiable, checks, such as, regexes or rules based checks or so on. Can see that if eval actually gets high correlation with LMR in our end, So we were looking at into this, and we found that, like, basic a line eval, which is our method, evaluates content, whereas, like, if eval evaluates instruction adherence, So the this seems like a complementary thing. So we thought, okay. Maybe we can combine them. We combine them and call this aligning valve plus and here we can like, this last two columns, can see that we get the highest correlation you do this. So, basically, in general, I want to argue that large you can evaluate alignment in nonverifiable tasks using evaluating LLMs as evaluators or as judges. And the implication on this is that it's much more efficient, because you can reuse the model's outputs and you only need to run the preference oracle once. And this becomes very important, especially if you are training a lot of, like, reasoning models. As it's become, very expensive to evaluate. And then we observe very high correlation with human ranking. Beams. So, yeah, this this is, the first work, and then the second work, I'm going to, shift gears a little bit and tell you a little bit about, scientific problem solving in LLMs and their reasoning can help and their knowledge appraisal look role. So the first thing we were, looking into in the work was, like, there's lots of benchmarks that people have created for, evaluating scientific problem solving. These are things like GPQA, LabBench, or MMLU Pro, things like that. That. However, like, if you want to evaluate LLMs very I it it's it becomes very expensive. Like, for example, if you get a broad set of these benchmarks, for for even a Gemini 2.5 model, if you want to evaluate that, you're looking into about 300, $136,100 dollars, of expense. Just around one model. You can imagine that, like, if you have a lot of more models than the, cost accumulate very quick. So we wanted to see how we can still do broad, benchmarking across a lot of tasks, but make it efficient. So we manually looked into a lot of these datasets and, only looked at those that are actually reasoning intensive. Then from each side, they set the sample, a smaller set of instances. Without hurting the, the confidence of evaluation. So, this basically allows us to cut the evaluation cost to one one third, to a half And then this doesn't reduce the evaluation confidence at all. So we call this sires. It's not like a new, like a newly annotated benchmark. It's just an effort to unify benchmarks And it's it is a spanning across multiple tasks, including general science reasoning, physics, focus, general instruction following for science, and, some broader subject like exam. So here are the performance. I I this is when you're looking into performance, you should always, like, consider the cost as well because we are talking about reasoning models. You can see here that, like, the best performances are for, reasoning models at the time, like Gemini point five pro or three and GPT five. But then the question I've even more interested in is, like, if we increase the reasoning effort, how does the performance change? And, can high reasoning effort actually improve performance? And this is not always the case it's very interesting to look at some of these models. For example, Gemini 2.5 pro, even if you, 10 x the reasoning effort, are not getting much out of it. And the performance doesn't improve much. Whereas for some other models like like o three, increased reasoning effort actually translates into increased performance. So this benchmark actually allows us to kind of see across the board, across a lot of different how efficient is as as cost efficient and capable. Are these models? And then here's another view of the the the the benefits that you can get from these recent models. Can see that, like, ideally, you wanna see the dots to be on upper left side of this graph. And you don't see that for all models, but for o three mini or like o three in general, you better performance. When you increase the reasoning effort. And then you might ask, okay. Why do I care about holistic performance? Maybe I just take two benchmarks that everyone evaluates. On. So we did a correlation analysis, and this actually shows that you kind of need this, type of holistic evaluation to tell you, like, in general, how good your model is at reasoning in knowledge intensive and environment. Because like on some data sets, it doesn't actually correlate with other data sets. And we looked at this more closely And, what's interesting is that we found that a lot of different models are tuned towards different datasets. For example, here we found that corn three has very high benchmarked scores for side bench. Whereas some other model like DeepSeq r one has very highest scores for MMLU pro, versus the average score at from other benchmarks. So it seems like this type of holistic benchmarking can tell tell us more about general reason capabilities of model. And this is, again, the performance cost trade For some models, you gain performance by paying more. But for some models, like this green bar, I think it's Gemini model. That you don't get much, input and this is performance improvements for reasoning models for math versus non math. Instances. And, as is also, reported in other papers, we see that, vesamil usually helps math questions more than other non math questions. The the next question we wanted to actually look, in this work is, how how is the interplay between knowledge and reasoning scientific problem solving, and where are the actual bottom lines. So in order to study this, because every model is trained on different data, we wanted to have a controlled setup. So we took, a quantum five model that is a base model and not reasonably trained, And then we wanted to control for in domain knowledge training. We train it ourselves, using back reasoning training on these three datasets. Math, stem, and synthetic one. Synthetic one is a dataset that other people have collected that has a long chain of thoughts, and it is distilled. I think it is this is from deep sea color. So we wanted to see if our own training actually, replicate replicate this. State of the art. And this was just a sanity check, and we was we were saying that our check points actually can match the performance of checkpoints that, exist out there and other people have reported on. So now we wanted to see how you can, like, separate or, like, kind of study knowledge versus reasoning. We introduced this, simple framework we call it CROX or knowledge and reason exam, which allows us to investigate, knowledge the role of knowledge in large chain of reasoning. So it works like this. You have a a question, so so we want to know what are the atomic knowledge units. Or knowledge ingredients, that are useful to answer this question. So we we kind of, extract these using a strong model of deep c CARBON. Basically, we generate the full chain of thought. We ask, deep sea carbon to extract this atomic knowledge unit. We call them knowledge ingredients or k i's in short. And then then we want to solve the question, we gave the question and these KIs or knowledge ingredients to another model to see if we can utilize these knowledge ingredients and solve the question. So the first research question using this framework is, can a base model that is not reason trained actually just use these knowledge ingredients and get to the answer? And the answer is actually yes, like we we we tested this. Like, the blue bar you see here is a base model. This is not reasoning for strength. And you can see that if you just give it these knowledge ingredients, it get huge improvements on different tasks. And the the other colors are reasoning post training. Models. So this basically shows that, like, actually high quality knowledge is important for models to solve the task. There's some caveat in these results, Some models like one two point five who knows what they use for their data? But there's some times that they might have some reasoning data in their mid training. So this kind of is not like a base based model. This kind of like a lightweight version of a reasoning. Model. But the second question we wanted to see if, like, does this same ingredients actually help, reasoning models as well. So the the format is like this. Basically, we extract these KIs, we give it to, the models like these are different models, base model or math train model or a stem post. Model are both them. And you see across the board that this type of knowledge ingredients help all their models. Both post trained models and the base models. And the improvements are actually consistent those. So, basically, this shows a practical path If we want, to improve models, we we need reason models, and we need high quality knowledge, ingredients. And you might ask, okay, well, where do I get knowledge in because yeah, I don't know where to search like this. This can be just retrieval augmented system. There might be some caveats about this result. You might say, okay. Some of these improvement might like, this knowledge in defense how do you know that they are not kind of narrowing the search space for the reasoning models. So, I'm going to talk about that now. So the next question we wanted to ask is, like, does reasoning focus post training help the model better surface parametric knowledge? So, to study this, we have this set of extracted knowledge ingredients again. But then we, we post train different models based on different data and we extract these knowledge ingredients from our own post trained models. And then we give these knowledge ingredients to a base model to see what knowledge this type of post training exposes to model. And in order to control for knowledge injection, we just look at the math. Data. So basically, we post train a model on math only. And then we extract the knowledge in GDS give it to a base model, to see how the performance changes. And then here's the results. You can see that even if train the model only on math, and then you extract these knowledge ingredients, it can still help improve the results. On on non math task in GPQI and MLU Pro. And, this is a very interesting finding. Me and I would like to type of reasoning training with long channel thought is like, helps us basically surface the knowledge that the model already has. We wanted to make sure that this type of post training is not injecting new knowledge to the model So we basically did some probing tests and then saw that actually, you know, the performance of the post train model, and the, base model are the same in terms of, like, their knowledge in non math. Type. So, basically, the key findings here is that this type of post training reasoning focused post training is helping the model surface the knowledge that they already have. So to recap, we we need a comprehensive benchmarking to better understand scientific problem solvings. And, we we find that models a lot of models are bottlenecked by knowledge. It's not the reasoning effort that always give you performance improvements. So it's, depending on your, tasks, it's good to consider retrieval augmented SIS. Systems. And, when knowledge is missing or hard to retrieve, scaling chain of thought alone is not sufficient. So hopefully, this gives, some, insights to the community when they're, like, developing new reasoning models to see how we can improve them in this type of knowledge intensive environment. And then the practical implication is that the good mechanisms to supply or retrieve the right knowledge can help us get better reasoning system. So thank you so much. I wanna also thank my students and collaborators. We will have a few question time and everyone who are interested for the talk. We're welcome to ask questions for Perf Ahmed. Hi. Thanks for the wonderful talk. I had a question on the first like section on meta evaluators of like, LLM as a judge. Have you looked at like, if you increase the reasoning level like does that actually improve LLM as a judge performance? I know you did it in the second one, but, like, for LLM as a judge, does like, improve? Yeah. That that's a good question. Like, that wasn't our focus in that. Four. But there are other works and people have studied reasoning models as judges, and they usually see improvements there too. Yeah. A good question. Okay. I have a similar question that is about prompt. Yeah. As I evaluate benchmark in the model, I found that a little slight change for a prompt will significantly affect the final performance and how do you think that what's the correct prompt for best evaluation or correct evaluation? Yeah. That's a good question. Look, the prompt that is used if the LLM judge performance quite a bit. Other people have a studied this. They actually also have a previous 40 studying that. But here, we fixed the prompt so that we can basically have a controlled evaluation, and we use the same prompt that the original dataset, like ALPACA e values. Okay. Thanks. We will for time leaving, I have one more question, and thanks, please. Thank you. Nice talk. Nice work. I have a question about the second part where specialized knowledge is being used to improve the utility. Right? What is the story for efficient reasoning? So so the story there is like, it you want to like, because a lot of times, you just throwing, inference on computes to improve, performance on tasks. This basically tells you, like, if the model doesn't have the knowledge, no matter how much, reasoning you put into it, it doesn't help and then in lot of tasks, the basic you're basically you're can get very good results by good retrieval setup. You don't necessarily to have a lot of like, reasoning. Okay. Thank you. Thank you. Thanks everyone and we are happy to honor to have Amir Khan here for the remarkable presentation. And I think the the next should be the oral presentation. And we will have two of the following oral presenter. To present their work. One the first one will be let me check. Sorry. Yeah. Yeah. Yeah. The first all in talk will be M1 towards scaling test time compute with MAMBA reasoning model. Yeah. Here comes our presenter for this work. Okay. Next So yeah. Let's start Okay. Yeah. So hello everyone. My name is Xun Shung. Sorry. I'm I'm a researcher in Together AI, and today I'm going to present my old paper. Towards scalable test time compute with one more reasoning models. Okay. So let's start with a quick trend that we have seen in the recent in the community. So this chart shows the AME accuracy. Versus compute during our training and compute during our test time. So the takeaway here is that if you allow the model to sync longer or sample more, and then you do the majority voting, you can get big gains So especially for those reasoning tasks, Yeah. So test time compute reflects better accuracy. So and we already seen many literature that linear models like mobile and hybrid models can have, like, great throughput. So they are much faster efficient than standard transformers, especially for the long sequences. However, they are worse like, in terms of the quality like, sometimes for the resume tasks, So the question that we are going to answer in this paper is that so under the same compute budget, can Mama and Stella reasoning models can match or beat transformers So the the motivation is that we can if those models are larger if those models are faster, we can generate more samples and fix computer budget. So it we we probably can get like, result by using the emergency voting. So then the overall accuracy can be improved Yeah. So as that as that time, there are some several challenges. The first is that there is no available, like, hybrid hybrid models. So this paper is, is just written after the the deep stake AI model. So there is no available reasoning models at that time. And, also, the second is that so we have seen some approach like, or, like, Lambda the distilled from general models, like like, lambda lambda based models. But those models, performs better on those, reasoning task So typically, they only get the 40 to 50 for the mass 500 tasks. Yeah. So, now I want to introduce this framework that we call the Mama in the Lamma framework. So it has a three stage pipelines. So first is that we start from a coherent transformer models, and we want to try to reuse the projector layers from these transformers. And then initialize those corresponding, like, projections from the and this, metrics from the to the mobile layers. And different from the the memo in the paper is that we use some mass corpus, which we call the open open math instruct, and then we do the reverse KOD distillation instead of the for forward KL distillation. Because we find that rewards KL is actually performance better. So and when we train, ten ten billion tokens And secondly second stage is that we do the supervised fine tuning on those reasoning corpus, like, so this this dataset is generated by the like, using the DeepSpeak r one and then the r one distillate models. The third stage is that we apply the GRPO to on the deep scale dataset, using After SMP models. So, actually, there are some, like, numerical issues because if you are familiar with like, linear like, like, one of my models, they have the different kernels in the training and in the generation. So there is a gen training and inference mismatch, and we spend some time to track all these system issues. Yeah. So, yeah, And, here is a zero shot result. Like, we test on those most popular math benchmarks, and then basically, we can see that we match the performance of the deep search Distilled like, 1.5 clean models. Right? So the DeepSick Distilled 15 model is fine tuning from the quid and using the same copper like, the similar copper, the deep DeepSpeak R1, distill corporates. So, yeah, so you'll see this blue, blue bar is at very competitive with this query. Yeah. I think it's the first sign that we have seen the good reasoning qualities while using alternative architectures and hybrid models In Yeah. So let's talk about speed here. So we benchmark our models, you compile with the the lama, like, three b models, and also the the Quinn models. Right? So we we use a a VRM, like to serve these transformer models, but so at that time, there is no, like, the VOM is, like, the hybrid models haven't been implemented good. So we just the, like, standard generation using the CUDA graph. For for our approach. So, the takeaway here is that I umbrella, like, our ROI is like, two two to three times faster for the very large batch size or for longer sequences. So Yeah. And now we are ready to answer this question. Like, given a fixed compute budget, how does the reasoning model compare with how does the hybrid model compare with the transformer models? So we fixed a number of generation tokens to eight k. So each model here is like, the generation lens is eight k, And then we vary the best size So so so the the figure in the left is showing that x axis is that the number of samples. Right? And then y axis is the majority voting scores. Using this number of samples. So as you see here, the transformer model is still the like, RNT still better compared with hybrid models. But if you count this for the if you normalize it for the speed, actually, the the hybrid models will be better. That's because so the how we run this experiment is that we we vary the number of best size like, from one to two and then to a very large numbers, and we get a optimal TPS for each of these models. And then we normalize by the its TPS. So the reason why we get a speed gains in the right figures is that, MRIs like two eyes, faster compared with deep search, distilled models. And, also, we run a similar experiment, but we focus on the length. Right? So basically, if you generate longer sequence days, the the benefit of these hybrid models can be better compared with transformer models. Yeah. So we have the same, conclusion here is that if you if you if you if your metric is like the generation NAND and then this accuracy is, like, slightly worse, But if you if you account for the the the time, it can be better. Yeah. So I also want to mention that the MOI, actually, approach has been scaled up by the this company. It's called ServiceNow. They actually train a 16 I think yeah, the 15, 15 b models. To they use like, they distill, so they have a transformer model, which is which is this one, 15 b synchre models. And then the they use our approach to remove as many transformer layers So so this is the the most, right one is the without any attention to yours. Right? And then this one is like have some attention to yours. And the the and this one is like, with three third of malware and one third of assault. Three fourths of attention layers and one one fourth of normal layers. And then if you do SFT, you can, like, the all this the the upper one is SFT model after doing this distillation. Yeah. So the yeah. And then the benchmark is like, this hybrid models and, the original transformer models, then they can show, like, a match on all all of those tasks. So Yeah. So, last thing I want to mention is that so nowadays, there are many, like, hybrid models and more accounts like Qoinix and Kimmy linear models. But there is one question. It's still I'm not so clear is that like, how does the speed compare with, like, the five let's say we have a five transformer optimizations, Like, by five transformer optimizations, I mean the technical, like specular decoding, can actually provide two times two to three times speed up compared with the one without speculating coding. Right? So in real inference service, you typically can get, like, two times speed boost by doing spec related coding. So So yeah. So, we we can now we can provide a very good hybrid models. But how can we actually run it very as fast as possible? Right? So, I want to talk a little bit about this direction because that is what I'm interested in right now. So So like, we are going to use a technical which we call a speculating coding. So which this is very popular inference technicals. So the way that it works is that we we have some like, we will we have a smaller model for the draft model, and generate generate some tokens. Right? And then we send these tokens to a verify, and then verify which tell tells you which which tokens I I prefer and which tokens I I don't like. And then we will reject those tokens, which don't like by the a verifier. Yeah. So, there is a challenge for the hybrid model or linear models is that so if you do this transform with transformers, it's very simple. Right? So you can KVACash will it's KPI cache will keep track the the kvC at each position. Right? So each token And then if the verify rejects a certain positions, you can easily retrieve the corresponding KB cache at the last, match prediction and the start from there and restore the restore the yeah, restore from there. And then you just keep verifying it. However, for linear models, this very hard. Right? So because you only keep a single single state. Right? So so let's say the very first only like, you you only have a single state the final stage It's very challenging to roll go back to a to a previous positions. So if you if you alternatively, you can keep track all the hidden state at each word. Right? But but then you can you will, like, have to materialize all the hidden all the SSM state or, like, the iron state. But they are typically very large, and then it will run some memory issues, and you can probably can not get any gain. If you just memorize all those So, that's the reason we introduce this multi step linear verification kernel set So basically, it can like, it can verify So so this kernel is that it does multistep generations. So, basically, like, the typical way that we do generation is that we we generate at each staff and and then, we we load these ways in from HBM to SRAM, and then we generate there. And then we send the send the state from, SM to HBM. But it's it's like it's not efficient. So we write a multistep kernels that can generate as many, like, here is like 16 or 32 steps. Yeah. And with with these kernels, we can actually do the speculated coding very efficiently. So basically, we let's see how can we use this, like, multistep generation multistep application to do the generation. So basically, you if we have three tokens here, and then we can like, if if let's say these two tokens are get verified and the last token get rejected, but in but we didn't advance our hidden state. Right? As hidden state is still the s s m SM state is still at the first positions. And in the last time in the next round, we will advance the hidden state using this, like, multistep multi step kernels. And then we also verify the the three three more tokens. And we'll also propose three more tokens. Right? So and then if we see the last two get rejected and then the the first one get a accepted. So we can, update the hidden state to the to the, like, the previous previous match positions. And Yeah. So so by doing something like this, we can actually make the specularity going very fast. So there are a lot of work which is, I think is very interesting that in today's this year, neurophys, this this is called the ST tree. It's actually doing speculating coding for the tree applications for the, hybrid SSM models. Yeah. So, I want to thank to all my collaborators here Also, just to let you know that together, AI is hiring, and then we are, like, top the most efficient yeah, among all those models. Yeah. Thanks for all the remarks. At the time limit, we will not have question time for the oral presentation. And now, let's welcome our next oral paper talk Generate Parallel Scaling with Independent Generalization. I may have one of my backpack if you don't have. One. Here. So sure. I didn't have Okay. What what second. Sure. No. No. No. Sorry, by the way. Everyone, my name is Harry. And today, I'll be talking a little bit about our paper generalized parallel scaling with interdependent generations. So, you know, we've seen throughout this conference in the past year, yeah, all on reasoning has know, there's been insane progress. A lot of this has been due to inference scaling. And we saw a little bit of that in the previous oral talk. Where, you know, for a particular prompt, you might scale in the sequence axis, in which you're sort of generating more and more Genesys. Thoughts. More tokens before you output your final answer. Another access that you might sort of consider is the parallel access, where for one particular question, you are going to generating multiple response and in the end, you might do some merging you might do some selection or majority voting. And so in this work, we're actually gonna be focusing mostly on parallel scaling. Here. And how we can improve that. For, reasons. Models. So To sort of introduce this, to three parallel scaling is, you're generating multiple responses. For one prompt. One question. And this is done usually independent What we mean by independence is every single sequence that you're generating, for this prompt. It's going to be independent of each other. So there's not gonna be any sort information transfer. So that means something like information that is derived in one thread is going to be inaccessible in another thread. So there's no information flow across these different threads. The and also for independent sampling, we have end to end. Which what we mean is that for end threads, we're getting end different answers. Another sort of, method of parallel scaling that has gone some recent traction this notion of, like, decomposition. So for a particular question, you might break it down into parallel subtasks. And each thread is kinda tackling each subtask independently of each other, And then at certain points, you might do some merging. And you might fan out again. Keep doing this until you get the final answer. There's going to be information flow throughout these threads. But in these cases, you're gonna have to you're using multiple threads, but you're producing one final answer rather than an different answers. In the extreme case, we also have interdependence. Where information flow is going to be consistently flowing between threads. And also, you're gonna have end separate responses. As well. So this is going to be something independent sampling where we have threads and responses. But here, the threads are sort of gonna be communicating with each other at regular and so this is gonna be our focus is how can we actually do this, in an efficient manner? And the way we do this is that we first take a look into the hidden states of these LOMs. And so, you know, these layers in an LLM where it's transformer based LLMs, they're gonna be mixing and to tensor. And so what the hidden states tensor sort of looks like, it's this three d sort of structure. Where we have the responses on one axis the tokens on one axis, and then the features on third access. And typically, what's gonna happen is that for attention, it's going to be mixing information along this token access. And then you have the feed forward layers that's gonna be sharing information between the different features. Usually, you know, there's nothing gonna be flowing in the response access, in batch inference, you're not really making any assumption there, in what's gonna be in that batch. So you wanna keep everything sort of independent with each other. But this is actually not the case in parallel scaling because in parallel scaling, these different responses are actually gonna stem from the same prompt. Right? So these sequences are no longer sort of completely independent They're gonna be highly correlated each other because they're they're trying to answer the same question. So what that means is that related sequences due to parallel scaling allows us to view hidden states. As a holistic tensor rather than just isolated matrix slices. So you can treat this as sort of one structure rather than n independent. Substructures. And so that's what our method is gonna be doing, is we're gonna be adding a third module. Specifically for parallel scaling. That is going to operate along this response or batch access here. So So, in a nutshell, our method is just an unmasked attention like operation along the dash axis. For all the sequences that send from the same prompt. So this is what it kinda looks like. So typical transformer block is gonna have the attention and the feed forward blocks. Just adding a third one, which we call bridge. And within that module, what gonna look like is that for a particular prompt so here in this diagram, we have prompt q and q prime, so you have two different prompts. So between different prompts, still, making them independent of each other. There's no communication. But, for all the rollouts for a particular prompt, they're going to be communicating each other every time step. And so this allows us to share latent information to other sequences, at each generation step. Our method is also fairly low cost. It's going to be adding around three to 5% new parameters. And we don't have to train from scratch. We can just actually add this to a pre trained model. And then just sort of fine tune do RL on top of that. It's also fairly, versatile. In the sense that we actually can train at a know, parallel width. And it can actually generalize to different widths both above and under what it was trained on. So for example, for some models, we trained on a width of four, and it can generalize all the way up to 16, or you can also use for, with a one, which is independent. Sample. So To give a more, deeper visualization of what's going on, so for self attention, what typically do is for a particular token, we're looking at the historical tokens. This particular sequence. So that's why I have kind of showed here in in this, matrix here. And for a self attention, you're gonna have to maintain a cubic cache because you have store all this historical information, and you also have to keep track of this notion of, like, time. With positional encoding. In Bridge, we what we're doing is we're actually going along that other axis along the sequence, not the sequence the batch axis. And because we're not really storing any historical information, there's not sort of a memory cost associated with that. So don't have to store some sort of KB cache. There's also position invariant, and what we mean by that is that if you permute the items in this batch you're gonna get the same answer. So there's no sort of, positional information there. And so when we combine this together, in an, an LLM, what we see is that this sort of token is going to be attending to the all the other tokens and that share this access. The act the axis being the dimension. Dimension. We this also changes the next token distributions a bit. So, typically, what's gonna next token is gonna be dependent on the prompt and then also all the previous tokens that the SQL has generated. But in our case, it's also gonna be dependent on this tokens. That were generated previously by all the other sequences. And so what this kinda looks like is that you think of this, you know, matrix of batch by, token, matrix, generating sort of one, takes a lot of time, but in Bridge, what we're doing is we're doing one column at a time. So each column is gonna be dependent on the previous column. Okay. So now, I'll get into some results really briefly. So so we're gonna show some math results. So just a brief overview how we trained it. We're using RLVR, and we're training with and without Bridge, using a dataset of math. Problems. And so I'll walk you a little bit through what, these baselines are. So we have the original sort of yeah, these are all, like, deep deep distilled models. We have RL VR only, which is just, you know, applying RL VR on top of these models. And then we also have this baseline called p match. Which just stands for matching the number of parameters and the number of layers that we induce adding these bridge layers. So in a sense, this is, like, the compute match baseline. And, what we actually end up seeing is that for all these all these different math tasks, a lot of these are competition math problems. We see that Bridge actually extends been we get from our old. So it extends the sort of headroom that we get Even though this was trained only on math, we can also evaluate on non math generalized in terms of you know, is it just learning to share information for math task, or is it also learning to share information beyond, math? And what we see is that it actually generalizes to non math tasks. Despite being only trained on math. So here, we have you know, some language tasks, stem, and puzzles as well. And even in the cases where Bridge is not doing the best, performance, it's still doing the equivalent. To the other baselines. Way that we can sort of evaluate, our method is at the set level. Right? Because we are generating n different responses, rather than compressing it all down to one response. So we can see how good this setup and responses are. And for this, we're looking at sort of three different things. We're looking at a simple spectrum. So from left to right. What I'm showing is the accuracy when we're only looking at one out of eight is correct. That's, like, equivalent to coverage. But we can be a little bit more strict And so the center is going to be out of eight correct in this set. And then the, rightmost is eight out of eight. So unanimously unanimously all correct that's a lot more strict. Of notion of accuracy. And what we can see is that, our method most consistently produces the most correct answers for difficult tasks. So and now, finally, I'll talk a little bit about some growth investments. What you mean by width is, you know, the parallelism sort of width. So we trained at four and, we can evaluate at different width. So here, I'm showing widths from one to 16, where one is essentially just independent generation. And then we actually end up seeing is that though it's only been trained out with four Bridge actually outperforms, any scenario, including, the baselines and also when bridges with a certain equal to one. So Okay. So, just to wrap up. So Bridge allows information flow between sequences, without Docker parallelism. It's a composable block that you can just kinda add into your LLM. And it's also fairly robust to different tasks and also generation with the second thing that we want to emphasize is that, you know, bridges only one way to sort of leverage this. Untapped structure that's in parallel scaling. All these sequences are correlated with each other. You know, certainly, this sort of observation can be used, for other methods as well. Some future directions that we think would be interesting to explore would be know, including this in, you know, early on training, say, in mid training or even in pre training, everything that we've shown here is all done in during post training. And then also, it might be pretty interesting to look at vision test just because in vision, there's gonna be a lot more correlations going on. Than a language. So. Yeah. That's it. And thank you so much. Yeah. Thanks for such a remarkable award and thank Harry for here as an oral presenter. And then we will move to the next part of the professor Wei Shi's talk. Yeah. Yeah. Let me give a brief introduction. Yeah. Wei Shi is the professor and the faculty member of School of Interactive Computer and Machine Learning Center, at Georgia Tech. Which is research lies on the interest of machine learning, neural language processing, and social media. We should also address the neuropathy x lab with focus on large language model here. Let's give our time to professor Wei Shi. Alright. Thank you, everyone. For coming. I'm going to talk a little bit different perspective of efficient reasoning. It's not about computational efficiency but rather how effectively, as a field, a field of 100,000 tons of thousand researchers that we can think differently and come out of creative ideas, in order to move the field efficiently forward other than doing similar things, of each other. So my talk will be, about probability reasoning for real world decision making. Which is beyond the logical and mathematic reasonings that you see a lot of benchmark are our related to So I want to, ask you what did you reason about today? Or maybe you will be reason about later today. So likely, if you haven't think about this, you will think about this later today or tomorrow. Is when should I leave my hotel room tomorrow morning or That's my flight. For the airport. Right? So the reasoning will be something like this. There's fifteen minutes drive. To the airport from about here, without traffic. It's now the major holiday, but there's a lot of attendance, of New Year's might be going to the airport at the same time. Is this easy to get a rideshare? Are you renting a car? Yeah. And how long it will take for wait in the line to check out the hotel if you really need you need to do that. Will look up at the terminal map to figure out how large is the the airport, because some of my airport from the security check to the gate is kind of a hack for more than a mile. With your luggage. So this determine, what I will do the day I head to to the airport after a conference. So this is what we read on daily basis. But what does our large number model reasoning benchmark of are working on? There is something more similar to this. Beside those mathematical questions, we may have some logical reasoning on on tasks like this from the big bunch actual hard release by Google Research. That used in, almost three recent release evaluation. So it's going to add some, region. It's a very little question, but these are on the toy exam. And the the top first dataset on benchmarked our Genesys Ray, in their release, they evaluated on, is this humanity's life exam. Has a lot of very interesting questions like this. That probably there's a a tiny little number of people in the world ever will be able to answer. authors on the paper that invited all the world expert of very, very niche domains in this very, very hard question. But this is not what we are reasoning on daily basis. So you can already see the difference of what I want to focus on today on this what you might really listen on a daily basis. What is, what are in the benchmark? Currently? So so the example I presented is more on the numerical reasoning and with some logical reasoning. But with a lot of uncertainty. This is what we we do reason as a human being. Even not as an expert of some of Honeywell. So this type of reasoning will require reasoning, so that you need to estimate some of the likelihood and have to make some This is what people do to make decisions. It's undeterministic. The most common places you will see in in the real world for some important decisions will be like legal decisions. If you want to convince a jury, you each jury will make different decisions based on the partial evidence presented to them. And if you work in marketing, or HR, you will do this on on at your job all the time. Is that what kind of product I manufacture, make sure design, and what color should we manufacture for these different colors of process. And more recently, as think for a lot of long mode, the most successful and more in comparison, more studied domain will be medical, diagnosences. That is always a probabilistic reasoning. Although the normally, the cause of a disease will be only one, but the similar syndrome and test result, there's could be a more like a distribution over market diseases. May may mount to a question, why so far of the current large number of reasoning benchmark is not the probability reasoning task that I'm talking about. A standby of a few related reasons. It's very hard to collect ground truth. I think medical field is easier to advance is because there's a lot of patient and a lot of medical that was able to have this more intensive evidences, and decisions being kind of our predictions being collected to evaluate. To work with. But what about whether we want to open, I want to open a business. I want to roast a certain restaurant with the multiple choices of location I have. It's hard. Right? You can't do AB test and location may be only one factors of the business success. And, of course, these kind of things are important. So a lot of our companies and private individuals, you have to do research, marketing research analysis. In order to so those are data are potentially of available, but it's normally in private holding. And it's incomplete. And that's and as a consequently considerably evaluation will be inherently challenging you may or may not actually know the right answer. One of the best attempt we have seen this year from at IKEA is from Dan Ross group at UIUC. So Dan Ross has a long term machine learning researcher. He has done this since probably twenty years ago. Before I even entered the field. This is his latest attempt with his student. It's still a little bit kind of a toy examples, makes some assumptions, it's kind of artificially created dataset. But this is one of the best attempts in order to make this type of task possible to study. For AI researchers, need quantitative requirements and, evaluation So what we can do? We we definitely need some real data also called benchmark in order to work on So what do we really looking for? Right? We want to look for something more realistic other than, just toy examples that no one knows the answer. Or the answer is kind of you don't know whether if you loosened it or it's it's been cracked or not. Right? So for example, we want to predict how to weigh you know, how likely a World War three will happen, which country or which region that will be a start of it. So it's hard to kind of, have the data to to to work with us. But we can bounce this type of problem based reasoning in everyday scenarios. But, another thing is, when do need to evaluate. We want the data to be public. That every researchers, you know, on the world on the Earth can can can get hands on and work with it. With reliable or at least efficiently reliable ground truth. For my own group or myself, personally, I want to work on something socially impact I don't want to just work on things because I can publish papers but rather I wanted something useful I can make It's different for everyday every part is, every user's life. In on Earth. So luckily, we did find one such use cases that fit all above three criteria. So this is the work my PhD student, presented here a few days ago at, NewRePS. So he's the only student officer, did the work and did all the work because all the rest of three of us, Alan Rita, Solvig, my collaborators, in machine learning field or and privacy field. They are all three of us are all professors. So Jonathan did 100 percentage of the work. So, the of course, you are curious what the use case would be, like, allowing us to study this. It's just actually, you probably encounter all the time as well. Reddit or social media, if you use it, you will post on it. You are very likely to talk about yourself or someone else. So Reddit is a pseudo anonymous web forum. So people quite often talk about themselves or their coworkers and so on and so forth. For example, this is one of them. But how risky it is actually to post a post already like this. So we had to detect later in ten, fifteen minutes, is to detect where part of the the tags people reviewed some details about themselves. So and, of course, what type of information what is related, for example, the location of them, the the job of them. So these are prior detected by a fine toned model. Then the question they ask is just how how many people, you know, are on the earth would fit this description. Of course, the smaller the number, the more risky it is. I will give you a second to read this post because I'm going to show the reasoning of it. In a second. So there's a few key, key point. It's like my son's take care also recently increased their rate. I only had four months of maturity leave. So that didn't say this is a mother, but it's very likely a mother of a newborn son. Presumably much less under the age of one. You can't enforce those. Of course, this is not only for social media. I need data you send you know, converts with or other kind of larger LAN memory or APIs depending on your configuration. You might give him a a way your, data, as well. And also, there's always a leak. Concerns. So, so for human and the OAM changes, there's also privacy concern like this. So this is what channel of sales will will give you. So this is a give the ready post and, of course, a lot of prompt on engineer as well as the detection of which part of the kind of the text I showed you, the highlight are provided in order to help the model, reason better. Of course, you can just throw the Reddit just zero shot prompted. That's also kind of will work a lot of cases and especially the simpler cases. So But there are just two arrows, in this reasoning. So here is one. I hope you're in this very short time on the slides, you will be able to recognize that. That's a little bit like a challenging task. But on a high level, the idea is that the this particular reasoning trend mentioned there's a the woman will be five 50% of the population of this, city. And the working in the tag, how many percentage that point one percentage of the population working tag in Australia. And then they multiply this two things together. But that's not very accurate. Because we all know that there's a gender imbalance in the tech, field. So is woman well not percent. Percentage of that. Population group. So is another, estimation that, in this case, GPT model made. This is how many they do realize this is like a a a child in daycare. So their estimation is based on how likely you have a toddler. Under three year old. But the woman is talking about maturity leave. So it's unlikely this mother more likely the mother have a is son not under a year old. So this overestimate underestimated the risk. Of the poster. So in order to do this reasoning more accurately, we Jonathan devised this, base based previous reasoning network with large enough model as a back bone. So the key idea is that as you all know about, base net, you have to model the strong probabilities in certain orders of a composition of the conditional probabilities. Of course, there are some assumption you can independent of assumptions you can made and some you can't. So for example, estimating how many women in the group of people working tech is a little bit easier than estimating among all the women how many are actually working the tag. So this ordering matters, and the decomposition matters. So there's numerous different ways you can construct a BSNAD based on the original post. This is the one parameter of the better ones that will in this better order that allow you to estimate more accurately and with a little bit more evidence you can collect. So this is would be one of the many constructions of BS9, but you do need your user model to predict to construct an graph. And also pick the one that will work better. And this will allow you to turn into sub questions that large number more likely can answer with evidence more accurately. So this is going to be a the questions that we are trying to answer. So this will allow you to estimate among the percentage of the world population that fit this description. Even with this decomposition to this level of sub questions, some of the question still will be harder to answer than others. So for example, how many percentage of the people in tech in this city in Australia have no landlord? This is what mentioned in the original post. There's normally there are no census data about this. Normally. Maybe there is, but it's very hard to to dig this out. So, we can further break down this question into whether this person on the their own property or they are leaving their visitor parents? But still, the model will have fairly low confidence or certainty to estimate this. Then you'll have to generalize it. And also, based on the model's own confidence, in order to to decide what to abstract this generalizes question to be. Until you can answer it with some level of certainty. So eventually, you will kind of two type of most common scenario, and then you'll sum them together, become this as Some other questions are supposed to be easier to answer is, for example, how many percent of the women in the city, that actually have a newborn sign? So if you send to a larger Frontier larger model, they actually do quite a bit thinking just in order to answer this. Single simple sub question. Based a lot of retrieval, searching through all the whole entire Internet, and I was in order to answer this accurately. And there's a lot of things they cannot talk about, but the most relevant one is the annual or number of annual courses. In this city. So the information is fairly accurate here, but it's mentioned as one of its largest public hospital in that city, which is one of the major one that's most the newborn son, up children come from. The twenty twenty three of they have a knowledge cutoff they don't they have only twenty twenty three data. However, if you really can actually have two uses and might accurately kind of have more structure data. Funded in the right places, you will realize that although the majority are children born in this city are from this public but there's a lot of private facilities also get take care of the newborn babies. So the actual number is larger than this. The number of the model estimated. So with all this, you are able to make an estimation. And eventually, you'll to put it into the right message equations. Are condition or independent or not. Do a little bit reduction or else in order to aggregate the number. So number is striking. Just just some kind of pretty normal sense of your world Internet. I don't feel that the original post is so kind of revealing. But if you actually do it, it's come down to 25 women in our city. It will be narrowed down to that level. If it is they talk about something else, for example, they engage in some political activist, In some other Reddit post, they reviewed that. They they can be narrowed down, if need possible. By people inspired who know a of them personal. So we we did some kind of evaluation using this new technical kind of, framework. For basic listening to evaluate this. So compared to your off the shelf channel of Salt, which a lot of prompt engineering, of course. This is kind of the result The different model will, actually, the model are doing pretty well in estimating these numbers. And, there's still a lot of space to further improve. Of course, the goal of this is now to try to identify that anonymous users, but rather to which part of the text they mentioned is indoors for themselves. So that they can change it or modify it in order to decide what they actually share. Also, for as we don't want to make Internet user to be more paranoid, what what about what they share on the Internet. So this project partially myself, personally, is I actually want to go on ready to help people to answer their questions. That I know very good answers for. But I don't want to be, de identified so I want to be able to enable a lot of people who are will be waiting to help people on the Internet to feel more comfortable to to do that. With some safeguard. But, of course, that's that's my personal interest in this product. So this is a little bit, analysis of the of the result. Of course, all the technical of my sorta evaluation are are in the paper. But this is a little bit on a high level idea. This is comparing branch, what's a chain of thought. Of course, if you're just doing dealing with the simple cases, there's several only three things the user mentioned about themselves. All the model work more or less as the same because they are easier, so the turn offs are any frontier model can do fairly well. But once you have a for example, a posting history, you'll have a lot of changes with larger model over time. Or you post things over the time. Those become much more complicated. In those cases, probably it's a reasonably framework as I just presented will make a huge difference from just simple prompting my sword. So So model do pretty well, mostly predicting, the x axis is the model predicting number of the k, how many number of people in the on the earth that fit that description, and the y axis is the ground truth, On the diagnosis are the pink colors. Ones are the one pretty clear very closely to the ground truth. With only half of the magnitude differences. What really make a difference on those much more complex cases this analysis is on a random sample of Rydays. So there are simple cases that both Maser worked really well and also have cases that more complicated at our master work a lot better. What's concerning is, like, the typical churn of salt or TPO from tier lateral model if you just do simple prompting. Just do prompting, even a very simple or more engineered complex. Prompts, is always underestimate, more likely than not. So that meaning they cannot give the k much larger k, than it actually is. So they would have it will indicate that, oh, there's a 3,000 people fit this description, and there are maybe just 30. So it's a little bit so being able to not avoid underestimation is crucial. So here is a a arrow case of where that happened. So this is actually a real data that of user data in, changes with the strategy PD that the data released by shared GPD, You may heard this story that the original trade GPT release actually included some of the journalists using it to go through or summarize some transcript they had with some interviewees, but, actually, you revealed anonymous, people's name that there's a be keep as a voice of the Lord or cannot people interviewed for a press release, privately. So these are real. We had to modify this post in order to show us here on the slides to This is kind of a semi fake and real, user conversation in GP. So it does a person talk about starting a business, and want to decide on which company name for the startup. And eventually, one of the company become the real company, and then it's not that hard to figure out who is the founder of it. So his case actually won. There's a single identifiable individual who made this type of changes. Okay. Just to wrap a little bit here, before I move on to something else, I need to do this quickly. What I want to make at a point is propriety reasoning is challenging for both AI and humans. We are very interested path to study. Very quick. I need to wrap up. So another work we did a already preceding this Work Is To Actually identify this this disclosures which actually crucial for both channel of thought type reasoning as well as our method to work for this. Task. This is actually hard. Detecting this accurately or even 90% accurate. As well. So thought it would with phantom beam. So, yeah, if you're interested to improve this, we encourage you to do that. And there are some other people who follow our work and make this to other languages other than English as well. So we did actually conduct a real user study with actual Reddit users, and that's them whether they like this thing. They they do. Some people have found it's not so so far. There are two out of the 12 21 people, but they were like, I'm already very careful. I don't need this to help me to make my calls. I I'm not talking much about myself. Reddit. But for my cousin for my, for my that is a tunator, this will be very very useful since that So they gave a lot of, good feedback and motivated us to continue working on this. My second to quickly mention actually, once you identify this this disclosures, the larger model are really good at at tracking it to make it safer to everyone. I don't know whether you use Grammarly before to correct your grandma errors. But if you simply have a Grammarly for privacy reasons, only text you send out or post on the Internet can be just modified to make it safer for everyone. That will be really nice. Then you can take control of your own data other than rely on registration to protect. Users. Privacy. This is going to be my last second slide. There are other things we are thinking about and we are working on. About reasoning, is that, cross lingo, cross culture scenarios, social scenarios, that require quite a bit of reasoning to understand social norms and understand not only the language, but also kind of some unwritten rules. There are other, paper way or are published, and we are working on this direction. Forward is also kind of a geolocation of the multimodal case scenario. If you post a have a image, can actually the frontier models can actually narrow down to the GPS coordinate that probably even at the the location to be close enough on the block or something like, accurate to that level. You're welcome to check our paper on that. To make it even kind of more accurately locate you is to have actual user to converge with the frontier model in market term conversations. That will even narrow down that accuracy of geolocation of the image. Towards a estimation accurate level. Of course, again, they want we don't really want to locate people, but we want to raise awareness. This is is possible. And we want to try to prevent this from happening. For example, if the user post a photo in front of tower, then that's perfectly fine. Right? Because the user know what they are doing. But if the user just have someone else, you take a photo with you and you are just on the other people. Background of someone on a random street. You don't want to be located necessarily to the level of the same the block. And the large and small actually do that. So how to kind of moderate a larger model to not answer questions certain questions about certain photo is So what I want to encourage everyone to think about is do we always need to reason in high resource language, mostly like in English or Chinese for our frontier models? I think the answer is no. Depending on what question it is about. And do do we want to consider to kind of human and AI to be a collaborative process in reasoning? I think the answer is yes. I hope to see more work in these areas, other than just the large number model benchmarks that we we are trying to climb that in the number four. So This is, just my group. Just, some two money. My student contributed to this talk, and I also work on a lot of other topics in the lab. Yes. Thank you, everyone, for for your attention, and, I'm happy will be around and answer any questions. I'm happy discussion. With you. Yes. Thanks for the talk. I have one question about Asian. You talk about a lot of privacy, yeah, and how I'm conveying the overall privacy for document. Yes. And how that be for Asia? You know, the agent, have capability tool for tool calling, agent has a capability maybe control your computer Would the privacy problem be more important in if you're using recent model agent? Yeah. That's a super question. It's how this kind of in two use cases and you'll have company have private data to do this. I think generally, there are two part of my answer. So one area we're actively working on in my lab is actually doing synthetic data generation. So in order to do just we our release data is actually synthetically generated that look really like the ready data, but they are all fake. Just like some the example showed on my slide are those. So you don't you couldn't tell there are fake ready calls. Right? So they are actually fake Reddit post. So so that's one thing I felt like, people can do, and they said, could be useful for pretraining and, in any stage of the model being trained. And the other thing is a little bit kind of afterwards, I after south. Right? That's a little bit like a geolocation situation that we want to know the context of this people whether this answer can be answered. Right? Depending on the context of that image and what the intention of the malicious users uses. Or who they are, and why why they the model may be need to abstain off certain questions. Yeah. Thanks. And due to time limit, we can only have one question. Thanks so much for professor Wichi's talk. Yeah. Thanks so much. We will have our next talk. From Baidu's Will team. Yeah. Wall is well known technology for reinforcement learning and recently model training. And Wang Zhang is the co teacher of world project. Especially promoting the system single control, etcetera. And he's the machine learning system engineer from Baiduancy, on building scalable infrastructure for reinforcement learning. Yeah. Yeah. Let's give our time to Quan Chung. Hello everyone. My name is Wong Zhang. I worked and ByteDance SE as a machine learning system architecture engineer. I'm also the co initiator of the VERO project and prototype this single controller architecture. Today, I would like to talk about the past the current status, the future of the VERA. Here's the outline for today's talk. So everyone attending this talk is likely well aware of the importance of reinforcement learning especially at the large scale. So I will quickly move through this background section. Like, a large scale reinforcement learning shine because pushed the model to reason significantly better. For example, without reinforcement learning, model like GPD four o hits the ceiling on math and logic task. But with large scale IO, like the o one, performance jumps dramatically Almost doubling on the benchmark like Amy and to reach close to ninety five on the math. So the takeaway is simple. Reinforcement learning and scale is a key in to that unlocks strong reasoning. And it is becoming increasingly important and for a gen tech task. However, running this reinforcement learning task and the skill is never an easy job. It is especially true for architecture engineers want to provide the flexibility for large language model researchers while maintaining hyper during the computation. The main reason for this difficulty is that IOW isn't just a single model training loop. It is a complex data flow connecting Mont models simultaneously. In the age of the agents, we are often managing hundreds or thousands of independent agents. Running alongside the model training loop. On top of that, we have multiple workloads happening at once. Generation, inference, training, and synchronization. All tightly coupled. So meanwhile, when we scale up this up to the large language model, each of those steps like the after generation or the reward model inference become a massive distributed job on its own. So instead of a simple loop with multiple models, we are actually orchestrating a set of large scale distributed workloads that all need to run smoothly together. We face the exact problem about two years ago. And I would like to share the story of how the very beginning to life. The story starts with my boss, One day, he came to me with two seemingly contradictory demands. Like, bosses often do. First, he wanted a reinforcement learning library that could run on can run an entire experiment on a single machine. With, you know, relatively small model and, limited data set. Second, once he proved his idea worked, he want to scale it up to the multi model cluster effortlessly. So to achieve the first goal, the main hurdle was to was the limit the GPU resources on a single machine. To solve this, we combine the training and the inference engine into a hybrid engine. Using the GPU time sharing. In this in the first part of each step, the hybrid engine work as an inference server. It performs a restarting process, optimize the for generation and training parallelism. Loading model weights from the previous iteration. And after the generation, the hybrid engine morphs into the training mode. The second demand fostered the hybrid controller. It allows OEM researchers, including my boss, to you know, use single controller mode to write their and the trials. They can then scale this to multiple nodes just like writing a single thread script. Meanwhile, the multi controller mode still allows system engineers like me to implement high performance computation and maintain the bugability for everyone. If you, okay. So if it's it's your first time here in terms like controller or multi controller, let me quickly revisit the classic design. Choices. In a single controller system, one centralized controller managed all work workers running different programs. It is very flexible. Examples including like TensorFlow, one or race Isle like On the other hand, a multi controller assistant has each worker running its own controller with same program on different data. This is much efficient, and it is designed for designed to seeing in frameworks like PyTorch and JAX. Whenever. This SPMD mode make it a difficult to data. Flow. Involving many interacting models. In VERA, we proposed a new paradigm called the high grid controller. The idea is quite simple. Combines the best of both a central controller just high level algorithm logic while the multi controller handles large scale parallelism. Acquisition. In this way, we get both flex and efficiency. From a researcher's perspective, we keep the interface simple. It looks like a single controller. You can write the entire IO loop in just a few lines of code while the Verint handles the heavy lifting underneath. As shown in the this example, a developer can simply write a core algorithm logic in a single process. And come the distributing training is obstructed away. On top of that, it is a wide range of algorithm from PPO and GRPO to newer methods like Prime and DAPO. For the efficiency, we fully leverage the multi control paradigm under the hook. And since release, very hands received tremendous support from developers in both ByteDance seed and, the community. I would like to highlight some recent updates and features recently in progress. So recently, there has been growing interest in reinforcement learning for agent take tasks. Traditionally, the rollout means lot language model generates a text token that went straight into the reward and the training modules. But the agent take scenarios the model generates a special tokens. Like code. And then get executed in a sandbox. The feedback might include a unit test results or compile errors. And the model may interact with the environment over several turns before the lead trajectory is training. The rich tool using Paradigm is powerful. Training large model with reasoning capabilities. But this raise raises a question. Should we provide a mechanism that continuously integrate all plug ins and tools. Into the interim system Or should we mimic how the application engineers build as a agent? Calling the tools within the application logic in And after the discussion internally with this we chose later. The client side choice. Which we call the agent loop. We believe this design offers clean intuitive mechanism for both application engineers and algorithm researchers to simulate and implement RM agent flows. And here is a code example showing how there is this tool calling program. So roll out is hand delivered by Vero in a server based manner. User can leverage the generate HCI to use underlay engines like VOM or SPEAKER two. Geelong. For token generation. They also have the flexibility to define tools, send a box, and the timeline logics. And our async IO APIs allows allow for high profile high concurrency, ensuring efficient use of the hardware resources. In addition to new features, we are reflecting better to improve interface and establish it as a standard library. I will skip this the deep dive into this slide for the second. Time. But please check the issue tracker if you are interested in this details. Our reason to road maps folks also focus on modular design with cleaner obstruction back ends like FSTP two and Metro. We are building partial and full async rollout pipelines and releasing new recipes such as we bench. We are also working on more efficient multi model. Data transfer. All of this is tracked on our GitHub road maps. And we'd love to for you to follow along this contribute. So with the support of internal and external contributors, our community has reached over 15,000 stores. 2,400 folks, and, almost 400 contributors. This has been this has far exceeded our initial expectation. As a co initiator of the VERA, the best way I can thank our community is to check those up upcoming challenges and new possibilities for the two thousand sixteen. I would like to focus on four areas. We'll believe will be the greatest challenge for Verint in 2000 Please note that this is not a roadmap. This there are explorations We want to see if this these directions can take a very to the next stage in coping with rapid evolution of RLO in large language model. The first challenge you may many user face come from the limitation of rate. Two major issue prevents us from scaling up aisles task further. The first is the single head architecture. Of the Ray cluster. The head node bears a heavy burden man managing logging and the meta data without high available backups. If the header node fails, the entire cluster and all the running jobs goes down. The second one is a built in gRPC protocol. It requires serialization and deserialization for every message. This gets worse when we trying to transfer a CUDA tensor The lack of GPU direct RDMA forces our device to host a copy should slowing down the training loop and increase the risk of out of memory errors on the driver obsessed. So to address this, the Monarch project from Meta PyTorch looks like a strong candidate. Monarch is a distribute. To the programming framework based on the scalability scalable actual messaging. It's RDMA native communication serves us from saves us from the serialization and the host device copy overheads. It also avoids rigid cluster architecture. Using our custom customizable service discovery mechanism. So in 2016, we plan to explore Monarch as an alternative back end to provide a better The second exploration in involves the dual engine architecture. Currently, most reinforcement learning framework use heterogeneous engines. One for training like the PyTorch, and the other for inference like VOM and g log. This is done to maximize the throughput and utilize the existing optimizations. Whenever. We want to see if a unified engine perhaps starting with the JAX is possible. The issue with a separate engine is implementation divergence. Our policy training can suffer when the training probability deviates from the inference probability. We currently handle this by recomputed log probabilities but and unified engine may eliminate this recomputation phase. While the computation graph for the training and inference might still suffer a unified implementation would help to tackle to tackle and forward tackle the forward path and reduce the numerical deviation. And in 2025, we we saw a surge in a synchronized solution like StreamElements, laminar aerial, etcetera. These solutions vary in their rollout policies, wait synchronization, and the replay buffer strategies. However, most of them can strike truly similar Most of them are structurally similar and can be transformed into one another with minor policy tricks. So this suggests that there this suggest universal infrastructure for async is possible. To build this, we need to we need the right program interface. A a purely decorative approach is clean. But the limit is the flexibility for researchers. So instead, after the discussion, we prefer a programming approach. Extended single controller API to include components like replay buffers and the version, the ways synchronization. Well, working with the volcano engine team, on this and hope to offer a solution to our researchers too. And finally, in 2026, we want to preserve the original vision of the wearer. Tall experiment locally and scale effortlessly. As the code base grows, we don't want it to become bloated. We plan to separate the valuable recipe while keeping the core functions in the main variable code base. This is this ensures the core remains stable. For production use while the ecosystem still grows. I would like to thank all our our contributors again. And, if you are interested contributing especially to the Unified exploration, please contact us. And thank you very much for your attention. Yes. Thanks so much for the remarkable talk. Yeah, we have several Yeah. We have much more time for the question mark you have. Question, want to ask the whole project, Hello. Yeah. This was a great talk. I had a, just a quick question. I've been seeing a lot of people talking about, like, training inference log prop mismatch. And I was curious your thoughts, like, how big of a deal is this in the RL training procedure? And is this something that you're team is working on? So far, we find the deviation from the training and the inference mainly come from the different node. Implementation. So far. And, the first one first approach, we're trying to fix it is not know, fix the kernel difference, but to recompute the log probabilities when training. So that offers one choice, but, sometimes it doesn't work. And, the performance may be reduced. Due to other reasons. So we are also working, you know, to figure out how to align the performance of each kernels and And even with the know, different engines on training and in But, I think it what we're trying to do in this exploration is to find another fundamental change to the dual engine architecture. And if a single engine layer can help. But we have to admit is there is a possible to, you know, to reduce throughput or the latency for the inference engine? Both are implemented with Jacks or PyTorch. But that's also we want to Okay. Thank you. Cool. Hello. Hi. I do inference. I'm kinda new to IO. I have a basic question. How do you know the performance gap is due to the differences between training and inference engine. The performance gap. Like like like if you have like a mismatch between training and and jade. We'll cause some issues. So Why why are you so sure that it come it comes from this mismatch? I see. So the issue caused by the difference inference and training loop training engine is mainly reflected on the the the the convergence So, you know, the score for the training loop the models policy model is not stoop. Is not growing as expected. So that is a major issue we found when you know, there's mismatch from the the training under the inference. But not the performance one. So so, yeah, by performance, I mean that. Oh, I see. I see. Yeah. Yeah. Yeah. So are seeing that you have some scenario where it's matched and doesn't collapse. And the versus this doesn't match and it has some issue with like, the loss. You know, a quite easy way is to, you know, to see we have already, adopted the reconfiguration phase for, you know, like many, open source open source implementation we also have a re computation in the project. And our you can, you know, skip you can disable the recomputation and see how the performance improves or this improves. So that that could be, you know, a reason approach to see if there is if it is caused by, you know, the mismatch between the two things. Yeah. That makes a lot of sense. Thank you. Thank you. Yeah. I have a question. I we know that Waw is a open source project. Yeah. And a lot of code are open source. Yeah. I'm wondering, what's your plan for the, like, large scale training, like, digital training? Like, what is the I do remember it only supports limit of digital turning skin. Do you have plan to, like, support MOE better? Yes. Support like, export better, So, for the exact plan, I probably cannot give you the, you know, exact deadline when we also what kind of model by what date. But, from from my side because I put up a high single controller part, what I'm trying to do is to solve the obstacles preventing researchers and engineers to adopt larger models. That is, you know, the limited of from the rate. And, we were trying to, you know, get rid of the single head architecture and see we can scale it up to, you know, more nodes and, provide a safe pass for, you know, up upgrading their model size to much larger one. Yeah. Thanks for waiting. Interesting talk. Yeah. We would like to thanks all our speaker in front of us, like, yeah. And now we are moving into our poster session. The poster session start from twelve and end on thirteen. Yes. And also, a good news is that we will also provide a lunch yeah, lunch taco lunch here. The lunch will arrive at, like, fifty, twelve. Yeah. We all call everyone to like, keep keep here and join our procession and also all interesting car in the afternoon. Yeah. Thanks, everyone. Let's start our post session. We'll start at 1PM. And, yeah, Everyone is welcome to take a seat and Professor Wei will give us remarkable talk. Thanks. Hello. Hello. Hello, everyone. Yeah. Let's start our afternoon session. Yeah. Our afternoon session will start, where we talk today and give by professor Yu Yu is associate professor and Institute of Interdisciplinary Information Science. Tsinghua University, obtained his PhD PhD from UC Berkeley and he was a researcher at OpenAI from 2090 to 2020. Yeah. His research focus around reinforcement learning, multi agent learning, and agent. Yeah. His representative work, including the value iteration network, the MAD DPG, MAPPO algorithm. Open eyes, hide and seek pojae, and aerial pojae. Yeah. He received the best paper reward at NeurIPS twenty sixteen, best demo reward at ICIA twenty twenty four MIT TR certified Asian pack, 2025 reward. Yeah. Let's welcome, professor Yu Wu, for today's afternoon talk. Okay. Thanks for the introduction. And, I'm Yi from Xin University. And, yeah, I know everyone is still enjoyed the poster, so I'll have some background part so we can probably still enjoy the posters, but there are some content afterwards. So today, I will talk about, the aerial, talk about efficient, how can we do flexible and efficient agentic reasoning for with reinforcement learning for large and with broader agents. So this is joint project with Tsinghua University and Ant Group Yeah. So and in this talk, I will talk about a few content, we'll talk about first, what is agent degree enforcement learning? And second, we'll talk about why do we really need reinforcement learning. For agents. Well, discuss that with a concrete example, a search agent. It's perhaps the simplest agent, but it's still very challenging. And also talk about, with particular example, talk about the challenges for efficient agent to reinforcement learning, and then we talk about how we tackle these all these challenges with the aerial system, which is designed the principle of make agentic reinforcement efficient and flexible. K. There's content. So first, let's start with what is agent triggering learning? We know, there are a lot of milestones with models. And most of the milestones are driven by the reinforcement techniques. For example, in 2022, we have JAG GPT which is actually driven by the technique called reinforcement from human feedback. '24, we have reasoning models which the model can think and that's produced by the technical reasoning ring or reinforcement learning with verifiable reward or something like that. And more recently, we have agents and we also have some techniques called agentic reinforcement learning. We want that to be efficient, to be flexible. We want really want the technique of reinforcement learning to make our agent products more powerful. So today, we'll focus on the very latest part, reinforce more in for agents. So So one is agents. Right? So perhaps this year, the the first and most successful agent product is deep research. Right? Basically, you can ask, the chatbot deep research product to help you do a lot of very deep search multi turn reasoning with the final very concrete report. Also, we have MANUS. Well, right. So the difference basically in deep research, the agent is first the first time can really call tools, to multiple search queries and call actual tools to put a lot of extra ex external content in the content window. So the window can leverage a lot of external resources to perform reasoning. Right? And for Menace, it's it's it's beyond that because the agent has a sandbox. So we can do a lot of extra works. You can create a file Right? Can do a lot of terminal operations because you have a sandbox. So at the LM, have a laptop. So the agent is more powerful to much more things beyond. A classical chatbot LM. So, we do believe, right, this is really a trend that you know, when you talk about AI, it's really talk about agent applications. So a lot of concepts for building agent products. Right? You have a really complex environment. Right? You have a sandbox. Basically a laptop. Right? You can have browsers, you have a lot of operating systems. You have multiple reasoning, right, to do a lot of operations, a lot of actions in the environment, and every action require reasoning process. And you can do a lot of tool use. And also, it's multi agent system. Right? You need a lot of different agents but sometimes different models to do a very complex to complex drops. So very complex. So agent is pretty much complex. Right? So what is a gentle ring force modeling? Right? So a genteel reinforcement learning AKA. Alright. This is analog to the classical video game reinforcement learning. Pretty much the same. You have an environment In the video game, reinforcement learning, you have a environment which you can be a sandbox browser tools, Right? And you can you need an agent workflow to wrap the LMs with some tool called API so the LMs can interact with the environment. Right? You need some agent framework like open AI agent SDK, land gen, whatever things. Alright. Developer agent workflow, and then you want to run reinforcement learning to train this agent workflow to scale it up, to make it more powerful. So that's basically the workflow of reinforcement agent take reinforcement learning. Right? And the first question is, why do we really need reinforcement learning? Right? Because we already have an ALM. We already have a very, you know, powerful things. And why do we still need reinforcement learning to make it scale up? Well, a short answer to this question is you have two important things. You can if you can really run reinforcement to scale this agent take workflow really well, You have two advantages, very big advantages. One is simplicity. While simplicity is this simplicity means a lot of things. It typically means efficiency. Means easy to deploy, it means is cheap. Right? So simplest is good. Everyone likes simplicity. It can make your agent work for really, really some The second thing, it can really give you generalization capabilities since a lot of time you can base model to learn a lot of new capabilities. Okay. So this is very high level, you know, a in a very high level words. So here we'll make this high level concept more concrete. We'll talk about a very con a very concrete example, a search agent example. So search agents passed the simplest the agent you can ever imagine. Right? Just give you a query. It's even simpler than deep research. It simply give you give the agent an inquiry, and agent search online give you the answer. May think this search agent scenario is super, super I already have Google. Already have chat g p t. We have a lot of things. Right? But still, this scenario, a lot of the questions or settings is much more complicated than an exam. Here is a very concrete example. Right? Actually from from XBench. So the question is how many gold medals does China team win at 2020, twenty twelve London Olympics? Right? You will think this question is super super simple. Right? You can Google, you can do whatever thing. So I tried that last night, with Chagibidi. Laser so it basically says 38 gold medals. I I I chat with, Germany, 30 gig eight gold medals. I chat with Groc, 38 gold medals. Super simple, super fast. But actually, this is the correct answer. If you are familiar with the background in history and particularly sports, actually, the at this moment, the correct answer is 39 gold medal, not 38. Why? Because if you're really familiar with that actually, the the the the woman's race working gold medal. And the civil medal was disqualified after, like, eleven years. For doping. Right? That's basically the result. So you can see like two Russian players was actually disqualified for doping. So at that time, at during the twenty twenty, year, right, China team got a bronze medal. But actually, eventually, the the the player got a gold medal after eleven years. So if you ask the question now, the correct answer should be 39. Right? Think about this question. Right? If you do some search online, you have a lot of the official resources to say 38. But actually, sometimes you can find some misleading or, like like, or conflicting information. Say, okay, sometimes 39 Right? So which is correct? 38 or 39? You need to do some deep research, some deep reasoning to say, okay. Probably there's some doping disqualification event happened. Okay. And you check the rule of IOC and really, it's it's a rule about doping. You want to make the conclusion that during twenty twenty year, right, the China team got 38 gold medal, but eventually at this year, of doping event, China team got 39 gold medal. That's the correct answer. So there's a much it's like about it. There are so many partial information. There's so many conflicting information online. You really need the agent to efficiently reason about these uncertainties and partial informations to make the final result. So this even simple is even search is such a simple question, it's much more complicated than X Men. Than anything imagine. Then, okay, think about if you really want to do a purely workflow based agent. How can you do this? Right? Think about this. Right? Probably the answer is you need a multigent system. Right? You need multiple modules. You need to agent. You need a agent to check the uncertainty. You need agent to check conflict information. You need a lot of modules. Right? And Okay. If you only if you are not allowed to train. Right? The a lot of agents. And but but what about a reinforcement based agent? Right? So we did a project which we released a few months ago. Called Assructure. It's basically we we use, like, end to end reinforcement military and a simple search agent. If you can do reinforcement pretty right in the right way, I would say, then the agent architecture can be super super simple. It's basically just a 32 billing it's just just an open source model with 32 billing parameters. It's a reasoning model. And I only have two tools, just a web browsing and search just two tools. So it's super, super simple. And the reasoning process can be very because it only have two models. Only two tools, a simple model. So it's super, super simple. Just do that and do wrong reinforcement training and well, this question is from the test set. It's not from training set. And eventually, can give you the final answer. So basically, the behavior is something like this. Like, the agent, like, search for, like, 60 steps and like doing a lot of cross checking type of verification, eventually give you the final answer. Like, after like 60 steps. 60 actions, Yeah. So really, the agent by reinforcement learns a lot of this kind of emergent double checking and cross checking behaviors. Make sure final answer is correct or not. Finally, we do some evaluations on very challenging benchmarks and and I think when we release this project, I think until, like, October like, really, until, like, the latest, the Gemini, and the latest, the cloud model released, the still this day of the art result, Just a 32 billing parameters. With reinforcement learning and with some test time enhancement. And you can achieve a STLVR result on this kind of search benchmarks, most of the challenging search benchmarks. Okay. So again, go back to the question, why do we need reinforcement learning? For agents? Right? If you can do reinforcement learning correctly, for agents, two advantages, you have simplicity, you have generalization It's can make everything much efficient and make your life easier. Okay. So that's the good part. And what are the challenges? What are the challenges that prevent you from applying reinforcement learning to this scenario? Well, first thing, is we have a very good observation. From this project is that so here is experiment, a test time experiment evaluation curve. So x axis is the number of terms. Or number of actions allowed to perform within an episode. Or trajectory or roll out. The y axis is the accuracy. And basically you find that for a simple model, if we can increase the number of turns, or number of actions, you have a clear trend that accuracy will improve. And that's the same thing for training. Right? If you really want the training, your reinforcement can incentivize reinforcement learning to learn some new capabilities, you need to allow the agent to explore sufficient That means during training process, need to allow the agent to do a lot of action turns. In order to make sure the agent can, if explore to discover new capability. So we do allow the agent to do a lot of actions. Actually, in eSearch project, we allow the agent to do a 128 actions. Then you have a more severe problem. That is you can you can observe a very severe long tailed distribution. So so here is the statistics. So y axis is the counts, and axis access is the number of ad tokens So the average search time well, average trajectory takes some tools to search online. So every the average time for every trajectory is like ten to twenty minutes It's pretty pretty much okay. Right? But, if you really check the long tail tail distribution, or some a lot like you can almost like, for every batch, of trajectory, it's like almost surely you will have some very long trajectory that will take, like, 100 actions, and that will take one to two hour. Right? One to two hours. So it's super expensive for some particular long tail distribution. So that means imagine. Right? So you only have twenty four hour a day. Only have twenty hours. A day. And every batch, there exist a roll out that will one to two hours. That means, you know, if you collect a batch to train, then, you know, for a day, you probably only can take like 20 batches, and that's super super slow. So this is kind of an agentic. Right? Because the the environment is super slow. You can't really do much things. You need large exploration. And everything will be super, super slow. So you will really have a slow training. Right? Because some prompt give you two hours some prompts give you like lot of prompts give you, like, the It's it's kind of like okay, but you have this kind of all wired prompt give you super slow. So whenever you collect a batch, every batch will have extremely long just a almost surely will have an extremely long allot. And, you know, then most of the GPU will be idle time. Right? And you're basically wasting a lot of computations. So, is kind of severe. One thing. So it's slow training. The training is very inefficient. I really want the training to be speed up. We want the reasoning process to be efficient. Right? The first thing. Is slow training. Training is inefficient. Second challenge is the workflow can be really, really, really complex. So I like, basically, quote a a tutorial. Like, a survey plot. Survey plot. Basically say, okay. There can be a lot of different modules of agents. Right? You need, like, planning module, You need tools. You need actions. You need, like, a lot of memory modules. Imagine when you want to train this complex agents, Right? There are a lot of modules. And every module probably need to train. Right? You probably need to adopt different models. Right? Need, very complex workflows. And many nested parts will be need to be trained. And so I want to say, training agents is much more complicated by simply training a reasoning model. Right? It's much beyond a simple single model for rollout, a single model to train. It can be a nested model. Some models are not to train, Some models need to train. You have a lot of models, and even for a rollout, it's a very complex process. So we have a cartoon, for this. Right? Basically, two challenges, low efficiency very slow, Right? You have a lot of your players. It's just like all of our friends just like playing cards and why use it. While you are like working hard. And sometimes, you know, when you want to train, because of agent, it's so complicated. So a lot of time, you'll develop you'll need a lot of engineering efforts It'll be very complicated. So, you know, train deployment gap, it's very it's very inefficient for development. Training is very inefficient because of this kind of long tail distribution. And that's what we really want to address. So we can have that. Have the idea of developing new system called iReal, So the Aerial, we really want to design for agent to reinforce more learning with sufficient and flexible in mind. So the key idea is we really want to decouple everything Really want to when you change one thing, the other part of the system or your code, you don't need to change anything. And that's what we want to present. And we believe that's the key for efficient agent development and efficient agent training. You need to decouple everything as much as possible. So the first thing is, we have an idea of asynchronous reinforcement learning. So we basically decouple training and inference. And the reason for this is for the training, you don't need to wait. So the trainers, the training resources just keep running on. Keep running on. And when you finish your rollout, just send to the trainer nodes. And the trainer nodes have a data buffer. Just keep whenever you have a batch, just train that. So in this way, you know, don't inference engine doesn't need to wait. Just continue generating new batch. Whenever you have a new set of parameters, you interrupt all the ongoing rollouts and sync them with the latest weights. And then, you know, you are there were there will be some trajectories that partially is generated by a old version and partially generated by a new version. It's kind of okay. So just continue doing this as continue training, continue to sync up the weights. In this way, the generation will have no idle time. The generation knows we'll have no idle time. So basically, break the long tail distribution in a particular batch because there's no batch generation anymore. It's continued generation whenever it's there's a new way to sync up with weights, and continue to generate continue to generate. So there's no gap. So with a proper separation, our decoupling with the training resources and generation resources, you can achieve no way to reinforce malaria. With fully utilized. GPU resources. And in in the e search example particularly, we have more than five x speed up in the training time. This is super, super fast. But every coin has two sides. So, you know, in this change, you have some rule out of a across different model versions. Some of the parts of the model is generated by the old version, and some are part of the model is generated by another version. So we have this kind of trajectories work across different model versions. So, we have a very classical important concept on idea or term in reinforcement called stillness. So we call a sample stillness with two is because the the the oldest, the model version generated by this sample The different the model difference between the oldest the model generate for this trajectory between that and latest model. So for this particular red trajectory, the stillness too. So you you know, in order to be fast, you need to really careful about staining the sample. Because if you don't do anything for the stilted sample, we have some experiment result basically saying that if you if the if the data stillness increases, then the Right? So that's what that's something you don't want. So you do we have some idea called system and algorithm co design. We design a system that can do efficient generation, streaming generation. It's pretty fast. Also need to do some tiny changes to the algorithm side. To make sure, the algorithm can tolerant such a still samples. Okay. So we do some simple changes. On the system side, we have a centralized controller. We'll basically check whether there's some very stilled sample. If they are really a very stale sample, we'll pass the generation process to wait until you have the longest sample to finish. Well, why we want to wait in instead of, like, just to throw it out? You need to wait. You can't throw out a very stale sample because you're throwing out a still sample, you are essentially putting a very bad bias the model. You're running a rejective sampling. So whenever you throw out the sample, you are basically telling the model never to generate this So never throw out any sample for a reinforcement. So but, you know, you don't really want a very still sample braking system So we have centralized control to monitor the still stillness of the samples. If you really have an off layer there, you have to wait. But kind of an off layer is very rare, so it's kind of okay. So the system side. The Argon side, I want go to the details, but basically we do a very simple change. A very simple change just like we basically decouple the sampling term and the trust of region term. If you are interested, you can check our paper. It's a very tiny one line of the code change. And then you can basically can ensure also basically ensure the model can tolerant of stillness, like, up to eight. 16. Stillness. So with that, everything right? You can basically have a very good training curve that can even observe emergent behavior in a agentic in a for the search agent. So the middle part is basically the average The purple curve is average of actions. Per episode. And the and the blue curve is the maximum actions per episode. You can see a very clear increase of emergent behaviors just like the the r one zero project. Okay. And also, the other part is we do a lot of we do a lot of optimization on the aerial system. So in our latest version, we can like, only support training of a a channel one two hundred and thirty five billion model. Was only six nodes. Which is, like, 48 GPUs. Okay. So that's really also resource efficient. So we really want everyone to have a chance to train large models. Because large models are the most powerful models. So this kind of a training curve. So release this feature in the next a few days. So, that's the algorithm part, but also we want to reduce the engineering efforts. Right? So we want to make everyone that can do minimum out coding change engineering change in order to run agent take reinforcement training. So we have a feature already there is you can develop a with opening an agent's SDK. And you can basically change your code with one or two lines, and you can run agent reinforcement learning with Ariel. So we can just use the open AI agent SDK to build an agent. And then you just define a reward model, then you can just launch. Everything is there. You could don't need to do anything. So the key thing is we really designed aerial in a principle of everything is service. Because an agent is an agent service, training is a training service. Everything is with some with with with kind of this kind of agent service interruption. So it's pretty much modular. So when you want to change the front end, you don't need to touch the back end. One one change the back end, don't need to touch the front end. And so, and more and more recently, we also support multi gene system and multi model training. So here is an example of TAU bench, TAU two bench, TAU square bench. So TAU 12 bench is like a customer service scenario. So you do need to and a service and a database. To do a lot of this kind of conversation. So you do need a lot of models So how can we do this? So we do support multi agent training. In like, particularly you can support multi model training. It's same thing, particularly if every model is just a service. You just launch service. Different models, and it's already there. You don't need to change too much code. It's already super simple. Have a one line code and launch a service. Another line of code to launch another service. Everything is there. And also, we have some, scripts language to specify, different allocation mode and parallelizing strategies for different mode. Different models. It's very, very convenient. So this multi agent training and also is all. End of this month. We'll even support a bit, like, all the agent framework beyond just the OpenAI agent SDK. So you can do whatever agent framework you want. We'll basically use URLs, HTTP URLs to redirect your messages to the inference models to our training engines. So we can base the with this, design, we can achieve, like, zero code change. To run agent t reinforcement learning. So we just defined whatever, agent workflow you want. And we will do everything in the back end. To ensure all the data will send it to the trainer engine and will update weights. So this kind of zero code reinforcement lanes, like, in experimental animal probably release that in the dismounts. Later this month. And also we can do multitask trainings very straightforward. And more importantly, because the back end is really decoupled, is very flexible, So then some people will think, you know, we have different GPUs. Right? We have inference GPUs. We have training GPUs. So, you know, inference GPUs is better for token by token generation, and training GPUs better than forward and backward. Right? And inference GPU is much cheaper. So we have a project called Aerial Hacks. So we are basically, you know, because we have such a flexible back end so we can use inference GPU to do generation. Training GPU to do training. Or profiling. So So if you can do a very well balanced, you can reduce the cost like 40%, the cost. So so, for example, for the, 14,000,000,000 model, 14,000,000,000 parameter model, if you use all the edge h hunt h the training GPUs, you may need, like, a $100 per year per per per like, per hour. But if you can use a mix using a real HEX, then you're only, like, $80 point. Power. It's basically, like, it's basically 40% cost reduction. Super, super cost efficient. And also we have some more, things to release. So in the following week, we have a lot of components. Well, will be open sourced. With our end, the engineers in in the end group. So to wrap up, so basically, it's a real So the design principle of ARRU is really to decouple everything. So we really want the engine so the users can change as fewer code as possible. So do what you want. So it's basically, we want to change from a sandwich to a tapas. So hope you enjoy tapas, particularly in San Diego. Right? Okay. And that's it. If you are welcome to use our project and we thank a lot of the team members for developing this project. And thank you. That's all. And, I think welcome to questions. Hello? Yeah. Thank you. Thank you so much for the amazing talk. So, I think you mentioned agent take r l is simple at the deployment time compared with multi agent system. At the very beginning of the talk. Yeah. So if you can run ring training, then you can simplify your architecture. But but late later of your talk, you also talk about like, a agentic RL system. Is complicated. So I kind of see the contradiction here. Oh, yeah. So definitely, I mean, so so the first of all, agents agent take a agent take system is already very complex. For example, Yeah. In in a lot of, like, example, like, the customer service scenario, you always need, like, different modules. You need different module during training. You need that model to simulate user, one model to simulate the customer. Sorry. That that service and the someone to maintain the database. So you always need a very complex But if you don't have reinforced learning, then the comp system can be more complex. I see. I see. I see. So it's basically make a life easier but life is already tough. Yeah. And I think yeah. I think that's a very good clarification that My second question is that I guess it's a follow-up question. Then how do you like, what are the criteria to to determine if I should use a multi agent system? Because I do believe that there's still some good use cases for the multi agent. System. And probably this like, agentic RL is only good for a subset of it. Can you give us a framework? Well, so my suggestion is always try to do as simple as possible. So, like, when only when add a new agent when you really need that. So because already a very complex system, so try to reduce the complexity as much as you you can. So that's my suggestion. Based on, like like, the customer service, scenario, you you at least needed two. Right? This can't be simplified. Yeah. Yeah. And and from that perspective, I do see that maybe for any real use cases when we need to deploy something let's say very fast to the production, Uh-huh. Always start from a multi Yes. Agent system. And if it doesn't work, or when the system complexity gets to some threshold, that is very hard to manage. Then consider migrate to the agentic URL which actually as another layer of complexity of, you know, training, curating your own data. So it it's still a cost you have to manage. Okay. My my suggestion is the opposite. Because adding things are always easier than removing things. If you really deploy something. So trust me. So it's like from system development, I think for system design principles, always suggest suggesting called never optimize too early or, like, or never do, like, never make your system too complex. Right? So it's it's I think start with a simple thing. Right? And then think I mean, look, I I agree that deploying an agentic RL training system it's simple at test time. Because you don't have a lot of modules. But developing that system is actually more complicated. Than a multi agent system. Because think about it. For the multisagent system, you don't have to fine tune your model at all. You just you know, have a bunch of agents which to in many cases just wrappers of the closed source model. It's very easy to deploy and if in some use cases, it solves the problem, I actually think that should be the first thing you try. Sure. Yeah. This this is something I totally agree is that I will always suggest to start without IRL. That's always true. Yeah. Because I always do, like, this just a complete layer. So I totally agree. I see. Oh, then then I think we're on the same page. If I have time, can I ask the second question? Yeah. The second question is that any support for the process reward when you do this, agent take our orcas? It's it's just support. Alright? Basically, rewards always is also a service in a real, so we can implement whatever you want. Process reward is I think we also have papers using Arial to do some process reward stuff. Okay. Sounds good. Thank you. Yeah. Thanks professor Wei for his Yeah. And then let's move on next to next century. Next, it will be host professor Yuan Dun Tian. Is ex research scientist adviser and His research direction cover multiple aspect of decision making including learning, planning and efficiency as well as the theoretical understanding of LR. Yeah. That's welcome, professor. Yandu. Not going Thanks for the introduction. I'm still setting up. And hopefully, we done soon. Okay. Okay. So I think I guess everyone can see the, see the presentation. Yeah. So good morning. Good afternoon, everyone. My name Nguyen Dong Tian. Today, I'm going to talk about efficient reasoning from in-depth understanding of large and more behaviors. So we all know that larger model has been, leveraged in many different scenarios. And, our previous group, Ian, Meta, has been working on reasoning and planning, for a while. And, of course, like, we probably wanna think about, like, what's the next step for reasoning and planning. Given that there's so much progress, in the last few years. So one thing one image I I really like is the image from Apple AI that shows the progress of large models. Year over year, you see, like, the training compute is exponentially increasing. Given the linear rate of the publication date. And, also, you will see the same behavior for data Right? So, year over year, you'll see the data use it also exponentially increasing. Of course, there's no enough data at some point. And the the the data that's being created by human is now obviously growing exponentially. It's growing, in a linear rate, so you appear if appear to be a logarithm, curve. So at some point, these two definitely will, come other. Right? So if we have been using all the possible data, in the Internet, what should we do? That's actually one of a question. But even if we already combine all the data, and all the compute and to train our AIs, And we have a very strong AIs in many different scenarios. And we have, GPT five. We have, Gemini three. Have lots of strong AIs, but still, if you compare different axis, the current AI is still not as strong as humans. So, for example, in the training data efficiency, we actually see, I mean, a thousand, a thousand x gap. Between SOTA, large models, and human brand. Human brand only need maybe, like, less than a two 10,000,000,000 text tokens. Maybe if you keep reading books for the entire for, like, for all of your seventy years, and you you you may be able to get to, like, ten ten billion tokens, in ten But if if you compare this with the SOTA IRMs training, it's actually a thousand three magnitude, three hour magnitude difference. And the power consumption is also a huge difference. And, not only that, but also the human AI system is actually super efficient. It will be you will see human can get to the points of the knowledge with just maybe a few samples. Maybe even like one or two samples. Right? But for the sort of other large models, you actually see the case that the model typically needs, like, hundreds of thousands of data points. And it still may fail to generate. In the cases that human think it's easy to generalize. So these are actually the big gap between the two. So I mean, of course, one question will be, how is human learning so efficiently? Right? So, human can generalize things very well. And the model does not do that. Right? So, I would imagine that at the beginning, we have GPUs. And, we use GPUs to, generate lots of data. In the exponential search space. Okay. For this given all these points, may come with some interpolation. To understand what's going on. But going forward, we're not everyone will become GPU poor. Everyone we want to, everyone want to explore so much possibilities in terms of the empirical results. And it's going to be very hard to use number of the the exponential number GPUs to fill in the entire space. Not possible. We probably don't cannot afford to use all the electricity, to fuel the GPUs in a planet. So instead, what we should do is we should think about open a black box, find marginalizable rules. Marginalizeable laws, so that given the few points provided by TPUs, we can do better in the air extrapolation. To cover larger subspace. But in order to do that, we really want to open a black box. So by opening a black box, what we are trying to do is that there are different level of it. Alright. So for example, instead of only staring at last term performance number, We should be able to check the patterns, output trained models, So we're gonna check weights, check activations, check attentions, check all the structures then maybe check, like, a pattern of layouts. Check all these, then you actually obtain more information from a single data point. Right? So given when GPU hour or several GPU hours, you actually get a lot of information compared to the case that you only care about the final numbers. So of course, these two are basically like a in kind of like a rudimentary way of understanding understand the mock how the model works. More advanced, going forward, what we can do is, we probably want to come with some mathematical theory and, some kind of framework that can understand how the model get learned, the dynamics of it, and use that dynamics to come with a better way explain how the model behaves. And finally, it can come up with a theory that govern, the how the pattern emerges. That's the final goal of it. So in this talk, I'm going to talk a little bit about examples, of this philosophy, and how this philosophy will give you, practical algorithms. There are short term and long term, access. The short term access is that, oh, we have already checked the patterns that you see in your experiments, and we want to find, for example, nice structures. Given nice structures, maybe you can develop algorithms that is very useful and can improve efficiency of current models. And the long term, would say, we want to actually understand what kind of representation the model learns, so that, the representation, can be learned better, and this will to fundamental changes algorithms in the future. So I will cover mostly short term, but also long term, in this talk. So, for short term, I'll give you some examples. The first example I tell you, I will give you is a is a very classic example. You leverage attention sparsity. So this attention sparsity structure is basically, like, being discovered in several of our previous works, shows that the attention will be sparse and over time during training. And, the the attention pattern will be dependent on the query. So, and, given these kind of findings, we actually have a previous work called Attention Sync. In that paper, we actually discovered that, the first few tokens, attract lots of attentions. And, by fixing them, we are able to find an algorithm, that can take only the first few tokens and the sliding window of the recent tokens, put them together, and this combination of kvCash is able to give you super smooth generations. And without memory outbound and without, capital tokens. And at the same time, we actually achieve constant memory usage and stable capacity at the same time, and you can basically extrapolate the slide down of the window to a very long window, and it's also faster. So this approach is actually showing that it's very useful, after two years, since it's released. So we have already accumulated more than a thousand citations, and, this approach and its extensions have been used in GPU models in pretraining. By basically, like, having a trainable attention sync tokens, and we are able to if the model is able to train smoothly, without the spikes, And, we also see that recently, QN's Getty Attention paper, which you'll see received, this year's in Europe's 2025, best paper award. Also, stem its own study from the attention sink phenomena. So basically, like, once we have a gated approach, we are able to, we'll be able to, like, get rid of the attention sink. And at the same time make the entire training much smoother and get a better results. So you can actually see that, with this kind of, sinking process, we are able to identify important patterns, during the training. And use that to come with a more efficient efficient, efficient reasoning algorithms. And at the same time, in the long run, improve the training process. So, we here, I'll give you another example which is called the DeepSync with Convenience. This is a very recent paper. That we published. So in that paper, what we do here is that when we do multiple rollouts in Resiliente, usually, we first do lots of robots, and then, we do majority of all, and then we get the final results. But, we find that it's interesting to see, doing the rollouts of these, thinking traces we don't even need any supervision to decide whether each of these robots are useful or not for the final, majority vote. We find that there actually exists, like, a very simple very simple criteria called confidence. And use that confidence, we can get rid of some of the thinking traces that are known to be non confident. And use the convenant syncing traces use them, in majority vote. And this actually give you better performance. It's a very simple approach, and, it actually works quite well. So, in AIM twenty twenty five, what you actually see is that the model that, coupled with the GPU assist one twenty b, models. It can achieve 99.9% of the accuracy, in AM 2025, with a majority of the load of five five hundred and twelve after the deep confidence filtering. At the same time, the usage of tokens is actually much fewer. Because most of the routes was being killed, and So we're able to queue up to, eighty four point seven of the tokens, and at the same time, you actually preapproved performance. So you can try the same thing for other models like DeepCAB, NVAC could give you also a pretty strong performance, 87.4, At the same time, you see, substantial drop in terms of token being generated. So that's, very good. You get basically better both worlds. So you get a better accuracy, and you get, an elastic token usage. So, what's the secret The secret is actually, very simple. We just check, okay, the confidence from the property distribution. So if we're to define confidence being, the sum of the log probability of all tokens, and we have average over the number of tokens. So it's very simple. So once you have that, then you can compute the confidence score for each of the routes. So, you see that, oh, if the route is correct, then the confidence tend to be high. And the loss is not correct, the confidence tends to be low. So this is happened this has seemed to be true for multiple definition of contents. So we would because we have confidence at token level, we can define multiple, different confidence for the sequence level. So there's different ways of defining those confidence. And we can basically filter out, the the the rollouts with different combinators. So these are basically the other criteria. You see that for all these three criteria, you see a clear separation between correct and incorrect. Robots. So this actually, give us a hope. Right? So then of course, we can leverage the confidence to filter out the the incorrect samples. Use only the correct raws to do majority of it. So, given the question, we first can do online generation, which is parallel parallizable, and then we check, okay, if the confidence is consistent and higher. Then we accept. Otherwise, we just reject these, generations. And then we only use the accepted draws to do majority vote. So that's, and we actually have very efficient implementations in VMs to implement that. So and we see that in both the offline and settings, the performance is actually, pretty good. Here, offlineonline basically say, we have to set some threshold for contents either in offline manner or in online manner. But in both kind of settings, we actually see pretty good performance compared to the baselines. It's a very simple approach, and it gives you a pretty strong boost. Yeah. Okay. So that's the example of, leveraging these short term nice structures of the, of the of the output. Of the neural network or the main representation of neural network, to improve your reading systems. And this actually actually give you pretty strong impact. So now I'm talking about long term So in the long term, we probably want to understand, like, what reputation we learned, during the training, so that we can leverage them. To develop new algorithms. So one paper I want to mention is a paper called the pass note taken. We study the delta w, the weight difference between reinforcement learning and So So this is actually a paper that's also present, in this workshop. So it's interesting to see that, if you actually check the, data weights of STT and data weights of reinforcement learning, they are actually having very nice, very different structures, and they learn different things. So of all, you basically check, okay, how much degree you have the weights have rotated after, this either eye or SFT. Training, you see that there's a huge difference between the two. For SFT, usually, the weights the singular space. In each of these weights of matrices. They rotate a lot. Especially, physically like a four four like, smaller eigenvectors, I notice even more for a large eigen eigen eigen eigen space, it should look at less, but it's still much larger than the reinforcement learning setting. So that's actually a very interesting observation. And, what happens is that, we actually check okay, for the data weights that reinforcement learning has already been changed, and you check what's the mask that correspond to the update. You see that the updated mask is actually correlated a lot with the low magnitude mask rather than the principal mask. Alright. So, by principal mask, it doesn't mean that, we first do we keep only the first few, singular vectors, and then we reconstruct back to the waste space. And check what's what's the magnitude of each of the, locations. So you see that the update mask is very well with a lower singular vector space. So that's actually very interesting. So IOA is actually take a detour. It actually optimize the low singular space, and keep the high singular space intact. That's different from SFT. SFT is basically changing the principle of the pixel mask. So So, we also do some experiments. So since we now know that the the the weights that IR has changed, Then what we can do is that, okay, how about we train and enforce money training and enforcement on ARMs, but we only ask the IR to change the principal weights. But not the low singular, vector weights. So it seems like if you ask the IR to do that, then it doesn't work. Very well. We actually tried LoRa versus Pizza. PISA is the the this kind of LoRa version that only focus on the first few, singular vectors of the weights. And you'll see that if you, run a piece, I always reinforce learning, and performance actually drops substantially. Sometimes, it will collapse. To zero. But, if you run LoRa, run higher with LoRa, which is allowed to change any, Wirespace, the performance is actually stable. And we actually see, like, if you actually check the KL divergence, KL loss, between, between the the model and the original model. And what you see is that the our if you train in this small space, this small single space, the the behavior is very similar. To the case that if you train with the entire space. But if you constrain that, the higher training is only focused on the principal weights, then the behavior is very different. Yeah. So that's actually interesting. So basically, that's why we call the the path not taken. So it seems as it's it's possible that we the ire is trained on a very different space, and that particular space is very useful. To keep generalization to keep the model not forgetting, any other previous things. And to make the entire training stable. So but on the other hand, for SFT, things that turn out sink sink in principal weights, it might destroy the backbone of the models, and it might give you, issues in my lead to overfitting, etcetera. Of course, we basically give you an idea about examples and observations. Right? So what's insight, can be further explored? So in this paper, we also give demystify why the gradient provide the weights updates can be sparse in reinforcement learning. This mostly is because of there's a DPF 16 position underflow. So because in reinforcement learning, lots of ways to basically change very small in a very small manner. So if you use b f 16, then the the amount of change won't change the quantization level. So, basically, all these dis weights appear not to change. So, and this is actually perceived very noticeably if you compare the base model and fine tune our reinforcement models. All the reinforcement models being reinforcement tuned, trend, for example, trained with JPO, with reinforcement plus plus or DPO, you see a huge sparsity. But if you train with SFT, you actually see very low sparsity, which means that many of the weights have been changed. So that's actually very interesting. I mean, how reinforcement learning can achieve much better results with much fewer changes of the weights, remains a mystery. And it can be a and it can be a further walk But the observation itself is quite interesting. Okay. So, finally, I want to also cover our previous work at the latent space reasoning directions. Which is also, true focus on, like, basically, using the representation to do reasoning. So if we understand the representation better, then maybe reasoning can be much more efficient. So, the latest based reasoning, is, one idea I try to replace the output tokens in the reasoning traces, with a continuous vector. So So if you use continuous vector, which is typically other vector, right before softmax, And then the hope here is that we can basically use more we can have more information for each output, and this information will be fed back to the large models. Without quantile without, like, sampling. So if you do softmax sampling, then, of course, you have to basically force the model to collapse into representation that is discrete. But if you use continuous factor, then there could be much more things inside a discontinuous representation. That is the idea. And the idea actually works to some extent So we actually have, initial results on this original coconut paper. That shows that it can actually worked quite well, for some of the datasets that contain tricky structures. So, basically, that that dataset has this graph reachability problem that asks the model to see, which destination can be reached. Given the starting point. In that particular tricky cases, we actually show that this approach can do very well compared to the, channel sorts. So so, and we actually have a theory basically tell you that for the the distributability problems, the channel faults, If you use the squid version of it, then you need, like, o n square. N being, like, number of nodes. Being the directed graph. In order to find, the reachability problems. But if you use continuous chain of thoughts, then, you actually can use order n o o d o n, number of steps. With o n embedded in dimensions to solve the problem. The reason why it can work is because continuous channel source, will store superposition of the embeddings. Right? So while the discrete channel source can only store one of the, node at a time. That's the key difference. So if you have a vector that can store multiple, possible frontier of your search, then what happens is that every time you do one step search, it basically expands its frontier to to the larger frontier in the graph. And then you don't need to worry too much about which syncing traces is the correct syncing traces, because we have already simultaneously enumerated all the possible syncing traces. With the superposition of all the node embeddings. And once, if you do this iteratively until the frontier of the node touch the final target, which is basically the target that you want to reason about, then the model realizes that, the model has reached the destination. And once the model know it reached destination, then it can back trace find what's the best syncing traces. That you can the the syncing should be able to follow. Then, the model the the problem has been solved. So I think, as a human, usually people also have this kind of experience that we first come with an idea and then know how to how it works, and then try explain how these things should work. With discrete, thinking traces, with language. Right? So but the the syncing tracer itself seems to be a black box. So we don't know how we think about that. Once we think about that, we can back trace and we give an explanation to other people who don't understand. So this, basically, tell you how this works. This gives you a mechanism to how this can work. Alright. The latent vector can store a superposition. For multiple possible sinking traces and multiple front terriers. Once you hit some hit the target, and then everything can be explained. So this this is much more efficient than if you do discrete thinking traces once at a time. If you do that, then you probably will hit that end, and you need to back trace, and hit that again again to back trace. But with this representation, the the signature the search becomes much more efficient. So So we actually do experiments to show that this is indeed the case, by checking the inner product between the can you source vector, and and the nullity values. You see that if the nodes are not reachable from a starting point, the attention the the inner product is actually almost zero. So very spread very spread and about central zero. But for nodes that are promising, they're frontiers, and they're optimal, then, the inner product actually is higher, which means that they can use sort of vector as a container key information. And the key information, over time, will become more and more salient that's that's interesting. Of course, people may ask, okay, why there's a difference between frontier and optimum? Right? Given the previous explanation, all the frontier nodes should have the same weights. Rather than optimal being being, like, having, like, a higher weight. So, interestingly that, you can actually think about other possible scenarios that the model can, find the channel source, to find answer of the reasoning, even without syncing traces. Right? So, we actually have a paper, already this year that explore these possibilities. This this is the same thing as setting as the previous settings. So that, you have all the you have the graph, and then you want the model to predict the shortest paths. But the shortest path, but we don't want the model to do any thinking. So there's no CoT. So in that case, it's interesting to see that the model is able to give you pretty good results Not only that, you can actually see there's actually a interesting correspondence between the representation, after the model had trend, and the representation as it computed from the graphing balance. So since there's a graph, a natural way to compute representation without doing any deep learning is to use, graphing bindings. So you have this graph, and then you do some transformations, you compute its Laplace, and you can get an embedding vectors. Now this is embedding vectors. This is embedding vectors. Vector that usually people will use to analyze the graph structures. But, but this the the top part has nothing to do with the neural torques. So this is some analysis people come up with as a mathematicians. But interestingly, when we actually train the models, without channel thoughts, the model actually figure out the representation that is very, very similar to the edge emitting lightings that we actually get. From the from the graph and operations. We if you actually check correlations, I mean, there's actually very high correlations between, the graphene runnings computed by graph La Plasian and the embedding that is, computed after you have trained this model. For long. So it's actually we can actually, basically, like, find a way to, basically, get dump these algorithms, used by the transformers. So from transformers, we can actually come with a new algorithm, and find shortcuts pass. So algorithm is very simple. Right? So you first compute the embeddings, and then you just do a greedy approach. So this this is an algorithm people never heard about. So this is a of course, it has no guarantees, for shortest paths, but interestingly, you can actually find optimal paths for a very small graph. To 99% of the time. So this algorithm is actually legit. If you do you apply this to a graph of size 200, 100, you get maybe 70% accuracies. So, then this basically tell you that the maybe the neural networks actually can do, some kind of reasoning, for this structured path. Even without using any channel source. So without using a channel of sorts, the model already come with some graph representations then use a representation. To, have a few candidates. For the shortest paths. Then can do a chain of thoughts to further refine them so that the final results become better. That could be one possibility. Yeah. So and the reason why you see, the optimal and the frontier are different. It's probably because the model additional learn these representations over time. So So it's actually very interesting, and we're going to continue to exploring, whether there is any principles for the channel source ideas. And how and when the model get the answer beforehand or when the model really needs the channel source. To give you the final solutions. Okay. So, it's the time. So summary is that first of all, if you want to explore exponential search space, everyone is GPU poor. Everyone is equal. It's it's very hard to use all the CPUs to even all the possible situations. So people have to think about how to find a way to generalize, how to find generalizable loss in order to get the right ideas with fewer samples. One way of doing that is to open a black box to get more examples, more informations. And how the model works. And the data algorithms, from these different sites. Right? So and this basically give us, like, a unique mode if we have unique understanding. Right? So in the future, if we don't really a lot of compute, we don't have lots of resources, maybe we should aim for more understandings. Then everyone will have their own ideas and come up with a newer solutions. So in the future, hopefully, instead of, like, printing front of computer, to say it works, we should open a black box of the neural networks and then, find something better. Thanks. Any questions? Hi. Yeah. I think you'll find good patterns on the attention sink and many other findings. I'm just wondering, can we design it's just like a pattern recognition Right? Can we design some of workflow or let AI to do some of this work? Okay. That's a great question. Yeah. I think in the future, definitely, it's possible. We can it's possible to use AI to, to to work on AI itself. So that, we actually have automatic pattern matching way to find find new discoveries. Yeah. So I think the future is possible. But I will also say it may be constrained by how current AI system can can find the patterns. Yeah. Because these patterns are very subtle, and the human has insights human. Human is human is super efficient on learning, so it can come with a new insights with only a few samples. Yeah. I feel like you kind of have a methodology of how to I also sir, listened to your podcast. You said use fewer data points to guess identify that pattern. Yeah. So maybe a good summary of these methodologies will be very helpful. Okay. For, like, us young people. I see. I see. Okay. Thank you. Thanks. I have one very short question. I know we're out of time. We tried coconut, We think that you know, it's it's it excels at many areas, but also struggle compared to the like a token space chain or T on others. In another word, it's very unstable. So in order to make it work and like, mass deploy into production, what do you think is the path forward? Somebody argued that we need to do that. At the pretraining stage. Do you agree? Yeah. I think how these a way of seeing papers that can can can do this in the pre training stage. And, for example, people propose to use, like, the interleaving channel of thoughts, continuous and, discretion of thoughts. In order to get this to work. I think one big problem is that we don't know how to supervise these latent vectors. Yep. Right. So and right now, people usually are using for example, a mixture of sub tokens. We have tokens, each have embeddings and mixture of embeddings. That that might be one of the way of doing that. This is also consistent with the self fulfilling prophecy position picture. That you can do. I think going forward, I mean, the long term run of course will be okay, we want the latent vectors, and we want latent vector to be super creative and to find the right representation so that the reason it become very efficient. So I think we want to explore many different kind of variations so that some of them can work better than the others. And I see. I think soft talk can do Can we work? Yeah. And it seems it seems that you do believe that a mixture of the token reasoning and the latent reasoning Combining them is the path forward. Yeah. That's combining them definitely will give you more efficiency. Because then you have partially autogressive and also partially fulfill. In this kind of stage. Got it. Yeah. This will definitely one way of doing that. Yeah. Thank you. Okay. So if you don't have more questions, then we can start this panel stage. Thanks. Yes. Test. Yeah. Thanks for your interest. Remark. We all know a lot from to make more reasonable explainable. So, yeah, and we will continue as our panel session. And Yuan Dong is also one of some of her most important panel session. Yes. And I will also introduce another panel session, and left one is Zhong Wei. He is a Baidu seed researcher. Many folks are on the post training Yeah. And the later one is the Nidiang. Yeah. He's flown Microsoft. Research of Microsoft M AI and doing most of also the post training work. And there's also. Yeah. We all know him. He he was the speaker. And Tao Jiawe, he's the meta researcher Many folks doing efficient reasoning. Yeah. I'm all about to host this session. And to ask question for all the panel speaker. Yeah. Okay. Now let's start our panel session. The first question will be continue with previous talk. Large language model remain fundamentally black ball system Yeah. As Randall's talk said, recently there are a lot of work trying to open a black box For example, security level analysis, Providence Life have began to reveal how the model store language, how the model doing listening, and how the model implement represent our task. So the question is, how far can opening the black box actually guide the design of more efficient reasoning system. Or in other word, can this insight translate to like, loyal design such as more efficient model with structured memory, like efficient k v cache management, or better attention magazine. Yeah. Let's give time to panelist. Thanks for the thanks for the question. So I think right now, we already have a lot of, work that, try to explore how the reasoning actually happens. In the models to understand what's going on and leverage that to improve our our model efficiencies. So I think, in my talk, I already mentioned some examples. We have more examples, due to the time limit, so we don't really have time and effort to to time to to explain. But I think a lot of work has already, been going along in different directions. Right? So I think many of the existing study existing, like, a mainstream architect design are also following that path. Right? So for example, the the reason why people use MOE is because FFN layer is huge. Right? So you have a lot of parameters, and you have to use a way to set specific parameter into different, GPUs to different machines. Right? So then that's why you people have tensor parison that's why people have MOEs, have routers, etcetera. So these designs are all constrained by that and constrained by other and insights. Of our models. And we find that, oh, doing the feed forward only a very fast number of neurons that get activated. Alright. So then, naturally, people will think, maybe should group them together into multiple Express. That's why m o e comes. So, basically, we find spot patterns and then we go to the MOS stage. So this is examples into how we leverage the insights to develop new acquisition approaches. So, and as to the more principle way of understanding the neural networks, I think it's a very hard problem. And but I would say, eventually, this will happen. I have I'm being very optimistic about this. Right? So and some people may not have the same optimism that as me. They will say, oh, model cannot be understood. But I would say I am optimistic. I think, eventually, it will happen. This is based on my of my simple reasoning. Right? So here's my reasoning. If AJI has achieved and the human they have nothing to do, then they will find they will try to understand what's going to make sure that he must be available. If the model cannot achieve AGI, then we get stuck and we know most CPU doesn't help. Then we have to sit down and understand what's going So in either cases, we need to understand what's going on. So that's the path that everyone need to go through. So I'm optimistic. Is there any other one, two, Yeah. Maybe I can continue. Yeah. I totally agree with agree with that. I think model improvement is definitely a good direction to go. But I'm not quite sure whether it is too long or too short. Maybe you need to take a long time to to find a new, component, but maybe just the single moment we come up with a new component, and that works very well. So But, from poster training perspective, it's better. I'm doing the RA infrastructure. So I feel like, when the model changes, RL system is quite sensitive to that model model modification. So so currently, some of the model people, they are they want to do some pre training exploration and just, in just, you know, maybe few days. They want to do the whole stack. Don't want to just focus on pre training, and then we try the post train and post train that So because for RL, I think yeah, if if we can memorize something and make the model smaller, that definitely work. But for RL, sometimes maybe negative samples is also important. Because the RL can if we can generate the negative samples, and RO will use that negative sample to avoid generating the same patterns. So So, from my current belief, I feel like making the smaller models like is more suitable for, like, we distill our large models to our smaller models, and use smaller model for inference or serving. But for audio training, I I guess we probably need more research. Yeah. Yeah. So I basically do search on pretraining and I got capture, and I look at example on intention mechanism. Like, so people are used to publish blog post on, like, a the understanding the cost and learning and the induction pass. They found that, you have two layers of transformer, attention will learn to have a pre fixed machine and then do a copying. But if you only have one layer of attention, then you cannot do this kind of induction test, which hurts the by a lot. So so so so so so this kind of a mechanism capability by doing some synthetic task pretraining actually helps the area to develop new mechanism to do the injection head in one layer so that we can have better capacity. So they actually use a very simple trick to actually to have, like, a short collision and to let the key vectors to be aware of the neighbor tokens. So that the attention can actually retrieve the in to do the prefix matching in one layer instead of two layers. It actually can help, like, to model to converge very faster. And it has been widely used user for letting your attention mechanism and the ARNN like a or like a a, something like that. And so it's actually mechanical interpreter has been very useful for architecture development. And I think it will be useful. But it also has some like, on drawbacks, like you do experiments on synthetic data, so hyperparameter will be very sensitive for your performance. So it it's not always like a very statistically significant correlation there. But, actually, I think actually, it inspired you to do something like to do some modification to architecture. So it's still have for even though it may not scale in some in some case. So, yeah, scaling is still very hard, like, to have some scalable predictions. With the mechanics and interpretive Okay. Yeah. Thanks for answering that question. Let's move on to the second question. Our second question is about the scaling door. Yeah. As the recent model skill are beginning to see some phenomena or even a law like the scoring or like one. For example, QA can sample multiple reason in pass, and aggregate them to majority watering. For example, if you test the pass one accuracy, only about 68% by field test pass 500 the accuracy increase to 80%. So, does the panelist believe that reasoning model has such scaling that we can eventually formulate it And could such scaling know go continue to go? And guide the design of more efficient reasoning model. Yeah. So I do agree that, for the post training, already inside the skin laws are very important, and, we should even design a new skin allowed for skin test time compute. So to to explain this questions, we can break into two parts, like, one is scaling by weights, and scaling by depth. So scaling by weight means we can, just, have parallel scaleings launching parallel test launching a a few chain of salt at the same time and do majority vote to other aggregation methodologies to aggregate the final answers. To build scaling by weight is is more like, like a sampling problems. We we it's actually the scaling, scaling by which is the same as sampling from a structured distributions. By with these heretical modes. If you view, if you view the reasoning as sampling problems given a fixed prompts and a fixed LOMs, we always have a sequential distributions based on the constructed by the next token prediction and distributions. And in that case, if we sample if we sample more rollouts, from the distributions, we have more realizations. And understanding this distribution better. In this case, we can, by doing by by launching more by launching more samples and more rollouts, can have better understanding of the underlying distributions, and and and doing a more accurate majority vote to boost the test time performance. That's, that's scaling by ways. And also, we can do scaling by depths. It's very important for very complicated reading task. Such as some some reasoning task, might require more compute, during test time, like scaffoldings and some planning searching problems. Those problem, usually, if you think harder or longer, you can almost, for sure, get a get a better results. In such scenarios, should definitely scale in by the depth. To thinking longer, so to to boost the performance. But there's a but there's a balance between both weights and depths. That's why I believe we should design some like, a like a new evaluation paradigms, like a new evaluation paradigms, like controllable thinking. You evaluations or benchmarks, should control the thinking budget at the same time to to measure how much the sinking effort and improvement over the depths or the weights. But overall, I do believe, we the the scaling loss exists for the reasoning models, and there will be more loss. To be studied over these paradigms. Yeah. Maybe I can share some thoughts from the infrared So I think, so we up when we trained the ARIA system, we observed that the the sequence is increasing, but when we add some like, the sequence lens panel like syncing budget, it decreased. But the thing is that the rollout is a main bottleneck of the ARIA system. And it take, if you want to generate just one training batch, it can take over one hour. So that means you can you can definitely increase the budget to the infinite. Infinite. Right? So I think it's a quite interesting question because, if you want to you want to find such a point where you can gain the good model performance, but also you can decrease the the time of the rollout. Yeah. So yeah, that's my thought. Okay. Yeah. Thanks for apparently for answering that question. So let's continue to our third question. Third question is about the agent. We all know that the agent workflow introduce new dimension of optimizing Such an operating is NILAR CHINING or PURE INFLENCE. For example, it require toll tour calling it require reasoning model planning, It require retrieval information. And it require a wide strategy in the middle of a task. So the question is, is there a key open problem in open night such agent system for efficient reasoning. And how the new tool, new plan, new task need to do for most efficient agent calling and listening. Yeah. Two quoting Okay. So the two calling channel. Yeah. Yeah. Two calling. So when we when we build, Viral, the RL framework, I think the agent the agent one, there were two main challenges. The first one will be whether the framework is holistic to the different agent work, The other one is for the framework part, I think so currently, the Vowel is using the the agent loop, like, single prompt is you can you can view view it like a process, and it will iteratively call the decoding and call the agent and then, wait when when it finish, it will finish that infinite that So But the agent core is very complex because, in the future, you might consider, like, different agent will interact with each other. And it will you will construct graph that connect different agents first prompts to the final response. So whenever you construct that graph, it can be a optimization question. Whether you can allocate different resources for different generation, whether you can allocate the resources for the rollout, whether for this rollouts, this prompt route, or the other rollouts, so you can schedule in some prompts, scheduling some agent core, and you can overlap the agent core with the the other decoding. So that's the second challenge, how to improve the performance. So I think currently, Viral is focusing on the first part and, do very well the first part. And, in our internal framework, we are focusing on the second. Part. Yep. I think another challenge is probably like there's a lot of tours in the possible agent behaviors and lots of tools Some of them, like, has very high latency. Some of them had low latencies. So, you probably want to design a synchronized reinforcement system so that it can cope with all the situation and try to hide all the tendencies. With sufficient large batch size and also diverse tool called structures. So I think first another issue is that how you want the how to sure that the model can be aware of all the tools It could be, like, a 100 a thousand tools. And you pull everything to the front and you're going to have super long contacts problems. And you're going to queue the coherence of the models. Usually, the model says claim has a million contents windows, but what really can be used for is maybe a 100 a 100 k or something. Right? So, that's actually one of the problem, that people will face So the longer the content window if you put, like, a lot of content in the content window, then the reasoning capabilities of the model for this long content window will decrease. That's actually not great. So how we deal with that? That's another big problem. Another and and finally, the context editing the context management is another one. You want all the information to be incorporating to the context window so that the model you know, the agent is aware. But at the same time, you have a limited context window. So how you prioritize which information is most important? Is another big challenge. Yeah. Thanks for all answering question. Now it's the following question and maybe the next question final question. So what do we believe the future of Efficient recently? What we expect is the next major efficient gain. Gaining from the future You see, the agent? Is he just a big model? You see the prompt, a better prompt. Yes? Or is he parallel thinking? Skilling, Yeah. It's open question and feel free to answer. Question of future of efficient listening. Yeah. Okay. So, I think an interesting challenge will be if you have a a lot of, small models, how are they going to collaborate and how are going to form together into a big model? So that, we can basically leverage all these models at the same time and do something bigger. That could be a very interesting thing. So one thing that I have one kind of crazy idea I have is that, oh, you might be able to train a 100 small models and if they put them together, then they become big model. They can do big things. If you put them into separate places, they become specialized model. They can do small things. So it's the true transformer. But, the it it will be very interesting to see if that could happen. So in that case, you might maximize our efficiency by using your small models at demand at the same time. Can put them together into big model for very high very high problems. So that could be interesting challenge. But this is just my crazy thoughts. I would love to hear about other people's ideas Yeah. I do think there's a large space for like a new architecture to be more efficient on, like, a long sequence reading, like, you know for when we're generating sort like 32 ks if you're using full attention, it will be very slow. And so we need to cut that that kind of cost so that we can have a good more effective and agent that can sync shorter and quicker? And also, like, question, like, how we do the continued learning, doing, like, inference time. So so they also needed, like, a an actual innovations like how to update your ways, like update architectures, like, in the during the test time. So I think there's still, like, a space, like, on both the efficiency side for your architecture for long lived and also on the capacity side, like, how to define develop more capable models to do, like, a continued learning at a time. Yeah. So I also agree that, the better agents system will be much, more important than like, just increasing the size of the models. We all know that the there will be diminishing returns if we just scale the model size. But on the other hand, if we have smaller model but with a better agent workflow and environment, it will be much easier for us to explore the the structure of the onlining agent task. This will be very useful for some coding coding task and some other relevant, scenarios. Thanks. Thanks for every panelist for sharing the insight about the current state of efficient listening and the future of efficient listening. Let's thank all our panelists for such a wonderful discussion with us. Yeah. Six. And, yeah, now we welcome our next speaker sorry. Yeah. Don't worry. Okay. Thanks thanks for speaking again, and let's welcome our next speaker, Hao Zhang. He's from UCSD He's a assistant professor at UCED. Mainly working on efficient listening and and he also is the inventor of war, and I will give my time to professor Hao Ta. For his speech. Okay. Thank you. Yeah. Know how to set up? Do I Okay? Okay. Yeah. Let's get started. Good afternoon, everyone. My name is Hong, and, I'm currently a faculty at the UC San Diego. Local here. I I hope you enjoy the weather. Okay? And, really excited to share our work on making large language models, specifically reasoning model, more efficient. Okay? So So, since the release of OpenAI o one, right, and virtually every major language model provider, including, like, OpenAI, interpreting, Google, They are kind of offering a reasonably enhanced language model. And compared to the typical, language model, Chapels, I think this model is extremely good at math. They are, like, achieving gold medal level, mathematical solving, and they are also literally powering the most widely called agents today. I think most of you, and including me, also use that cursor. Cloud code. Okay? So the key enabler for this is basically a so called test time screening law. Which means that you need to allocate the extra compute during inference in order to yield higher accuracy. And, and, roughly speaking, we converted into two ways of scaling, test and compute. The first way is basically channel software. Which basically generates, explicit longer reasoning steps. And the second way, which is also quite popular, is self contingency. That, you keep sampling one more accurate, and then you perform a majority voting to get the answer. While this approach is, improved accuracy, actually, both measures demand generating many, many tokens, which is a major bottleneck. Okay? And as you probably know, like Arm Inference is actually a big field, and and the reason that is actually I mean, inference is inherently very expensive. And slow. And, and reasoning models makes this problem, even worse. Because reasoning model need to generate more tokens. So here's an experiment we, We, we we basically performed on the mass dataset, compare a standard QIN 2.5 model to its original variant. And, reasoning basically achieves higher accuracy. But, if you lock the accuracy at 80%, the standard model use 78% less tokens. And a similar problem can be observed in another paradigm, self consistency. So typically, when we use SC, the standard practice is to specify fixed number, say, 64 trajectories. And then you start using different random states to keep sampling. Until you finish it. Right? And then you do majority voting. But if you go inspect the those trajectories, you will find that two patterns. So for example, for hard problems, 85% of the traces are actually wasted. Some failed very early. And some derailed in the middle way. And and and or containing man minor errors, which not going to give you the correct answer. And for those easy problems, however, many solutions are actually redundant. Already get that in in your first directory. Okay? And this massive token consumption, redundant or low quality traces, is a also not quite sustainable. So basically, both COT and, SC actually waste a lot of tokens and trajectories. So so then here comes to our problem. Right? So want to do more efficient reasoning. Okay? The main theme of this workshop. And, ideally, we want to achieve two outcomes. First, we want to generate fewer tokens for the same accuracy. Right? That way, I'll lower our cost. Second, we want to achieve more accuracy if we have a larger budget of general auto tokens. Compared to the current status quo. K? And this is crucial because, if we can, improving this kind of reasoning efficiency actually directly reduces, both cost and latency. Which is a big part of that behind today's language model based applications. So the question is, can we achieve this? And, and the goal of this talk is that I want to give you a definitely yes answer And our idea in this talk is actually we will be able to use the model's internal certainty okay, to assist reasoning. I think in the literature, there are quite a lot of approaches make this more efficient. But in this talk, we are going to focus more on, uncertainty or confidence. So, the high level intuition is basically language model often knows what they know. Especially when they are confident. Okay? And we can try to kind of, like, quantify the signal and use that signal as a as a as a signal to to gather your austere in a reasoning. And, and here, we we try to extract that confidence signal at either or cross screen answer level or fine grained the token level. And this signal actually allows us to develop a mechanism that for example, like SIS to language model, you should stop here because you are confident. And it's unlikely you are changing your answer. Or, like, you are confident that you are not going to solve this problem. Why don't you stop here? Something like that. Okay? So, okay, the first project I want to introduce is basically a certain And this is a paper that we posted on our archive, actually, about almost one year ago, but it was recently accepted to to neuro loops and presented here. And the TLDR is basically we can use a coarse grain, the answer level certainty. Metric to trigger an early exit from the reasoning process. So let's take one step back. I to illustrate why we, generated so many tokens, we can look at this case study. For this given question, which is on the upper top upper right a long reasoning model outputs a correct answer in roughly 300 tokens. And the reasoning model would generate over one k tokens for the same problem, and the key observation here is the model gets the correct answer. I mean, for the original model, it gets the correct answer very early on. Which I marked. With a red red box But then it continues to double check self dot and re verify its thought. And it builds confidence until it eventually stops. Right? And this repeated ray analysis actually consumes a massive amount of unnatural tooling. And this is we believe, the source of the overheads. Okay? Want to, basically, point out that this original model's long narrative is not a backup because when we use to incentivize reasoning, we're all basically kind of incentivize the model to have this kind of behavior. Okay? But this is basically the model being comprehensive, trying to basically explore different solution path and eventually get a more common answer. But, here the idea is if we could detect the moment of early correctness we would save substantial compute at inference time. So how do we quantify this phenomenon? To quantify this kind of, like, self doubting, we develop a probing technique called a prop in the middle. So for the long reasoning trees, after for example, after every of your tokens say, after every 128 tokens, we basically append a prop like, We just directly prompt it to say, what's your answer Okay. Could you answer now? And to force the model to reveal the current solution, And by inserting this kind of prob, small prompts, we force the model to always output the current answer no matter where the progress is currently at. And this allows us to accurately, track the reading path. And within, connect the model's intermediate answers and check them against ground truth. So here, I want to point out that this is an ideal Oracle driven test So in reality, when you do inference, you actually don't know the answer of your question. But, for some data set, we can do this kind of like a study. And we'll try to inspect the the port this phenomenon. Okay? So, basically, we we insert a lot of pro problems, and we try to connect the answer, and then we get all the answers. And we check all the answers against the ground truth. And this experiment, actually revealed the results. So I will spend a little bit more time on this. So this experiment took two minutes to decide. Against the the DeepSig r one, our very famous reasoning model. And for the baseline, the model basically runs naturally to finish. We just make it run. We never interrupt it. Okay? And for probing, we basically interrupt every one twenty eight tokens, as I said. And we connect the answer and check for correctness, and then we terminate it. If the answer is correct, Okay? And we record we also record the tokens used for both baseline. Lines. So let me explain these two figures. So the x axis here is basically a number of tokens we spend on particular question. And the y axis is the question ID. And the color of each cell, represents how many runs. The correct result up to how many runs has the correct result up to this point? For each question, we basically run 10 times in order to get a some statistical significance. Okay? And the color red basically means that all rents are wrong. And the the green means turn auto turn on correct. And you can actually figure out color spectrum. And if you compare the left and right one, the left is baseline. The right is basically the one that we insert all the props. So the result is quite striking. Right? So the baseline model actually spend a medium 2.7 k token. To get to correctness. And the property manager has spent only 800 tokens. Okay. That is a three x gap. Okay? Which means three x latency reduction. Right? And and this clearly shows that, the reasoning model can arrive at the correct answer. Much earlier than the default termination point. Okay? And this actually motivates the need for a very automatic and practical certainty based early exiting mechanism, which I'm going to develop next. Okay. So now, the practical challenge is, how do we apply this reality This in reality? Because, like I said, the previous experiment that we use Oracle, but, in practice, we don't have the granules. Tell us when to stop, because we are not able to check our answer correctness. Right? So we need some kind of signal that other than checking against the ground truth, Okay? And it turns out that, this is where the certainty model certainty, becomes useful. Okay? And we can basically measure certainty of a sliding window of the red linear trajectory. So here, we define, three concepts here. The first one is the search, certainly window w. Which is the last w answers we prob and collect to evaluate uncertainty. Here, apparently, the w equal to three. The second metric is entropy term, entropy symbol, h. Which is roughly measures your certainty. Right? So which defines how we calculate the certainty. Okay? Here we define it as a number of answers that is the same to the last. Like, how many times the answer the same with the last answer? Okay? And, and this is measuring the window. So has to be normalized by the window size. So the value is always from zero to one. Okay? And finally, we have, like, a configured certainty threshold. To determine when to early access it. For example, if your certainty is greater than the threshold, then you'll Okay. Very, very simple. So let's work through a very, very quick example with Windows as a three and a threshold of one. Means that you check the last three answers, and all these rounds are need to be equal, then you exit. Right? So at the start, the model answers are inconsistency. Inconsistent. So the last answer is forty forty two. As you can see. So the uncertainty measure is basically one third. Okay? And it's below threshold one. Okay? Which means that it's not searching yet. And, therefore, we ask a model to continue. Okay? Okay? Okay? And then it continues. And after another 28 tokens, it generates an answer, 42. We check again. Right? We connect two answers, which is the same with the last But the certainty is two third. It's still smaller than the threshold one. Okay? Therefore, we generate another one twenty eight tokens. Okay? And finally, in the last step, see the model answer 42. Then we basically think about the it actually matches our threshold. It actually exists. We stop it. Okay? Is a very, very easy problem and already active mechanism. So why does certainty work in this case? We can dive deeper a little bit here. Okay? So in this figure, we plot the creation. Between steps to yield a correct answer. And the answer network certainty. So the x is basically the steps Steps is is basically proportional to the compute. Right? To some compute. And the y axis is basically, like, after you're going this many steps, what's your current So you can see that, this scatter plot is roughly l shaped. And this is what we want, because that means when certainty is high, the number of compute needed to get a correct answer is is low. Right? And it means it means that we will get to a certain answer very soon. Okay? And also, when the certainty is low, we generally require more more steps to to to get an answer. Okay? And I look at more compute, we are likely improve the answer for these questions. Okay? And this plot actually explains the kind of statistical signal behind this. And we actually applied this certainty based determination to quite a few, reasoning models, and the result is quite promising. We consistently achieved from 12% to our setups, so we're maintaining the same accuracy. Right? And this demonstrates the power of this certainty. To improve efficiency without actually compressing performance. And this approach is quite a welcoming because you don't have to train your Okay? It's an inference time approach. So for so far, we have focused on Chiang Salt. But many other more sophisticated reasoning or test time, skimming method exists. Right? One, famous one is the one I mentioned, the self consistency. Which samples multiple trajectories. And there are also some other one, like tree based m MCTS. They are slightly not that popular today, but IBD will reemerge again. Okay? And all these advanced paradigms still rely on, increasing the test time compute to improve accuracy. So the good news is the certain measure I just defined is actually broadly applicable it is agnostic to reasoning algorithms. Because as long as you are able to probe your customer, are able to get an answer. And as long as you can get answers, you can basically evaluate the answer network certainty. Right? So we can generalize our answer network certainty to apply across all these programs. And for self consistency, it will be just like you extract the answer for each directory. Before you spawn more more directories. Right? And you measure your certainty and and try to say if it exists your threshold. And for MCTS, depends on the algorithm you use. But for some MCTS, you also can extract answers. But if for those, you cannot extract answers, you cannot do prob. You can use a reward score, because in many MCTs, algorithm, you have a reward model associated. Okay? So here, the keynote I want to give is basically that this certainty actually serves as a unified algorithm agnostic measure to measure this answer confidence. And this answer confidence directly correlates with the step to correctness. Which can be used to reduce token consumption. Okay. And, I talk about how how you perform this kind of kind of, like, acceleration for single reasoning, request. Right? But I think that in practice, many of the time you deal with, like, a serving systems, like a BOM and kind of system. You have to serve a lot of requests. You put your model on top of that. So we can actually generate generate this answer and have a certainty from a basically, a single request to many more many requests and generate it from a single request inference to serving. So this is this is what we did. This is a certain DAX, because we want to generalize this into different algorithms. And multiple requests. So this sorting acts basically its power actually extends beyond a single request to language model serving systems. So consider, you know, a batch request. Right? You you are given a batch of written queries. And and these certain tags can be used for dynamic token allocation across requests. So now please consider a language model serving use case where you to generate tokens for for a request I illustrated here, p one, p two, p three, p four. And, typically, we would set a maximum token limit for each query. Right? That's that's some something we always do in streaming systems. And also consider that, assuming this p one, p two, p three, p four, they have different difficulty. So p one is a is a request. P two is a is an okay request. P three is a a little bit difficult, but if you spend more tokens, maybe the model will survive. But the p four essentially lies at the impossible region which is beyond this language model's capability. So so for this easy question p one, we if we have a certainty, right, we can early terminate and reallocate the unused tokens. To a harder request p three. Okay? Might need to need more compute. In your original allocation, like I like I said, shared in this part, you would allocate a equal amount of tokens for each request. And, therefore, you use more tokens for p one. You you are able to sort p one p two by you you just don't have enough token for storing p three. Right? And same thing. Sometimes when the language when the language model is confident, it is also confident in no. That means like, you you measure its confidence. The answer is always wrong, but the confidence is means the language model is trying to demonstrate a signal that I'm not able to solve that question. And for this kind of thing, you can also calculate it because if the modeling model already show a signal that it can answer this question, why don't Right? That's for p four. Okay? You can reallocate this token for p four. Back to p three, and that will that will help you actually solve it under query. Right? Increase accuracy. It's a more efficient token usage. Okay. This gives you a high high level idea of how you can incorporate this kind of, like, certain signal in a more complicated, serving pieces. So our evaluation, in real serving engine comes these benefits. So for batch processing, we achieve up to 50% of computer saving on on some, like, datasets. Including the famous Mac mass metal data sets. And for other serving, using, consistency, we can achieve up to 1.6 higher like, program rate, per per second. Is your throughput. Okay? Compared to baseline. You don't do dynamic token allocation. Okay. If you need to know more details, feel free to check our paper. And I hope I convinced you that this reasoning model and reasoning program actually weighs level tokens. And we can do much better. And one possible way I I try to allocate here is basically using this cross screen answer level certainty. Okay. And in fact, we are able to push this, answer level certainty into one of the leading serving engine, TRT Arm by NVIDIA. And if you go check their code, there is already some code that we committed Okay? Okay. That that basically wraps up my first part of the talk. And I want to turn to the second part, So deep sync with confidence, and this is actually a collaboration with our panelists here, Yuan Dong, and, led by my student. And Deepgram is an improvement on self consistency. Okay? And, it actually leveraged Fang Green model level of certainty, token level of certainty, to address the massive efficiency or parallel thinking, with generating hundreds of tokens. So So So, so in the first part of my talk, I already defined one kind of competence, which is the entrepreneur We try to extract all the answers, and we try to calculate sort of an entropy across answers. But I would say that certainly is a coarse grain level, because you'll have to get answers in order to calculate uncertainty. But sometimes some application especially in online generation, you want to know something like a very fine grained signal in order to stop or early access a token level. And, and the next, we are going to basically develop a more fine grained certainty. From the model internal itself. So So besides this, coarse grain, the answer to our uncertainty, in fact, the language model also signals their certainty at every step. Right? Every time when a general token, they are signaled presented. Why? Language model is doing next token prediction. And before you sample the token, you actually first across the vocabulary. And that distribution is where you can actually extract a token out of certainty. Okay? So, basically, a sharp distribution means that the model is highly confident about that token. Okay? And it indicates that you already maybe cracked a step. Okay? A flat distribution is all that. The model is fine to output any token. Right? Many token have equal probability. Therefore, the model is less certain. Which often corresponds to reasoning hesitation or a potential error. K? And we basically try to quantify this using token network metrics, like entropy or more effectively. Token confidence. Which is basically the average log probability of of of that. Distribution. Okay? At that token position. So so how can we develop this kind of token, our confidence to help with parallel thinking? So we need to first develop a a develop a certainty metric to tell the confidence of a trees inside of a token. Right? We only have the confidence of token. We want to generate that into a tree. So So to get a quality score, for the entire tree, the simplest method is basically average trace confidence. Okay? Which I define here as the main of all token level confidence values, actually, this definition has been used in a lot of papers published earlier. And, people have have been finding this quite useful. And this is the faculty, but it also has a critical limitation. That is, it is basically a global averaging of the confidence across all tokens in the trees. Which means that it is quite prone to local failures. Sometimes, the language model can go back and forth, and you actually get the answer. If you average the confidence occurs entirely accurate, it's likely like it's done the align with the final answers to confidence. Okay? So a single critical step with low confidence can actually can be diluted and missed when average over a very long trees. And this motivated with the need for a more granular approach. So basically, this work, Deepakanth introduced a new metric to, better capture local reasoning behavior. Basically, we use a so called a group confidence is a sliding window average over nearby tokens? And, to smoothify the noise and track intermediate step confidence. And here, we can even generalize, the group confidence to something like a bottom 10%, group confidence. Which basically computes the average of the lowest scoring confidence groups. Okay? And it captures the weak points that a global that a global average misses. And and this load is the group of confidence that uses the absolute minimum value. Which is particularly useful sometimes in certain applications. We can also use something like basically, we have modified like a trajectory algorithm to to basically make it capture a lot of locality. We can use a tail confidence, which basically focus only on the last few tokens. And this local patterns are far more effective than global average across entire trees. Okay? So why I introduced so many terms? Because, this slide actually explain why. So we can also do a very, straightforward correlation plot. Right? So this figure virtually confirms, the improvement of this new definition of metrics. So the mean confidence is a left figure. Which is basically global averaging, the token level confidence across all tokens in the directory. And either basically shows a noisy, overlap distribution for correct incorrect. Which is green and orange. Traces. So here, again, we are doing an oracle study. That is, we know the answer. And we try to classify answer into correct and incorrect and try to study speculation. Between the actual correctness and the and the confidence value. Okay? And and in in contrast, if you compare the middle figure to the left figure, this bottom 10% of confidence is much better. Right? At least, like, the first impression is either separates the orange and green much better than the left. Than the first figure. This is why this is why we need to refine this metric. Okay? Because our end goal is basically we try to map this confidence to the uncertain crime means. And we try to use this as a signal to help us identify those per missing traces that would yield a correct answer. Okay? So, so then, that that basically gives us a few definition about this onshore network trace level certainty. So you are probably wondering why do we care about developing another fine grain of confidence at a trajectory error, Here, I want to provide a few argument. So fine grained uncertainty actually enables two key improvements. First, we can do offline aggregation. We can use the confidence as weights. For voting to to basically outperform a standard consistency where you don't have a waste of waste, have greater majority voting. Right? So every battery is equal weight. Okay? Secondly, we can also use this, like, a trace level certainty. Which is calculated online. To do online steering of the region. Okay? So we can continuously monitor the competence during, generation, Early stop those low competence increases, and then reallocated the computer, budget to more promise increases. This is calculated and maintained at a token level, it's very, very, like, responsive. You can stop at any time. Okay? It's very rare since it's And this basically leads us to our recent extension, to the first work. And I probably will skip a lot of details, but I'll just give you a highlight. Of of this work. So this is Deepgram. And as a highlight, basically, Deepgram achieves a a stable accuracy on 2025. I think it's the first work that reports 99.9% digital accuracy. On '25. Okay? And, and using the GPT OSS one twenty b. Okay? Which arguably is not a wireless phone model today. Okay? And while at the same time, reducing token, cost by up to 85 percentage. And it basically features two modes, offline mode, which use confidence weighted voting on four traces, to achieve better self consistency. That is for each recovery, you you, you you do the company's measurement offline. You try to assign a weight to each trajectory. And you remove those low confidence traces, and you keep those high confidence traces. And once you figure out a subset of high confidence traces, you use that trace, confidence as a weight to weight the combined answers and to get your final answer. Okay? That is a direct improvement of standard self consistency. Right? Another way is online mode, which basically monitors confidence in real time to early stop, no confidence traces, and a safe compute. And this is already similar to the first word I just introduced. Where you keep monitoring the confidence. And whenever the confidence drops to a certain threshold, you give up, and you try to spawn under pieces. To restart. Okay? And, and the result of this online deep Deepgram IV are the most impressive, which I want to highlight So basically, we achieved huge token reduction. Between 32 percentage and 85 percentage fewer tokens across various datasets. Such as a 76% reduction on the actual MIMT 25. And, more importantly, this massive, computer reduction the accuracy remains competitive. And sometimes, you can even improve the accuracy compared to a standard majority voting it is because you, indeed, use this confidence to filter somewhat noisy trajectory. You only aggregate those high quality answer across important trajectories. Okay? So in conclusion, for this work, we developed another kind of confidence metric our parallel generation. And we argue that the parallel generation, with tokens due to diminishing returns, or having more accurate accuracy performing majority only. On them. And Deepgram basically tried to solve this, and the online mode actually receives about computer without the extra training. And the offline mode was the first that basically make a ME twenty twenty five benchmark not useful. Okay? You basically saturate that. And, I think my student and the collaborators, they are also pushing pretty hard to put this Deepgram into TRT ARM and VOM. And we are working together to integrate this kind of like a test time to steering method. Okay? Thank you. That's all my talk. Thank you. So we will have questions. Time for one question. Yes. Thanks. Hi, professor. Professor, thank you very much. Come to hear. My name is Ming. Ming Qing come from 08/2008. And the first part you mentioned, like, my impression is it talk about how to achieve a reasonable accuracy by measure and token in the reasoning model? Consumed I'm not sure how you conduct this research I think there are lots commercial vendor. Mhmm. And my working environment, they always charge me, like, how many token I do the inference. So when you do your research you really relay on the API provider provider from the vendor to get every benchmark how much tokens they give to you? Or you have some kind of your self module you do that kind of research. My question is, how I can predict like, a confidence the vendor provided this reduced token is reliable. That Yeah. Yeah. So, you know, you know, first method I introduced, I think, you need to you do not rely on getting the logins. Language model. Right? I think ARM providers, they are not exposed end point for you to get logins. You can only get tokens. But in the first one, I introduced you don't need that. You just need to probe and get answers. That's a pure pure token. Which is fine. In the second work, I think, is slightly more complicated because the way you aggregate a trace level confidence is you need to get the log token level distribution. Which you are not able to gather from API providers. So if you want to apply the second word, you have to serve your own language model. Yeah. Okay. Yeah. I have a second question. That's my peer to proceed. First. Hello. I had a question regarding Datacom. Does it assume that the models are properly calibrated As in that there won't be underconfidence or overconfidence issues with token For example, for some erroneous tokens, the models may be overconfident. And maybe in that case, choosing the highest 10 or the lowest 10% tokens may result in, like, a bad confidence estimate. Yeah. So in my talk, I I actually simply have a lot of details. I tried to present a metric as simple as possible. But in fact, there are a lot of parameters that you need to tune. To make sure that, for example, bottom 10% of that 10 is arguable. Right? And you probably want to tailor that percentage against your application. Yeah. It's a limited data set specific, I have to say. Yeah. Yeah. That makes sense. Thank you. Okay. Cool. Thank you. Just last one question, right. So in other people has come here. So my last question is, I saw your did a research. From, like, a commercial perspective. Right? And organization business user, Is there is a way to create a real time or some kind of, like, orchestra. Right? Like you have a credit score company and give your credit score evaluation. You You're zero with this moving forward, we can have some kind of company merge out? Hey. You have different commercial moduling. I have this service to measure the efficiency even like you you give you example, You're forced to interrupt get a final result. You pro you forcing like, hey. Stop zero. Where is this good enough? User zero is going to create a platform or aux tool. I can dispatch my like you some people maintain this kind of benchmark. To testing, to give the people, say, hey, You can use at this moment, maybe this vendor is there resource consumption is less a reasonable like, a this standard. So customer will say, hey. I don't need to pay too much based on this kind of reference. So so that's what my crazy idea is in Because I think it's long term this going to be a society resource. Like, how we efficiently to use them We need a reliable, neutral like, aging in ZER. Say, hey, I can tell you in this current time for this mass, like, a benchmark the foundry rich company, you can spend less money and the the, I think, win win situation for less Yeah. Indeed. I think that's reasonable. Yeah. And I think people have been approaching this from both inference and training. On inference, that's basically kind of like my work. Where you try to do inference time steering. Like I said, we are able to put these methods into TRT RMM, and a lot of companies are actually using TRT ARM two. Serve their models, right, as a infrastructure. On the training side, I think, I don't know which company is the best, but a lot of people to train this kind of confidence based early early exit into the model. Okay? So basically, the next generation model, which IBD is basically adaptive thinking. Right? So a lot of commercial endpoint already have that option that depends on your problem of difficulty. It's going to generate more or less tokens. Yeah. Yeah. Yeah. Okay. Thank you. Thank you. Yeah. Thanks, Pu Hao, for his remarkable talk, and we will walk on to our next thanks. Yes. And now we welcome our next speaker, NEE class banner. Class is student at Stanford, and he will talk about his work on test test sculling But due to some yeah. You can see Viglas on the screen. Yeah. Hi, Viglas. Yeah. Due to some personal issue, Viglas cannot join us, but today for personal talk, but we still kindly invite V class for the motor. So, yeah, now I will give my time to Nicklaus. For his presentation. Thanks. I'm Nick Closon. I'll I'll talk about test time scaling. Wanna thank the organizers for having me, and arranging me online. I I'm sorry. I injured myself, so I couldn't make it person. But I'm excited to talk to you about test time scaling. And I wanna start with this plot from from Jensen Huang, the CEO of NVIDIA earlier this year. I liked it because it frames test on scaling as a continuation of the scaling paradigm. So we started off with pretraining scaling and then post training and now test time scaling. And I think what's interesting about test time scaling is that it it's quite different to the previous two because there's two axes of compute that we're scaling. For pretraining and posttraining, we're scaling training each time. It's about training compute. But for a test I'm scaling, we are scaling training compute as well. So this is a plot from from the o one release last year. And at the same time, we're also scaling test time compute. So we need to scale train time compute first to get the model to use a lot of reasoning and thinking. And then once it has this capability, we can vary, test, and compute. So there's two axes we're we're scaling, which I think is is quite interesting. So there's a lot more compute that we need because we need to scale both of those unlike for the prior paradigm. So we we just scaled the training compute. And earlier this year, we released a paper called s one, which I think provides two useful baselines for both of those parts. So for the train time scaling and the the test part. So for training, the paper showed that you can do simple SFT, supervised fine tuning, on 1,000 samples to get this good reasoning performance comparative to o one and some other models. So here on the x axis, we have the number of of training samples and then reasoning performance on the y axis. And it got pretty good with just 1,000 samples. So this is a useful baseline for that training aspect of test time scaling. So how you set up the model to be able to reason and and think for a long time and and use a lot of computer test time. And then for the testing part, so then once you have this trained model, how do you make it use varying amounts of computer test time? We introduced budget forcing which is, again, very simple The idea is that given some question that you want the model to reason about, such as how many are you we let the model first think about it, and then if we want the model to to be faster, so reason for a short amount of time, then we just cut it off and directly force it to go into its answer. If you wanted to think longer such as in this example, so extrapolating then rather than letting it finish, we'll have something like wait or something. We'll replace this finish token with some other token that will lead the model to continue reasoning. So here in this example, the model actually wanted to finish after the sentence, The number of r's in Raspberry is two, so it's, like, the eighth or ninth line. But then rather than letting it finish, we just ignore this finishing token and and put a weight in there. And then, of course, the most logical thing to generate after weight is additional reasoning tokens. And so the model continues reasoning. And then actually, in this instance, it even fixes its answer from two to three. So in the final answer, then gets it correct thanks to this additional thinking stage. And if we if we use this for varying budgets, we get pretty good test time scaling plots. Like shown here. So for for math, PhD level science questions, a bunch of different things from from benchmarks you probably know about. And if we zoom into one of those, we so Amy, for example, then a bunch of those thoughts are cutting the model short. So for example here, forcing 2,048 or 4,096 maximum thinking tokens. So they're when the model reaches this budget, then we force it to stop and start answering based on what it's got so far. So it'll just look at its current reasoning, Chase, and then give it the best shot. And then toward the right, we let it think longer and longer. And then even extrapolating by not letting it finish its thinking, but rather a pending weight multiple times. And here for Amy, this competition math benchmark, it it can improve performance. This is how this looks in in detail. It does give us a good and useful baseline for for test time scaling. But, obviously, I think there's many problems with it, such as if we would go further to the right, then it doesn't work forever eventually. The model gets gets sick of all the weights, and it'll do some not very good stuff. Then also the forcing, of course, is a bit arbitrary. The model might be in the middle of something important, So it is very useful as a baseline, but probably not the final solution. But nonetheless, I think these are two pretty useful baselines, so we have the training baseline, SFT on few samples, and then the testing baseline of of using budget forcing, a very simple baseline that doesn't require any training. You can just take any existing models and and implement it. And since this release, it's been very cool to see a lot of works of bashing our baselines and getting much better things. So I wanna briefly go through some of them and then think a bit about what is still missing and and what we should do next potentially. So on the training site, I think there was this paper Open Thoughts earlier this year, and they just show that you can scale up as as Feet and get much better performance. So, obviously, 1,000 examples it's nice. But it won't you can probably do much better with much more. It's it's good because it's very easy, But, for example, if you look at GPQA, then even though at the 1,000 range on the x axis, the s 1.1 dataset is is not bad. It definitely doesn't scale as far as the dataset they created. And this is one way, I think, to to improve on the training part. So just scaling SFT further, And, of course, in parallel, we can also scale RLs, so there have been some works scaling up RL even earlier like deepseag and recently for Meta. And yeah, I think there's also a way more gains to be had if you if you scale up our a lot. So this is probably also way more than a thousand training samples. So the progress there has been great. I think what's still missing on the training part is we can probably still scale further on the RL side. But even here at the end, think there's a lot of extrapolation happening. There's not We haven't they don't didn't put all the dots train all the dots there. So going further there, seeing what's actually the limit, in an open setting, it is where very interesting, I think. Maybe the some of the labs might have already figured this out but haven't shared it. So scaling further I think, is is one exciting thing on the performance part. And to improve the efficiency of training, I think probably process rewards. Figuring that out seems like an important step. So Ilya was recently talking about this that currently the models will do no learning at all. Until you come up with a proposed solution. So we only give it the reward at the end. Even though there's been some some work on process rewards earlier, in the past couple of years, I think we haven't quite figured out how to use it at scale. And so I think these are two aspects that are important there. And then on the testing part, I think there have also been a bunch of pretty cool improvements I wanna take a look at. So this work from from Salesforce they have a very simple idea of rather than just having one budget for the the reasoning trace, we also have another budget for the response. And this gives the model more flexibility And so they show in their paper that while it's one, it lets you have decent control over the tokens used. So tokens used here on the x axis, it performs worse than their separate budgeting method. So the the yellow line because you get more flexibility for the model by having these two two separate budgets. And then going even a step further and integrating it partly into the training, I think there was there were a bunch of papers on this. One of them was l one, which I enjoyed. Where they show that you can train a model with reinforcement learning to adhere to these budgets. So rather than just after training it, forcing the model to stop after a specific token count They put those into the prompt and then train the model with it, and then it turns out that the model learns to adhere to these budgets pretty well. So in their plot as one, is very, very controllable. So the purple line very controllable across the tokens used and performance improves But the performance is not as good as if you tell the model in the prompt. And I think the reason this is the case is the model knows in the prompt already what its budget is gonna be, not gonna start off and do something wildly different. If it knows it has a very very small budget for this reasoning chain, then it won't start a very long a very, very long reasoning effort. But it it knows, okay. I have to be quick. So it'll jump to the solution much faster. Versus for budget forcing just cutting it off in the middle of its reasoning chain expect it to not be as good, I think. I think this is a important improvement, letting the model already know what its budget is gonna be. And then another paper recently I think, was also interesting where they let the model check for its budget during inference. So you can let the model know at the beginning but then counting how many tokens it has generated thus far is not very easy, and I'm not sure it's something that we should teach these models to do. And so what they do is they simply have a tracker where the model can check its budget anytime during the reasoning. And then knows, okay, how man how much have I already you have I already used? How much do I have left? And this kind of saves the model from having to count the tokens itself Arguably, in this s l one approach, model somehow learns during our l to count its own tokens. And that might take away from other capabilities it could learn during RL. And so I think this is pretty useful and they wanna extrapolate in this work, they also use this weight approach, but add some additional stuff to use some tools or do some other stuff. To let the model know that it it can think longer. And they show that this is better than than not having this budget, and it can even extrapolate quite well. So earlier with the way to approach, we also got some extrapolation. But here, I think maybe partly because they the model used tools and it has a lot more things that it can it can play with. They might be getting even better extrapolation I think the last line the last connection here is is extrapolation. So y axis is performance, x axis is how many how many tools it can use. So quite some extrapolation there. I think this is one obvious improvement. So we probably want to combine them. So having the model know what its budget is already in the prompt and sort of training it a little bit with learning to use that. But then during generation, it can check at any point in time, okay, where am I at? How much more do I have left? A bit like in exams, I think we also are usually allowed to check what's the time, how long have you already been working on the exam, how much more time do we have left. I think that's quite useful. I think what's next on the testing side for performance we probably need to work even more on the extrapolation. So here, I assume that if we go further to the right, so beyond the 200, they probably saw it also flattening out. Similar to what we had earlier with s one. So figuring out how it doesn't flatten out is is quite important. And especially if you if you train and then test and during training, you might not be able to let the model reason for very long, say, for for a day or two days because then you would have very few updates. So training with shorter reasoning chains and then being able to extrapolate to much longer One's a test time, I think, is is a quite promising direction. And then another pretty obvious one is that if we want these models to test time scale to days or weeks in the future where they think for really, really long amounts of time. Then we probably have to make it more efficient because with the current full attention it'll get prohibitively expensive for very, very long chains because the model needs to keep everything in memory. So we need to figure out some ways to to do this. And I think there have already been some some exciting works on this, but I think there's more to do. Figuring out how to how to remove things or maybe changing it to some linear attention approach I think a lot of things there. And so in summary, think what's interesting with test set scaling is that unlike before, unlike for pretraining scaling and for the brief post training scaling, though you might argue this is still some form of post training, But unlike those two, there's now two scaling axes. So we gotta scale up the the training part to teach the model how to reason, and to think And then the test time part where the model then uses this to think for very long and so problems. And for training, I think the two things that we can work on on the performance and efficiency sides are scaling for performance, figuring out how to scale further, And then on efficiency, probably letting the model learn from its intermediate reasoning steps and not just a reward at the very end. And on the on the testing part of test time scaling, we probably need to get better extrapolation so the model can the models can think for longer potentially longer than than they have been trained for. And we also for efficiency, we have to eventually solve the full attention complexity, I think, if we want to test time scale to days or weeks. Thanks. Thanks, Miklas, for, such a wonderful task. We have time. For question, if you can. Question after welcome to come here to ask the class about Ted Tai's coding. Oh, praise, please. Hello. Can you hear me? Yeah. Thanks. I had a question regarding s one paper. Did you guys have the time to try something like a dynamic token budget? As in token budget, which depends on maybe like the difficulty or the confidence of recent trace, for example. The difficulty of the problem. Yeah. That's a good point. I I think we we didn't try that. So one thing is, yeah, you gotta know the difficulty ahead of time, which I guess is is kind of hard, or you use the confidence of the model. So then it would be, I guess, during the reasoning, you figured the model sort of figures out how how difficult it is, how confident it is, and then you let it think longer. I think that's an interesting direction, but I haven't haven't tried it. Thank you. Thank you. Maybe I have a question for you. Yeah. Do you agree for that for the future of test type skilling, is it continuous scaling? Is it the cloud way or for the scaling or it has some limit like scaling has limit like the training, GPT, GPT four, by five, or one may now reach the reach the end of scale in all. Yeah. Do you have that belief similar thing will happen in test time scaling or no? Yeah. Yeah. It's a good question. I personally think we can scale a lot further. So looking at some of the current RL plots, at least here on the right side, it doesn't seem like it's it's flattening out. From other people's work. I think on the training side, like, training the model to reason, we can scale further. But, yeah, maybe that one least I think in this paper, they say it approaches the sigmoid curve, so it it sort of starts flattening out. But I think the evidence isn't super clear on that yet. So we should test it and there's it might scale further. And then on the on the testing part, so just letting the model think it after it has been trained, I think I'm pretty confident we can scale a lot further. Than currently the models aren't thinking for super long yet. And at least personally, solving a really hard problem, I think you could I could work on it for weeks or something. So until we get there, I think we can can scale a lot further. Yeah. And I think it's it's pretty exciting to to try to get us there. Yeah. Thanks. And that's the next question. Hi. Thanks talk. Interesting type. Area to I had a question about essence, in the presentation you were talking about budgets right? Computer budgets presumably. Whether you see the work going on in the industry as well as maybe a bit in academia on LLM routers. Whether that complements the scaling that you're doing within And if so, how? Oh, the rudders? So, like, GPT five having multiple separate models, and then they decide which one to route to. So the goal is to given a particular compute budget. Right? Try to figure out which of the various options you have would be most effective? Using an LLM or much smaller model that routes the request to whichever model seems most appropriate given the budget. Yeah. I think that's interesting. So I think some of the approaches there are OpenAI is doing with g p five, I guess, and then also what Google is doing with this metformar approach, I think. And, yeah, I've I've also worked a bit on MOEs before. I think there's things you can do there. So I'm not sure it has to be separate models. It could just be one big MOE and then it has some some experts that are much faster than others. I guess that's a bit closer to Matt Former. Yeah. I think that's also promising, but I'm not sure we can get a lot of, or at least current works, I think, don't promise a lot of extrapolation. Which is arguably the most exciting part. So how do we get the model to think for very long to solve very, very hard questions? So for saving compute, it's probably great. But for thinking longer and solving really hard questions, it would the architecture would probably need some some recursiveness or something to enable that. If if it if it's purely an architecture based approach, I think. If we stick with this sort of token based scaling approach, then I think the methods we we just talked about makes sense. But for for something that scales by the architecture, I think these MATFORMER MOE's multiple models, there needs to be some some recursive component to to do that, if that makes sense. Might be interesting to look into. Thanks. Yeah. Thanks, Satya. Hey. Great talk. I may have missed the beginning, so you didn't go over this. But I really agree with the need for process reward models. I'm curious if you've seen or aware of like, interesting or promising approaches to a general process for model you know, made for even non verifiable domains? Yeah. I So I've read some of the earlier works on it, And I think initially, when we worked on this and we're trying to reproduce o one, we thought, okay. It has to be using process rewards. And so I read a lot of these and and tried to play with it, but then figured out it it's too complex. It probably not using that. I think it turns out that a one one the the follow ons, they didn't use any process rewards, I think. At least that's what most people say. So just the based on the the final result, And so, those earlier works be interesting. So I think, like, let's verify step by step in those papers. And then more recently, I think Deepseak had a paper on on proving on a prover model that used some form of process rewards. That could be interesting to to read in more detail. And other than that, I I think I haven't come across much and maybe I missed some things. But I think there we have still haven't figured out a the final recipe here, so I think it's exciting to to do more research on this. Awesome. Yeah. Thank you. Thanks. Hi. Thanks. Great call great talk. I wanted to ask do you think that the that the performance of the model that's trained on the SFT reasoning traces is upper bound by the performance of the model that generated the reasoning traces? Yeah. That's a great question. I think I think you can get better. So because imagine this scenario, you have a model that's really, really sometimes really good, but sometimes really bad. And so if you evaluate it on Amy, say it gets 50%, like, 50% of times, it's really good, and the other half, it's really bad. And then you distill from it, so you generate a lot of SFT samples from it. But you only take the 50% that are really good. And and more that look like those. And so now you get a distilled model that, say, it gets 100% on Amy because it hasn't gotten any of the the bad samples. So it's always really, really good. So I think if you if you distill and you have some form of rejection sampling to only take out the good parts, you can get a model that will be better on benchmarks, I think. But if if it's if it's better in the limit, so in terms of say, pass it k, probably probably not. Right? So if you generate a lot with first model with the big model, then eventually, it'll get the right solution just that it's not at maybe its first solution. And so it's it's, pass it k where its coverage will still be really, really good. Like, it'll it'll solve all of those if you give it enough tries. And then the small model would be the same. So I think in terms of coverage, you probably won't get a better model. But in terms of pass it one, I think you can distill a better model. If that makes sense. That makes sense. Yep. Thanks. Hi, Nicholas. Nice to see you. So I have like two possible directions of scaling that might be, related to today's talk. One is the scaling of, external contacts So for example, Alpha Evolve has this, database, external database that could expand and, evolve during infrastime. So that could be, like, one direction of scaling, and the other one can be like, the scaling of recurrent depth where you, use the same like, the blocks of LLMs and then scale the depth of repetitions that it goes through. Doing inference. Like, for these two directions, what's your take? Yeah. Thanks for bringing them up. I should have meant I'm mostly focused on sequential scaling here and obviously missed those two and also missed, I think, parallel scaling, so just scaling up majority voting. So I think there's other exciting axes. I think just my personal thoughts, the alpha evolve approach, I think it's very exciting. But, it does seem to me that at some point, the model will would need to learn like, you need to update the parameters at some point. So just purely scaling that way. I'm not I'm not sure. It gets us all the way, but I think it does make a lot of sense And for recurrent depth, I I like solutions that are really simple. And so if you just see the model's reasoning, in text, then that's really nice and simple. You can just read it. And and personally, could imagine myself just thinking in in text and I think I would be would be just as it could solve similar problems. I'm not sure how much a latent reasoning I'm I'm personally doing versus just thinking in text in my head. So I'm not sure we we need the recurrent depth approach. I I think it it could be that, like, both of us could lead us to the to the final solution. Either one is fine. And then maybe the text one is a bit nicer because you can read it and it's a bit simpler. But maybe the work in-depth approach will will win out in the end. I think, yeah, both are very promising. Okay. Thank you so much. Yeah. Thanks. Thanks for all the question, and thanks, big SaaS again. For his wonderful talk. Yeah. Let's sense together. Yeah. Thanks, Victor. And then let's move on to next part. Our next is two oral presentation. First one is oral paper talk when listening meets it slow. We welcome our two oral speaker. I see. Okay. Hi, everyone. My name is Yijing Yuzhang, and I'm master student in computer science at UIUC, and also a PhD applicant this year. And, also, I would like to introduce Yifan, a PhD student at UIUC. Today, today, we would like to present our paper when reasoning Mason's laws. And first, I would like to introduce, our motivation for studying the Risen laws. I'm actually inspired by the physical laws in our natural world. As we know from classical to modern physics, physical laws have shaped our world, as we know it can allow us to form fundamentally understand predict, and transform our natural world. So maybe a question could be, should AI have its own laws and, specifically, if there exist recent laws for LOM so we can better understand and improve their reasoning capabilities. And in our paper, we will primarily focus on the recent process and generation of the large RASING models. And despite the stronger performance in solving complex problems, there exist a a challenge that even powerful LMs will exhibit abnormal, behaviors that deviate from, typical human reasoning patterns. And here is a illustrative example deep seated r one, we can see in this example, the DeepSig r one will tend to generate like, 300 more recent tokens. Than its competitor question. But also with a lower accuracy, on this simple sub question. So this mismatch with human reasoning will reduce an abnormal reasoning patterns present in current LRM. And I also would like to explain the potential reasons I couldn't account for that. Because our researchers will generally overlook the high viral data during the training phase. So it will lead to inefficient allocation of computation either underthinking or overthinking. So in view of this challenge, our first research question can be can we theoretically formalize the model reasoning to ensure, desirable behavior? So we present Law of Reasoning which is a unified framework that systematically formalizes the relationship between the complex complexity as well as the model reasoning behaviors. And And in the LOA framework, we will focus on two key concepts reasoning compute and accuracy. And we will present the central compute law as as well as the complementary accuracy law. So before we talk about these two laws, there are three key concepts here. So complexity, which is ideal definition, are the compute. And in our paper, we use the expecting number of token as and accuracy, which is intuitive to understand. So let's come to our first law, the compute law. Oh, here is a general hypothesis without any constraints that I assume the reasoning compute has has a linear relationship with the In another words, the reading model allocates its reasoning computing efficiently The amount of compute is expected to scale proportionally with complexity. As the complexity is difficult to measure in practice, so we introduced two trackable properties. Monotonicity as well as the compositionality as proxy. To studying our compute law. And for the compositionality, we assume that the compute is monotonically non decreasing with complexity and it should be easy to understand because more complex questions naturally require more reasoning. So here, we also present an example For the first question, we need to perform one metric operation. And second, has one more operation. So ideally, t one should, t two should be larger than t one. And for the compositionality, if x one and x two are independent, their answer to question should exhibit the additive compute. And I also, have an example here For the first for the first question, we need to calculate the n and send question, the 10 digits, and also the concept question. So ideally, the thinking token length of the computer question should be approximately equals to t one plus t two. And here is overview of our compute's law. A linear relationship hypothesis and two trackable properties as proxy. Which is similar to the accuracy law. And for our accuracy law, we also have a general hypothesis that the overall accuracy is expected to, decrease exponentially with complexity. And it also, for the optimal reasoning model, it will satisfy two fundamental properties. Monotonicity and the compositionality, which is very similar. So when we have have our comprehensive framework, how can we evaluate whether car or whether popular ORMs will follow these proposed principles. So in response, we present a comprehensive benchmark that examine monotonicity and compositionality in our ends. And let's even introduce a benchmark. Thank you. So as for Lori Mono, we first ask the questions from four domains, Mass science, language, and code. Then for each seed question, we expanded it into a series of variants, with strictly increasing complexity. For example, variants were here requires to do the matrix operation once, variances two twice, and variances thirty thirty times, we recorded the Spearman correlation coefficient between the variant index and the quantity we care, namely the reasoning compute and the log accuracy. As for LORI Compose, it's actually more straightforward. Using Math 500, we randomly sample two questions and the concatenated them using a predefined template, asking the large reasoning model to answer them in order. Now that we can get to the sub questions and the competitive questions, ideally, if conversationality holds, this absolute difference should be close to zero. To account for the unit difference, we use the normalized MAD as the final metric. Now comes to the benchmark results. Nearly all large reasoning models exhibit a strong positive correlation between reasoning compute and the variant index. The only exception is DeepSeq. Our one, there's still q one one dot five b, which demonstrates unexpected patterns in sound domains. Namely language and code. In If an our reasoning model adheres to the compositionality law, most points would align closely with the y equals x line. In practice, however, the majority of points deviate substantially. Also, you can observe that for larger models, they don't necessarily show better compositionality. Suggesting that size is not the only factor here. So this now that we have the conclusion that current large reasoning models generally satisfy more than ethnicity, but failed to exhibit conversationality, So there's enforcing conversationality can further improve the general reason capacities. So we want to investigate this question as our third research question. So we introduced SFT Compound, a simple yet effective supervised by tuning master to provoke the composition conversational behavior in large reasoning models. Specifically, we just used the same way we construct construct our low recompile. We have the subcautions and their competitive versions. And for each question, we do multiple rollouts. Specifically, we do k model outputs from a large reasoning model. And finally, we select a combination that fit. Fast to satisfy the conversationality condition. You can understand that this is a very, very simple method. And finally, we just performed SFT on the selected robots. That's our very simple SFT component. And our stream should introduce the experimental results. Yeah. So to empirically evaluate our SFT Compose, our aims to address two research questions. And first is, does acetone perform effectively in enforce the compositionality? So I'm finding is the compositionality of the recent compute can be improved with such simple fine tuning approach, which is obvious in our in our figure, which demonstrates deep r one one point five b significant significantly improves. And the second part, I think, is the most important part. Thus, that enforcing compositionality can lead to general reasoning better general reasoning capabilities. So we find that enforcing it can can actually lead to better reasoning capabilities. Across different math and science benchmarks. In we also add a control baseline here, SF And the only difference between the SFT and SFT Compose is performing a uniform sample or selecting the combinations that best satisfies the com. Compositionality. So from our result, SFT can outperform the SFT in all cases. And showing that these gains are not just from this from a stronger model, but from better compliance with compositionality. And finally, I will like to like to discuss an interesting, synergistic effects in our experiments. Because we find that our model can yield broader improvement across different properties and laws For example, enforcing compositionality in, recent compute can improve its monotonicity. And also, enforcing compositionality in reasoning compute can improve compositionality in accuracy. So finally, as a comprehensive study from theoretical hypothesis, we advanced the theoretical perspective grounding human reasoning for understanding and improve the reasoning in current ORMs. And it also let us to think about how structural properties can lead to improved general reasoning beyond just scaling or scaling data or the model size. So we hope Lori can inspire more potential strategies that guides models towards their optimal paradigm of thinking. And here is our paper our website, and GitHub. And thank you for your listening. Amazing work. Thank you. For the presentation. I have a question on the two models you showed that doesn't conform this law. Any insights on why? Oh, or what two models didn't? I I think you mentioned but that page went through super fast. I'm not entirely sure that's what you mean. You said there are two, like, one math model, one coding model that where this law doesn't We're Yeah. I think this this page This one? Go Yes. You bring the monotonicity of 1.5 b. There are two models that are set in Well, that's probably another page. You you you said that there are oh, yeah. Current failed to exhibit. Yeah. Oh, any insights on why? Oh, so you mean why they failed to accept the conversationality? Yeah. Actually, it's explained a little bit before before this before our framework. I think it typically can be accounts that researchers also like, will collect data during their training phase. Do well or heurously curate by the human annotators, or generated through online rollouts So it really constrained by explicit rules. So, I think that can come for that because for simple problems or the different problems, human didn't aware that the difference and use like, very similar like, generation forces two different questions. So I I think that's why it will all fail to exhibit the compositionality. Yeah. Got it. Thank you. Hi. Great work, by the way. So for your SFT and SFT combo comparison where you show that compositionality helps in reasoning. Right? Yep. Can you is the data matched? Like, is the number of data points matched? Yeah. They are same. The only difference is the SFT is perform the uniform sampling, and the SFTP compose selecting the data that the best set is the condition compositionality. Data other set and datasets size. Is totally the same. Got it. So thousand points. One is random selection, and one is selecting based on your Yeah. You can see here, the rep the mock as red box is satisfied with the compositionality. But for the SFT component, I will uniform sample from others. Yeah. Thank you. Thanks for the oral paper presentation. And thanks. Please come. For yeah. Here we welcome our next oral paper talk. Inflow Agent System Optimization for efficient planning and tool use Yes. Let me help you Put in for 20 Alright. First, it would be our great honor to be selected as So I'm pan I'm Stanford University. This is Jofo and Houxiang who work with us this summer. So today, we are very, glad to present our recent work agent flow. In the flow agent system optimization effective planning and to use So this work was partially supported by AI Lab, high renaissance officer of Naval research and cast. So to power empower a Lange model with specialized skills, one common line of work. Was to is to augment language model with internal tools like web search, Python encoder for knowledge retrieval, and precise computation. So basically, there are two paradigms So first one is a large net model agent. So typically, an agent consists of a planner, for making decisions, the next steps. And tool, tool set to power spatial skills and memory to store intermediate results. So so recent work has extended reinforcement learning in this field with rewards to to guide the learning model, learn how to use tools and when to use tools by interleaving reasoning and a generation. However, there are some limitations for this kind of paradigm. So first, as you can see, they are just one single engine model to do the planning, execution, verify and the final solution generation So it introduced a lot of, overhead our single model. So it may perform very bad. Bad in, downstream task, or long context reasoning task or, the the scenarios that require much more turn reasoning or compressed reasoning. So with that, so built up built out there is another paradigm that that is much higher my tech agent assist terms. So are usually, built on top of a single language model, agents. So with that, you can see there are multiple agents, and each agent may work on just one role or one skill for and the different agents can collaborate in our sequential way, in a hierarchical way, or customized way. So with so here, the structure of these methods. So you can see there are different modules. They can work sequentially or Yammer. Structured, complex way. So with that, you can see that they can compose they can compose very contest test complex test into multiple steps. And they can solve the problem step by step. So it's more powerful So you can see that these systems have shown the mark for capabilities in many complex tasks, So for example, you can see deep search by OpenAI. So by integrating different tools like web search, and other specialized tools, they can do can integrate resources from different websites and have a very comprehensive solutions And more recently, you can see these paradigms have been applied in scientific discovery, For example, you can use, developed magic agent system to discover new antibodies. And more risk like virtual lab. And more recently there are BioMini devices So it's, general purpose biomedical AI agent that can support, hundreds of tools, databases, and softwares and can do a lot of scientific discovery in medical domains. But although this, agent assist system has shown a marked up performance, you can see they are still very fragile. In various scenarios. For example, they may make mistakes, For example, the system might might be very fragile and cannot not very reliable in different scenarios. Scenarios. So and there are sometimes there are limitations in the communication across different modules or systems, Sometimes, it's very hard to verify if the intermediate results are good or not. So with that, we propose agent flow. Well first in the flow of multi agent systems. You can see our, system is very complex. But it's very well structured. We can support diverse tools, like Google Search, Python encoder, Wikipedia search, and more. There are four specialized agents like planner, executor, verify, and the generator. And each is responsible for one specific function. For example, planner, the planner can generate the overall plan, for the complete task and decide how to break down these complete tasks into multiple steps. As a curator, we convert each step into concrete tool calling with the sub goal, and we will have to verify to verify if the intermediate results are good enough or it requires additional tool cost to make the the task solvable. Finally, we have a generator to summarize all the trajectory in the memory and generate final solution through original query. So with that, we have, involving memory. It stores it stores intermediate results, to make the communication across different agents more smoothly. So we can take, the we can take moving into more technical details about our agent flow system. So you can see there are multiple terms giving the one query and to a state it may perform much turned more turns to solve problems. So for the first term, it caused a planner agent to generate the action for each step. Then the executor converts this texture action into more executable way giving the tool selector tool and the sub goal. And then we have to verify to verify if the kind of step is good or not. If it's complete, we we can generate to the final solution. If not, it will repeat this loop. In multiple turns. For example, we have a second turn, we have the the t turn. So at this time, the verifier is like, very verify checks that it has got, enough tools to solve the problem, and it's a good time to stop to generate the final solution. So you can see, our agent assist 10 features multiple tools agents, we have the memory to make this process working very smoothly. And and in our system, we fine tune the planner because it's the one of the most important components in these systems. So, yet, I'd like to give more details about our system so we have a planner. So it takes the the query analysis, sub the original query, and all the skill all the tools in the toolset. To make the decision for each step. So it will generate the sub goal the select tool, along with the contest for this sub goal. And we and then we have an executor, so it takes sub goal and action generated by the planner and converts to execute way to call external tools And we have verified to verify if each step is complete or if there are any ambiguities needed to be addressed with further tool calling. And the older agent old agents have been updated in the memory, So information in in different terms can be shared So with that, we, our system can feature a diverse range of tools. First, we have base generators, so it is a very basic tool that can take any search query. And answer the questions step by step. We have present quarter, so it can generate precise compute perform very precise logistic reasoning or, mathematical reasoning. And we have a course search. To provide real time information from the Internet without with some citation support. We have Wikipedia search, so it can provide very specialized knowledge feature in the Wikipedia. This, too, might be very helpful if you are working on special domain like medical domain or scientific domain. And finally, we have web search. So it's very, well designed, giving the website. URL. It will understand the whole test and return a summarized information given your user query. So next, I'd like to take a concrete example give you more high level high level understanding. Of how agent flow works. So here's a question, computer, the track digit. For this trend hippocampus ID for this funky, would help if it were ISBN 10 number. So basically, there are three steps First, you need to, maybe, have the Google search to return this TID for this funky order. Then you need to convert it into ISBN 10 number. And finally, needed to compute this digital check digit. So here, we we show the our system without the hour of untuning. You can see, for the first step, it caused Wikipedia search and unfortunately, it returns the wrong information. But our system has improved or saved ability to refine the steps And this time, it turned to turned to Google search. And you can see it returned useful information like a TD. And for next step, it will cost PAS and Coder. But you can see for this step, there are some error for, the tool coding. Unfortunately, because the system has not been fine tuned, so you can see it has limited limited, planning, and making, the repeated arrows and unfortunately, it returns wrong solution. So it it motivates us to introducing our AI fine tuning that we can introduce in the rewards from the final solutions and encourage the engine model to enhance the engine model's abilities in planning and to usage. Next, I'd like to, hands on. Things to Joe Fong. And for our IO tuning and more experimental results. Yeah. I want to talk more about why and how to reinforcement learning to the planner. This technique, you know, for multi agent system, we can't do multi agent RL. On this agent system. Right? But, you know, this this is very resource consuming. And it's hard to optimize. Yeah. Hard to achieve the global optimize. Yeah. And meanwhile, according to our observation, in our journey system, or even for most system like CloudSearch, Cloud Code, or OpenID deep research, the, you know, sorry. Like, you know, Cloud Code, Open Air Deep research is the most critical agent is the planner. Since, you know, it's responsible for task you know, planning and scheduling for all subsequent agents. Okay. Therefore, it's the one that truly did training and the other agents can steal its cues instruction effectively. Without additional training. Okay. So how can we treat we can collect some data and then perform supervised flight tuning followed by you know, reinforcement learning and directly on the planner. This large model. Right? And then plug this print agent back into the system. This is very simple way, but this approach sounds promising. In practice, it usually doesn't work as if Since during our training, it cannot aware other agents' context. This will cause a significant misalignment between training and inference. Okay. So what's the best way to optimize Our method will directly optimize planner agent using the system in Online Fusion. To be more specific, we draw out the full agent flu system. Clicked, and filter the plan nurse. Actual trajectory of states. Action, and true events. It induces. And then use this online trajectory to update the planner on policy. Yeah. We will call this paradigm as in the flu reinforcement learning. Yeah. And, here's our formula. Yeah. Now we have a general idea on in the flow. But how to realize it. There are two main challenges for First, the rule generated by achieving this system are multi term. Which can cause model's context window to explode. Second, the the real world signal for a generic system are extremely sparse. To solve this, we propose flow GOPO. First, we transform the multi terreprocessing learning into a series of single terre update. At each term, the planner has access to the memory contents Then we compute a single and variable final outcome based reward, to the entire trajectory. And then we will put to every turn. Using this style, you know, coupled with group normalized advantages to stabilize training. Enable robust credits assignments, and allows the planner to, you know, if effective nonhorizons strategies. From Spass feedback. And next part, Haochang will introduce our experiment part. Yeah. Thanks, Shuo Fu. And let me give you a glance of our experiment results. And as we train our gensetec system or only mass data, mass hard and acoustic search search data, We test them on four domains, including mass, scientific, agentic search, dialects, and search domain. And overall, again, from the inference only agent flow based on 2,570,000,000 instructs is reported here, and we can see that our flow GOPO tuning makes this work. And all domain sky is diverse. And as a result, compared to the larger, proprietary models such as four o, g p four, open source, Queen 2.5, and Billingstruct, holistic paradigms such as search r one, t r TIR, and training free agent take systems, such as autogenerate. Our tuned agent flow surpasses all these baselines. Which also prove proved that, oh, with the reinforcement learning of the genetic system, system, it works. And with further analysis, analysis, the distribution shift of the tool used on some of our testing. Datasets. Here, we found some interesting changes. The model can dynamically play explore a combo of, like, Wikipedia plus web search, that showed on my QA, the right side of picture. And, oh, they can first search the Wikipedia search and got the snippets, and then delve into one of the pages of the Wikipedia, and further delving to the information we need. And on some specific search task, such as the two Wiki. A huge leap in using Google Search which is much stronger than Wikipedia, can be automatically optimized by the planner model. Okay. And also our further analysis found that our paradigm can increase performance and mitigate error through calling with a larger maximum turn limit. And also, it can reach a better performance with an extent of the larger term limit. K. And furthermore, as our system design handmade handmade planner, to and tuned the output of only the planner, so the reward assignment can be only assigned into the part of the planning part which means that there's no need for gradual overland response aligning to GRP o one, such as the go to gradually longer and longer. Response. Which may also have a bridging on our training system, and other work all well. That in contrast, our planner only needs to sync properly. Like showing the picture of the left with the steps gradually longer response were not exceed that. Such as maybe into the 150 output tokens, and then it will gradually get down. And furthermore, as compared to other holistic TRR reasoning models such as the two REL, our genetic system has efficient training. Process. And finally, test on different model sizes, such as 2.5, 3,000,000,000, and 7,000,000,000, instruct. And, also, we have conducted also other performance of model families, such as lama. And also prove that our reinforcement learning process, JRPO, agentetic system, can work. And I can have a final Yeah. So finally, let me quickly wrap up a project. So we provide a lot of accessible resources including our code base, live demo, YouTube tutorial, and interactive Wiki documentation. So if you're feel free to resources. To know more details Or your future development. And we are very happy that our project has been drawn attention in both at industry and So thank you so much for having us. So and to know more details feel free to check our website. Thank you. Thank you. Yeah. Thanks so much for the oral paper talk. And we thank you again, and we welcome our next speak invited speaker. Quick question. Do we have a So so that we don't have time for question. You can divert it. You can directly ask them Yeah. Yeah. Thanks. Yeah. So we are inviting our next speaker, professor at CMU, and he will give us very interesting talk about about skilling. Test times. Yeah. Hi, everyone. I'm Betty. I'm assistant professor and CMU. Thanks for staying late. My talk. So you will be about beyond flops and opportunities and challenges of the test time scaling on more than hardware. So this will be a shorter version of our tutorial. On Tuesday. So, I will try to stay very high level. If you're interested in the details and analysis, feel free to watch our, two hour tutorial. This Tuesday too. So, let me get started on, reviewing all the scaling law the first stage of the selling law around 2017 to 2020 is about the model parameters. People are excited about increasing the number of parameters, so we got more intelligence. And the second axis is around the data. Because lack of the scaling law study, we found that not only my parameters should be the x that's being scaled, but your data should also be scaled accordingly as well. And then we hit the CERT stage of the scaling, which is the context lens We see from 02/2000, 4,000, to like 1,000,002 contact lens days. So, we kind of see that this is the key enabler for all these kind of tasks on Asian tech LMS. Program automation. And now we have like super, long generation as well, which we enter the last era, like the most current era of test time scaling. We see this thinking models are released from, last year, October From, like, o one and deep think, cloud, and also the great open source models. Like r one, Quinn, and Kimmy, and series. And they're also widely deployed in different applications including the coding, math, trading, and search. So people are very excited about the performance of it. And, so the opportunity is currently, we kind of see that our LMs and agents and according to all the great talks, before me, they're kind of working. Right? The exciting part is we're no longer at this pre training of evaluating my perplexity or the token per second. We're now just focusing on the soft rate and also the task per second. So that is what we care about. However, the challenge is, we know that, for all these reasoning models, especially in cop with this Asians, So it has challenges in both training and inference. Inferencing in the sense of, like, we know that users are using all these agents, you have to wait for really long. If it's calling the tools, if it's ringing for really long. For the training part, we know that, all the great, software engineers and ML engineers are set up all this environment and, develop this task so that the models can, learn them and then mid training and all the rest are kind of, focusing on making the models actually usable and solve all the tasks. So the key part, we're gonna go through today in my talk is about how to improve the scalability of the test and compute for both of the training and, inference. So let's do a brief review on all the test time compute kind of strategies. So we have, like, sequential search, which is the low c o t everybody is familiar with. And we have the parallel search, which is known by Basil Finn, so you don't necessarily only do one trial especially it's extremely long. Although it has the benefit of it's doing some reflection and is enabling some stronger capabilities of the models. But it's also you can do it at, like, multiple times And with this verifier, as long as you are doing one of them correct, you are able to solve the problem. Right? And we also have more advanced strategies like tree search. So not only you can do independent search, you can have, building the tree, which is more scalable, and each of the branches interacting with, each other. So the outline for today is we're gonna do a introduction on the test and scaling laws several of them, and the the challenges on the modern hardware. And then we're gonna have like several lines of work The first line is the parallel reasoning. How to improve the scalability. And the efficient architecture, how to improve scalability. And finally, we're gonna talk about the applications of, Tessman's compute. So So let's first get to the first part. So we see like the scaling law analysis have like two exciting factors. The first one is from the Brown et al paper. That Besselfeld, of course, it will have the potential of increasing the capabilities of the model, but it's not guaranteed that you will continue growing the small model's ability kind of on par with the large model. So we're excited to see that this growth continues. Although not in a very scalable way, but it continues. In And then in the second scaling law paper on It is imposing some kind of new paradigm on Potentially, we can also add the infra time using a smaller model by doing, a 100 times more test and compute at the inverse time, and it's gonna match the large model's performance up by being more efficient, if we're doing this in a compute optimal way. But the question is, is the compute optimal? Actually the right way of And By computer, we mean the floating points for each of the computation of the models, However, if you're gonna take a look at the plot on the left, the x axis is the accuracy and, we're plotting the parallel frontier of the model selection, and, y axis is the latency where, the seconds. So you kind of see that we are contrasting two scaling laws, which use the compute optimal and the e flops which is the compute and memory access optimal. And you have large difference on the model selection. So, if you look at the line with the reddish color, it is trying this is the previous scaling law, which is the compute optimal. To reach accuracy of 70%, you want to select example, around like a four b model, However, if you're looking at our scaling log, which is the greenish color, it's gonna select the 14 b model. So what do you mean by, like, selecting this model is, like, under the same cost under under the same accuracy of the task. What is the optimal cost if you select different models. You guys select a small model, with a lot of generated tokens and test them strategy. You can also select your large model with a smaller generated, number of tokens. They might result in the same cost but who is actually give you a better reduce the cost? Depending on your cost model. So if you're doing the computer optimal only, you're actually do a sub optimal selection. So that was the key takeaway from this slide. Basically, the high level idea is we overlook the critical memory access bottleneck introduced by the Ambridge time scaling strategies and overestimate the smaller model impact So let's take a closer look at what do I mean by EFOS and why this is in for the test time scaling on modern hardware. So eFlows is computed by the cost of the compute, plus the cost of the memory access, times the arithmetic intensity of your hardware. So on the left hand side is the GPUs, and right hand side is the TPUs. The number just shows you that when the newer generation of hardware being developed, is easier to improve your flops than your memory bandwidth. That's why your arithmetic intensity is actually going up So with the newer generation of the hardware, you actually want some kind of the workload that is more compute bound. Rather than the memory bound. And this is the reason why in the previous scaling law study, if you're using a different cost model, you're gonna you're gonna have a different optimal selections on what to use at the test time. And the high level bit of them next several slides about, like, the challenges and opportunities is the key is this memory access is very important because when you are generating a lot of tokens, in your KaaS admin pued, which you would do is increasing your memory access a lot. So that cost is much more than the flop if you do the evaluation in this way. So that's why, this talk is oriented about something beyond flops, the cost beyond flops, but the memory access. Let's see why for the test time strategies, when you have more tokens, your memory access increases. The reason why is if you make the analysis about the cost model, the high level idea is, your attention, kvCash accesses are gonna dominate. Your test time cost and remember that we're gonna have this software then task second. So basically, we're mainly looking at the cost and cannot be amortized by more users using it. If it's the memory accessory models, can see, okay. I'm just hosting one model. If a lot of users are using it, the cost is amortized. We're good. But if you're, but the cost of the kvCash cannot be amortized because the more users or more rightsize you are using it, this is an independent and you are introducing more memory access. So that's precisely the kind of the major bottleneck of the cost for, test that compute when you generate a lot of tokens. And this is some two, detailed analysis plots on how how serious this is in practice. On the right hand side, this is benchmark on h 200. And you see all these green bars is at, different models, different sequence generation lengths, and it's dominating your cost. And on the left hand side, this is saying that when you're generating tokens, are grows longer, your attention dominates more. So the loss, we just saw on the left hand side of this plot, now it makes sense. Right? So previously, to reach a certain accuracy, the compute optimal base selection, or select the small model with a lot of tokens because we thought the generated tokens are free. If you were to just compute the flops. But if you consider the memory accesses, you would rather select a larger model with a fewer generated token. To reach a certain accuracy. So that's why there's a major difference on if your cos model is different, what your scalar law will change. So basically, in summary, what we're seeing for this scaling law, if you if you consider the practical efficiency, the theory usually use flops, but reality is bounded by IO, Your memory bandwidth, because of the tension quadratic in rows, during the long generation, and deployment reality is like we're out of bandwidth instead of the compute. So let's see, like, what are the key challenges introduced by this, practical scaling analysis. The first one is course, is due to the loan COT, You will have, like, very low paralysism, for this model, so you have to generate one by one. And even you increase the number of GPUs or GPUs is not gonna help with this problem. So it's very slow. Second one is the memory wall. We're talking about this like k v cache. Memory excesses. And when your hardware generations, become latest, or more advanced, it's not gonna resolve your problem. Just make it even worse So you cannot leverage the high fluff. Of your newer generation of the hardware Instead, we're still bonded by the, bandwidth. And, you know, that bandwidth grows slower than the flops. And when you're generating tokens, grow, your attention will gonna be more dominating. And your asthmatic intensity is lower. So this is the reverse way of the hardware growth. And that's why this is preventing the test time compute strategy to be scalable. And this is just two slots to, two figures to explain this problem in details. The left one is, in the x axis, is our smacking intensity increases across hardware generations, and, it's just saying that when new generations your attention dominates more. And on the on the right hand side, it just says that that your MOE or that mixture of experts does not help, but actually exacerbate the problem. And there are several challenges. Usually, we're gonna deploy this custom compute. In, training time. Right? Like, the test time training. Which is you're gonna collect all these trajectories and rewards and go back using reinforcement learning and train a model and update your model. And that's imposing additional challenges beyond the first two we we talked about. Is the irregular workflow. Because we see this like agent workflow and task. They're like not, the same time they're gonna end, so their irregular workload is copying is causing all these bubbles in the scalability of the test time training. So So next, we're gonna talk about, like, three lines of work. Kind of, resolving these problems. Including the parallel reasoning, efficient architecture, and we're gonna see that in the application of RL, how to scale reinforcement learning training. So So the parallelization can enable the scalability. It's never new. And it has been leveraged with all this system great work and map reduce Spark, and all the rest. And let's see that the analysis quad on the right is saying that if you do your parallelization properly, for example, previously we would do the analysis of, let's say, 32 k generation. You'll be the green line. Which means that your newer generation of hardware appears. You are gonna be more and more memory bond and it's not scalable. But if you're just generating the same number of tokens, let's say, 32 k, but with in parallel. Like, a group of eight. This will resolve the problem make the model much better or discolipically much better. But the question is like, how do we do that? Right? So basically, there are like several ways of doing the parallel generation. So compared to the autograft modeling, first of all, the diffusion language model, hopefully. Can resolve this problem a little bit better. And, I understand that because of like the discrete deficient problems, you need more iterations to actually do it. And currently, we haven't found the best way of dealing with this But with the algorithm make, improvement, hope on that. And especially with different hardwares, for example, TPUs. They will actually have higher automatic intensity, so even more compute. As long as it's parallelizable, is very scalable. So that's why probably Google is very excited about this direction. And, so we're gonna talk about today is the multiverse model, which was presented in this conference by amazing students. So the is your language model is secretly decides how to merge generation. And our task is to design a task to train the model in such a way that they can internalize this, capability. Basically, we did analysis on the recent traces of the r one model. When it came out. So, the question is, how many times or how many of these trajectories are actually doing parallelizable computation even they're doing the generation autoregressively. And surprise Not surprisingly, actually, 99% of this trajectory have some kind of, parallelizable branches. Even is is autograftly generation. They're independent. Locally. So there are several of them, including the one that you executing one task. But there are several steps you can do it in parallel. Or it can be your executing one task, but you wanna just try different variables or different trials. But anything that you can parallel analyze it. It exists. In this regional traces naturally. So the next question is, do model know they should parallel or merge? Unfortunately, they don't. So we can do the probing test, at the point that we know that it should parallel. Or it should kind of merge. Generations. It seems like the model does is not aware of that due to the autoregressive training. Of course. So how to do that? We are machine learning people. Of course, we'll do the design on the task to enforce this parallel structure into it. So what we do is, we are rerate rewriting the traces with, Gemini model. And maintaining the quality and content and also the the lens of it, but insert keywords at the moment when it should do the parallel generation, the moment it should merge, And it works pretty well. If you just change your engine and also your attention mask because you wanna see that, in a intermediate or, like, the local parallel parts, the two parallel sentence should not be able to each other. So you have to change your attention mask and your inference engine. According to that. So and the results show that on different, type of task, and a b 24, 25, 500, and GBQA. Doing very well on the reasoning task. And here's demo how exactly we're doing that. So here's a generated summary of the test. And it's doing like three parallelizations. And it knows that you had to write a conclusion. And then it starts to branch out again. And there's a branch in the branch, and those branch in the merge. And conclusion. So this is exactly how it is a operating. So let's get to the second part. Which will will seek how we can incorporate the efficient model architectures to resolve the scalability problems of, the test time compute on the hardware as well. So So recall that we're seeing this memory wall due to the memory access of the kvCash, So the other way, like, the since the attention cost dominating, if we can reduce that, that naturally help with this bottleneck. Right? So in fact, there are a lot of hybrid models where far as attention model came out, with this open source, models, like mini max m one, and point next thinking. Or deep seek 3.2 using the SPARS retention. And our lab has been working on Sparse Attention, and also efficiency and architecture for long contacts. It seems like it's extremely useful for long generation as well. It's it's actually even more. Important. So Smart Meetings is actually not very new. If we're considering this, scalability. It has been used as regularization before 2012. And efficiency for a really long time, including make sure of the experts And we see that it can even the capability of the test on compute because nowadays, we're doing the reinforcement learning and agent end task. And we just same amount of time, if you can generate more tokens, your ability just increases, or with the same amount of time, you can train or get more rewards your model will be stronger. So scalability or, like, sparsity is no longer efficiency. It is the capability increase. So, the TLDR for this is we did same analysis with the kinetics scaling law. Just change the attention to test time's first attention. The very simple version of blocked top k. It seems like it's doing like, extremely well. And a interesting observation is with the more compute, when you double when you when we study the scaling law, we usually study the you double a compute, where do you increase. Right? So the interesting thing is, when we're doing the double the compute, we're not aiming to actually increase the sparsity budget. Or pushing sparse attention to dense. Rather, we want to spend more computer and memory access or resources to the number of generated tokens. Meaning that if you are doing like a massive generation and aiming for really high accuracy region, on sparse is even more important. There. And there are some results on the, additional benchmarks. Iron is completely different scaling beyond dance for test time to compute. Amy and Lifcode Bench, and believe in other agent testing will be very similar. And this is some evaluation on the mixture of experts model as well. And like what we mentioned earlier, mixture of expert model is even worse is even more memory bonded. So that's making this attention better will also, like, resolve the problem for that. And there's several obligations on, test time. The inference time, attention. We compare like the top k dance and blocks bars and local. It seems like the the block stars is doing reasonably well. Pretty close to the true fine grained top k attention. Well, unfortunately, local fell, but we know that those hybrid, attention will be like required to be training from scratch, but this is like test time. Another exciting thing is, the on the right hand side of the evaluation on the latency. Of different KB budget and different speed up we can get for the test time compute. This is showing that not only this is theoretical analysis, but a simple implementation on on top of, like, page retention, shows that this can actually be realized faster on GPU. And also, like, we just wrote a interface which you can write like 200, 20 lines of code to call the word packs. You can write 20 lines of code over any kind of sparse attention variance because we do the abstraction, do the heavy lifting, for you on the current service system engine, which basically their the intuition is like you can decouple the, attestation back end and the caching systems. And write the abstraction code on top of that, and it's super simple. And this is a very important for what we're gonna talk about next because once you do the test on, as far as attention, you wanna use it somewhere no matter is in the agent workflows or reinforcement learning. You actually need to be integrated into the service system. Otherwise, even algorithmically, you are doing like five x faster. But the continuous batching and scheduling all the rest, will ease up all these advances. So it is very important to actually integrate all these variants or algorithm change in the serving system for the sake of test time compute, test time compute applications or reinforcement learning. So let's get into the, last part, which is the application of reinforcement learning. So say that implemented well on as the back end, we wanna do a reinforcement training. For on the top of this like Sparse Attention enabled model, what do we do? So the challenge is at same challenge as the sparse model which is like MOE, We know that because the inconsistency between the inference and FSDP, there's could be activating different experts. When you're doing the rollout when you're doing the training. So there's, like, a divergence on the distribution. Of your rollout and your training model. Right? And as far as retention suffers from very very similar problem. And also quantization and all the rest. But luckily, we have a lot of great work including the aerial flash aural, GSPO, ISFOP, and even deterministic kernels trying to resolve this problem. To mitigate this, inconsistency. Or harness of training these different architectures. Or lower bid architectures in, reinforcement learning. You can see that with a simple strategy. It is matching the dense reinforcement learning results. So those are like training free sparse attention not only can maintaining and with the same kind of cost even doing much better. In the infra time. It is also on par with the but being much faster in the during the reinforcement learning duration. And we asked the question that since we can fix the model, for this distribution misaligned problem, what if? I want to do an even more aggressive experiment on I want to roll out with a smaller model because it's being faster. Right? So all this training or post training is bottlenecked by the exploration of finding the reward at all. So if I can use a smaller model or different model to the rollout, but I can still train with the trajectories that is explored by this smaller model. And it is much faster than you have to stay fully on hold. Right? So, we asked this question, but the existing distribution alignment strategies including all this clipping or sentence wise rejection and all the rest, will fail here, because, this distribution mismatch is too large. The distance or the k l divergence of your small model and large model will be a very large gap, and it's almost impossible to do that reinforcement learning on the top of that stable way. So the idea of our work to be released it's called jackpot, The insight is instead of doing the clipping, after the important sampling, of, PPO or GRPO. What you can do is you can directly align your inference and your training models. That distribution. By rejection sampling. But, of course, we know that rejection sampling is not very efficient. There's a trade off between the sampling complexity and all this distribution matching. Right? You can exactly have the same distribution, but you won't have any tokens left. They're all being rejected. So to solve this problem, there was a amazing 2024 Icemail paper on optimal budget rejection saying that, you don't have to be exactly the same distribution of your roll out model and your training model. But you can try to set up like, there's a face to gap. Or for any kind of acceptance rate that you enable, you can do a different sampling with some kind of hyper parameter tuning on this rejection sampling algorithm. And to do that, you can control how much how far away you want to control these two distribution online so that you can balance your sample complexity and your distribution distance. So that, in the end, what we can do is, if you do that properly, you can actually the line of algorithm is the yellow one. Is not like exactly matching the on policy, but it's already amazing that you will continue to train not collapse. If you are just doing the rollout of a one b model and train a seven b model on top of that. And you see that this, like, k l diverges was kind of maintained. This this yellow line. Across the training. So the last part of the, reinforce challenges of the irregular workload we talked about. So you could be like, when you are doing exploration, you could call a lot of tools or agent tasks, and you are some kind of, like, the waiting time for some of the examples are very long, you will have all this dullness. So this is a classic asynchronous examples. So how to allow more asynchronousations since now we're doing the agent check RL training without hurting the training accuracy. So our observation is in fact, you just need to prevent some of the bad tokens or distributions misaligned that is very severe So in the usual case or usual steps of the training, if you don't do any kind of the clipping, which is what is trying to resolve this asynchronous problem, you can train well, and your performance won't be bounded. But the trade off is you can just crash. If you don't do the clipping properly. But there's a kind of a tension between how much you and what we are clipping. Because, we know that they are some kind of the high entropy tokens by the earlier work. That is very important for your RL training. However, if you clear it's very strict. For your trans region. Your actually very likely to clip all this important tokens. So the idea is we need some kind of algorithm which you can do the clipping the ones that are extremely bad so that your training won't collapse. But at the same time, you have to maintain and not being too constrained on the high entropy tokens which could be very, very useful for your training. We proposed this second momentum trust policy optimization, which is depending on the k l square. Are details in the paper. I won't have the time to go over the talk. But here's the result. Amazingly we can enable the stable training for 256 steps. And asynchronously. Yeah. So I think to conclude, today we go over the scaling laws of test and compute on modern hardware, and the practical efficiency is very due to the attention dominating for the AV memory accesses. And it's not very scalable with the hardware development because the flops goes faster. Than the memory bandwidth. And then we talk about three challenges on they are problem Oh, they are problem for parallelism, there are problems for the quadratic memory access. There are problems for the irregular workload. And when you talk about three lines of work, which can do the parallel reasoning, can be sparse attention or efficient architectures, and, also, like the stable reinforce stable scalable learning as the application in the end. Thank you so much for your attention. I'm happy to take questions if there are any. Please Hi, professor. Thanks for the great talk. I have a question regarding the small model rollouts idea. So people say that the upper boundary of reinforcement learning is somehow concerned by the explore So wouldn't we be constrained by the capabilities of 1.7 b base models in this setting here. Yeah. So, this is exactly the problem that we will worry initially. And in the end, what amazing We're still like doing the experiments, for longer training and larger scale. But what what we reserve is you will continue to grow even like if you're using the 1.7 b model to do the roll out, and four b or seven b to do the training. As long as it's not collapsing, your community will grow slightly slower, if you see like the yellow line and, the purple line, So it's it will be like slightly slower and lower, but if it continue to grow, I kind of see that it is still catching up and matching. And, we're also investigating why this is the case, and it seems like we're also doing the reverse k l. On the top of this. To make the smaller model doing much better than its own RL training as well. Mhmm. So I think there are like mechanisms that we don't fully understand here, but the takeaway is even enabling this kind of a like, really large diversions of, the two model training. Is very, very interesting. And, we're looking forward to see in the end, it cannot fully match the large model, maybe there are other follow-up works that we can do to fix it. Thanks for your question. Thanks for the explanation. For example, you can use trees. Or even more scalable Python compute for the model to do. The rollout. That could be one idea. Yeah. Thanks for the question, Anthony. And do everyone have more question? Feel free to ask Oh, yeah, next one more. Yeah. Maybe one last. Okay. Thank you, Betty, for the great talk. I just have a question. So what's your intuition on, like, how much, hardware utilization we can do when you this kind of scaling. We also all the method proposed here cannot fully utilize the hardware. For example, if you do parallel generation, it's always constrained by the longest, path. Right? By the critical path. So there's still a lot of resource that are idle. So based on your experience, like, how many how much performance have we extracted from the underlying hardware? Yeah. That's one of the most pain point our lab is encountered. So So that's why we're also building the interface or abstractions on the top of that. So that including the multiverse, including the Kinetics, burst of attention, could it be actually be scalable and leveraging the development of the services system. You're right on the naive implementation. Is not gonna leverage on not even the hardware. But the server and system advantages. It's very hard to do that, and we're working on it. And for the smart assistant, it's very similar. So you have to, they are special kernel, specialized kernels that you have to integrate and there's like a lot of lines that you have to write. To even get a properly or correct. And of course, when the hardware shifts, it will be another, like, length of work. So we're trying to actually after doing all this amazing algorithms and, scalability, exploration, the next mission is to build an interface or, or, like, the front end so that all this method can be actually properly compiled. And implemented properly in this different hardware or service system agronomists. Yeah. Great. Thank you. For the question. Cheer. Thanks for the question and asking. And thanks, professor Baide again. This. Awesome cog. And, yeah, Testing Yeah. Thanks everyone for coming here. We have one last session about the best paper of the NeurIPS efficient listening workshop. Yeah. This year, we have normally four of the paper as best paper nomination. That include when listening, it's slow, and other paper journey parallel scaling with independent generation. Yes. Also, in other paper, Inflow agent system optimization. For efficient planning and tool use. And also M1 towards scale test time compute with MAM based learning model. And we finally have one of one best paper that is M1, yeah, towards skill, time, test time computer with mobile and we are We're honored to invite the paper also here to take our best paper award. Yeah. Yeah. Thanks, Junshiong Wang, from Together AI, who got the best tech early award this year. Yeah. You can give a sell award. Yeah. You. Thank you for the organizers. Yeah. And thanks for everyone. Yeah. That's all of today's efficient work research work about agenda. Yeah. Thank everyone for coming here to join our Infusion listening, and we have also have a lot of poster here after the whole session. You're welcome to like see all the poster, and thanks everyone again for joining us here. We can now make such a big workshop without you. Thank you so much.