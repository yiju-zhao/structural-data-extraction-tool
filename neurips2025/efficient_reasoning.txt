WEBVTT

00:33:48.831 --> 00:33:51.111
Hi. Hello, everyone. I think that,

00:33:51.411 --> 00:33:53.441
our workshop will be starting

00:33:53.441 --> 00:33:55.521
now. Yeah. First is our opening remark.

00:33:55.521 --> 00:33:57.681
Yeah. You, everyone, for attending our

00:33:57.681 --> 00:33:59.760
workshop today. I'm Xin Yuan, and this is

00:33:59.761 --> 00:34:02.090
Chunghwa. Yeah. We are the organizer and

00:34:02.090 --> 00:34:04.021
the moderators today for our workshops.

00:34:04.340 --> 00:34:06.580
Yeah. We are the first workshop on the efficient

00:34:06.581 --> 00:34:08.661
reasoning. Yeah.

00:34:08.661 --> 00:34:10.980
We know that, these days, that foundation

00:34:10.981 --> 00:34:13.181
models are everywhere, and we have so

00:34:13.181 --> 00:34:15.351
many large reasoning models. Yeah. But,

00:34:15.881 --> 00:34:18.120
when people apply them in real world applications

00:34:18.121 --> 00:34:20.370
and in many settings, that well, people will

00:34:20.530 --> 00:34:22.611
complain about their efficiency. So we

00:34:22.611 --> 00:34:24.620
want to talk about how to make this large

00:34:25.101 --> 00:34:26.401
reasoning models more efficient.

00:34:27.990 --> 00:34:30.311
In detail, we are talking about four questions

00:34:30.311 --> 00:34:32.610
in our workshop today. The first is

00:34:32.611 --> 00:34:34.871
how to design, create, and maintain high quality

00:34:35.010 --> 00:34:37.281
data for training and evaluating these large recent

00:34:37.281 --> 00:34:38.961
models on challenging recent task.

00:34:39.521 --> 00:34:41.621
And the resource constraints contribute

00:34:41.760 --> 00:34:43.841
to engines, of these directions, including

00:34:43.841 --> 00:34:45.771
maybe long context reasoning, simple

00:34:46.331 --> 00:34:48.650
planning, multi hop deduction, and real time

00:34:48.651 --> 00:34:50.440
decision making on diverse domains.

00:34:51.751 --> 00:34:53.910
And for the second question, we also want

00:34:53.911 --> 00:34:55.531
to develop maybe more advanced

00:34:56.291 --> 00:34:58.451
training algorithm for maybe supervised fine

00:34:58.451 --> 00:35:00.483
tuning or doing some reinforcement fine

00:35:00.484 --> 00:35:02.581
tuning to and some efficient type

00:35:02.901 --> 00:35:05.321
time inference methods such as pruning, compression,

00:35:05.921 --> 00:35:08.321
progressive generation, and search based strategies

00:35:08.321 --> 00:35:10.511
to reducing the time or space

00:35:10.511 --> 00:35:12.491
complexity without, diluting

00:35:12.551 --> 00:35:14.331
the accuracy of s phoneme.

00:35:15.451 --> 00:35:17.051
And the third question is about,

00:35:21.591 --> 00:35:23.811
Yeah. And and it's okay?

00:35:24.051 --> 00:35:26.171
He'll he'll he'll operate the I

00:35:26.171 --> 00:35:28.491
see. I see. Yeah. Yeah. And I think the first question

00:35:28.491 --> 00:35:30.831
is about we want to, implement

00:35:30.831 --> 00:35:33.050
of efficient audio training systems

00:35:33.051 --> 00:35:35.290
and the long term source inference engines to

00:35:35.290 --> 00:35:37.080
support contract reasoning models.

00:35:37.641 --> 00:35:39.881
We're unlocking new reasoning potentials on

00:35:39.881 --> 00:35:41.341
commit accommodated GPUs

00:35:41.931 --> 00:35:44.331
technicals may include dynamic QV cache placement,

00:35:44.331 --> 00:35:46.421
context graph exclusion, and on device

00:35:49.441 --> 00:35:51.621
two, topic about both the aerial

00:35:51.681 --> 00:35:53.891
and world. They are two very

00:35:53.891 --> 00:35:56.051
famous, RL training systems these

00:35:56.051 --> 00:35:58.091
days. And the last question

00:35:58.091 --> 00:36:00.251
is about if we also want to adapting this

00:36:00.251 --> 00:36:02.421
larger recent models to real world

00:36:02.421 --> 00:36:04.921
reasoning capabilities in real world results constrained

00:36:05.061 --> 00:36:07.341
scenarios. Such as clinical decision

00:36:07.341 --> 00:36:09.731
making, robotics, autonomous driving,

00:36:09.951 --> 00:36:12.031
developing world health, and maybe on

00:36:12.031 --> 00:36:14.101
orbit, augmented We will just

00:36:14.101 --> 00:36:16.260
want to use these models to be really useful

00:36:16.260 --> 00:36:18.481
in real world. And today

00:36:18.481 --> 00:36:20.101
that we will have nine speakers,

00:36:20.561 --> 00:36:22.431
from both academia research.

00:36:22.671 --> 00:36:24.751
And the industry. Yeah. They are

00:36:24.751 --> 00:36:26.771
professor Bedi Chen. Yeah. Professor

00:36:26.831 --> 00:36:28.371
Aman Kuhn. Professor

00:36:29.040 --> 00:36:30.741
and, Nicholas

00:36:31.681 --> 00:36:34.101
professor not professor. Yeah. Researcher.

00:36:35.001 --> 00:36:37.321
Yeah. Professor Wei, professor Yu,

00:36:37.321 --> 00:36:39.181
professor Zhang Hao, and

00:36:40.651 --> 00:36:42.691
Wang Zhang from Badang State. Yeah.

00:36:43.491 --> 00:36:45.351
We will also have four panelists

00:36:45.571 --> 00:36:47.631
from the industry to talking about

00:36:48.191 --> 00:36:50.530
some frontier research regarding to the reasonableness.

00:36:52.311 --> 00:36:54.391
And today and we are also very

00:36:54.391 --> 00:36:56.400
happy to see that this year, we

00:36:56.401 --> 00:36:57.961
have received submissions.

00:36:58.421 --> 00:37:00.430
We have received nearly 300

00:37:00.431 --> 00:37:02.451
submissions. I think it's a very huge number

00:37:02.591 --> 00:37:04.891
for our workshops days, and then we have maybe

00:37:04.891 --> 00:37:06.751
182 acceptance.

00:37:07.230 --> 00:37:09.631
Among them, we have select, maybe not 22,

00:37:09.631 --> 00:37:11.571
32 spotless and four hours.

00:37:11.911 --> 00:37:13.511
And today, we will also,

00:37:14.151 --> 00:37:16.391
give the best paper award to one of the other

00:37:16.391 --> 00:37:18.719
paper. And our schedule

00:37:18.720 --> 00:37:20.681
is that, in the morning, we will have

00:37:20.921 --> 00:37:23.001
four invited talk and two hour talks and

00:37:23.001 --> 00:37:25.141
then maybe the post session one. And in

00:37:25.141 --> 00:37:27.341
the afternoon, we will have the five, another

00:37:27.341 --> 00:37:29.571
invited talks and one panel discussion

00:37:29.631 --> 00:37:32.031
and two oral talks, and also our panel

00:37:32.031 --> 00:37:34.171
section two. And here are

00:37:34.171 --> 00:37:36.191
other organizers. Yes. Thanks their contribution

00:37:36.331 --> 00:37:38.431
to this workshop and the thanks to our

00:37:38.431 --> 00:37:40.810
audience. Yeah. Let's start

00:37:40.811 --> 00:37:43.001
maybe our first, speaker. Yeah.

00:37:43.161 --> 00:37:45.661
This will come professor, Jun Escaping.

00:38:46.511 --> 00:38:47.011
Alright.

00:38:49.371 --> 00:38:51.320
Yeah. I think I'll just get started with you.

00:38:54.221 --> 00:38:56.231
Alright. Yeah. Thank you all for coming.

00:38:56.871 --> 00:38:58.951
Very efficient of you to come so early in the morning to the

00:38:58.951 --> 00:39:00.690
workshop. So,

00:39:01.331 --> 00:39:03.441
gonna talk a few about a few things a bit more towards research

00:39:03.441 --> 00:39:05.251
and a bit more towards

00:39:05.631 --> 00:39:07.790
maybe future architectures of reasoning and of efficient

00:39:07.791 --> 00:39:09.851
reasoning. Because, I'm pretty interested in

00:39:10.571 --> 00:39:12.630
recurrent depth architectures. And, I'll tell you

00:39:12.631 --> 00:39:14.710
what that means and why I think it's interesting

00:39:14.710 --> 00:39:15.900
for reasoning in this talk.

00:39:17.720 --> 00:39:19.920
Before we start, talk about

00:39:19.921 --> 00:39:22.021
a few key concepts And I think I can

00:39:22.021 --> 00:39:24.021
only do this wrong, but, these will be the

00:39:24.261 --> 00:39:25.671
concept of which I'm working with.

00:39:26.471 --> 00:39:28.821
So for the purpose of this talk, what's test

00:39:28.821 --> 00:39:30.841
dot compute? For the purpose of this talk,

00:39:30.841 --> 00:39:33.161
we'll say that test time compute is just the ability of machine

00:39:33.161 --> 00:39:35.191
learning model to use more computation at test time.

00:39:35.271 --> 00:39:37.374
We'll make no further, like, ideas of, like, what

00:39:37.374 --> 00:39:39.407
kind of computation that would be or what shape it

00:39:39.407 --> 00:39:41.440
takes or whether there's a chain of thought or not.

00:39:42.481 --> 00:39:44.721
And, but maybe the bigger question is

00:39:44.721 --> 00:39:46.261
like, what actually even is reasoning?

00:39:47.461 --> 00:39:49.531
And, it's quite good to have the slide

00:39:49.531 --> 00:39:50.611
at the beginning of this

00:39:51.651 --> 00:39:53.910
workshop because I really don't have a good answer

00:39:53.911 --> 00:39:55.950
Like, because I think like, many

00:39:55.950 --> 00:39:57.990
of us are familiar with this kind of definition

00:39:57.990 --> 00:39:59.991
of reasoning. Right? Where we say, maybe reasoning

00:40:00.311 --> 00:40:02.470
is just learning to pattern match abstract concepts to

00:40:02.471 --> 00:40:04.091
meta strategies to learn logic and

00:40:04.571 --> 00:40:06.701
structure and planning, It's not just memorizing facts,

00:40:07.101 --> 00:40:09.221
But if we investigate it for a second, what does it

00:40:09.221 --> 00:40:11.491
what does it actually mean? Right? It just means

00:40:11.491 --> 00:40:13.661
something more along the lines of, it's

00:40:13.661 --> 00:40:15.786
just everything where the model doesn't know the

00:40:15.786 --> 00:40:18.071
answer. Right. So but maybe that's actually

00:40:18.071 --> 00:40:19.511
not a crazy definition. Right? Because like,

00:40:20.261 --> 00:40:22.501
we see a lot of, works today where

00:40:22.501 --> 00:40:24.281
reasoning really is just logical inference.

00:40:25.481 --> 00:40:27.641
And, I think

00:40:27.801 --> 00:40:29.471
have a good answer here what really is reasoning.

00:40:30.351 --> 00:40:32.511
But I think, with these working definitions, this

00:40:32.511 --> 00:40:34.391
will be what the talk will be about.

00:40:35.191 --> 00:40:37.421
So So because what

00:40:37.421 --> 00:40:39.861
we are very familiar with over the last now,

00:40:40.001 --> 00:40:42.341
one and a half years, is we've seen a lot of verbalized

00:40:42.481 --> 00:40:44.011
reasoning. Right? We we have a model,

00:40:44.521 --> 00:40:46.731
the model gets a question. The question is very hard.

00:40:46.891 --> 00:40:49.070
Here's something about planning. Right?

00:40:49.071 --> 00:40:51.231
And the model runs for minutes. Now, for maybe

00:40:51.231 --> 00:40:52.931
from more modern models for hours.

00:40:53.241 --> 00:40:55.081
And the model does logical inference.

00:40:55.641 --> 00:40:57.901
It run it runs some sub components. It thinks about the question.

00:40:57.991 --> 00:41:00.151
It really looks like whether it has some set of premises, which

00:41:00.151 --> 00:41:02.220
starts out with. It runs a lot of

00:41:02.220 --> 00:41:04.321
computation to, like, work on

00:41:04.321 --> 00:41:06.451
those premises until it arrives a solution to the problem.

00:41:06.851 --> 00:41:09.059
And this is really how we have been able to

00:41:09.059 --> 00:41:11.121
scale these models to many domains

00:41:11.121 --> 00:41:13.151
that we have never seen them work before.

00:41:13.151 --> 00:41:15.311
Like, even two years ago. Right? Like, if you remember back

00:41:15.311 --> 00:41:17.461
to GPT three, it was terrible

00:41:17.461 --> 00:41:19.811
math. And now we have Math Olympiad models

00:41:19.811 --> 00:41:21.210
just based on verbalized reasoning.

00:41:23.371 --> 00:41:25.440
But why is also super important to

00:41:25.441 --> 00:41:27.621
investigate why would this even be necessary?

00:41:28.261 --> 00:41:30.341
Why do we need reasoning these models? Why do we need

00:41:30.341 --> 00:41:31.091
verbalized reasoning?

00:41:32.846 --> 00:41:35.040
Actually straightforward. We

00:41:35.040 --> 00:41:37.113
have these multi hop problems. For example, I

00:41:37.114 --> 00:41:39.190
like we like this this test sentence

00:41:39.190 --> 00:41:41.431
from this one paper, which is called John's father's mic.

00:41:41.431 --> 00:41:43.530
Mike's father is Tom. John's grandfather

00:41:43.591 --> 00:41:45.341
is Of course, the answer is Tom.

00:41:45.981 --> 00:41:48.051
If you think about this, in a

00:41:48.051 --> 00:41:50.081
transformer, then you need

00:41:50.081 --> 00:41:52.121
a certain number of layers to

00:41:52.181 --> 00:41:54.361
even encode these three relationships.

00:41:55.081 --> 00:41:56.551
And here, if you

00:41:57.121 --> 00:41:58.980
have a model that has fewer than four layers,

00:41:59.220 --> 00:42:01.591
you can't robustly do this sort of inference.

00:42:01.671 --> 00:42:03.701
You actually need a certain amount of depth in

00:42:03.701 --> 00:42:06.101
the model to do a certain amount

00:42:06.321 --> 00:42:08.781
of inference. That's why we've been so successful

00:42:08.841 --> 00:42:11.029
with verbalizing of thought as a community because

00:42:11.030 --> 00:42:13.110
we can use the verbalized gen of thought to sort of

00:42:13.111 --> 00:42:15.271
extend this graph that I'm showing you here more

00:42:15.491 --> 00:42:17.571
like horizontally, so more to the

00:42:17.571 --> 00:42:19.881
right. Right? Because we we put in more tokens

00:42:19.881 --> 00:42:22.011
in the middle, and we can use those tokens

00:42:22.171 --> 00:42:23.481
to have a longer multi hop.

00:42:24.521 --> 00:42:26.611
Now what I think this graph also

00:42:26.611 --> 00:42:28.020
implies is that,

00:42:28.811 --> 00:42:30.961
scaling computation only horizontally

00:42:31.361 --> 00:42:33.381
is just one way of extending this graph

00:42:33.381 --> 00:42:35.181
Right? We could also just make our models deeper.

00:42:36.101 --> 00:42:37.801
Also left in some fun models that trained

00:42:38.121 --> 00:42:40.200
over the last months where, like, none of the models are hundreds of

00:42:40.201 --> 00:42:41.521
layers deep or 200 layers deep.

00:42:42.401 --> 00:42:44.480
So there's a second access of compute scaling. We can't

00:42:44.641 --> 00:42:46.811
we can scale, of course, always the sequence

00:42:46.811 --> 00:42:48.901
dimension But

00:42:48.901 --> 00:42:50.980
these kind of problems suggest that we can also scale in the

00:42:50.980 --> 00:42:52.981
depth dimension. And how would we do that?

00:42:53.481 --> 00:42:55.021
So And,

00:42:56.661 --> 00:42:58.651
why would we do that? And how is it different? Right?

00:42:58.731 --> 00:43:00.789
Think it's often interesting to think about that. We

00:43:00.789 --> 00:43:02.841
scale in a depth dimension. As a as

00:43:02.841 --> 00:43:05.261
a form of continuous reasoning

00:43:05.261 --> 00:43:07.571
or a form of latent reasoning meaning that we

00:43:07.571 --> 00:43:09.671
put a part of this reasoning chain that we

00:43:09.671 --> 00:43:11.731
used to do in the SQL dimension,

00:43:11.731 --> 00:43:13.931
we used to verbalize this reasoning. We put

00:43:13.931 --> 00:43:16.071
a part of it into the depth of the model to

00:43:16.071 --> 00:43:17.001
make it more efficient.

00:43:19.321 --> 00:43:21.480
So how would we do that and what's what are approaches

00:43:21.481 --> 00:43:23.701
to that? So what I'll put I'll talk

00:43:23.701 --> 00:43:26.201
about today recurring in-depth. And

00:43:26.261 --> 00:43:28.356
what that means really is that it's a model that

00:43:28.356 --> 00:43:30.481
just has a recurrence where it has a

00:43:30.481 --> 00:43:32.570
repetition. In the depth of its layers. So it has

00:43:32.571 --> 00:43:34.331
some layers or maybe all of its layers

00:43:34.970 --> 00:43:37.001
and these layers are repeated means that you

00:43:37.001 --> 00:43:38.991
can make the model very

00:43:39.361 --> 00:43:41.400
deep if you repeat the layers a lot of times.

00:43:41.401 --> 00:43:43.561
Or you can make it very shallow if you repeat them only

00:43:43.561 --> 00:43:45.681
once. Right? In this way, you can sort like

00:43:45.681 --> 00:43:47.700
adaptively change the depth of

00:43:47.701 --> 00:43:49.661
the model and you can make very

00:43:49.721 --> 00:43:51.932
deep models that you couldn't otherwise really

00:43:51.932 --> 00:43:53.791
handle terms of number of parameters.

00:43:54.191 --> 00:43:55.891
Right? Because in a standard transformer,

00:43:56.510 --> 00:43:58.671
if we make it very deep, we have a lot of layers.

00:43:58.671 --> 00:44:00.860
So we actually have to either make it very thin

00:44:01.260 --> 00:44:03.331
which is very bad for optimization, or we have

00:44:03.331 --> 00:44:05.391
to make the models very, very large,

00:44:05.551 --> 00:44:07.631
it's also hard to handle for efficient reasoning for

00:44:07.631 --> 00:44:09.660
many inference domains. So

00:44:09.661 --> 00:44:11.820
just having a smaller subset of layers that

00:44:11.821 --> 00:44:13.881
we repeat seems to be

00:44:13.951 --> 00:44:16.071
very promising. But

00:44:17.811 --> 00:44:20.041
and what's also interesting

00:44:20.041 --> 00:44:22.051
about this is that this is really like such a

00:44:22.051 --> 00:44:24.131
foundational concept of machine learning. Right? Like, you may

00:44:24.131 --> 00:44:25.961
have seen this idea of a current in-depth

00:44:26.361 --> 00:44:28.431
universe transformers. Or you might have

00:44:28.431 --> 00:44:30.451
seen this idea, right, as like loop transformers

00:44:30.511 --> 00:44:32.851
or as representation recycling like an alpha fold

00:44:32.970 --> 00:44:35.030
or as an end to the computation

00:44:35.030 --> 00:44:37.111
time from the before transformers era

00:44:37.781 --> 00:44:39.940
as equilibrium models, as diffusion models,

00:44:39.941 --> 00:44:42.441
as the infinite refinement, implicit neural nets.

00:44:42.511 --> 00:44:44.500
Or maybe just even older

00:44:44.741 --> 00:44:46.831
Not sure who's in the room. I say,

00:44:46.831 --> 00:44:48.570
really it's a type of hot field model sometimes.

00:44:49.661 --> 00:44:51.821
But why why do we keep on going back to

00:44:51.821 --> 00:44:53.921
recurrence? Right? Why do we keep on making our

00:44:54.421 --> 00:44:56.131
neural networks to be recurrent?

00:44:56.651 --> 00:44:58.811
And I'm gonna bring up a few interesting motivations

00:44:58.811 --> 00:45:01.081
that I think may be clarified from

00:45:01.081 --> 00:45:03.361
different angles why we think this is interesting.

00:45:04.416 --> 00:45:06.611
So So the first motivation,

00:45:06.751 --> 00:45:08.411
the very classical one in our

00:45:08.891 --> 00:45:11.131
I'll show you a bit brief about it, and you don't have to suffer

00:45:11.131 --> 00:45:13.281
through a lot of neuroscience. But the

00:45:13.661 --> 00:45:15.731
classical the classical motivation

00:45:15.790 --> 00:45:17.971
for recurrence in neural networks

00:45:17.971 --> 00:45:20.061
is neuroscience. It's the idea that we have recurrent

00:45:20.061 --> 00:45:21.461
firing patterns in the brain

00:45:22.581 --> 00:45:24.921
it don't just use every neuron once in the brain. Right?

00:45:25.321 --> 00:45:27.351
And, there's

00:45:27.411 --> 00:45:29.561
even like from, like, if you look at this, like,

00:45:29.561 --> 00:45:31.661
from more more like a zoomed out pops up perspective, right, you can

00:45:31.821 --> 00:45:33.901
probably heard about brainwaves. So the idea that you have,

00:45:33.901 --> 00:45:36.130
like, different patterns in the brain.

00:45:36.530 --> 00:45:38.836
If someone uses this paper, this is a paper from

00:45:38.941 --> 00:45:41.091
a few years ago that shows these

00:45:41.091 --> 00:45:43.281
like metastable patterns. Here

00:45:43.281 --> 00:45:44.101
in the cortex.

00:45:45.361 --> 00:45:47.421
And this is interesting. Right? Because we have these

00:45:48.061 --> 00:45:50.491
different frequencies and this is like a classical,

00:45:51.511 --> 00:45:53.600
like, classical approach to motivating recurrence

00:45:53.601 --> 00:45:55.700
is from neuroscience. But, let's skip

00:45:55.701 --> 00:45:56.201
that.

00:45:57.791 --> 00:45:59.871
A more recent approach to motivating recurrence

00:45:59.871 --> 00:46:01.011
is the idea of universality.

00:46:02.131 --> 00:46:04.221
Especially from this paper, also

00:46:04.361 --> 00:46:06.621
aptly named universal transformers

00:46:06.681 --> 00:46:08.831
from about 200,018.

00:46:09.171 --> 00:46:11.251
Right. And this is just a classical transformer

00:46:11.251 --> 00:46:13.511
like the one that you probably still learn for interviews.

00:46:13.740 --> 00:46:15.090
And don't probably don't use anymore.

00:46:15.760 --> 00:46:17.920
It's just both blocks, the encoder blocks and the

00:46:17.921 --> 00:46:19.181
decoder blocks are both recurrent.

00:46:20.421 --> 00:46:22.211
And why did they build it like this?

00:46:22.541 --> 00:46:24.551
Right? If you look into the appendix of this paper, they

00:46:24.551 --> 00:46:26.611
actually have a very short proof section.

00:46:26.611 --> 00:46:28.691
That this actually is the only architecture

00:46:28.691 --> 00:46:30.961
that they think that will give

00:46:30.961 --> 00:46:32.921
you a form in their, like,

00:46:33.241 --> 00:46:34.971
like, there's like hand wavy,

00:46:35.541 --> 00:46:37.658
of proving this. This is the only architecture that

00:46:37.659 --> 00:46:39.621
will give you a universal computation.

00:46:40.661 --> 00:46:42.981
Because you can basically write as we come back

00:46:42.981 --> 00:46:44.691
to before, you can't represent

00:46:45.280 --> 00:46:47.321
all simply in this you can't represent

00:46:47.431 --> 00:46:49.311
all symbols. You have a fixed number of computational

00:46:49.451 --> 00:46:51.481
steps. If you have a model where you

00:46:51.481 --> 00:46:53.261
can just put in more computational steps,

00:46:53.661 --> 00:46:55.781
by which the human depth then you can

00:46:56.021 --> 00:46:58.131
represent some form of Turing machine that you

00:46:58.131 --> 00:47:00.071
couldn't do finite precision in a transformer.

00:47:00.710 --> 00:47:02.391
So here's sort of an argument

00:47:02.726 --> 00:47:04.911
that you need recurrence to implement

00:47:04.911 --> 00:47:05.491
unit facility.

00:47:07.601 --> 00:47:09.627
And, has

00:47:09.627 --> 00:47:11.771
been made a bit more precise in recent years. For example, there's

00:47:11.771 --> 00:47:13.320
a paper really like from the spring

00:47:13.801 --> 00:47:16.120
called A Little Depth Goes A Long Way, The Power of Love

00:47:16.121 --> 00:47:18.250
Depth Transforms. And what they show in this paper

00:47:18.250 --> 00:47:20.330
is that if you have

00:47:20.331 --> 00:47:22.530
certain forms of complexity classes, going

00:47:22.530 --> 00:47:24.851
to that too much in this talk, but basically transformers

00:47:24.911 --> 00:47:27.411
are in TC zero which is a lower

00:47:27.791 --> 00:47:29.911
complexity class. And if you want to solve harder things,

00:47:29.911 --> 00:47:31.925
you want to solve TC one, is for example,

00:47:31.926 --> 00:47:34.300
is regular languages. And, like,

00:47:35.101 --> 00:47:37.221
the can be a regular language is just always like a lot

00:47:37.221 --> 00:47:38.691
of bracket resolving. I think that's a

00:47:39.251 --> 00:47:40.760
reasonable languages are.

00:47:42.151 --> 00:47:44.281
Wanna do a lot of packet resolving, you can

00:47:44.281 --> 00:47:46.381
train a very wide model which I

00:47:46.601 --> 00:47:48.620
have here in so you

00:47:48.621 --> 00:47:50.801
have a very wide and very fixed step deep

00:47:50.861 --> 00:47:52.891
model. Which I have in blue here. Your scaling

00:47:52.891 --> 00:47:55.060
is pretty bad. You have to make a very deep model

00:47:55.780 --> 00:47:58.081
to have the model learn even a certain

00:47:58.561 --> 00:48:00.749
small subset of vocal languages. And you can

00:48:00.750 --> 00:48:02.780
also use chain verbalize general font And

00:48:02.780 --> 00:48:04.651
with verbalize chain of thought, you have a linear scaling

00:48:05.211 --> 00:48:07.301
But if you scale

00:48:07.361 --> 00:48:09.407
in-depth, you actually have an architecture

00:48:09.407 --> 00:48:11.280
where it's only logarithmic. So you

00:48:11.441 --> 00:48:13.330
only need a little amount of extra layers

00:48:13.490 --> 00:48:14.730
have a much broader

00:48:16.081 --> 00:48:17.941
complexity class that you can use with your model.

00:48:18.741 --> 00:48:20.771
So this is so these papers are

00:48:20.771 --> 00:48:22.471
both arguments towards, from

00:48:22.841 --> 00:48:24.871
the computation from universality to for why

00:48:24.871 --> 00:48:26.011
you would want to use recurrence.

00:48:28.141 --> 00:48:30.301
But maybe the part three that I like also a lot

00:48:30.301 --> 00:48:32.499
is part based on deep thinking

00:48:32.500 --> 00:48:34.201
literature. And what this means is,

00:48:35.331 --> 00:48:37.441
it's really a question of, like, if we

00:48:37.601 --> 00:48:39.221
so we often we wanna learn patterns.

00:48:39.621 --> 00:48:42.121
Like, a lot of custom machine learning is about learning patterns, learning associations.

00:48:42.741 --> 00:48:44.870
But what if we really wanna learn algorithms?

00:48:45.246 --> 00:48:47.301
And more explicitly, what if

00:48:47.301 --> 00:48:49.521
we really want to learn iterative algorithms?

00:48:49.741 --> 00:48:51.821
And we wanna make it really make it very hard for the

00:48:51.821 --> 00:48:53.701
model to memorize? And,

00:48:53.941 --> 00:48:56.101
there's been a lot of great work doing this on, like, smaller

00:48:56.101 --> 00:48:58.571
setups. Example, one that I've even illuminating

00:48:58.711 --> 00:49:01.041
for this is this example of maze finding.

00:49:01.361 --> 00:49:03.561
So these are all like tiny little incidents. They're

00:49:03.561 --> 00:49:05.821
recurrent. And these models

00:49:05.821 --> 00:49:07.901
are trained to just solve this maze

00:49:07.901 --> 00:49:08.570
here. Right? They

00:49:09.931 --> 00:49:11.761
Get the maze definition in the middle.

00:49:12.001 --> 00:49:13.601
And they have to produce a path in the bottom.

00:49:14.240 --> 00:49:16.601
It's a very toy task. Right? And

00:49:17.571 --> 00:49:19.651
then, people in this paper here, they train these maze

00:49:19.651 --> 00:49:21.961
models on mazes of any sizes

00:49:21.961 --> 00:49:24.111
between like nine and thirteen pixels, right, so like

00:49:24.270 --> 00:49:25.930
kind of the size that's here on the

00:49:27.331 --> 00:49:29.751
on the slides. Right? These are tiny mazes.

00:49:29.941 --> 00:49:32.101
And it's just a recurrent model. It trains a lot in these

00:49:32.101 --> 00:49:34.108
mazes. But why do they do that? Right? They

00:49:34.108 --> 00:49:36.041
do that because they want the model to not just

00:49:36.681 --> 00:49:38.971
pattern match to how a maze social looks like,

00:49:39.211 --> 00:49:41.531
but they want the model to learn the algorithm that solves

00:49:41.531 --> 00:49:43.800
MACE finding. So they find in these papers

00:49:43.801 --> 00:49:46.271
is that, if these models are recurrent,

00:49:47.411 --> 00:49:49.221
then the model actually can learn the algorithm.

00:49:49.781 --> 00:49:52.061
And then after the model has learned the algorithm,

00:49:52.220 --> 00:49:54.460
at test time, it can be deployed to solve harder problems

00:49:54.460 --> 00:49:56.541
that it saw during pretraining. Or I guess

00:49:56.861 --> 00:49:58.980
back then training. Right?

00:49:58.981 --> 00:50:01.121
Like, this is a much longer maze. But because

00:50:01.121 --> 00:50:03.351
the model just has learned the algorithm, here,

00:50:03.351 --> 00:50:05.101
because it has learned paralyzed back end filling,

00:50:05.421 --> 00:50:07.070
The model can just solve any

00:50:07.551 --> 00:50:09.749
maze of an arbitrary hardness here, where

00:50:09.750 --> 00:50:12.010
hardness here now we define this by size. Let's go

00:50:12.010 --> 00:50:14.090
even further. Right? Like, there's a an appendix in

00:50:14.091 --> 00:50:16.220
the paper where it's an massive maze of, like,

00:50:16.221 --> 00:50:17.641
a thousand by a thousand pixels.

00:50:18.481 --> 00:50:20.741
And I think there's a big promise here from

00:50:20.751 --> 00:50:22.190
depth occurrence that

00:50:22.991 --> 00:50:25.091
maybe this is a way that we can really learn algorithms

00:50:25.601 --> 00:50:27.691
And if we can get our models to learn

00:50:27.691 --> 00:50:29.711
algorithms, then we can get them to

00:50:29.711 --> 00:50:31.986
learn solutions and generalizable not

00:50:31.986 --> 00:50:34.141
just between different domains,

00:50:34.381 --> 00:50:36.441
also between different hardness levels. Because

00:50:36.441 --> 00:50:38.530
the model can run can learn the algorithm on easy problems

00:50:38.530 --> 00:50:40.720
and ideally just execute

00:50:40.721 --> 00:50:42.761
the same algorithm on a much harder

00:50:42.761 --> 00:50:44.908
problem. And like just in the same way that maybe

00:50:44.908 --> 00:50:46.531
all of us We've learned addition.

00:50:47.171 --> 00:50:49.441
And we haven't learned addition for, like, let's

00:50:49.441 --> 00:50:51.530
say, like, 2,000 digit numbers. But all

00:50:51.530 --> 00:50:53.611
of you could sit down with a pen and paper in enough

00:50:53.611 --> 00:50:55.941
time and I could motivate you to do a 2,000

00:50:55.941 --> 00:50:58.261
digit addition. Because you know the algorithm for addition,

00:50:58.901 --> 00:51:01.041
you can just slowly execute it. And I think that's very

00:51:01.041 --> 00:51:02.981
promising that we're gonna want is to do something

00:51:03.121 --> 00:51:05.191
like that. So how do

00:51:05.191 --> 00:51:07.591
we do that? And what's one concrete approach towards

00:51:07.591 --> 00:51:09.520
that? I And,

00:51:10.001 --> 00:51:11.441
let me take a sip sip of a slide.

00:51:12.171 --> 00:51:14.271
So what's one approach towards that? So one

00:51:14.271 --> 00:51:16.351
approach that we talked about yesterday in the post session a

00:51:16.351 --> 00:51:18.361
bit over the year, so that's been a lot

00:51:18.361 --> 00:51:20.181
of work on this for many other great people,

00:51:21.701 --> 00:51:24.030
is to use these sort of recurrent transformers,

00:51:24.030 --> 00:51:26.151
these universal transformers, and to, like, bring them into

00:51:26.151 --> 00:51:28.091
modern setups train them with modern data,

00:51:28.171 --> 00:51:30.310
see what they would do. And there's a few

00:51:30.311 --> 00:51:32.131
things that you can build into these that

00:51:32.451 --> 00:51:34.570
like an upgrade from classical universal thermostats

00:51:34.571 --> 00:51:36.651
that make us more stable because there's been a lot of great work on

00:51:36.651 --> 00:51:38.699
the small scale models in the last five

00:51:38.700 --> 00:51:41.020
years and what you would need to make this recurrence

00:51:41.020 --> 00:51:43.071
more stable. There's a few

00:51:43.071 --> 00:51:45.191
interesting things here. So, in

00:51:45.191 --> 00:51:47.581
this particular architecture, have three kinds

00:51:47.581 --> 00:51:49.660
of blocks. And each of these blocks is

00:51:49.661 --> 00:51:51.801
made of a number of layers. And that's

00:51:51.801 --> 00:51:54.271
interesting because, so when you in the original

00:51:54.411 --> 00:51:56.471
Universal Thomas, you just had one layer that you've

00:51:56.471 --> 00:51:58.490
been recurring. But, we find that you often

00:51:58.491 --> 00:52:00.771
you want to learn a more complex nonlinear

00:52:01.311 --> 00:52:03.401
operator. And you really want to have depth in your

00:52:03.401 --> 00:52:05.431
model. So here, actually,

00:52:05.431 --> 00:52:07.861
these all of these recurrent blocks themselves

00:52:07.921 --> 00:52:09.990
are deeper. They are, like, here are four layers. They can

00:52:09.991 --> 00:52:11.671
learn them more complicated from the operator.

00:52:13.271 --> 00:52:15.371
And then what we also have here is that

00:52:15.931 --> 00:52:18.010
classically in something like universal, almost you have

00:52:18.010 --> 00:52:19.970
a sort of forgetting problem. Where,

00:52:20.681 --> 00:52:22.581
if the model recurs for a long amount of time,

00:52:23.621 --> 00:52:25.701
then input data is very far away from the

00:52:25.701 --> 00:52:27.759
end. Right? Because you injected the input data

00:52:27.759 --> 00:52:29.841
only at the beginning, of the recurrence. And you

00:52:29.841 --> 00:52:31.911
run the recurrence for a long time, And if you think about

00:52:31.911 --> 00:52:34.010
this like a dynamical system, then this is a bit

00:52:34.010 --> 00:52:36.071
complicated because you feed in

00:52:36.071 --> 00:52:38.471
the input data only at the beginning or so

00:52:38.471 --> 00:52:40.571
only at the boundary of them. Of the model. And

00:52:40.571 --> 00:52:42.590
what's much more stable actually it's not an

00:52:42.591 --> 00:52:44.811
accident, but this looks much more like gradient descent

00:52:44.811 --> 00:52:46.511
or it looks much more like a diffusion model,

00:52:47.141 --> 00:52:49.210
that you have a sort of import injection where in

00:52:49.210 --> 00:52:51.290
every step of this recurrence, so we have this recurrent

00:52:51.290 --> 00:52:53.371
block of green, right, which you repeat, every

00:52:53.371 --> 00:52:55.661
step of the recurrence, you condition

00:52:55.961 --> 00:52:57.260
on the input embeddings.

00:52:58.030 --> 00:52:59.900
And the second part of this thing here is that

00:53:00.540 --> 00:53:02.531
there's there's an intralation of the state here.

00:53:02.691 --> 00:53:05.021
Right, so there's a state variable. It's called s. And

00:53:05.131 --> 00:53:07.271
we keep on refining the state variable

00:53:07.271 --> 00:53:09.331
by running this recurrence. And it makes a

00:53:09.331 --> 00:53:11.411
lot of sense based on prior literature use

00:53:11.411 --> 00:53:13.451
actually a random initialization for the state variable.

00:53:14.091 --> 00:53:16.251
This means that we train a model that from arbitrary

00:53:16.251 --> 00:53:18.400
initial states, it just learns to refine

00:53:18.401 --> 00:53:20.500
these states. And it's a very desirable property

00:53:20.500 --> 00:53:22.581
because we want the model to learn iterative

00:53:22.641 --> 00:53:24.531
refinement. Right? We want to have a prior towards iterative

00:53:25.571 --> 00:53:27.711
aspects. And

00:53:27.851 --> 00:53:30.040
then there's a number

00:53:30.040 --> 00:53:32.161
of ways to train these. We found to be really

00:53:32.161 --> 00:53:34.331
scalable is to just train them with the simplest thing, which is

00:53:34.331 --> 00:53:36.651
just to have migration end

00:53:36.651 --> 00:53:38.741
to end And here, this is actually a truncated back

00:53:38.741 --> 00:53:40.871
propagation where you take

00:53:40.871 --> 00:53:43.101
your model doing training every time

00:53:43.101 --> 00:53:45.120
you see data, you just randomize the number of

00:53:46.101 --> 00:53:48.111
you'll do. And then just back up.

00:53:48.111 --> 00:53:50.051
In this way, the model sort of has to handle

00:53:50.740 --> 00:53:53.091
all recurrences between

00:53:53.091 --> 00:53:55.490
here and 200. And

00:53:55.491 --> 00:53:57.811
we train a model that does not that doesn't just work

00:53:57.811 --> 00:53:59.901
for if you repeat it four times or repeat it

00:53:59.901 --> 00:54:02.341
16 times. But it works for any number of occurrences.

00:54:02.481 --> 00:54:04.701
Is important for efficiency. There's

00:54:04.701 --> 00:54:07.201
a number of different alternatives that you could do here

00:54:07.321 --> 00:54:09.452
where there's a lot of work in this field and there's a lot of different

00:54:09.452 --> 00:54:11.530
models and they're differ a lot on based on what

00:54:11.530 --> 00:54:13.540
objective they use. To

00:54:13.540 --> 00:54:15.801
train these models. And you can ask

00:54:15.801 --> 00:54:16.631
me all that later.

00:54:17.911 --> 00:54:19.991
And, does this actually work? Am I wasting your time

00:54:19.991 --> 00:54:22.221
with talking about all of this stuff? Interestingly,

00:54:22.361 --> 00:54:24.161
this does work. And,

00:54:24.970 --> 00:54:26.750
this leads you to models that have

00:54:28.061 --> 00:54:30.546
aside from their actual performance,

00:54:30.681 --> 00:54:32.770
have interesting properties. So what these

00:54:32.770 --> 00:54:34.931
models learn, if you just train them just at large

00:54:34.931 --> 00:54:36.391
scales of like a few billion parameters,

00:54:36.941 --> 00:54:39.051
maybe a trillion tokens just on web text. Is

00:54:39.051 --> 00:54:41.131
that they learn different behaviors on different kinds of

00:54:41.131 --> 00:54:43.381
data. But if you remember during training,

00:54:43.601 --> 00:54:44.901
the model just saw

00:54:46.291 --> 00:54:48.391
randomized occurrences. So the model never

00:54:48.391 --> 00:54:50.470
had any the model didn't we didn't do

00:54:50.471 --> 00:54:52.541
more computers during training on harder problems.

00:54:53.341 --> 00:54:55.221
We just did random compute everywhere.

00:54:55.461 --> 00:54:57.470
But the test server we see is

00:54:57.470 --> 00:54:59.551
that the model actually converges quicker on easier

00:54:59.551 --> 00:55:01.461
problems. Like, and especially on problems like,

00:55:02.351 --> 00:55:04.511
like knowledge based problems here, like ARC challenge as

00:55:04.511 --> 00:55:06.551
a knowledge base base benchmark.

00:55:07.131 --> 00:55:09.391
These benchmarks are models of, like, saturates

00:55:09.391 --> 00:55:11.480
quickly, either the model knows the effect or the

00:55:11.481 --> 00:55:13.641
model does not know the effect. And then more computation doesn't help

00:55:13.641 --> 00:55:14.861
you to not more affect more.

00:55:16.141 --> 00:55:18.281
But on other tasks like here on GSM at K,

00:55:19.081 --> 00:55:21.341
the model actually can

00:55:21.401 --> 00:55:23.801
use and can leverage an iterative

00:55:23.861 --> 00:55:25.941
algorithm to solve the task. And we see that it doesn't

00:55:25.941 --> 00:55:27.971
learn so just from data. Which I think is pretty

00:55:27.971 --> 00:55:30.101
interesting. Alright. Here's a few more

00:55:30.101 --> 00:55:32.261
examples where, like, all the interests

00:55:32.261 --> 00:55:34.341
in the top really are a bit more knowledge heavy where

00:55:34.341 --> 00:55:36.691
the model just needs a few recurrences

00:55:36.691 --> 00:55:39.011
because either it knows how to solve the benchmark question,

00:55:39.011 --> 00:55:40.911
like from SEQ to the science fact. Question,

00:55:41.071 --> 00:55:43.430
or it doesn't know the question. But on benchmarks

00:55:43.431 --> 00:55:44.971
where the model can leverage computation,

00:55:45.460 --> 00:55:47.601
it seems to have learned to do so. It's pretty interesting.

00:55:48.961 --> 00:55:51.241
And especially if you compare this to like a

00:55:51.561 --> 00:55:53.796
baseline trained with the same amount of parameters, the same

00:55:53.796 --> 00:55:55.991
set up, same data, then these two models

00:55:56.131 --> 00:55:58.241
are often similar. It's a knowledge heavy

00:55:58.241 --> 00:56:00.251
benchmark. But on task like g s m eight

00:56:00.251 --> 00:56:02.341
k, quorum model is five times better than the baseline.

00:56:04.581 --> 00:56:06.821
And what is it doing? If you look at this,

00:56:07.220 --> 00:56:09.431
we see that the model really is learning

00:56:09.431 --> 00:56:11.470
some sort of recurrence. That depends

00:56:11.471 --> 00:56:13.761
on the number of on how hard the token is.

00:56:14.241 --> 00:56:16.661
So here, what you see is that I put down a sentence.

00:56:17.341 --> 00:56:19.421
And I show you the convergence of

00:56:19.421 --> 00:56:21.611
this latent state. So I show you

00:56:21.611 --> 00:56:23.361
like how quickly these latent states

00:56:24.481 --> 00:56:26.592
stop moving because we have a recurrence. Right? So

00:56:26.592 --> 00:56:28.830
we have an initial latent state. Random, and then we run

00:56:28.831 --> 00:56:30.961
the recurrence. And we run the recurrence,

00:56:31.321 --> 00:56:33.421
And then some point, this model

00:56:33.561 --> 00:56:35.701
may converge. And this is interestingly

00:56:36.661 --> 00:56:38.911
very token dependent because the And this is surprising

00:56:38.911 --> 00:56:41.071
because we didn't train for

00:56:41.071 --> 00:56:43.191
this behavior. Right? We just training, we just

00:56:43.191 --> 00:56:45.270
took a batch of data and we

00:56:45.270 --> 00:56:47.460
said, okay. Do 17 recurrence

00:56:47.461 --> 00:56:49.700
on this whole batch. But now at test

00:56:49.701 --> 00:56:51.260
time, we see actually the model has learned

00:56:51.741 --> 00:56:53.380
just from scale and just from training it.

00:56:53.831 --> 00:56:55.901
That on easier tokens, it can

00:56:55.901 --> 00:56:58.301
converge quicker. And on harder

00:56:58.301 --> 00:57:00.190
tokens, it can converge slower.

00:57:00.991 --> 00:57:03.411
But here, for example, you see especially that for the colon,

00:57:04.071 --> 00:57:06.411
model converges much slower because often, like, colons

00:57:06.471 --> 00:57:08.530
or commas are occasions where you have

00:57:08.530 --> 00:57:10.666
to figure out really what comes next. You have to plan a bit

00:57:10.666 --> 00:57:12.759
ahead. Versus, if you write the first word

00:57:12.760 --> 00:57:14.771
of methodical, kind of obvious that

00:57:14.771 --> 00:57:16.891
you wanna finish the word methodical, so you don't need a lot

00:57:16.891 --> 00:57:17.671
of computer to finish that.

00:57:19.271 --> 00:57:21.591
Right. And these kind of like graphs to me like show

00:57:21.591 --> 00:57:23.601
love promise of recurrence and,

00:57:23.601 --> 00:57:25.681
especially, also, promise of activity and

00:57:25.681 --> 00:57:27.521
compute. Right? Because it's still kind of crazy that

00:57:27.841 --> 00:57:30.091
it's the year 2025. We have

00:57:30.091 --> 00:57:32.250
these super large models of, like, even the smallest

00:57:32.251 --> 00:57:34.801
ones that you probably are using is, like, are 7,000,000,000

00:57:34.801 --> 00:57:37.031
parameters or something, 8,000,000,000 parameters. On

00:57:37.031 --> 00:57:39.111
every token, the model produces it

00:57:39.111 --> 00:57:41.121
runs all APLIM parameters. No

00:57:41.181 --> 00:57:42.870
matter if the token is the answer to the

00:57:43.431 --> 00:57:45.451
real hypothesis or not. Or the

00:57:45.451 --> 00:57:47.601
token is just you had

00:57:47.601 --> 00:57:49.760
for dinner today and how are you doing, and thank you for your

00:57:49.760 --> 00:57:52.151
answer. And yes, you're absolutely right. The model

00:57:52.151 --> 00:57:54.321
uses the same amount of compute all these tokens.

00:57:54.960 --> 00:57:56.991
And maybe graphs like this indicate that this

00:57:56.991 --> 00:57:58.171
maybe is not necessary.

00:58:00.011 --> 00:58:02.151
And you see more of a slide of how

00:58:02.151 --> 00:58:04.321
these trajectories work. We're doing this interesting

00:58:04.321 --> 00:58:06.211
because, this is sort of like a top down view.

00:58:06.611 --> 00:58:08.690
Because you have the you have PCA

00:58:08.750 --> 00:58:10.921
interactions of the latent state on

00:58:10.921 --> 00:58:13.071
the x and y dimension. And you have

00:58:13.071 --> 00:58:15.121
the sequence now on the z direction.

00:58:15.121 --> 00:58:17.370
So, we go from the bottom up

00:58:17.611 --> 00:58:19.691
This is the sequence. And what you see here

00:58:19.691 --> 00:58:21.891
is that these, all these latent

00:58:21.891 --> 00:58:23.971
states, because you do the recurrence by every

00:58:23.971 --> 00:58:26.321
token, all these latent

00:58:26.321 --> 00:58:28.151
states, they converge to different points

00:58:28.391 --> 00:58:30.551
but they because this is a still a transformer, right,

00:58:30.551 --> 00:58:32.690
and they attend to each other, They form these

00:58:32.690 --> 00:58:34.911
structures as you go along

00:58:34.911 --> 00:58:35.680
the sequence.

00:58:38.081 --> 00:58:39.991
And maybe we can zoom in in

00:58:40.151 --> 00:58:42.450
few of these. Because what is interesting

00:58:42.451 --> 00:58:43.411
here is that this model,

00:58:44.720 --> 00:58:47.060
is not trained to always be

00:58:47.061 --> 00:58:49.200
convergent. Right? There's some some earlier work on

00:58:49.201 --> 00:58:51.280
equilibrium models or on fixed point models where we

00:58:51.280 --> 00:58:53.651
really want our recurrent models to always

00:58:53.651 --> 00:58:55.801
converge to a fixed point. But

00:58:55.801 --> 00:58:58.141
because this model is just trained at scale with vectoration,

00:58:58.681 --> 00:59:00.779
this is not a necessary requirement. And we

00:59:00.780 --> 00:59:03.061
find that the model often still converges

00:59:03.061 --> 00:59:05.221
to a fixed point. That's a very same implementation that

00:59:05.221 --> 00:59:07.436
you can do in in

00:59:07.861 --> 00:59:09.941
have a dynamical system. Right? So here, we see that for

00:59:09.941 --> 00:59:12.250
some tokens, I'm showing you the first six BCAA directions.

00:59:12.731 --> 00:59:14.971
And you see how this model just converges to a fixed

00:59:14.971 --> 00:59:17.000
point. What I'm showing here really is that these

00:59:17.000 --> 00:59:19.250
are the different states that the model

00:59:19.271 --> 00:59:21.451
runs through. As it answers what should

00:59:21.451 --> 00:59:23.631
come next for this token. So the darkest

00:59:23.631 --> 00:59:25.621
color is the first state

00:59:25.901 --> 00:59:27.771
and the brightest color is the last state.

00:59:28.331 --> 00:59:29.900
And these all just converge to the middle.

00:59:30.951 --> 00:59:32.891
Interestingly, on a number of

00:59:33.770 --> 00:59:35.800
hard tokens, number of fin math and

00:59:35.801 --> 00:59:37.820
in reasoning, The model learns

00:59:37.821 --> 00:59:39.921
all kinds of dynamical behaviors. Where

00:59:39.921 --> 00:59:42.181
we also see that for example here on

00:59:42.321 --> 00:59:44.421
this token, really is number three. The

00:59:44.421 --> 00:59:46.500
model has learned to find this orbit, and it

00:59:46.501 --> 00:59:48.691
traces out this orbit in

00:59:48.691 --> 00:59:50.790
latent space. And, we're

00:59:50.791 --> 00:59:52.811
still not entirely sure how to mix in

00:59:52.811 --> 00:59:54.931
these orbits. But, there seems to

00:59:54.931 --> 00:59:57.051
be a way to implement, for

00:59:57.051 --> 00:59:59.010
example, like, some some modular arithmetic

00:59:59.661 --> 01:00:01.691
or some functions where you have a set of of these

01:00:01.691 --> 01:00:03.931
interlocking orbits. Way

01:00:04.151 --> 01:00:06.421
you can implement more complicated

01:00:06.721 --> 01:00:08.420
functions in

01:00:08.821 --> 01:00:10.520
latent space, right, in your general system.

01:00:11.241 --> 01:00:13.320
And I think the neuroscience people are pretty excited

01:00:13.321 --> 01:00:15.220
about this. Because we have a sort of

01:00:15.331 --> 01:00:17.391
you know, I have a frequency. But I think for

01:00:17.391 --> 01:00:19.421
us, it's just interesting that the model does the

01:00:19.421 --> 01:00:21.431
model just learns from data, and appropriate

01:00:21.571 --> 01:00:23.241
behavior for this dynamical system.

01:00:23.721 --> 01:00:25.591
Of the recurrence. Its state

01:00:25.841 --> 01:00:27.731
states. So

01:00:27.891 --> 01:00:29.971
also other tests to this stuff like slider, where the

01:00:29.971 --> 01:00:32.351
model is learned to just like slowly slide into one direction.

01:00:33.071 --> 01:00:35.288
Which we think often as relates to counting. Operations

01:00:35.288 --> 01:00:36.061
on the model.

01:00:38.301 --> 01:00:40.489
What I take with some of his pleasures It's interesting

01:00:40.490 --> 01:00:42.270
there's like so much complexity that we just

01:00:42.511 --> 01:00:44.771
can learn in these recurrent models. Just

01:00:44.801 --> 01:00:46.520
from pretraining. And we have these different

01:00:47.001 --> 01:00:48.861
terminal behaviors that emerge just from scale.

01:00:49.931 --> 01:00:52.191
It's it's hard to analyze these and we require

01:00:52.331 --> 01:00:54.621
more tools from representation analysis,

01:00:54.621 --> 01:00:56.801
more probing, really understand what's going on

01:00:56.801 --> 01:00:59.091
here. Because the model can collect more functions in

01:00:59.711 --> 01:01:01.730
one part of this recurrence. And

01:01:01.730 --> 01:01:03.421
it's a bit more challenging to analyze.

01:01:04.990 --> 01:01:07.301
But I also

01:01:07.361 --> 01:01:09.371
want to talk to you about here in this workshop is

01:01:10.411 --> 01:01:12.701
you aside from all this about recurrence,

01:01:13.021 --> 01:01:14.801
what are the benefits for inference?

01:01:15.251 --> 01:01:17.191
And why is recurrence very efficient?

01:01:17.710 --> 01:01:19.790
So I'm gonna give you a few examples for why I think it's actually a

01:01:19.790 --> 01:01:22.180
surprisingly efficient architecture. You can do interesting

01:01:22.181 --> 01:01:23.641
things once you have this recurrence.

01:01:24.791 --> 01:01:26.850
So one thing that's very straightforward is you

01:01:26.851 --> 01:01:28.560
can just do self parallel decoding.

01:01:29.121 --> 01:01:31.279
Classically, in speculative coding, you have a draft model.

01:01:31.280 --> 01:01:33.690
You have to load the draft model. It's a different

01:01:33.691 --> 01:01:35.901
model. You maybe have has a have to hope it has a

01:01:35.901 --> 01:01:37.661
SIM tokenizer. Have to do all this

01:01:38.271 --> 01:01:40.351
engineering to have the draft model make

01:01:40.351 --> 01:01:42.451
sense and speculate the next four eight sixteen tokens.

01:01:42.451 --> 01:01:44.531
Right? You have this recurrent depth model,

01:01:44.531 --> 01:01:46.611
you can just use itself as speculator. Because you

01:01:46.611 --> 01:01:48.770
can just you can just run the model with only a few

01:01:48.770 --> 01:01:50.920
number of iterations to speculate. And then you

01:01:50.921 --> 01:01:53.160
verify with many iterations. And you can even

01:01:53.161 --> 01:01:55.321
reuse the same computation that you did already just to continue

01:01:55.321 --> 01:01:57.341
running on them. Very simple.

01:01:58.091 --> 01:02:00.191
What's also interesting is that, if

01:02:00.191 --> 01:02:02.431
you think about how you do inference with these kind

01:02:02.431 --> 01:02:04.460
of recurrent depth transformers, and if you

01:02:04.460 --> 01:02:06.495
do inference naively, you'll find that

01:02:06.495 --> 01:02:08.501
the k v state you have a k v state

01:02:08.501 --> 01:02:10.741
every time you run the recurrence, That the k v state

01:02:10.741 --> 01:02:12.791
grows with number of recurrences. And

01:02:12.791 --> 01:02:14.871
that sounds kinda bad because then, that means if we do a lot

01:02:14.871 --> 01:02:16.760
of compute, we have to

01:02:16.901 --> 01:02:18.941
also allocate a lot of memory to store

01:02:18.941 --> 01:02:21.000
all our kv states. And

01:02:21.001 --> 01:02:23.080
what we find in the paper is actually that's just

01:02:23.081 --> 01:02:25.051
not necessary. What we found in the paper is actually that's just not necessary. Because all the recurrence

01:02:26.411 --> 01:02:28.571
are in the same space. Right? It's just the same recurrent block

01:02:28.571 --> 01:02:30.721
that's repeated. That means that all the

01:02:30.721 --> 01:02:32.841
k v states match to each other. And what we can

01:02:32.841 --> 01:02:35.010
just do is we can just throw away

01:02:35.011 --> 01:02:36.571
all the early k v states.

01:02:37.371 --> 01:02:39.480
And for every token in the sequence, we

01:02:39.481 --> 01:02:41.671
just store the last k v state. No

01:02:41.671 --> 01:02:43.731
matter whether this was recurrence four

01:02:43.951 --> 01:02:46.030
or recurrence 137, store the

01:02:46.030 --> 01:02:48.111
last k v state. And then once

01:02:48.111 --> 01:02:50.230
we run that for occurrence again on a on a new token,

01:02:50.791 --> 01:02:52.951
we still attend to all prior k v states no matter

01:02:52.951 --> 01:02:55.251
what their position was. So we always just

01:02:55.391 --> 01:02:56.611
default to

01:02:57.731 --> 01:02:59.011
we always default to,

01:03:00.671 --> 01:03:02.691
attending to the most computed state.

01:03:03.021 --> 01:03:05.121
And it's pretty interesting that it works. Even

01:03:05.121 --> 01:03:07.041
works a bit better if you have only a few occurrences.

01:03:09.441 --> 01:03:11.520
And I've shown you this graph before. Here's another one

01:03:11.521 --> 01:03:13.841
of them. Just repeating this. What this

01:03:13.841 --> 01:03:16.061
graph also implies is that we can just

01:03:16.061 --> 01:03:18.301
use, really, we can we can use per token

01:03:18.301 --> 01:03:20.310
exits. Right? We can just look at

01:03:20.311 --> 01:03:22.411
this and we can see when our recurrence converges.

01:03:22.871 --> 01:03:24.951
And once it converges, or it stops changing in some

01:03:24.951 --> 01:03:26.781
way that we have defined. We exit.

01:03:27.021 --> 01:03:29.101
And in this way, we can we can have different compute

01:03:29.101 --> 01:03:31.341
per token. Which leads us to more

01:03:31.481 --> 01:03:33.581
efficient inference. Now,

01:03:33.821 --> 01:03:36.070
some of you can see this here. This is some are MLM

01:03:36.071 --> 01:03:38.261
new categories. In this

01:03:38.261 --> 01:03:40.421
example, you see that on some MLM categories, like for

01:03:40.421 --> 01:03:42.551
example here on on easier

01:03:42.551 --> 01:03:44.871
math, you see that the model uses maybe more

01:03:44.871 --> 01:03:46.961
like a median of eight or

01:03:46.961 --> 01:03:49.151
nine occurrences. And if it's a harder problem,

01:03:49.151 --> 01:03:51.101
maybe it's like it's logical fallacies and the

01:03:51.661 --> 01:03:53.861
middle right. The model even uses a middle

01:03:53.861 --> 01:03:56.061
median of like about 20 recurrences.

01:03:57.740 --> 01:03:59.840
To hit the same convergence threshold.

01:04:03.831 --> 01:04:06.101
Also, what what was also interesting about been up to

01:04:06.101 --> 01:04:08.491
very recently with a bunch of collaborators, so also in the audience,

01:04:08.761 --> 01:04:10.881
is that, we classically,

01:04:10.881 --> 01:04:13.356
recurrence also is very the recurrence

01:04:13.361 --> 01:04:15.630
the model is, of course, very sequential,

01:04:15.631 --> 01:04:17.660
so it's also very slow. So

01:04:17.661 --> 01:04:20.060
what's pretty interesting is that,

01:04:21.290 --> 01:04:23.450
classically, you run inference. Right? So now I've I'm

01:04:23.450 --> 01:04:25.540
always gonna be drawing these two d graphs. You

01:04:25.540 --> 01:04:27.621
have the token positions, we have the sequence on the

01:04:27.621 --> 01:04:29.711
x axis. And you have the recurrence

01:04:30.901 --> 01:04:32.281
on the y axis. Right? And classically,

01:04:33.711 --> 01:04:35.141
pointer. Don't know a pointer.

01:04:35.911 --> 01:04:37.861
Like classically, if you just

01:04:38.341 --> 01:04:40.501
inference model, you would first be talking

01:04:40.501 --> 01:04:42.651
about this one. And then you would run the recurrence

01:04:42.711 --> 01:04:44.971
one here in this example. One, two, three, four, five, six, seven.

01:04:45.531 --> 01:04:47.740
Six a six times. And then you go to

01:04:47.740 --> 01:04:49.211
the next token, you're under recurrence.

01:04:49.861 --> 01:04:52.200
And this is where you're talking at with very flop

01:04:52.341 --> 01:04:54.591
efficient. But you can always see this is not very palisable.

01:04:54.591 --> 01:04:56.771
And it's very slow to run inference at this scale.

01:04:56.931 --> 01:04:58.871
Especially if we have very fast

01:04:59.010 --> 01:05:00.300
GPUs. Just very slow setups.

01:05:01.341 --> 01:05:03.481
So there's a lot of exciting literature in the

01:05:03.481 --> 01:05:05.751
fusion models. And there's some interesting connections

01:05:05.751 --> 01:05:07.601
between these recurrent models and diffusion models.

01:05:07.841 --> 01:05:09.861
That we can just take samples from

01:05:09.861 --> 01:05:11.801
diffusion and apply similar principles here

01:05:12.280 --> 01:05:14.301
And in diffusion, this is called a diffusion

01:05:14.361 --> 01:05:16.451
forcing sampler. But what it just means is just

01:05:16.451 --> 01:05:18.411
entirely encapsulated in this figure.

01:05:18.651 --> 01:05:20.731
Because what this means is that instead of normally, they're just

01:05:20.731 --> 01:05:22.801
turning the currents like a a

01:05:23.020 --> 01:05:25.180
like a u shape, you know, we go we have to, like, always

01:05:25.181 --> 01:05:26.930
run the whole recurrent before we do the next token.

01:05:27.811 --> 01:05:29.851
Here, we just immediately decode the

01:05:29.851 --> 01:05:31.861
next word. After we do one recurrent step,

01:05:31.861 --> 01:05:33.940
we decode the next word. And now we have two

01:05:33.941 --> 01:05:36.061
states that are uncapped data like. Semi

01:05:36.061 --> 01:05:38.371
finished So we run the records on both states.

01:05:38.530 --> 01:05:40.581
And then we get the third one. We run it

01:05:40.581 --> 01:05:42.781
again. So we get a fourth state. Right? And

01:05:42.781 --> 01:05:44.931
so we we run this wave over the computation. Also,

01:05:44.931 --> 01:05:46.341
I have another picture here.

01:05:48.020 --> 01:05:50.280
What this picture again shows you is that there's the sequence.

01:05:50.481 --> 01:05:52.211
On the x axis.

01:05:52.611 --> 01:05:54.751
And there's the compute steps on the y axis.

01:05:55.231 --> 01:05:57.391
And what you see here is there's almost like a wave of quotation

01:05:57.391 --> 01:05:59.530
running over the model like a diffusion model. Right? And

01:05:59.530 --> 01:06:01.691
you see that like at some point the tokens

01:06:01.691 --> 01:06:03.481
are frozen. That's why these are like just

01:06:04.041 --> 01:06:06.141
virtual lines and they stop changing. But there's

01:06:06.141 --> 01:06:08.300
a region where they are the model is still

01:06:08.301 --> 01:06:10.411
trying different different tokens and it's it's

01:06:10.411 --> 01:06:12.711
like it's generating from motor positions at once.

01:06:13.351 --> 01:06:15.461
So it's through the recurrence, we

01:06:15.461 --> 01:06:17.540
get a bit of a step we get a few steps away

01:06:17.540 --> 01:06:19.461
from the autorogas if need nature of the model.

01:06:19.801 --> 01:06:21.871
So we can run this sort of, like, parallelized

01:06:21.871 --> 01:06:24.081
inference. And this actually does make the models

01:06:24.081 --> 01:06:26.481
about four times faster. Even on

01:06:26.481 --> 01:06:28.911
domains like GSMED k or math where we really need

01:06:29.391 --> 01:06:31.171
to have the model be sequential sometimes.

01:06:32.351 --> 01:06:34.451
Then we can still use, this parallelization

01:06:34.831 --> 01:06:36.851
on parts where this is just fine. Like,

01:06:36.851 --> 01:06:39.000
we have a sampler where it just

01:06:39.241 --> 01:06:41.311
figured out based on convergence Can

01:06:41.311 --> 01:06:43.391
I advance the sample or not? Actually see this here

01:06:43.391 --> 01:06:45.281
in the in the middle of this

01:06:45.761 --> 01:06:47.921
crazy chart because there there's a part where

01:06:47.921 --> 01:06:49.621
it just it just goes down vertically.

01:06:50.500 --> 01:06:52.547
On the right hand side. Right. Well, that

01:06:52.547 --> 01:06:54.021
that that just means that there's like a

01:06:54.821 --> 01:06:57.080
a certain amount of uncertainty over the sequence.

01:06:57.321 --> 01:06:59.801
And the model is not the sampler is not advancing

01:06:59.801 --> 01:07:02.141
the sequence, not making this this uncertain

01:07:02.601 --> 01:07:04.611
region wider, before it has resolved this

01:07:04.611 --> 01:07:06.681
uncertain part. And after it has

01:07:06.681 --> 01:07:08.881
resolved this uncertain part, again,

01:07:09.021 --> 01:07:11.021
the the sequence can grow again.

01:07:11.101 --> 01:07:13.181
And the model keeps on running generating.

01:07:16.061 --> 01:07:18.281
So in summary, aside from all

01:07:18.281 --> 01:07:20.141
maybe our hopes and maybe interests

01:07:20.761 --> 01:07:22.780
in recurrent models just for

01:07:22.780 --> 01:07:25.111
reasoning, I think it's interesting that they also simplify

01:07:25.171 --> 01:07:27.191
a bunch of things that are kind of tricky in inference.

01:07:27.571 --> 01:07:29.671
Like speculative coding, like QVC sharing,

01:07:30.311 --> 01:07:31.871
adaptive commutation and parallelization.

01:07:33.551 --> 01:07:35.211
Let me do a few summary talk points.

01:07:37.531 --> 01:07:39.521
So why are we interested in recurrent

01:07:39.761 --> 01:07:41.771
and not just do genophage? We can

01:07:41.771 --> 01:07:43.260
of course always do chain of thought.

01:07:43.901 --> 01:07:46.091
As we do normally, as we do right now. It's a it's a pretty

01:07:46.091 --> 01:07:48.171
powerful paradigm and I don't expect it to go away

01:07:48.171 --> 01:07:50.261
at all. Recurrence is also a

01:07:50.261 --> 01:07:52.681
super interesting paradigm to maybe be complementary

01:07:52.821 --> 01:07:55.121
to that. And we just see that

01:07:55.121 --> 01:07:56.841
just scaling it and just applying it with

01:07:57.321 --> 01:07:59.479
based on all literature, it seems to scale surprisingly

01:07:59.480 --> 01:08:01.581
far. There's still a big

01:08:01.581 --> 01:08:03.661
question in this field of how we get from

01:08:03.661 --> 01:08:05.671
these kind of occurrences to something that

01:08:05.671 --> 01:08:07.731
you've seen more for the mazes. Where we can get

01:08:07.731 --> 01:08:09.911
some sort of arbitrary explanation, extrapolation

01:08:09.971 --> 01:08:12.199
compute. Where these

01:08:12.199 --> 01:08:14.268
aren't just just sigmoids, but we can actually

01:08:14.269 --> 01:08:16.080
run the recurrence for longer and longer

01:08:16.401 --> 01:08:18.481
and then continue to improve performance on some

01:08:18.481 --> 01:08:19.191
benchmark task.

01:08:20.631 --> 01:08:22.720
So the question of, like, how would you set up this

01:08:22.721 --> 01:08:24.541
objective? How would you set up the architecture of this model?

01:08:25.021 --> 01:08:27.261
To encourage the sort of extrapolation from easy

01:08:27.261 --> 01:08:27.820
to hard?

01:08:29.580 --> 01:08:31.761
So Also, some fun practical questions on how to

01:08:31.761 --> 01:08:33.811
post train these. And I think this

01:08:33.811 --> 01:08:35.890
is maybe a complementary part of scaling model

01:08:35.890 --> 01:08:38.021
performance and verbal constriction of thought. Right? That

01:08:38.021 --> 01:08:40.360
we don't have to just have two access to scale compute.

01:08:40.761 --> 01:08:43.001
So, model scale, model size and verbal chain

01:08:43.001 --> 01:08:45.131
of thought. Maybe a third axis in which we

01:08:45.131 --> 01:08:47.141
can also scale compute and just have these three

01:08:47.141 --> 01:08:48.131
axes together.

01:08:49.651 --> 01:08:51.811
And with this, I'm done and I think there is time for

01:08:51.811 --> 01:08:54.001
questions maybe. Thank you so much for

01:08:54.001 --> 01:08:54.450
your time.

01:09:06.781 --> 01:09:08.900
Hi. Thank you so much for this

01:09:08.901 --> 01:09:11.401
really super interesting and fascinating presentation.

01:09:11.461 --> 01:09:12.721
I really appreciated it.

01:09:13.921 --> 01:09:15.781
I have two rather short questions.

01:09:16.261 --> 01:09:18.390
The first question is, so you

01:09:18.391 --> 01:09:20.811
talked about these, recurrent

01:09:21.301 --> 01:09:23.361
algorithms. Being able to

01:09:24.160 --> 01:09:25.900
learn some sort of a linear

01:09:30.931 --> 01:09:33.191
repeatedly. And and and now I'm wondering,

01:09:33.991 --> 01:09:36.391
how how important is it that it was

01:09:36.631 --> 01:09:38.571
trained for that particular

01:09:38.711 --> 01:09:40.161
linear update algorithm

01:09:40.801 --> 01:09:42.990
Or or versus versus how could is

01:09:42.991 --> 01:09:45.030
it generalizing these sort of

01:09:46.551 --> 01:09:48.630
algorithms? So say for instance, you have

01:09:48.631 --> 01:09:50.491
your pre your your your training data,

01:09:50.761 --> 01:09:52.330
for this whole, I'm I'm I'm

01:09:53.371 --> 01:09:53.871
architecture.

01:09:55.471 --> 01:09:57.891
Does it then generalize across, like,

01:09:58.551 --> 01:10:00.830
very wide set, of

01:10:00.830 --> 01:10:03.311
of of of of of such linear recurrent

01:10:03.370 --> 01:10:05.491
algorithms. That you

01:10:06.051 --> 01:10:07.961
can see if, I don't know, in benchmarks or so without

01:10:08.281 --> 01:10:10.591
being explicitly trained for this. Like,

01:10:10.651 --> 01:10:12.431
how how how well that does generalize?

01:10:13.701 --> 01:10:15.861
The first question. Yeah. Let me just first because I'll forget the

01:10:15.861 --> 01:10:18.351
other one. Otherwise. So, seems

01:10:18.411 --> 01:10:20.661
pretty generalizable. Also, but technically this is not

01:10:20.661 --> 01:10:22.681
a linear this is not a linear recurrence. So the

01:10:22.681 --> 01:10:24.651
operation is right? Like, if you go back here,

01:10:26.891 --> 01:10:28.431
So, let me go back a bit.

01:10:31.390 --> 01:10:33.660
Right. Because because this all these blocks

01:10:33.661 --> 01:10:35.041
are in nonlinear operations,

01:10:35.721 --> 01:10:37.911
repeating these nonlinear blocks this is

01:10:37.911 --> 01:10:40.091
not a linear recurrence. Right? It's a nonlinear recurrence.

01:10:41.370 --> 01:10:43.071
But, this is just nitpicking. And,

01:10:43.600 --> 01:10:45.381
Vitu, your more harder question? Yep.

01:10:45.681 --> 01:10:47.741
No. No. Yep. No. No. But it's here. These benchmarks, it seems to generalize

01:10:47.741 --> 01:10:49.881
to And like like we For example, we have this like

01:10:50.201 --> 01:10:52.281
this mastermind benchmark, which is just lot it's a

01:10:52.281 --> 01:10:54.371
logic puzzle from the sixties or seventies.

01:10:55.251 --> 01:10:57.401
And the model has never seen that. It seems

01:10:57.401 --> 01:10:59.640
to still use recurrent in a good way for that master mind

01:10:59.640 --> 01:11:01.910
benchmark I find very surprising. Actually. Nice.

01:11:02.210 --> 01:11:04.301
Your second question. Nice. Yeah. And the

01:11:04.301 --> 01:11:06.540
second question is very pragmatic in the sense of

01:11:06.541 --> 01:11:08.761
of let's say, I have an amazing benchmark

01:11:08.761 --> 01:11:10.671
for for exactly this kind of problem.

01:11:10.831 --> 01:11:12.911
And I would want to try how good such a recurrent

01:11:12.911 --> 01:11:14.921
model is working. Are there any kind

01:11:14.921 --> 01:11:17.000
of pretrained models that I could just get

01:11:17.001 --> 01:11:19.151
and then and then have a good idea?

01:11:19.391 --> 01:11:21.711
Do you have to train everything here from scratch, basically,

01:11:21.711 --> 01:11:23.725
if I want to you could just download the model

01:11:23.725 --> 01:11:25.701
I talked about in this talk. Sorry?

01:11:26.421 --> 01:11:28.411
You can just download the model I talked about in this talk.

01:11:28.571 --> 01:11:30.600
Okay. Yeah. Yeah. It also

01:11:30.600 --> 01:11:32.760
has like like you can just use it for inference. It has like

01:11:32.761 --> 01:11:34.220
a bunch of code for these different like

01:11:34.781 --> 01:11:36.870
specific decoding or like kvCash sharing

01:11:36.870 --> 01:11:39.281
in a VLM

01:11:39.501 --> 01:11:41.640
plugin. Should work. Otherwise,

01:11:41.640 --> 01:11:43.671
just reach out. Nice. Thanks.

01:11:43.671 --> 01:11:46.071
Thanks so much. Hi.

01:11:46.071 --> 01:11:48.491
Yeah. I have one question. So

01:11:48.861 --> 01:11:50.901
about skill renewal, you are talking

01:11:51.120 --> 01:11:53.440
about deeps and impact of deeps,

01:11:53.441 --> 01:11:55.871
how how it impact the modeling

01:11:56.091 --> 01:11:58.331
model. I'm wondering, do you observe some

01:11:58.331 --> 01:12:00.440
of the the law of

01:12:00.441 --> 01:12:02.559
scaling for the deeps, like

01:12:02.560 --> 01:12:04.801
how much deep we should skill and

01:12:04.801 --> 01:12:06.920
the lore will this law

01:12:06.921 --> 01:12:09.160
keep going, if we can scale much

01:12:09.160 --> 01:12:11.161
much more deeper, all

01:12:11.221 --> 01:12:12.861
the recently performance can

01:12:13.421 --> 01:12:15.821
be also be improved with the Deepu test

01:12:15.821 --> 01:12:17.591
or it has the end

01:12:17.991 --> 01:12:20.151
that's killing the escalating are going to end.

01:12:20.151 --> 01:12:22.471
So I as the deep for

01:12:22.611 --> 01:12:24.770
the deep scaling for real time model also

01:12:24.771 --> 01:12:27.081
going to end. Yeah. That's my question.

01:12:27.301 --> 01:12:29.620
Yeah. Yeah. There there some papers where,

01:12:30.881 --> 01:12:32.961
the people treasurer drive adaptive skilling off of big

01:12:32.961 --> 01:12:35.080
steps and trauma. But it's pretty high rate because

01:12:35.080 --> 01:12:37.121
it's it's pretty domain specific. If you go back

01:12:37.121 --> 01:12:39.361
to the original Kaplan paper, they they said, okay,

01:12:39.361 --> 01:12:41.381
depth doesn't really matter. But, it's also

01:12:41.381 --> 01:12:43.881
because the back in the Kaplan paper, they really cared a lot about complexity.

01:12:44.401 --> 01:12:46.901
And more about generic model points about about memorization

01:12:46.961 --> 01:12:49.080
for these small transforms. And there,

01:12:49.081 --> 01:12:51.421
if you want to know a lot of facts, it just doesn't

01:12:51.481 --> 01:12:53.521
help you to be deep. Deep. And then maybe there's just

01:12:53.521 --> 01:12:55.681
been a few more more recent studies where people show that, okay.

01:12:55.681 --> 01:12:57.791
Depth could be good. If you really wanna

01:12:57.791 --> 01:12:59.711
do reasoning, if you wanna solve math problems.

01:13:00.191 --> 01:13:02.611
But I don't think there's been like a super comprehensive

01:13:02.911 --> 01:13:04.961
study what exactly the scaling curve would be.

01:13:05.841 --> 01:13:07.871
But we we have these like we have these theoretical results,

01:13:08.501 --> 01:13:10.740
but many real problems are not regular languages,

01:13:10.741 --> 01:13:12.810
unfortunately. And like, maybe real maybe

01:13:12.810 --> 01:13:14.910
many real problems are a bit of a mix of knowledge

01:13:15.370 --> 01:13:17.451
some reasoning. So it's not such to me as

01:13:17.451 --> 01:13:19.471
like comprehensively like what the scaling

01:13:19.731 --> 01:13:21.971
coefficient should be that we should rely on our practice. I think that's future

01:13:21.971 --> 01:13:24.160
work hopefully for someone in the Yeah.

01:13:24.301 --> 01:13:25.560
Thanks. Yeah.

01:13:27.241 --> 01:13:29.251
Peace. Thank you for the great talk. As far

01:13:29.251 --> 01:13:31.351
as you understand, you are doing recurrent computation

01:13:31.351 --> 01:13:33.600
for each and every token. Is it possible

01:13:33.600 --> 01:13:35.540
to internalize the token generation?

01:13:35.711 --> 01:13:37.706
Into the model representations?

01:13:37.721 --> 01:13:39.890
I think it will be interesting as well. Like

01:13:39.890 --> 01:13:42.031
here, we're really bound to something that

01:13:42.031 --> 01:13:43.941
looks like a causal and somber but if you squint at it,

01:13:44.341 --> 01:13:46.250
because we can scale that training up

01:13:46.731 --> 01:13:48.840
and train it by next token prediction. But

01:13:48.841 --> 01:13:50.920
if you figure out how to, like, have different

01:13:50.921 --> 01:13:53.201
training objectives, you can also have the recurrence

01:13:53.201 --> 01:13:55.541
over multiple tokens. Right? Towards a classical

01:13:55.541 --> 01:13:57.621
maybe like recurrent t five or something. I

01:13:57.621 --> 01:13:59.601
think it would be interesting as well. Okay.

01:13:59.976 --> 01:14:02.211
And one final question. You

01:14:02.211 --> 01:14:04.361
talked about, generalizing this

01:14:04.361 --> 01:14:06.441
to an objective, but can allow generalization

01:14:06.580 --> 01:14:08.890
to more much more infinite depth. Yeah.

01:14:09.211 --> 01:14:11.370
Can you talk about, like, what are the major bottle bottlenecks

01:14:11.371 --> 01:14:13.420
that you face if you simply come up with

01:14:13.421 --> 01:14:14.481
the objective like that?

01:14:17.551 --> 01:14:19.591
Right? So so for this model we always see this as sort of

01:14:19.591 --> 01:14:21.890
the sigmoid curves. Where it uses compute

01:14:21.890 --> 01:14:24.041
for a while. And then it does the performance at

01:14:24.041 --> 01:14:26.181
some point saturates. And I think

01:14:26.181 --> 01:14:28.221
it's a very big question like why it saturates. You

01:14:28.221 --> 01:14:30.381
could think about this as like, maybe this is just a model

01:14:30.381 --> 01:14:32.001
has learned to do addition perfectly.

01:14:32.810 --> 01:14:35.021
And between iterations one

01:14:35.021 --> 01:14:36.691
and three two, it runs the addition.

01:14:37.411 --> 01:14:39.571
And then here, because this is just a MKW, the multilayer has to,

01:14:39.571 --> 01:14:41.651
like, do some inference. Multilayer has to figure out

01:14:41.651 --> 01:14:43.741
the benchmark question. Maybe the model

01:14:43.741 --> 01:14:45.870
figured out the benchmark question wrong. And it just does

01:14:45.870 --> 01:14:48.126
the wrong addition for future steps.

01:14:48.321 --> 01:14:50.110
And then it gets that's why deployments is not

01:14:51.151 --> 01:14:52.931
peaked already, or not at at a ceiling yet.

01:14:53.250 --> 01:14:55.350
Or it could be that this model has learned this

01:14:55.351 --> 01:14:57.631
addition algorithm, but has learned the

01:14:57.632 --> 01:14:59.721
algorithms in exactly. And if it

01:14:59.961 --> 01:15:02.120
repeats the algorithm, repeats the algorithm, there's a certain error rate that the

01:15:02.120 --> 01:15:04.421
model can't get around yet. Because we

01:15:04.421 --> 01:15:06.640
we're here, so it's a bit funny because we

01:15:06.641 --> 01:15:08.961
have these discrete algorithms like addition,

01:15:08.961 --> 01:15:11.121
for integers, and we implement them in

01:15:11.121 --> 01:15:13.181
continuous models. And there can be a certain

01:15:13.181 --> 01:15:15.401
amount of error on how the model implements this algorithm,

01:15:15.721 --> 01:15:17.801
It's not so clear whether it's more of the first, whether model

01:15:17.801 --> 01:15:19.918
makes some sense premise and then just runs

01:15:19.918 --> 01:15:21.480
a lot of computation in the wrong direction.

01:15:22.041 --> 01:15:24.541
Or the model just runs competition but in exactly.

01:15:25.931 --> 01:15:27.081
Okay. Thank you.

01:15:28.201 --> 01:15:28.670
Sure.

01:15:31.611 --> 01:15:33.870
Great talk. Very interesting idea.

01:15:34.061 --> 01:15:35.551
My my question,

01:15:37.071 --> 01:15:38.301
the the kvCash,

01:15:39.101 --> 01:15:41.261
the recurrent And and you mentioned,

01:15:42.161 --> 01:15:44.221
performance is preserved if you sort of

01:15:44.221 --> 01:15:46.390
truncate the cache and only keep the last step.

01:15:46.551 --> 01:15:48.870
They're in here so states. I'm just wondering

01:15:48.871 --> 01:15:51.031
how do you avoid, like, a context rift in

01:15:51.031 --> 01:15:53.091
that scenario where you might have a you know,

01:15:53.091 --> 01:15:54.771
a complicated system prompt

01:15:55.251 --> 01:15:57.401
if you can only look so far back,

01:15:57.401 --> 01:15:59.541
how do you keep keep it on task?

01:16:01.801 --> 01:16:03.960
So, yeah. I think, yeah, I think I was I was a bit quick in

01:16:03.961 --> 01:16:05.994
that section. So it's important here that there is

01:16:05.994 --> 01:16:06.931
still one kV state

01:16:08.921 --> 01:16:10.580
Right? So, you don't really lose any

01:16:11.141 --> 01:16:13.381
it's a long system problem, you still have one k v state per

01:16:13.381 --> 01:16:15.681
token. But here's the question.

01:16:15.681 --> 01:16:17.871
You can sort of like I think

01:16:17.871 --> 01:16:19.981
about the recurrence as just having

01:16:20.461 --> 01:16:22.480
a large number of layers. Right? And you have

01:16:22.480 --> 01:16:24.550
one k v state per layer. So what we're

01:16:24.551 --> 01:16:26.711
what we're saying here is that we don't need to store the k v

01:16:26.711 --> 01:16:28.230
state for all these like fake layers.

01:16:29.031 --> 01:16:31.531
We just need to store the k v state for the last

01:16:31.741 --> 01:16:33.781
recurrence. So for the last layer in some sense.

01:16:34.100 --> 01:16:35.881
So we still have a k v state for every

01:16:36.161 --> 01:16:37.921
in the sequence. We don't lose any information.

01:16:38.961 --> 01:16:41.301
But we still like, we don't need to store these intermediate

01:16:41.361 --> 01:16:43.381
computations. Like we would in an autonomous one where we store the

01:16:43.381 --> 01:16:44.921
k v state for all layers.

01:16:46.910 --> 01:16:48.611
Thanks. Thank you.

01:16:50.551 --> 01:16:52.091
Hey. Thank you for the talk.

01:16:54.251 --> 01:16:55.871
I understand that

01:16:56.451 --> 01:16:57.991
according to my understanding, you're

01:16:59.091 --> 01:17:01.111
don't really train a model to be

01:17:01.111 --> 01:17:03.290
adaptive beyond just sampling it.

01:17:03.676 --> 01:17:06.151
Pretraining. From different lengths?

01:17:06.311 --> 01:17:08.441
And I think one interesting thing with these

01:17:08.441 --> 01:17:10.521
type of models is

01:17:10.521 --> 01:17:12.611
that you might want to kind

01:17:12.611 --> 01:17:14.831
of control this, that actually is adaptive.

01:17:16.001 --> 01:17:17.781
Have you thought about any reinforcement

01:17:18.321 --> 01:17:20.331
learning other type

01:17:20.331 --> 01:17:22.451
of things that could be done to encourage

01:17:22.591 --> 01:17:24.830
the model to train to, like, do more recurrences on

01:17:24.830 --> 01:17:27.080
more difficult tasks until you get some

01:17:27.080 --> 01:17:29.221
reward or Yeah. Things like that? So

01:17:29.221 --> 01:17:31.620
I think that that's a super that's a super interesting

01:17:31.620 --> 01:17:33.640
question. Right? Because if if you go back to

01:17:33.640 --> 01:17:36.101
the older papers like University of Thomas, this is

01:17:36.101 --> 01:17:38.171
something they specifically trained for

01:17:38.171 --> 01:17:40.511
and it's like all of this is about

01:17:40.511 --> 01:17:42.406
adaptive compute. Modules.

01:17:42.661 --> 01:17:44.761
Which are, like, parts of the model that really decide

01:17:44.981 --> 01:17:47.061
end to end, should I exit here

01:17:47.061 --> 01:17:49.131
or should I run more compute? In our experiments,

01:17:49.131 --> 01:17:50.620
we found this pretty hard to scale.

01:17:51.181 --> 01:17:53.421
But this could be like a skill issue on my part really.

01:17:53.421 --> 01:17:55.431
Right? That's like, it's pretty hard

01:17:55.431 --> 01:17:57.171
to fast to these very large models.

01:17:57.731 --> 01:17:59.740
And then because training, we really want the

01:17:59.741 --> 01:18:01.431
model to be better at hard on everything.

01:18:01.830 --> 01:18:03.910
And we have to train this ACT module, which really wants

01:18:03.910 --> 01:18:05.670
to, like, do as least amount of work as possible.

01:18:06.231 --> 01:18:08.301
So we found it really hard to do that during pre training.

01:18:08.861 --> 01:18:10.661
But I think maybe it's promising to

01:18:10.901 --> 01:18:13.060
this in a post training stage where we, like, now the model is trained,

01:18:13.061 --> 01:18:15.281
we can then train an exit condition

01:18:15.281 --> 01:18:16.080
later, maybe

01:18:17.441 --> 01:18:19.441
Or there is a better way to do it during pre training?

01:18:19.601 --> 01:18:21.741
It's it's a min max game somehow, There should be a very

01:18:21.741 --> 01:18:24.060
interesting solution to that. Yeah. I was also

01:18:24.061 --> 01:18:26.241
thinking more about post training. Thanks.

01:18:29.870 --> 01:18:31.950
Sort of think we can talk. I'm gonna read

01:18:31.951 --> 01:18:33.981
this because I wrote the sound. Do you think

01:18:33.981 --> 01:18:36.161
recurrent, depth health transformers

01:18:37.281 --> 01:18:39.321
approximate reasoning, for specialized

01:18:39.381 --> 01:18:41.501
algorithms. In a more compute efficient

01:18:41.501 --> 01:18:43.511
way. Have we have you guys tried some

01:18:43.511 --> 01:18:45.890
experiments in that area? Because that's very interesting

01:18:45.890 --> 01:18:47.790
because we started with a puzzle and the maze,

01:18:47.951 --> 01:18:49.991
That was really amazing to see that. What

01:18:49.991 --> 01:18:52.071
if we can do it for very advanced task and very

01:18:52.071 --> 01:18:53.771
advanced algorithms? Yep.

01:18:54.221 --> 01:18:56.081
Think we it's really like we really struggle

01:18:56.301 --> 01:18:58.321
with like yeah, probably can't give

01:18:58.321 --> 01:19:00.491
you like a concrete answer, Intuitively,

01:19:00.791 --> 01:19:02.951
the the this seems sensible and the model should have this

01:19:02.951 --> 01:19:05.221
capability, but then we find this in theory,

01:19:05.461 --> 01:19:06.791
in this, like, theory part,

01:19:08.551 --> 01:19:10.351
Oh, let me just call all the way there. Right? Yep.

01:19:10.591 --> 01:19:12.591
Here, this is true. Right? Like, here,

01:19:12.751 --> 01:19:14.991
it is more efficient to learn

01:19:14.991 --> 01:19:16.661
regular languages by recurring in-depth

01:19:17.221 --> 01:19:19.231
than making an order wider using verbal chain of

01:19:19.231 --> 01:19:21.361
thought, Well, this works in practice, I think this

01:19:21.361 --> 01:19:23.441
remains to be seen yet because it's a very often it's

01:19:23.441 --> 01:19:25.650
a very hard apples comparison. So

01:19:25.651 --> 01:19:27.750
we train a lot of these recurrent walls, and they're kinda

01:19:27.971 --> 01:19:30.200
small. And at a small scale, it's

01:19:30.520 --> 01:19:32.600
pretty hard to make good verbal Chernoffov models if

01:19:32.600 --> 01:19:33.930
you're used to, like, I don't know,

01:19:34.761 --> 01:19:36.840
five is in the clouds, then you go back to the open source models and get

01:19:36.841 --> 01:19:38.910
them to do real translation. It can

01:19:38.910 --> 01:19:40.941
be a bit dicey sometimes. I think there's a lot

01:19:40.941 --> 01:19:43.140
of interesting work to be done here to really

01:19:43.140 --> 01:19:45.211
figure out how these in practice,

01:19:45.211 --> 01:19:47.430
and I don't know yet. Okay. Thank you.

01:19:50.531 --> 01:19:52.471
Hi. Thanks for the great talk. This is super

01:19:52.691 --> 01:19:54.981
interesting. Would be interested to

01:19:55.201 --> 01:19:57.500
understand bit a little bit better, like, the differences between

01:19:57.501 --> 01:19:59.630
the different dimensions in which you can scale.

01:19:59.631 --> 01:20:01.871
So, like, one would be the recurrence, the other one would

01:20:01.871 --> 01:20:04.200
be the autoregressive. Thing. One difference

01:20:04.201 --> 01:20:06.301
that I can see is that, like, for the autoregressive

01:20:06.301 --> 01:20:08.701
dimension, you sample. So you actually, like, committing

01:20:08.761 --> 01:20:11.050
to one specific trace where here,

01:20:11.211 --> 01:20:13.711
the recurrence basically like keep converging to

01:20:14.671 --> 01:20:16.961
something. Yeah. Could

01:20:16.961 --> 01:20:19.040
compare the two? I think that that that's a very

01:20:19.040 --> 01:20:21.221
good assessment of these different trade

01:20:21.221 --> 01:20:23.241
offs. Right? I think, there's

01:20:23.241 --> 01:20:25.390
this common intuition in the community that running

01:20:25.391 --> 01:20:27.321
the recurrence in-depth, It's a con

01:20:27.401 --> 01:20:28.510
it's a very large

01:20:29.491 --> 01:20:31.761
whatever, had a major hidden spade. States.

01:20:31.761 --> 01:20:33.840
So, we can the model could learn to implement many

01:20:33.841 --> 01:20:35.881
algorithms in parallel. But since it makes

01:20:35.881 --> 01:20:38.040
sense to actually symmetry break all of this and to

01:20:38.040 --> 01:20:40.081
to pick a certain

01:20:40.401 --> 01:20:42.461
and then go down that route? So, that's

01:20:42.461 --> 01:20:44.131
maybe these two axes

01:20:45.071 --> 01:20:46.991
are complementary. Have you tried it out to

01:20:47.231 --> 01:20:49.541
use both? Like, for demo, this example here,

01:20:49.541 --> 01:20:51.701
like, this like these models still run run

01:20:51.701 --> 01:20:52.471
verbal chain of thought.

01:20:53.901 --> 01:20:55.951
Like for example, like here, Like

01:20:55.951 --> 01:20:58.111
Tesaski's GSM case Jyn'Thought. Which

01:20:58.111 --> 01:21:00.411
just means that this model actually is running like per token,

01:21:00.991 --> 01:21:03.091
depth. It still writes out the answer in

01:21:03.091 --> 01:21:05.201
like oh, j s k. I can like

01:21:05.441 --> 01:21:07.599
three ampersand triples. It does both here. Actually,

01:21:07.600 --> 01:21:09.711
this example And it's it's very natural for the model

01:21:09.931 --> 01:21:11.601
side because like, you can always

01:21:12.161 --> 01:21:13.871
be more better by still writing out the answer.

01:21:14.431 --> 01:21:16.380
And here, that is actually skilled in both dimensions.

01:21:16.781 --> 01:21:17.481
Thank you.

01:21:18.821 --> 01:21:20.941
Yeah. I think I'll get

01:21:20.941 --> 01:21:23.191
away from now, and we can see what comes next.

01:21:34.031 --> 01:21:36.021
And We we are saying,

01:21:36.171 --> 01:21:38.251
to us, professor for

01:21:38.251 --> 01:21:40.751
a really interesting talk, and we will have

01:21:41.551 --> 01:21:43.400
sharp morning break and next week.

01:21:43.961 --> 01:21:46.621
Plant center will come here at 10AM.

01:21:46.921 --> 01:21:49.401
It's Amin Kuhan,

01:21:49.461 --> 01:21:51.681
professor from Yale University

01:21:51.991 --> 01:21:54.041
Yes. And thanks everyone and

01:21:54.041 --> 01:21:56.289
let's restart at

01:21:56.290 --> 01:21:57.110
10AM.

01:41:20.521 --> 01:41:21.021
Testing.

01:41:22.561 --> 01:41:24.641
Our next speaker will start at

01:41:24.641 --> 01:41:26.571
10AM, and we are

01:41:27.451 --> 01:41:29.470
everyone to like, go back to the

01:41:29.471 --> 01:41:31.250
sea and waiting for

01:41:31.620 --> 01:41:33.321
professor Amir Khan's

01:41:34.381 --> 01:41:35.010
remarkable talk.

01:46:19.691 --> 01:46:22.011
Hi, everyone. Now

01:46:22.231 --> 01:46:24.711
we honored to invite professor

01:46:24.771 --> 01:46:26.791
Amin Ke Huang. Yeah. He's professor

01:46:27.091 --> 01:46:29.151
in the computer science department

01:46:29.921 --> 01:46:32.081
in the Yale University, and

01:46:32.081 --> 01:46:34.231
he did the natural

01:46:34.231 --> 01:46:36.291
language processing app. And

01:46:36.291 --> 01:46:38.391
today, he will give us a very

01:46:38.771 --> 01:46:41.181
remarkable talk about efficient listening.

01:46:41.691 --> 01:46:43.551
As welcome, professor Amy Kehan,

01:46:48.511 --> 01:46:50.751
Thank you for the introduction and

01:46:50.751 --> 01:46:51.890
for having me.

01:46:53.120 --> 01:46:55.360
So today, I'm excited to tell you

01:46:55.361 --> 01:46:56.400
a little bit about,

01:46:57.381 --> 01:46:59.080
frameworks for better understanding

01:46:59.881 --> 01:47:01.821
evaluation and scientific

01:47:02.041 --> 01:47:04.071
reasoning. It's kind of a

01:47:04.071 --> 01:47:06.230
a different flavor of a talk in in

01:47:06.231 --> 01:47:08.120
terms of, like, efficiency.

01:47:08.421 --> 01:47:10.501
I'm not gonna talk about new methods

01:47:10.661 --> 01:47:12.811
to improve efficiency. What I'm gonna tell

01:47:12.811 --> 01:47:14.901
you how we can efficiently

01:47:15.201 --> 01:47:17.081
evaluate reasoning and

01:47:17.671 --> 01:47:19.611
give you some insights on where

01:47:19.830 --> 01:47:22.071
more reasoning could help and where it might

01:47:22.071 --> 01:47:24.211
not. Help. So to

01:47:24.211 --> 01:47:26.371
motivate this a little bit, I I think

01:47:26.371 --> 01:47:28.830
I had this venue, everyone

01:47:28.890 --> 01:47:31.210
knows that reasoning models rely on

01:47:31.211 --> 01:47:33.341
long chain of thoughts or

01:47:33.341 --> 01:47:35.361
test on compute to do very well.

01:47:35.501 --> 01:47:37.570
And, we see that this

01:47:37.571 --> 01:47:39.431
type of inference really pushes

01:47:40.001 --> 01:47:42.081
the boundaries of, performance across many

01:47:42.081 --> 01:47:44.111
different tasks. However,

01:47:44.330 --> 01:47:46.431
the the issue is, like, this type of,

01:47:46.901 --> 01:47:48.336
inference time compute

01:47:49.051 --> 01:47:51.290
is also making evaluations much

01:47:51.290 --> 01:47:53.601
more expensive. So

01:47:53.661 --> 01:47:56.051
many times when we are developing

01:47:56.051 --> 01:47:58.531
new models, if you want to evaluate them, it becomes

01:47:58.531 --> 01:47:59.971
very costly as well.

01:48:01.410 --> 01:48:03.350
The other issue is about the alignment

01:48:03.651 --> 01:48:05.881
evaluation. And here

01:48:05.881 --> 01:48:08.301
by alignment, I mean, like, the general alignment

01:48:08.361 --> 01:48:10.541
that we want models in

01:48:10.541 --> 01:48:12.730
nonverifiable tasks to kind

01:48:12.731 --> 01:48:13.951
of follow human

01:48:15.231 --> 01:48:17.140
as in their output. And

01:48:17.281 --> 01:48:19.440
compared with verifiable tasks like math

01:48:19.441 --> 01:48:21.491
and, like, coding, evaluations

01:48:21.631 --> 01:48:22.991
of such systems is,

01:48:23.711 --> 01:48:25.790
challenge. So, in

01:48:25.790 --> 01:48:27.950
this, talk, I'm going to

01:48:27.951 --> 01:48:30.171
talk about two challenges. One is

01:48:30.171 --> 01:48:32.031
just this evaluation in nonverifiable

01:48:32.821 --> 01:48:35.221
tasks, and second is, about

01:48:35.291 --> 01:48:37.461
scientific problem solving. The

01:48:37.461 --> 01:48:39.481
first one, we want to know if models,

01:48:40.711 --> 01:48:42.791
can follow human preferences and if

01:48:42.791 --> 01:48:45.111
we can do this type of evaluations

01:48:45.410 --> 01:48:46.151
more efficiently,

01:48:47.431 --> 01:48:49.511
And in the second one, I'm going to tell you

01:48:49.511 --> 01:48:51.401
about more, efficient benchmarking

01:48:51.861 --> 01:48:54.161
methods and where actually

01:48:54.301 --> 01:48:56.381
inference time compute can help in this

01:48:56.381 --> 01:48:58.121
sub of complex tasks.

01:49:00.421 --> 01:49:02.521
So specifically, first, I'm

01:49:02.521 --> 01:49:04.301
going to tell you about, evaluating

01:49:04.681 --> 01:49:06.701
LLM alignment through evaluation

01:49:07.131 --> 01:49:09.281
of LLMs as judges.

01:49:10.640 --> 01:49:12.761
And the the insight that I wanna

01:49:12.761 --> 01:49:14.611
highlight is that we can

01:49:15.071 --> 01:49:17.221
reuse computation for better and more

01:49:17.431 --> 01:49:19.551
efficient, benchmarks. And

01:49:19.551 --> 01:49:21.331
second, I'm going to tell you

01:49:22.251 --> 01:49:24.331
how we can design frameworks to know

01:49:24.331 --> 01:49:25.921
where reasoning actually

01:49:26.561 --> 01:49:28.581
is limited and how we can actually

01:49:28.671 --> 01:49:30.600
improve. That. So

01:49:30.921 --> 01:49:33.020
the first word is, on evaluating the

01:49:33.080 --> 01:49:35.100
element alignment. By evaluating the elements. So

01:49:35.100 --> 01:49:37.180
as judges, actually, we presented it, two

01:49:37.181 --> 01:49:39.261
days ago here. At

01:49:39.261 --> 01:49:40.721
Nuurex. So basically,

01:49:42.330 --> 01:49:44.730
LMM evaluation in nonverifiable tests

01:49:44.730 --> 01:49:46.600
looks like this. You have a

01:49:47.081 --> 01:49:49.160
question, but it doesn't have, like,

01:49:49.161 --> 01:49:51.501
objective correct. Answer, but then

01:49:51.501 --> 01:49:53.601
you want to basically compare models

01:49:53.661 --> 01:49:55.480
and how good or bad they

01:49:56.521 --> 01:49:58.221
they follow user preferences.

01:49:58.911 --> 01:50:00.991
And usually, how people evaluate

01:50:00.991 --> 01:50:03.491
this is just, you collect user votes

01:50:03.551 --> 01:50:05.651
or human votes,

01:50:05.651 --> 01:50:07.811
and then you aggregate these votes into a

01:50:07.811 --> 01:50:10.021
leaderboard. The famous one is

01:50:10.021 --> 01:50:11.781
LM Arena, that every new model is

01:50:12.341 --> 01:50:14.491
evaluated. On. And currently, I just

01:50:14.491 --> 01:50:16.751
took this today, like, Gemini

01:50:16.891 --> 01:50:18.811
three Pro is top of this leaderboard.

01:50:20.171 --> 01:50:22.031
However, because this is very,

01:50:23.211 --> 01:50:25.531
expensive to do, like and then you cannot

01:50:25.531 --> 01:50:28.020
just always collect human feedback.

01:50:28.321 --> 01:50:30.761
What people do is just automating this process

01:50:30.761 --> 01:50:32.841
by replacing the human waters with,

01:50:33.080 --> 01:50:35.310
elements. And this is actually

01:50:35.370 --> 01:50:37.471
done in many benchmarks in alignment

01:50:37.611 --> 01:50:38.901
evaluations, such as El Pollock

01:50:41.541 --> 01:50:43.861
And then you can do the same process, have the LMM

01:50:43.861 --> 01:50:45.931
judge to just you know, specify

01:50:45.931 --> 01:50:47.661
the preferences. You aggregate the

01:50:48.221 --> 01:50:50.540
results and get to a leaderboard such as this one

01:50:50.541 --> 01:50:52.351
or in a hard that you see. Here.

01:50:53.871 --> 01:50:56.111
And so how does this work exactly in

01:50:56.111 --> 01:50:58.191
practice? It's like you have a bunch of

01:50:58.191 --> 01:50:59.971
models that you want to evaluate,

01:51:00.621 --> 01:51:02.890
I'm listing four of them here. A

01:51:02.891 --> 01:51:04.581
Cohen, Lamma, Alma, and Gemma.

01:51:04.901 --> 01:51:07.321
And then you have a, set of input instructions.

01:51:07.621 --> 01:51:09.870
You you run these input instructions through

01:51:09.871 --> 01:51:11.951
your models, get the outputs, and then ask an

01:51:11.951 --> 01:51:14.101
LMM based evaluator to give you this

01:51:14.421 --> 01:51:16.691
scores or do some sort of pairwise comparison.

01:51:17.181 --> 01:51:19.321
And then you aggregate these rankings and

01:51:19.321 --> 01:51:21.330
its scores and you get to, a

01:51:21.330 --> 01:51:23.421
final ranking of which model is better

01:51:23.421 --> 01:51:24.901
than. Other model.

01:51:27.761 --> 01:51:29.560
So, LLMs can be also evaluated

01:51:30.020 --> 01:51:31.641
as a judge And,

01:51:32.421 --> 01:51:34.580
this means that we want to know how they

01:51:34.580 --> 01:51:36.911
perform if they're used as an evaluator.

01:51:37.861 --> 01:51:39.870
And, this is typically done in meta evaluation

01:51:39.870 --> 01:51:42.120
as studies. The setup

01:51:42.121 --> 01:51:44.330
is like this. You have still a set of prompts,

01:51:44.331 --> 01:51:46.260
but then you also collect a set of

01:51:46.560 --> 01:51:48.701
models. These are just used to create

01:51:48.701 --> 01:51:50.951
the pool of instances that you want

01:51:51.171 --> 01:51:53.461
to evaluate and then you usually do

01:51:53.761 --> 01:51:55.711
a pairwise comparisons between

01:51:56.031 --> 01:51:57.261
samples of these models.

01:51:58.620 --> 01:52:00.731
And then, basically,

01:52:01.031 --> 01:52:03.386
you collect, ground truth

01:52:04.011 --> 01:52:05.551
votes for for which ones is

01:52:06.036 --> 01:52:08.311
preferred. Can do this by human evaluators or

01:52:08.821 --> 01:52:11.061
as is typically done, you use a strong

01:52:11.061 --> 01:52:13.061
model, or LLM to

01:52:13.941 --> 01:52:16.100
to use as a preference oracle or

01:52:16.101 --> 01:52:18.011
as a ground truth.

01:52:18.251 --> 01:52:20.330
Then let's say, you want to know

01:52:20.330 --> 01:52:22.401
if these models, like

01:52:22.401 --> 01:52:24.581
Quinellama or or more good as judges.

01:52:25.111 --> 01:52:27.611
What you can do is just using them as judges,

01:52:27.831 --> 01:52:29.611
to also collect their votes here,

01:52:30.251 --> 01:52:32.411
and then, just compare how they

01:52:32.411 --> 01:52:34.400
correlate with the ground truth, which,

01:52:34.481 --> 01:52:36.561
which could be a LMM evaluated or

01:52:36.561 --> 01:52:38.901
human. Evaluated. And then based

01:52:38.901 --> 01:52:41.301
on this, we can get rankings again. And

01:52:41.301 --> 01:52:42.951
then this ranking tells us

01:52:43.350 --> 01:52:45.600
which one of these models, Cohen Lam, are

01:52:45.600 --> 01:52:47.761
almost better than better at,

01:52:49.551 --> 01:52:51.810
judging, the outputs of other LMM.

01:52:53.051 --> 01:52:55.291
LMS. So the key question we're asking here

01:52:55.291 --> 01:52:57.341
is, like, if an LMM is good at

01:52:57.341 --> 01:52:58.881
generation, is it also good at

01:52:59.441 --> 01:53:01.491
evaluation? So

01:53:01.491 --> 01:53:03.431
we call this generator, evaluator

01:53:03.651 --> 01:53:04.091
consist

01:53:06.231 --> 01:53:08.441
short, which is a correlation of performance, which

01:53:08.681 --> 01:53:10.541
between evaluation and, generation.

01:53:11.251 --> 01:53:13.411
And the setup is exactly like I

01:53:13.411 --> 01:53:15.451
described before. I just put

01:53:15.451 --> 01:53:17.331
both of them in this figure below.

01:53:17.811 --> 01:53:20.151
And, it is a bit different than

01:53:20.391 --> 01:53:22.631
like, other types of consistencies people

01:53:22.631 --> 01:53:24.721
have studied. Like like, there there

01:53:24.721 --> 01:53:26.960
are some works that looked into if an

01:53:26.961 --> 01:53:29.161
LLM prefers its own output

01:53:29.161 --> 01:53:31.271
because it generates, the same output

01:53:31.271 --> 01:53:33.341
itself. But this is like

01:53:33.341 --> 01:53:34.961
looking in ranking evaluation

01:53:36.091 --> 01:53:38.341
and then seeing if rankings are correlated

01:53:38.341 --> 01:53:40.390
with between generation and

01:53:40.391 --> 01:53:42.461
evaluation. So and

01:53:42.461 --> 01:53:44.581
you might ask, okay. Why would I care about this?

01:53:45.201 --> 01:53:47.360
So the the you

01:53:47.921 --> 01:53:50.111
if we find that there's high

01:53:51.181 --> 01:53:52.960
then we can use LLMs,

01:53:53.421 --> 01:53:55.621
as judges and evaluate them as

01:53:55.621 --> 01:53:57.691
judges, and then that tells us how

01:53:57.691 --> 01:53:59.891
good they are in generation as well.

01:54:00.171 --> 01:54:02.651
So it helps us approximate general

01:54:02.651 --> 01:54:04.461
alignment events. Evaluation.

01:54:05.791 --> 01:54:07.861
So let let's see how we can do this.

01:54:08.181 --> 01:54:10.521
So basically, we started this by,

01:54:10.660 --> 01:54:12.781
like, using a control setting,

01:54:12.781 --> 01:54:14.805
using a strong LLM like

01:54:14.806 --> 01:54:17.231
GPT four o at the time, as

01:54:17.451 --> 01:54:19.691
a preference oracle for both the generation and

01:54:19.691 --> 01:54:20.191
evaluation

01:54:21.741 --> 01:54:23.841
oracle. And then we use, Alpaca

01:54:23.901 --> 01:54:25.691
eval and Arena Heart as

01:54:26.041 --> 01:54:28.071
input instructions. These are

01:54:28.071 --> 01:54:30.071
instructions in general tasks,

01:54:30.311 --> 01:54:32.691
that are instruction following and alignment, and

01:54:32.691 --> 01:54:33.961
people use that for eval.

01:54:34.921 --> 01:54:37.001
Then we use 15 LLMs to evaluate,

01:54:37.001 --> 01:54:39.291
and then we for the

01:54:39.351 --> 01:54:41.071
generator ranking, we basically

01:54:41.471 --> 01:54:43.541
gave the model, both

01:54:44.001 --> 01:54:45.941
pairs like we also repair reversed

01:54:46.080 --> 01:54:48.331
this to eliminate the position

01:54:48.331 --> 01:54:50.361
bias. For the

01:54:50.361 --> 01:54:51.341
evaluator ranking,

01:54:52.611 --> 01:54:55.031
we get the, evaluation task instances.

01:54:55.461 --> 01:54:57.641
By pairwise comparisons of LLMs

01:54:57.701 --> 01:55:00.101
that we want to evaluate, versus a baseline

01:55:00.241 --> 01:55:02.511
LLM. This could be something like a GPT.

01:55:02.991 --> 01:55:05.144
Four. And then for evaluation metric, we use

01:55:05.145 --> 01:55:07.641
a basically, a correlation,

01:55:08.261 --> 01:55:10.651
or inner agreement between the

01:55:11.091 --> 01:55:13.361
preference oracle and our LLMs judges.

01:55:13.361 --> 01:55:15.511
And we do a step

01:55:15.511 --> 01:55:17.320
of consistency filtering

01:55:17.561 --> 01:55:19.721
and this this, this is important. I

01:55:19.721 --> 01:55:21.901
will, I will touch on

01:55:21.901 --> 01:55:23.341
this in the next slide.

01:55:24.701 --> 01:55:26.221
So these are the results,

01:55:26.951 --> 01:55:28.731
On left, you can see alpaca

01:55:29.111 --> 01:55:31.351
eval. On right, you can see areina heart. So

01:55:31.351 --> 01:55:33.651
this is kind of dataset dependent, but in

01:55:33.651 --> 01:55:35.931
both cases, we see high correlation

01:55:35.931 --> 01:55:38.191
between generation and evaluation performance

01:55:38.251 --> 01:55:40.681
of models. And, especially

01:55:40.741 --> 01:55:42.821
for the RNA heart, we see very high

01:55:42.821 --> 01:55:43.321
correlation.

01:55:44.951 --> 01:55:47.191
I mentioned about consistency filtering, and

01:55:47.191 --> 01:55:48.331
this is actually pretty

01:55:52.331 --> 01:55:54.491
we the oracle is very confident

01:55:54.491 --> 01:55:56.501
and like which one is correct.

01:55:56.501 --> 01:55:58.550
To order to limit ourselves to

01:55:58.551 --> 01:56:00.951
those type of instructions, we basically reverse

01:56:00.951 --> 01:56:03.181
it outputs and then saw if

01:56:03.181 --> 01:56:05.551
the model flips its decision with

01:56:05.921 --> 01:56:08.160
got rid of that instance. And that is

01:56:08.160 --> 01:56:10.243
important. As you can see here, if we don't do

01:56:10.243 --> 01:56:12.311
this filtering, we will

01:56:12.311 --> 01:56:14.251
see that the performance or, like, the

01:56:14.841 --> 01:56:17.241
correlation between generation and evaluation is,

01:56:18.151 --> 01:56:20.141
lower and actually significantly lower.

01:56:22.061 --> 01:56:24.210
So again, like, you might

01:56:24.211 --> 01:56:25.911
say, why would I care about this?

01:56:26.781 --> 01:56:28.481
We call this, kind of framework,

01:56:28.861 --> 01:56:31.309
align eval. Which, helps

01:56:31.310 --> 01:56:33.471
us evaluate alignment by

01:56:33.531 --> 01:56:35.741
evaluating LLMs as judges. Which,

01:56:36.120 --> 01:56:38.221
helps us approximate the generator

01:56:38.441 --> 01:56:38.811
ranking.

01:56:41.131 --> 01:56:42.891
So bay basically, the setup,

01:56:43.141 --> 01:56:45.241
I I just described was but

01:56:45.241 --> 01:56:47.361
for more details, we use RNA hard at

01:56:47.921 --> 01:56:49.941
instructions. Set. And use GPT four o

01:56:49.941 --> 01:56:52.331
or CLOS 2.7 Sonnet as

01:56:52.841 --> 01:56:54.931
the preference oracle. Were the best models

01:56:54.931 --> 01:56:57.191
at the time that we were conducting the experiments.

01:56:58.401 --> 01:57:00.521
So, basically,

01:57:01.571 --> 01:57:03.591
recall that in generation evaluation,

01:57:03.891 --> 01:57:05.911
for every new model that you get,

01:57:06.151 --> 01:57:08.251
you need to generate a lot of outputs

01:57:08.571 --> 01:57:11.051
and then you need to send them to an LMM

01:57:11.051 --> 01:57:14.051
based evaluator like a GPT 4.5

01:57:14.181 --> 01:57:16.341
or GPT or or clot

01:57:16.581 --> 01:57:19.081
point five or something to get the scores.

01:57:19.711 --> 01:57:21.790
And, because whenever you have a new

01:57:21.790 --> 01:57:23.321
model or new check point,

01:57:23.931 --> 01:57:25.871
you need to do this LMM based evaluation.

01:57:26.201 --> 01:57:28.520
And you can see that this this can be kind

01:57:28.521 --> 01:57:30.530
of expensive. But in

01:57:30.531 --> 01:57:32.769
our paradigm, this is different. You only

01:57:32.770 --> 01:57:35.261
run the LMM based evaluator

01:57:35.401 --> 01:57:37.621
once. This is like just to

01:57:37.621 --> 01:57:38.761
get the gold,

01:57:39.921 --> 01:57:42.081
labels here. And then you have that and you

01:57:42.081 --> 01:57:44.181
save that and then you don't need to for any

01:57:44.181 --> 01:57:46.340
new model that you want to evaluate, you don't

01:57:46.341 --> 01:57:48.661
need to send it through API

01:57:48.661 --> 01:57:49.951
to evaluate it.

01:57:50.750 --> 01:57:52.990
So as you can see here, like, the

01:57:52.991 --> 01:57:54.891
cost of align eval, the last

01:57:55.291 --> 01:57:57.261
row, is zero API cost.

01:57:57.580 --> 01:57:59.201
Whereas, like, for the other benchmarks,

01:57:59.741 --> 01:58:02.231
you can see that because they use LLM judges,

01:58:02.871 --> 01:58:04.971
you need to pay API costs to

01:58:04.971 --> 01:58:07.170
evaluate. It. And

01:58:07.171 --> 01:58:09.191
then, you you might ask, okay.

01:58:10.621 --> 01:58:11.981
$10 is not damaged.

01:58:12.731 --> 01:58:14.890
But but assume that you're, like, training

01:58:14.891 --> 01:58:16.991
a new RL model, like a new DPU

01:58:17.291 --> 01:58:18.691
or and you're evaluating your

01:58:19.611 --> 01:58:21.811
checkpoints every 100 steps or so.

01:58:21.971 --> 01:58:24.001
Then this adds up very quickly. So

01:58:24.001 --> 01:58:26.241
what you're arguing is that you can use a line eval

01:58:26.241 --> 01:58:28.491
and then you don't pay that cost. And then it

01:58:28.491 --> 01:58:30.631
tells you pretty well if your model

01:58:30.631 --> 01:58:32.871
is good or not. So,

01:58:33.251 --> 01:58:34.951
how do we know if this is reliable?

01:58:35.521 --> 01:58:37.611
We do correlation analysis. We LMR

01:58:37.611 --> 01:58:39.730
in our ranking, the self controlled

01:58:39.731 --> 01:58:41.810
version. This is similar to what other people

01:58:41.810 --> 01:58:43.790
do. We use 23

01:58:43.931 --> 01:58:45.830
LMMs to rank, and then, the

01:58:46.231 --> 01:58:48.601
compared with, few different benchmarks as

01:58:48.601 --> 01:58:48.921
base.

01:58:51.100 --> 01:58:53.491
So here you can see the correlation

01:58:53.870 --> 01:58:56.264
with l m arina, which is like human

01:58:56.351 --> 01:58:58.451
votes. And you can see that, like,

01:58:58.451 --> 01:59:00.321
our, align eval achieves

01:59:00.640 --> 01:59:02.461
pretty good correlations.

01:59:02.711 --> 01:59:04.731
It's it's a not as high

01:59:04.731 --> 01:59:06.801
as the judge based bench.

01:59:07.151 --> 01:59:09.261
Like a GPT four o as a

01:59:09.261 --> 01:59:11.441
judge or, in a hard style control,

01:59:11.971 --> 01:59:13.321
but it is very competitive

01:59:14.201 --> 01:59:16.061
Compared with judge free benchmarks,

01:59:16.661 --> 01:59:18.821
we get higher correlation. So, for example, there's

01:59:18.821 --> 01:59:20.741
mixed eval, which is another benchmark that is

01:59:21.461 --> 01:59:23.471
created from objective task. And here we can

01:59:23.471 --> 01:59:25.781
see that, the output performing. That.

01:59:25.781 --> 01:59:27.821
There's this benchmark if you're familiar with it

01:59:27.821 --> 01:59:29.991
called If Eval. Which is basically,

01:59:30.611 --> 01:59:32.620
followed instruction following

01:59:32.620 --> 01:59:34.241
evaluation through verifiable,

01:59:35.271 --> 01:59:37.431
checks, such as, regexes or

01:59:37.431 --> 01:59:39.521
rules based checks or so on.

01:59:39.681 --> 01:59:41.590
Can see that if eval actually gets

01:59:42.151 --> 01:59:44.041
high correlation with LMR in our end,

01:59:44.521 --> 01:59:46.841
So we were looking at into this, and we found

01:59:46.841 --> 01:59:49.031
that, like, basic a line eval, which

01:59:49.031 --> 01:59:51.430
is our method, evaluates content,

01:59:51.431 --> 01:59:53.301
whereas, like, if eval evaluates

01:59:53.601 --> 01:59:55.841
instruction adherence, So the this

01:59:55.841 --> 01:59:57.961
seems like a complementary thing. So

01:59:58.201 --> 02:00:00.361
we thought, okay. Maybe we can combine

02:00:00.361 --> 02:00:02.121
them. We combine them and call this

02:00:02.591 --> 02:00:04.725
aligning valve plus and here we can like,

02:00:04.725 --> 02:00:06.791
this last two columns, can

02:00:06.791 --> 02:00:08.331
see that we get the highest correlation

02:00:08.831 --> 02:00:11.071
you do this. So, basically,

02:00:11.071 --> 02:00:13.401
in general, I want to argue that large you

02:00:13.401 --> 02:00:15.660
can evaluate alignment

02:00:16.241 --> 02:00:17.311
in nonverifiable tasks

02:00:19.181 --> 02:00:21.371
using evaluating LLMs as

02:00:21.371 --> 02:00:23.591
evaluators or as judges. And

02:00:23.591 --> 02:00:25.841
the implication on this is that it's

02:00:25.841 --> 02:00:27.921
much more efficient, because you can

02:00:27.921 --> 02:00:30.321
reuse the model's outputs and you only

02:00:30.321 --> 02:00:31.111
need to run the

02:00:32.341 --> 02:00:34.541
preference oracle once. And this becomes very

02:00:34.541 --> 02:00:36.781
important, especially if you are training a lot of, like,

02:00:36.781 --> 02:00:38.921
reasoning models. As it's become,

02:00:39.161 --> 02:00:40.621
very expensive to evaluate.

02:00:41.311 --> 02:00:43.471
And then we observe very high correlation with

02:00:43.471 --> 02:00:45.891
human ranking. Beams. So, yeah,

02:00:45.891 --> 02:00:47.910
this this is, the first work,

02:00:47.991 --> 02:00:49.851
and then the second work, I'm going

02:00:50.231 --> 02:00:52.301
to, shift gears a little

02:00:52.301 --> 02:00:54.561
bit and tell you a little bit about, scientific

02:00:55.031 --> 02:00:56.541
problem solving in LLMs and

02:00:57.101 --> 02:00:58.910
their reasoning can help and their

02:00:59.311 --> 02:01:00.561
knowledge appraisal look role.

02:01:02.001 --> 02:01:04.241
So the first thing we were, looking into

02:01:04.241 --> 02:01:06.741
in the work was, like, there's lots of benchmarks

02:01:06.881 --> 02:01:08.901
that people have created for, evaluating

02:01:08.961 --> 02:01:11.021
scientific problem solving. These are

02:01:11.021 --> 02:01:12.721
things like GPQA, LabBench,

02:01:13.421 --> 02:01:15.451
or MMLU Pro, things like

02:01:15.451 --> 02:01:17.521
that. That. However, like, if

02:01:17.521 --> 02:01:19.601
you want to evaluate LLMs

02:01:19.601 --> 02:01:20.751
very I

02:01:23.041 --> 02:01:25.461
it it's it becomes very expensive. Like,

02:01:25.671 --> 02:01:27.951
for example, if you get

02:01:27.951 --> 02:01:30.061
a broad set of these benchmarks,

02:01:30.301 --> 02:01:33.180
for for even a Gemini 2.5

02:01:33.181 --> 02:01:35.481
model, if you want to evaluate that, you're

02:01:35.481 --> 02:01:38.041
looking into about 300,

02:01:39.146 --> 02:01:41.321
$136,100 dollars,

02:01:41.721 --> 02:01:44.091
of expense. Just around one model.

02:01:44.091 --> 02:01:46.201
You can imagine that, like, if you have

02:01:46.201 --> 02:01:47.959
a lot of more models than the,

02:01:48.481 --> 02:01:50.571
cost accumulate

02:01:50.571 --> 02:01:52.600
very quick. So we

02:01:52.600 --> 02:01:54.901
wanted to see how we can still

02:01:54.901 --> 02:01:57.061
do broad, benchmarking across a lot

02:01:57.061 --> 02:01:59.081
of tasks, but make it efficient.

02:01:59.721 --> 02:02:01.861
So we manually looked into a lot of these

02:02:02.421 --> 02:02:04.411
datasets and, only looked at

02:02:04.651 --> 02:02:06.351
those that are actually reasoning

02:02:06.811 --> 02:02:09.001
intensive. Then from each side,

02:02:09.001 --> 02:02:10.660
they set the sample, a smaller set of

02:02:12.171 --> 02:02:14.281
instances. Without hurting

02:02:14.660 --> 02:02:16.781
the, the confidence of

02:02:16.781 --> 02:02:19.061
evaluation. So, this

02:02:19.061 --> 02:02:20.741
basically allows us to

02:02:21.381 --> 02:02:23.121
cut the evaluation cost to one

02:02:23.601 --> 02:02:25.701
one third, to a half

02:02:26.020 --> 02:02:28.520
And then this doesn't reduce the evaluation

02:02:28.821 --> 02:02:31.271
confidence at all. So we call this sires.

02:02:31.271 --> 02:02:32.611
It's not like a new,

02:02:34.491 --> 02:02:36.431
like a newly annotated benchmark.

02:02:36.651 --> 02:02:38.931
It's just an effort to

02:02:38.931 --> 02:02:40.966
unify benchmarks

02:02:41.401 --> 02:02:43.631
And it's it is a spanning across

02:02:43.631 --> 02:02:46.051
multiple tasks, including general science reasoning,

02:02:46.111 --> 02:02:48.341
physics, focus, general instruction

02:02:48.660 --> 02:02:49.781
following for science, and,

02:02:50.701 --> 02:02:52.311
some broader subject like exam.

02:02:53.751 --> 02:02:56.171
So here are the performance. I

02:02:56.311 --> 02:02:58.386
I this is when you're looking

02:02:58.386 --> 02:03:00.390
into performance, you should always, like,

02:03:00.581 --> 02:03:02.760
consider the cost as well because we are

02:03:02.761 --> 02:03:04.921
talking about reasoning models. You can

02:03:04.921 --> 02:03:07.071
see here that, like, the best performances

02:03:07.290 --> 02:03:09.491
are for, reasoning models

02:03:09.491 --> 02:03:11.561
at the time, like Gemini point five pro or

02:03:11.561 --> 02:03:12.991
three and GPT five.

02:03:13.790 --> 02:03:15.951
But then the question I've even more interested in is,

02:03:15.951 --> 02:03:18.027
like, if we increase the reasoning effort,

02:03:18.028 --> 02:03:20.001
how does the performance change?

02:03:20.761 --> 02:03:22.971
And, can high reasoning effort

02:03:22.971 --> 02:03:25.035
actually improve performance? And this is

02:03:25.036 --> 02:03:27.410
not always the case it's very interesting

02:03:27.410 --> 02:03:29.751
to look at some of these models. For example, Gemini

02:03:29.751 --> 02:03:32.241
2.5 pro, even if you,

02:03:32.701 --> 02:03:34.241
10 x the reasoning effort,

02:03:34.741 --> 02:03:36.881
are not getting much out of it. And

02:03:36.881 --> 02:03:38.711
the performance doesn't improve much.

02:03:39.031 --> 02:03:40.731
Whereas for some other models like

02:03:41.671 --> 02:03:43.711
like o three, increased

02:03:43.931 --> 02:03:45.921
reasoning effort actually

02:03:46.271 --> 02:03:48.290
translates into increased performance. So

02:03:48.291 --> 02:03:50.281
this benchmark actually allows us to

02:03:50.441 --> 02:03:52.600
kind of see across the board, across a lot

02:03:52.600 --> 02:03:54.801
of different how efficient is

02:03:54.801 --> 02:03:56.961
as as cost efficient and

02:03:56.961 --> 02:03:58.521
capable. Are these models?

02:03:59.881 --> 02:04:01.941
And then here's another view of

02:04:01.941 --> 02:04:04.151
the the

02:04:04.151 --> 02:04:06.651
the the benefits that you can get from these recent

02:04:06.711 --> 02:04:08.791
models. Can see that, like,

02:04:09.231 --> 02:04:11.270
ideally, you wanna see the

02:04:11.921 --> 02:04:14.031
dots to be on upper

02:04:14.031 --> 02:04:16.131
left side of this graph. And you don't see

02:04:16.131 --> 02:04:18.451
that for all models, but for o three mini

02:04:18.451 --> 02:04:20.571
or like o three

02:04:20.571 --> 02:04:22.771
in general, you better performance.

02:04:23.011 --> 02:04:24.321
When you increase the

02:04:25.041 --> 02:04:27.201
reasoning effort. And then you might

02:04:27.201 --> 02:04:29.411
ask, okay. Why do I care about holistic performance?

02:04:29.551 --> 02:04:31.721
Maybe I just take two benchmarks that everyone

02:04:31.721 --> 02:04:33.821
evaluates. On. So we did

02:04:33.961 --> 02:04:36.201
a correlation analysis, and this actually shows that

02:04:36.201 --> 02:04:38.421
you kind of need this, type of

02:04:38.641 --> 02:04:40.316
holistic evaluation to

02:04:40.891 --> 02:04:43.381
tell you, like, in general, how good your model

02:04:43.621 --> 02:04:46.051
is at reasoning in knowledge intensive and environment.

02:04:46.291 --> 02:04:48.611
Because like on some data sets, it doesn't

02:04:48.611 --> 02:04:50.830
actually correlate with

02:04:50.831 --> 02:04:53.121
other data sets. And we looked at this

02:04:53.341 --> 02:04:55.441
more closely And, what's interesting

02:04:55.441 --> 02:04:57.500
is that we found that a lot of

02:04:58.461 --> 02:05:00.801
different models are tuned towards different

02:05:00.801 --> 02:05:02.981
datasets. For example, here we found

02:05:02.981 --> 02:05:04.991
that corn three has

02:05:05.211 --> 02:05:07.201
very high benchmarked

02:05:07.421 --> 02:05:09.731
scores for side bench. Whereas

02:05:09.791 --> 02:05:11.991
some other model like

02:05:11.991 --> 02:05:14.389
DeepSeq r one has very highest scores

02:05:14.390 --> 02:05:15.840
for MMLU pro,

02:05:16.401 --> 02:05:18.881
versus the average score at from other

02:05:18.916 --> 02:05:20.991
benchmarks. So it seems like this

02:05:20.991 --> 02:05:23.341
type of holistic benchmarking

02:05:23.341 --> 02:05:25.491
can tell tell us more about general

02:05:25.491 --> 02:05:27.171
reason capabilities of model.

02:05:28.531 --> 02:05:30.471
And this is, again, the performance

02:05:30.611 --> 02:05:32.271
cost trade For some models, you

02:05:32.751 --> 02:05:34.831
gain performance by paying more. But for some

02:05:34.831 --> 02:05:36.881
models, like this green bar,

02:05:36.881 --> 02:05:39.021
I think it's Gemini model. That

02:05:39.021 --> 02:05:40.581
you don't get much, input

02:05:42.561 --> 02:05:44.801
and this is performance improvements for reasoning

02:05:44.801 --> 02:05:46.931
models for math versus non math.

02:05:47.491 --> 02:05:49.631
Instances. And, as is

02:05:49.631 --> 02:05:51.910
also, reported in other

02:05:51.910 --> 02:05:54.361
papers, we see that, vesamil

02:05:54.421 --> 02:05:56.231
usually helps math

02:05:56.556 --> 02:05:58.881
questions more than other

02:05:58.881 --> 02:05:59.941
non math questions.

02:06:01.301 --> 02:06:03.381
The the next question we wanted to

02:06:03.381 --> 02:06:05.591
actually look, in

02:06:05.591 --> 02:06:07.691
this work is, how how is the

02:06:07.691 --> 02:06:09.861
interplay between knowledge

02:06:09.861 --> 02:06:11.911
and reasoning scientific problem solving, and where

02:06:11.911 --> 02:06:12.991
are the actual bottom

02:06:14.031 --> 02:06:16.350
lines. So in order to study this, because every

02:06:16.350 --> 02:06:18.480
model is trained on different data,

02:06:18.481 --> 02:06:20.560
we wanted to have a controlled setup. So

02:06:20.560 --> 02:06:22.851
we took, a quantum five

02:06:22.946 --> 02:06:24.981
model that is a

02:06:24.981 --> 02:06:27.241
base model and not reasonably trained,

02:06:27.621 --> 02:06:29.861
And then we wanted to control for in domain

02:06:29.861 --> 02:06:31.941
knowledge training. We train it ourselves,

02:06:32.511 --> 02:06:34.931
using back reasoning training on these three datasets.

02:06:36.101 --> 02:06:38.421
Math, stem, and synthetic one.

02:06:38.421 --> 02:06:40.200
Synthetic one is a dataset that

02:06:40.841 --> 02:06:42.921
other people have collected that has a

02:06:42.921 --> 02:06:44.930
long chain of thoughts, and

02:06:44.931 --> 02:06:47.091
it is distilled. I think it is this is from

02:06:47.091 --> 02:06:48.021
deep sea color.

02:06:49.621 --> 02:06:51.701
So we wanted to see if our own training

02:06:51.701 --> 02:06:53.705
actually, replicate replicate this. State of

02:06:53.706 --> 02:06:55.831
the art. And this was just a sanity

02:06:55.831 --> 02:06:58.060
check, and we was we were saying that our check

02:06:58.061 --> 02:07:00.351
points actually can match the performance of

02:07:01.071 --> 02:07:03.321
checkpoints that, exist out there and other

02:07:03.321 --> 02:07:04.701
people have reported on.

02:07:05.741 --> 02:07:07.979
So now we wanted to see how you can, like,

02:07:07.980 --> 02:07:10.011
separate or, like, kind of study knowledge

02:07:10.011 --> 02:07:12.321
versus reasoning. We introduced this,

02:07:12.561 --> 02:07:14.741
simple framework we call it

02:07:14.741 --> 02:07:16.250
CROX or knowledge and reason

02:07:17.611 --> 02:07:19.471
exam, which allows us to investigate,

02:07:21.311 --> 02:07:23.390
knowledge the role of knowledge in large

02:07:23.391 --> 02:07:25.441
chain of reasoning. So

02:07:25.441 --> 02:07:27.601
it works like this. You have a

02:07:27.601 --> 02:07:29.801
a question, so

02:07:30.041 --> 02:07:32.301
so we want to know what are the atomic

02:07:32.441 --> 02:07:33.421
knowledge units.

02:07:34.651 --> 02:07:36.831
Or knowledge ingredients, that are

02:07:37.391 --> 02:07:38.811
useful to answer this question.

02:07:39.691 --> 02:07:41.711
So we we kind of, extract

02:07:41.851 --> 02:07:43.860
these using a strong model of deep

02:07:44.421 --> 02:07:46.531
c CARBON. Basically, we generate the full

02:07:46.531 --> 02:07:48.631
chain of thought. We ask, deep sea carbon

02:07:48.771 --> 02:07:50.830
to extract this atomic knowledge unit. We

02:07:50.830 --> 02:07:53.210
call them knowledge ingredients

02:07:53.210 --> 02:07:55.301
or k i's in short. And then then

02:07:55.301 --> 02:07:57.328
we want to solve the question, we

02:07:57.328 --> 02:07:59.541
gave the question and these KIs

02:07:59.541 --> 02:08:01.831
or knowledge ingredients to another

02:08:01.831 --> 02:08:03.971
model to see if we can utilize these

02:08:04.031 --> 02:08:06.060
knowledge ingredients and solve the

02:08:06.060 --> 02:08:08.099
question. So the first research

02:08:08.100 --> 02:08:10.051
question using this framework is,

02:08:10.520 --> 02:08:12.561
can a base model that is

02:08:12.561 --> 02:08:14.641
not reason trained actually

02:08:14.641 --> 02:08:16.721
just use these knowledge ingredients and get

02:08:16.721 --> 02:08:18.841
to the answer? And the answer

02:08:18.841 --> 02:08:20.921
is actually yes, like we we we

02:08:20.921 --> 02:08:23.171
tested this. Like, the blue bar you see here

02:08:23.171 --> 02:08:25.351
is a base model. This is not reasoning

02:08:25.870 --> 02:08:28.030
for strength. And you can see that if you just give

02:08:28.031 --> 02:08:30.381
it these knowledge ingredients, it get huge

02:08:30.381 --> 02:08:32.741
improvements on

02:08:32.801 --> 02:08:34.901
different tasks. And the the other

02:08:34.901 --> 02:08:37.100
colors are reasoning post training.

02:08:38.061 --> 02:08:40.221
Models. So this basically shows that, like,

02:08:40.221 --> 02:08:42.001
actually high quality knowledge

02:08:42.641 --> 02:08:45.021
is important for models to solve the task.

02:08:45.821 --> 02:08:47.660
There's some caveat in these results,

02:08:48.811 --> 02:08:50.861
Some models like one two point five

02:08:52.211 --> 02:08:54.530
who knows what they use for their data? But

02:08:54.531 --> 02:08:56.571
there's some times that they might have some

02:08:56.811 --> 02:08:58.511
reasoning data in their mid training.

02:08:58.841 --> 02:09:01.031
So this kind of is not like

02:09:01.031 --> 02:09:03.041
a base based model. This kind of like

02:09:03.041 --> 02:09:05.070
a lightweight version

02:09:05.071 --> 02:09:06.611
of a reasoning.

02:09:07.491 --> 02:09:09.651
Model. But the second question we wanted

02:09:09.651 --> 02:09:11.591
to see if, like, does

02:09:11.811 --> 02:09:13.910
this same ingredients actually help,

02:09:15.681 --> 02:09:17.791
reasoning models as well. So

02:09:17.791 --> 02:09:20.111
the the format is like this. Basically, we extract

02:09:20.111 --> 02:09:21.881
these KIs, we give it to,

02:09:22.311 --> 02:09:24.631
the models like these are different models, base model

02:09:24.631 --> 02:09:26.856
or math train model or a stem

02:09:26.856 --> 02:09:29.040
post. Model are both them. And

02:09:29.041 --> 02:09:31.146
you see across the board that this type

02:09:31.146 --> 02:09:33.261
of knowledge ingredients help

02:09:33.660 --> 02:09:35.751
all their models. Both post

02:09:35.811 --> 02:09:37.601
trained models and the base models.

02:09:38.561 --> 02:09:40.941
And the improvements are actually consistent

02:09:41.661 --> 02:09:43.171
those. So, basically,

02:09:43.901 --> 02:09:45.831
this shows a practical path

02:09:46.071 --> 02:09:48.230
If we want, to improve models, we

02:09:48.231 --> 02:09:50.611
we need reason models, and we need high quality

02:09:50.611 --> 02:09:52.681
knowledge, ingredients. And

02:09:52.681 --> 02:09:54.790
you might ask, okay, well, where do I get knowledge in

02:09:55.971 --> 02:09:58.331
because yeah, I don't know

02:09:58.391 --> 02:10:00.731
where to search like this. This can be just retrieval

02:10:00.871 --> 02:10:01.611
augmented system.

02:10:02.991 --> 02:10:05.091
There might be some caveats about this result.

02:10:05.811 --> 02:10:07.901
You might say, okay. Some of these improvement might

02:10:08.631 --> 02:10:10.711
like, this knowledge in defense how do you

02:10:10.711 --> 02:10:12.951
know that they are not kind of narrowing the search space

02:10:12.951 --> 02:10:14.671
for the reasoning models.

02:10:15.361 --> 02:10:17.440
So, I'm going to talk about

02:10:17.441 --> 02:10:18.131
that now.

02:10:19.571 --> 02:10:21.711
So the next question we wanted to ask is, like,

02:10:21.951 --> 02:10:24.160
does reasoning focus post training

02:10:24.161 --> 02:10:26.421
help the model better surface parametric

02:10:26.561 --> 02:10:28.561
knowledge? So,

02:10:29.041 --> 02:10:31.151
to study this, we have this set

02:10:31.211 --> 02:10:33.541
of extracted knowledge ingredients again.

02:10:34.491 --> 02:10:36.810
But then we, we post

02:10:36.811 --> 02:10:38.911
train different models based

02:10:38.911 --> 02:10:41.011
on different data and we

02:10:41.011 --> 02:10:42.871
extract these knowledge ingredients

02:10:43.461 --> 02:10:45.610
from our own post trained models.

02:10:46.011 --> 02:10:47.870
And then we give these knowledge ingredients

02:10:48.091 --> 02:10:50.441
to a base model to see

02:10:50.441 --> 02:10:52.481
what knowledge this type of

02:10:52.481 --> 02:10:54.641
post training exposes to model.

02:10:55.041 --> 02:10:57.461
And in order to control for knowledge injection,

02:10:57.601 --> 02:10:59.561
we just look at the math. Data.

02:10:59.951 --> 02:11:01.981
So basically, we post train a

02:11:01.981 --> 02:11:04.191
model on math only. And

02:11:04.191 --> 02:11:06.221
then we extract the knowledge in GDS give

02:11:06.221 --> 02:11:08.461
it to a base model, to see how the

02:11:08.461 --> 02:11:09.121
performance changes.

02:11:10.761 --> 02:11:13.000
And then here's the results. You can see

02:11:13.000 --> 02:11:15.081
that even if train

02:11:15.081 --> 02:11:16.761
the model only on math,

02:11:17.640 --> 02:11:19.660
and then you extract these knowledge ingredients,

02:11:20.521 --> 02:11:22.901
it can still help improve

02:11:22.961 --> 02:11:25.401
the results. On on

02:11:25.621 --> 02:11:27.811
non math task in GPQI and

02:11:27.811 --> 02:11:29.991
MLU Pro. And, this is

02:11:29.991 --> 02:11:32.051
a very interesting finding. Me and I would like to

02:11:32.291 --> 02:11:34.411
type of reasoning training with long channel

02:11:34.411 --> 02:11:36.471
thought is like, helps us basically

02:11:36.471 --> 02:11:38.591
surface the knowledge that

02:11:38.671 --> 02:11:40.151
the model already has.

02:11:40.871 --> 02:11:42.971
We wanted to make sure that this type of

02:11:43.211 --> 02:11:45.290
post training is not injecting new

02:11:45.290 --> 02:11:47.301
knowledge to the model So

02:11:47.301 --> 02:11:49.661
we basically did some probing tests

02:11:50.301 --> 02:11:52.541
and then saw that actually, you know, the performance

02:11:52.541 --> 02:11:54.971
of the post train model, and

02:11:55.111 --> 02:11:57.231
the, base model are

02:11:57.231 --> 02:11:59.411
the same in terms of, like, their knowledge in

02:11:59.731 --> 02:12:00.971
non math. Type. So,

02:12:01.830 --> 02:12:04.210
basically, the key findings here is that

02:12:04.211 --> 02:12:06.656
this type of post training

02:12:07.911 --> 02:12:10.151
reasoning focused post training is helping the model

02:12:10.151 --> 02:12:12.441
surface the knowledge that they already have.

02:12:14.201 --> 02:12:16.350
So to recap, we we need

02:12:16.350 --> 02:12:18.591
a comprehensive benchmarking to better

02:12:18.591 --> 02:12:20.611
understand scientific problem

02:12:20.611 --> 02:12:22.691
solvings. And, we we

02:12:22.691 --> 02:12:24.531
find that models a lot of models are

02:12:24.866 --> 02:12:27.121
bottlenecked by knowledge. It's not the reasoning

02:12:27.421 --> 02:12:29.121
effort that always give you performance

02:12:29.661 --> 02:12:31.991
improvements. So it's, depending

02:12:31.991 --> 02:12:33.291
on your, tasks,

02:12:34.001 --> 02:12:36.241
it's good to consider retrieval augmented

02:12:36.241 --> 02:12:38.171
SIS. Systems. And,

02:12:38.410 --> 02:12:40.591
when knowledge is missing or hard to

02:12:40.651 --> 02:12:43.051
retrieve, scaling chain of thought

02:12:43.271 --> 02:12:45.331
alone is not sufficient. So

02:12:45.331 --> 02:12:47.751
hopefully, this gives, some, insights

02:12:47.890 --> 02:12:50.001
to the community when they're,

02:12:50.001 --> 02:12:51.871
like, developing new reasoning models

02:12:52.701 --> 02:12:54.781
to see how we can improve them in this type

02:12:54.781 --> 02:12:56.401
of knowledge intensive environment.

02:12:58.241 --> 02:13:00.531
And then the practical implication is

02:13:00.531 --> 02:13:02.532
that the good mechanisms to supply

02:13:02.532 --> 02:13:04.533
or retrieve the right knowledge can

02:13:04.534 --> 02:13:06.581
help us get better

02:13:06.581 --> 02:13:08.601
reasoning system. So thank

02:13:08.601 --> 02:13:10.761
you so much. I wanna also thank my students

02:13:10.761 --> 02:13:11.341
and collaborators.

02:13:20.031 --> 02:13:22.201
We will have a few

02:13:22.421 --> 02:13:23.961
question time and

02:13:24.830 --> 02:13:26.760
everyone who are interested for the talk.

02:13:27.071 --> 02:13:29.371
We're welcome to ask questions

02:13:29.371 --> 02:13:30.111
for Perf Ahmed.

02:13:41.161 --> 02:13:43.311
Hi. Thanks for the wonderful talk. I

02:13:43.311 --> 02:13:44.931
had a question on the first

02:13:45.731 --> 02:13:47.991
like section on meta evaluators

02:13:48.211 --> 02:13:49.991
of like, LLM as a judge.

02:13:50.471 --> 02:13:52.870
Have you looked at like, if you increase

02:13:52.870 --> 02:13:55.070
the reasoning level like does that

02:13:55.071 --> 02:13:57.171
actually improve LLM

02:13:57.171 --> 02:13:59.330
as a judge performance? I know you did it in the second

02:13:59.330 --> 02:14:01.301
one, but, like, for LLM as a judge, does

02:14:01.541 --> 02:14:03.941
like, improve? Yeah. That that's a good question. Like,

02:14:04.181 --> 02:14:06.421
that wasn't our focus in that. Four.

02:14:06.421 --> 02:14:08.721
But there are other works

02:14:08.721 --> 02:14:09.851
and people have studied

02:14:11.211 --> 02:14:13.231
reasoning models as judges, and they usually

02:14:13.301 --> 02:14:15.631
see improvements there too. Yeah.

02:14:16.251 --> 02:14:17.231
A good question.

02:14:19.160 --> 02:14:21.241
Okay. I have a similar question that

02:14:21.241 --> 02:14:23.331
is about prompt. Yeah.

02:14:23.811 --> 02:14:26.081
As I evaluate benchmark

02:14:26.561 --> 02:14:28.641
in the model, I found that a little slight

02:14:28.641 --> 02:14:30.180
change for a prompt will

02:14:30.781 --> 02:14:32.901
significantly affect the final

02:14:32.901 --> 02:14:34.861
performance and how do you think that

02:14:35.501 --> 02:14:37.681
what's the correct prompt for best evaluation

02:14:37.901 --> 02:14:39.041
or correct evaluation?

02:14:40.251 --> 02:14:42.440
Yeah. That's a good question. Look, the

02:14:42.441 --> 02:14:43.690
prompt that is used if

02:14:44.621 --> 02:14:46.849
the LLM judge performance quite

02:14:46.850 --> 02:14:49.021
a bit. Other people have

02:14:49.021 --> 02:14:51.231
a studied this. They actually also have a

02:14:51.231 --> 02:14:53.091
previous 40 studying that. But

02:14:53.310 --> 02:14:55.451
here, we fixed the prompt so that we

02:14:55.451 --> 02:14:57.640
can basically have a controlled evaluation,

02:14:57.861 --> 02:14:59.841
and we use the same prompt that

02:15:00.561 --> 02:15:02.901
the original dataset, like ALPACA

02:15:03.041 --> 02:15:04.601
e values. Okay.

02:15:05.291 --> 02:15:07.451
Thanks. We will for time leaving, I

02:15:07.451 --> 02:15:09.700
have one more question, and thanks,

02:15:09.701 --> 02:15:11.941
please. Thank you. Nice talk. Nice

02:15:11.941 --> 02:15:14.110
work. I have a question about

02:15:14.111 --> 02:15:16.321
the second part where specialized

02:15:16.321 --> 02:15:18.501
knowledge is being used to improve the utility.

02:15:19.041 --> 02:15:19.541
Right?

02:15:21.171 --> 02:15:22.711
What is the story for

02:15:23.271 --> 02:15:23.871
efficient reasoning?

02:15:25.711 --> 02:15:27.731
So so the story there is like,

02:15:27.871 --> 02:15:30.106
it you want to like,

02:15:30.751 --> 02:15:33.131
because a lot of times, you just throwing,

02:15:34.311 --> 02:15:36.331
inference on computes to

02:15:36.391 --> 02:15:38.321
improve, performance on tasks.

02:15:38.640 --> 02:15:40.721
This basically tells you, like, if

02:15:40.721 --> 02:15:42.961
the model doesn't have the knowledge, no

02:15:42.961 --> 02:15:45.181
matter how much, reasoning

02:15:45.241 --> 02:15:47.306
you put into it, it doesn't help

02:15:47.471 --> 02:15:49.561
and then in lot of tasks, the basic

02:15:49.635 --> 02:15:51.911
you're basically you're can get very good results

02:15:51.911 --> 02:15:53.971
by good retrieval setup. You

02:15:53.971 --> 02:15:56.081
don't necessarily to have a lot of

02:15:56.081 --> 02:15:58.300
like, reasoning. Okay.

02:15:58.301 --> 02:15:59.381
Thank you. Thank you.

02:16:03.571 --> 02:16:05.830
Thanks everyone and we are happy

02:16:06.131 --> 02:16:08.001
to honor to have Amir

02:16:08.381 --> 02:16:10.771
Khan here for the remarkable presentation.

02:16:11.661 --> 02:16:12.161
And

02:16:16.121 --> 02:16:18.321
I think the the next should be

02:16:18.321 --> 02:16:20.431
the oral presentation. And we will

02:16:20.431 --> 02:16:22.560
have two of the following

02:16:23.021 --> 02:16:25.461
oral presenter. To present their

02:16:25.461 --> 02:16:27.781
work. One the first one will be

02:16:28.101 --> 02:16:29.651
let me check. Sorry.

02:16:30.171 --> 02:16:30.671
Yeah.

02:16:36.400 --> 02:16:38.501
Yeah. Yeah. The first all in talk will

02:16:38.501 --> 02:16:40.601
be M1 towards scaling test

02:16:41.001 --> 02:16:43.441
time compute with MAMBA reasoning model.

02:16:43.841 --> 02:16:46.081
Yeah. Here comes our

02:16:46.081 --> 02:16:47.400
presenter for this work.

02:17:38.971 --> 02:17:39.471
Okay.

02:18:00.056 --> 02:18:00.521
Next

02:18:19.811 --> 02:18:21.772
So yeah. Let's start

02:18:22.081 --> 02:18:24.380
Okay. Yeah. So

02:18:24.381 --> 02:18:26.722
hello everyone. My name is Xun Shung.

02:18:27.790 --> 02:18:30.270
Sorry. I'm I'm a researcher in Together

02:18:30.271 --> 02:18:32.691
AI, and today I'm going to present my old paper.

02:18:33.301 --> 02:18:35.461
Towards scalable test time compute

02:18:35.461 --> 02:18:36.961
with one more reasoning models.

02:18:37.601 --> 02:18:39.840
Okay. So let's start with a quick

02:18:39.841 --> 02:18:42.021
trend that we have seen in the recent

02:18:42.421 --> 02:18:44.091
in the community. So this

02:18:44.650 --> 02:18:46.470
chart shows the AME accuracy.

02:18:46.870 --> 02:18:49.350
Versus compute during our training and compute

02:18:49.351 --> 02:18:51.351
during our test time. So the takeaway

02:18:51.411 --> 02:18:53.571
here is that if you allow the model to sync

02:18:53.571 --> 02:18:55.431
longer or sample more,

02:18:55.990 --> 02:18:58.150
and then you do the majority voting, you can get

02:18:58.150 --> 02:19:00.541
big gains So especially for those

02:19:00.541 --> 02:19:02.521
reasoning tasks, Yeah.

02:19:02.581 --> 02:19:04.982
So test time compute reflects

02:19:05.041 --> 02:19:05.861
better accuracy.

02:19:07.780 --> 02:19:10.200
So and we already seen many literature

02:19:10.961 --> 02:19:13.041
that linear models like mobile and hybrid

02:19:13.041 --> 02:19:14.660
models can have, like, great

02:19:15.701 --> 02:19:18.071
throughput. So they are much faster efficient

02:19:18.071 --> 02:19:20.232
than standard transformers, especially for

02:19:20.232 --> 02:19:22.071
the long sequences.

02:19:22.711 --> 02:19:25.001
However, they are worse

02:19:25.820 --> 02:19:27.521
like, in terms of the quality

02:19:28.111 --> 02:19:30.300
like, sometimes for the resume tasks,

02:19:30.860 --> 02:19:33.261
So the question that we are going to answer

02:19:33.261 --> 02:19:35.319
in this paper is that so under

02:19:35.320 --> 02:19:36.621
the same compute budget,

02:19:37.501 --> 02:19:39.741
can Mama and Stella reasoning models

02:19:39.741 --> 02:19:41.281
can match or beat transformers

02:19:43.431 --> 02:19:45.591
So the the motivation is that we can

02:19:45.831 --> 02:19:48.151
if those models are larger if those

02:19:48.151 --> 02:19:50.321
models are faster, we can generate

02:19:50.321 --> 02:19:51.976
more samples and fix

02:19:52.611 --> 02:19:54.931
computer budget. So it we we probably

02:19:54.931 --> 02:19:56.701
can get like,

02:19:57.102 --> 02:19:59.181
result by using the emergency voting.

02:19:59.981 --> 02:20:02.061
So then the overall accuracy can be

02:20:02.061 --> 02:20:02.551
improved

02:20:05.271 --> 02:20:07.570
Yeah. So as that as

02:20:07.570 --> 02:20:09.831
that time, there are some several challenges. The first

02:20:09.831 --> 02:20:11.671
is that there is no available,

02:20:11.971 --> 02:20:14.186
like, hybrid hybrid models.

02:20:14.431 --> 02:20:16.511
So this paper is, is just written

02:20:16.511 --> 02:20:18.591
after the the deep stake AI model.

02:20:18.591 --> 02:20:20.511
So there is no

02:20:20.731 --> 02:20:22.900
available reasoning models at that time. And, also,

02:20:22.900 --> 02:20:24.021
the second is that

02:20:24.981 --> 02:20:26.501
so we have seen some approach like,

02:20:27.860 --> 02:20:29.951
or, like, Lambda the

02:20:29.951 --> 02:20:31.461
distilled from general models, like

02:20:32.401 --> 02:20:34.181
like, lambda lambda based models.

02:20:34.431 --> 02:20:36.511
But those models, performs better

02:20:36.511 --> 02:20:38.271
on those, reasoning task

02:20:38.591 --> 02:20:40.761
So typically, they only get the

02:20:41.721 --> 02:20:43.821
40 to 50 for the

02:20:43.821 --> 02:20:45.441
mass 500 tasks.

02:20:46.651 --> 02:20:47.631
Yeah. So,

02:20:48.691 --> 02:20:50.852
now I want to introduce this framework that

02:20:50.852 --> 02:20:52.932
we call the Mama in the Lamma framework.

02:20:52.932 --> 02:20:55.071
So it has a three stage pipelines.

02:20:55.951 --> 02:20:58.111
So first is that we start from

02:20:58.961 --> 02:21:01.201
a coherent transformer models, and we want to try

02:21:01.201 --> 02:21:03.650
to reuse the projector layers from these

02:21:03.870 --> 02:21:06.290
transformers. And then initialize those corresponding,

02:21:07.181 --> 02:21:09.291
like, projections

02:21:09.432 --> 02:21:11.511
from the and this, metrics from

02:21:11.512 --> 02:21:13.941
the to the mobile layers.

02:21:15.482 --> 02:21:17.181
And different from the

02:21:17.691 --> 02:21:19.631
the memo in the paper is that we use

02:21:19.771 --> 02:21:21.831
some mass corpus, which

02:21:21.831 --> 02:21:24.071
we call the open open math instruct, and then

02:21:24.071 --> 02:21:25.711
we do the reverse

02:21:26.171 --> 02:21:28.611
KOD distillation instead of the for forward

02:21:28.671 --> 02:21:30.719
KL distillation. Because we find that rewards

02:21:30.720 --> 02:21:32.720
KL is actually performance better. So

02:21:33.069 --> 02:21:35.164
and when we train, ten

02:21:35.164 --> 02:21:37.251
ten billion tokens And

02:21:37.251 --> 02:21:39.571
secondly second stage is that we do

02:21:39.571 --> 02:21:41.481
the supervised fine tuning

02:21:41.780 --> 02:21:43.781
on those reasoning corpus, like,

02:21:44.031 --> 02:21:46.021
so this this dataset is

02:21:46.820 --> 02:21:48.921
generated by the like,

02:21:48.982 --> 02:21:50.991
using the DeepSpeak r

02:21:50.991 --> 02:21:52.771
one and then the r one distillate models.

02:21:54.051 --> 02:21:56.131
The third stage is that we apply the

02:21:56.131 --> 02:21:58.601
GRPO to on the deep scale

02:21:58.601 --> 02:22:01.001
dataset, using

02:22:01.001 --> 02:22:03.150
After SMP models. So, actually,

02:22:03.150 --> 02:22:05.231
there are some, like, numerical issues because

02:22:05.231 --> 02:22:07.181
if you are familiar with like,

02:22:07.481 --> 02:22:09.580
linear like, like,

02:22:09.580 --> 02:22:11.740
one of my models, they have the different kernels in

02:22:11.740 --> 02:22:13.801
the training and in the generation. So

02:22:13.801 --> 02:22:15.961
there is a gen training and inference

02:22:15.961 --> 02:22:17.871
mismatch, and we spend some time to

02:22:18.811 --> 02:22:20.721
track all these system issues.

02:22:22.641 --> 02:22:24.461
Yeah. So, yeah,

02:22:24.781 --> 02:22:27.201
And, here is a zero shot result.

02:22:28.301 --> 02:22:30.481
Like, we test on those most popular

02:22:30.541 --> 02:22:32.671
math benchmarks, and then basically, we

02:22:32.671 --> 02:22:34.932
can see that we match the

02:22:34.932 --> 02:22:36.751
performance of the deep search

02:22:37.146 --> 02:22:39.221
Distilled like, 1.5

02:22:39.221 --> 02:22:41.461
clean models. Right? So the DeepSick Distilled

02:22:41.461 --> 02:22:43.561
15 model is fine tuning

02:22:43.561 --> 02:22:45.921
from the quid and using

02:22:46.141 --> 02:22:48.471
the same copper like, the similar copper, the deep

02:22:48.471 --> 02:22:50.781
DeepSpeak R1, distill corporates.

02:22:51.591 --> 02:22:52.091
So, yeah,

02:22:53.611 --> 02:22:55.691
so you'll see this blue, blue

02:22:55.691 --> 02:22:57.892
bar is at very competitive with

02:22:57.892 --> 02:23:00.051
this query. Yeah. I

02:23:00.051 --> 02:23:02.290
think it's the first sign that we have seen the

02:23:02.290 --> 02:23:04.671
good reasoning qualities

02:23:04.671 --> 02:23:06.531
while using alternative architectures

02:23:07.261 --> 02:23:09.441
and hybrid models In Yeah.

02:23:09.441 --> 02:23:11.421
So let's talk about speed here.

02:23:12.301 --> 02:23:14.451
So we benchmark our

02:23:14.511 --> 02:23:16.751
models, you compile with

02:23:16.751 --> 02:23:18.861
the the lama, like, three b models,

02:23:18.861 --> 02:23:19.861
and also the

02:23:20.901 --> 02:23:23.291
the Quinn models. Right? So we we use

02:23:23.431 --> 02:23:25.621
a a VRM, like to serve these

02:23:25.621 --> 02:23:26.791
transformer models, but

02:23:27.831 --> 02:23:29.530
so at that time, there is no, like,

02:23:30.331 --> 02:23:32.721
the VOM is, like, the hybrid models

02:23:32.721 --> 02:23:34.881
haven't been implemented good.

02:23:34.881 --> 02:23:36.991
So we just the, like,

02:23:37.631 --> 02:23:39.870
standard generation using the CUDA graph.

02:23:40.341 --> 02:23:42.231
For for our approach. So,

02:23:42.851 --> 02:23:45.312
the takeaway here is that I

02:23:46.281 --> 02:23:48.321
umbrella, like, our ROI

02:23:48.321 --> 02:23:50.631
is like, two two to three times

02:23:50.631 --> 02:23:52.931
faster for the very large batch size

02:23:52.931 --> 02:23:54.566
or for longer sequences.

02:23:55.982 --> 02:23:58.021
So Yeah. And

02:23:59.671 --> 02:24:02.071
now we are ready to answer this question. Like, given

02:24:02.451 --> 02:24:04.791
a fixed compute budget, how does the reasoning

02:24:04.791 --> 02:24:05.821
model compare with

02:24:06.861 --> 02:24:09.121
how does the hybrid model compare with the transformer

02:24:09.262 --> 02:24:11.472
models? So we fixed a number of generation

02:24:11.772 --> 02:24:13.861
tokens to eight k. So each model

02:24:13.861 --> 02:24:16.101
here is like, the generation lens is eight

02:24:16.101 --> 02:24:18.331
k, And then we

02:24:18.331 --> 02:24:20.651
vary the best size So

02:24:20.651 --> 02:24:22.891
so so the the figure in the left is showing

02:24:22.891 --> 02:24:25.160
that x axis is that the

02:24:25.160 --> 02:24:27.392
number of samples. Right? And then y axis is

02:24:27.392 --> 02:24:29.581
the majority voting scores. Using

02:24:29.767 --> 02:24:32.051
this number of samples. So as

02:24:32.051 --> 02:24:33.991
you see here, the transformer model is still

02:24:34.446 --> 02:24:36.732
the like, RNT still

02:24:36.732 --> 02:24:38.891
better compared with hybrid models. But

02:24:38.891 --> 02:24:40.831
if you count this for

02:24:41.291 --> 02:24:43.261
the if you normalize it for the

02:24:43.646 --> 02:24:45.791
speed, actually, the the

02:24:45.791 --> 02:24:48.221
hybrid models will be better. That's because

02:24:48.951 --> 02:24:51.110
so the how we run this experiment is

02:24:51.110 --> 02:24:53.182
that we we vary the

02:24:53.182 --> 02:24:55.371
number of best size like, from

02:24:55.371 --> 02:24:57.611
one to two and then to a very large numbers,

02:24:57.611 --> 02:24:59.931
and we get a optimal

02:25:00.551 --> 02:25:02.581
TPS for each of these

02:25:02.821 --> 02:25:05.046
models. And then we normalize by

02:25:05.046 --> 02:25:06.551
the its TPS.

02:25:08.751 --> 02:25:10.972
So the reason why we

02:25:10.972 --> 02:25:13.111
get a speed gains

02:25:13.111 --> 02:25:14.892
in the right figures is that,

02:25:15.352 --> 02:25:16.732
MRIs like two eyes,

02:25:17.450 --> 02:25:19.511
faster compared with deep search, distilled models.

02:25:19.831 --> 02:25:22.070
And, also, we run a similar experiment, but

02:25:22.070 --> 02:25:23.921
we focus on the length. Right? So

02:25:24.081 --> 02:25:26.581
basically, if you generate longer sequence

02:25:26.641 --> 02:25:28.711
days, the

02:25:29.651 --> 02:25:31.561
the benefit of these hybrid models

02:25:31.801 --> 02:25:33.880
can be better compared with transformer

02:25:33.881 --> 02:25:36.051
models. Yeah.

02:25:36.051 --> 02:25:37.991
So we have the same, conclusion

02:25:38.201 --> 02:25:40.341
here is that if you if

02:25:40.341 --> 02:25:42.601
you if you

02:25:42.660 --> 02:25:44.700
if your metric is like the

02:25:44.700 --> 02:25:46.780
generation NAND and then this accuracy is, like,

02:25:46.780 --> 02:25:48.830
slightly worse, But if you if

02:25:48.830 --> 02:25:50.931
you account for the the the time,

02:25:51.070 --> 02:25:53.061
it can be better. Yeah.

02:25:53.361 --> 02:25:55.301
So I also want to mention that

02:25:55.911 --> 02:25:58.071
the MOI, actually, approach has been scaled

02:25:58.071 --> 02:26:00.381
up by the this company. It's called

02:26:01.181 --> 02:26:03.041
ServiceNow. They actually train a

02:26:04.161 --> 02:26:05.381
16 I think

02:26:06.941 --> 02:26:09.200
yeah, the 15, 15 b models.

02:26:10.351 --> 02:26:12.501
To they use like, they distill, so they

02:26:12.501 --> 02:26:14.561
have a transformer model,

02:26:14.561 --> 02:26:16.731
which is which is this one, 15

02:26:16.731 --> 02:26:18.891
b synchre models. And then

02:26:18.891 --> 02:26:20.801
the they use our approach to

02:26:21.312 --> 02:26:23.651
remove as many transformer layers

02:26:24.086 --> 02:26:26.261
So so this is the the most,

02:26:26.661 --> 02:26:28.261
right one is the without

02:26:28.761 --> 02:26:30.921
any attention to

02:26:30.921 --> 02:26:32.411
yours. Right? And then this one is like

02:26:33.051 --> 02:26:35.052
have some attention to yours. And the

02:26:35.052 --> 02:26:37.191
the and this one is like, with

02:26:37.591 --> 02:26:39.642
three third of malware

02:26:39.701 --> 02:26:41.201
and one third of assault.

02:26:41.841 --> 02:26:43.941
Three fourths of attention

02:26:44.001 --> 02:26:46.241
layers and one one fourth of normal layers.

02:26:46.241 --> 02:26:48.400
And then if you do SFT, you can,

02:26:48.400 --> 02:26:50.511
like, the all this the the upper

02:26:50.511 --> 02:26:52.981
one is SFT model after doing this distillation.

02:26:53.111 --> 02:26:55.151
Yeah. So the yeah.

02:26:55.432 --> 02:26:57.772
And then the benchmark is like, this

02:26:58.331 --> 02:27:00.570
hybrid models and, the original transformer

02:27:00.570 --> 02:27:02.821
models, then they can show, like, a

02:27:03.281 --> 02:27:04.991
match on all all of those tasks.

02:27:06.591 --> 02:27:08.721
So Yeah. So, last thing

02:27:08.721 --> 02:27:11.151
I want to mention is that so nowadays,

02:27:11.151 --> 02:27:13.461
there are many, like, hybrid models

02:27:13.461 --> 02:27:15.541
and more accounts like Qoinix and

02:27:15.541 --> 02:27:16.841
Kimmy linear models.

02:27:17.771 --> 02:27:19.861
But there is one question. It's still

02:27:20.022 --> 02:27:21.341
I'm not so clear is that

02:27:22.381 --> 02:27:24.421
like, how does the speed compare with, like,

02:27:24.421 --> 02:27:26.731
the five let's say we have a five transformer

02:27:27.066 --> 02:27:29.261
optimizations, Like, by five

02:27:29.262 --> 02:27:31.411
transformer optimizations, I mean the technical,

02:27:31.411 --> 02:27:33.751
like specular decoding, can actually provide

02:27:33.811 --> 02:27:35.831
two times two to three

02:27:35.831 --> 02:27:38.171
times speed up compared with the one

02:27:38.171 --> 02:27:40.301
without speculating coding. Right? So in

02:27:40.301 --> 02:27:42.471
real inference service, you

02:27:42.610 --> 02:27:44.751
typically can get, like, two times speed boost

02:27:45.071 --> 02:27:46.611
by doing spec related coding.

02:27:48.012 --> 02:27:50.031
So So yeah. So, we we can

02:27:50.191 --> 02:27:52.501
now we can provide a very good hybrid models.

02:27:53.141 --> 02:27:55.261
But how can we actually run it very

02:27:55.461 --> 02:27:57.591
as fast as possible? Right? So,

02:27:57.831 --> 02:27:59.991
I want to talk a little bit about this direction

02:27:59.991 --> 02:28:02.041
because that is what I'm interested in right now.

02:28:02.041 --> 02:28:04.392
So So like, we are

02:28:04.451 --> 02:28:06.851
going to use a technical which we call a speculating

02:28:06.991 --> 02:28:09.001
coding. So which this is very popular

02:28:09.691 --> 02:28:11.751
inference technicals. So the way

02:28:11.751 --> 02:28:14.011
that it works is that we we have

02:28:14.232 --> 02:28:16.371
some like, we will we have

02:28:16.371 --> 02:28:18.431
a smaller model for the draft

02:28:18.431 --> 02:28:20.651
model, and generate generate some tokens. Right? And then we

02:28:20.812 --> 02:28:22.892
send these tokens to a verify, and then verify

02:28:22.892 --> 02:28:24.211
which tell tells you

02:28:25.251 --> 02:28:27.411
which which tokens I I prefer and which

02:28:27.411 --> 02:28:29.561
tokens I I don't like. And then

02:28:29.561 --> 02:28:31.121
we will reject those tokens, which

02:28:31.982 --> 02:28:33.682
don't like by the a verifier.

02:28:35.171 --> 02:28:36.781
Yeah. So, there is a

02:28:37.262 --> 02:28:39.501
challenge for the hybrid model

02:28:39.501 --> 02:28:41.580
or linear models is that so if

02:28:41.580 --> 02:28:43.900
you do this transform with transformers, it's very simple.

02:28:43.900 --> 02:28:44.721
Right? So you can

02:28:46.171 --> 02:28:48.272
KVACash will it's

02:28:48.272 --> 02:28:49.812
KPI cache will keep track

02:28:51.041 --> 02:28:53.281
the the kvC at each position. Right? So

02:28:53.281 --> 02:28:55.631
each token And then

02:28:55.691 --> 02:28:57.871
if the verify rejects a

02:28:57.871 --> 02:28:59.911
certain positions, you can

02:29:00.551 --> 02:29:02.896
easily retrieve the corresponding KB

02:29:02.896 --> 02:29:05.061
cache at the last,

02:29:05.381 --> 02:29:07.591
match prediction and the start from

02:29:07.591 --> 02:29:09.691
there and restore the restore

02:29:10.392 --> 02:29:12.580
the yeah, restore from there.

02:29:13.461 --> 02:29:15.961
And then you just keep verifying it. However,

02:29:16.181 --> 02:29:18.401
for linear models, this very hard. Right?

02:29:18.401 --> 02:29:20.481
So because you

02:29:20.481 --> 02:29:22.561
only keep a single single state.

02:29:22.561 --> 02:29:24.621
Right? So so let's say

02:29:24.621 --> 02:29:25.530
the very first

02:29:27.051 --> 02:29:29.091
only like, you you only have a

02:29:29.091 --> 02:29:31.311
single state the final stage It's

02:29:31.311 --> 02:29:33.481
very challenging to roll go back to a

02:29:33.481 --> 02:29:35.011
to a previous positions.

02:29:36.530 --> 02:29:38.691
So if you if you alternatively, you

02:29:38.691 --> 02:29:40.361
can keep track all the hidden state

02:29:40.842 --> 02:29:42.081
at each word. Right? But

02:29:43.121 --> 02:29:45.302
but then you can you will, like, have to

02:29:45.302 --> 02:29:47.541
materialize all the hidden all the

02:29:47.541 --> 02:29:49.911
SSM state or, like, the iron

02:29:50.031 --> 02:29:52.251
state. But they are typically very

02:29:52.251 --> 02:29:54.500
large, and then it will run some memory

02:29:54.500 --> 02:29:56.636
issues, and you can probably can not get any

02:29:56.636 --> 02:29:58.891
gain. If you just memorize all those

02:29:59.291 --> 02:30:01.471
So, that's the reason we introduce

02:30:01.530 --> 02:30:03.602
this multi step linear

02:30:03.602 --> 02:30:05.931
verification kernel set So basically,

02:30:06.171 --> 02:30:07.900
it can like,

02:30:09.512 --> 02:30:11.091
it can verify

02:30:12.211 --> 02:30:14.631
So so this kernel is that it does multistep

02:30:14.771 --> 02:30:16.621
generations. So, basically,

02:30:17.661 --> 02:30:18.001
like,

02:30:19.740 --> 02:30:22.081
the typical way that we do generation is that

02:30:22.221 --> 02:30:24.181
we we generate at each staff

02:30:24.501 --> 02:30:26.661
and and then, we we load

02:30:26.661 --> 02:30:28.671
these ways in from HBM to

02:30:28.671 --> 02:30:30.411
SRAM, and then we generate there. And then

02:30:30.971 --> 02:30:32.812
we send the send the state from,

02:30:33.131 --> 02:30:35.312
SM to HBM. But

02:30:35.371 --> 02:30:37.151
it's it's like it's not efficient.

02:30:37.522 --> 02:30:39.682
So we write a multistep kernels that

02:30:39.682 --> 02:30:41.661
can generate as many, like,

02:30:41.982 --> 02:30:43.531
here is like 16 or

02:30:44.071 --> 02:30:45.751
32 steps.

02:30:46.951 --> 02:30:49.031
Yeah. And with with these kernels, we can

02:30:49.031 --> 02:30:51.171
actually do the speculated coding

02:30:51.171 --> 02:30:53.226
very efficiently. So basically,

02:30:53.226 --> 02:30:55.501
we let's see how can we

02:30:55.741 --> 02:30:58.161
use this, like, multistep generation multistep

02:30:58.301 --> 02:31:00.161
application to do the generation.

02:31:00.571 --> 02:31:02.771
So basically, you if we have three tokens here,

02:31:02.771 --> 02:31:05.101
and then we can like,

02:31:05.101 --> 02:31:07.501
if if let's say these two tokens are get verified

02:31:07.501 --> 02:31:09.511
and the last token get rejected, but

02:31:09.511 --> 02:31:11.751
in but we didn't

02:31:11.751 --> 02:31:13.991
advance our hidden state. Right? As hidden state

02:31:13.991 --> 02:31:16.150
is still the s s m SM state is

02:31:16.150 --> 02:31:18.240
still at the first positions. And

02:31:18.240 --> 02:31:20.681
in the last time in the next round, we

02:31:20.740 --> 02:31:22.841
will advance the hidden state using this,

02:31:22.841 --> 02:31:23.581
like, multistep

02:31:24.921 --> 02:31:27.120
multi step kernels. And then we also

02:31:27.120 --> 02:31:29.131
verify the the three

02:31:29.131 --> 02:31:29.871
three more tokens.

02:31:31.191 --> 02:31:33.451
And we'll also propose three more tokens.

02:31:33.451 --> 02:31:34.562
Right? So

02:31:35.671 --> 02:31:38.171
and then if we see the last two get rejected

02:31:38.551 --> 02:31:40.871
and then the the first one get a accepted.

02:31:41.272 --> 02:31:43.611
So we can, update the hidden

02:31:43.671 --> 02:31:45.772
state to the to the, like, the

02:31:45.772 --> 02:31:47.921
previous previous match

02:31:47.921 --> 02:31:49.491
positions. And

02:31:50.291 --> 02:31:52.451
Yeah. So so by doing something like this, we

02:31:52.451 --> 02:31:54.240
can actually make the specularity

02:31:54.541 --> 02:31:56.552
going very fast. So there are

02:31:56.552 --> 02:31:59.011
a lot of work which is, I think is very

02:31:59.011 --> 02:32:01.031
interesting that in today's

02:32:01.051 --> 02:32:03.141
this year, neurophys,

02:32:03.141 --> 02:32:05.351
this this is called the ST

02:32:05.351 --> 02:32:07.711
tree. It's actually doing speculating coding for

02:32:07.711 --> 02:32:09.621
the tree applications for the, hybrid

02:32:10.741 --> 02:32:12.871
SSM models. Yeah.

02:32:12.871 --> 02:32:14.951
So, I want to thank to all my

02:32:14.951 --> 02:32:16.171
collaborators here

02:32:17.251 --> 02:32:19.551
Also, just to let you know that together,

02:32:19.551 --> 02:32:21.491
AI is hiring, and then we are, like,

02:32:22.021 --> 02:32:24.321
top the most efficient

02:32:25.352 --> 02:32:26.812
yeah, among all those models.

02:32:29.232 --> 02:32:29.691
Yeah.

02:32:35.551 --> 02:32:37.631
Thanks for all the remarks. At the

02:32:37.631 --> 02:32:39.351
time limit, we will not have

02:32:40.370 --> 02:32:42.551
question time for the oral presentation.

02:32:43.501 --> 02:32:45.761
And now, let's welcome our next

02:32:45.971 --> 02:32:48.101
oral paper talk Generate

02:32:48.101 --> 02:32:50.201
Parallel Scaling with Independent

02:32:50.341 --> 02:32:50.841
Generalization.

02:33:06.911 --> 02:33:09.091
I may have one of my backpack if you don't have. One.

02:33:09.571 --> 02:33:10.981
Here. So sure.

02:33:49.262 --> 02:33:51.461
I didn't have Okay. What what

02:33:53.061 --> 02:33:53.511
second.

02:33:57.110 --> 02:33:57.521
Sure.

02:34:49.321 --> 02:34:51.591
No. No. No.

02:35:20.831 --> 02:35:21.972
Sorry, by the way.

02:35:24.051 --> 02:35:25.911
Everyone, my name is Harry. And today, I'll be

02:35:26.392 --> 02:35:28.511
talking a little bit about our paper

02:35:28.511 --> 02:35:30.841
generalized parallel scaling with interdependent generations.

02:35:33.211 --> 02:35:35.290
So, you know, we've seen throughout this conference in the past

02:35:35.290 --> 02:35:36.781
year, yeah, all on reasoning has

02:35:37.651 --> 02:35:39.801
know, there's been insane progress.

02:35:39.801 --> 02:35:41.900
A lot of this has been due to inference

02:35:42.041 --> 02:35:44.191
scaling. And we saw

02:35:44.191 --> 02:35:46.271
a little bit of that in the previous oral talk.

02:35:46.910 --> 02:35:49.251
Where, you know, for a particular prompt,

02:35:49.812 --> 02:35:51.892
you might scale in the sequence

02:35:51.892 --> 02:35:53.411
axis, in which you're sort of

02:35:54.291 --> 02:35:56.411
generating more and more Genesys. Thoughts.

02:35:56.651 --> 02:35:58.791
More tokens before you output your final answer.

02:35:59.671 --> 02:36:01.691
Another access that you might sort of consider

02:36:01.831 --> 02:36:04.181
is the parallel access, where for one particular

02:36:04.401 --> 02:36:06.281
question, you are going to

02:36:06.522 --> 02:36:07.521
generating multiple response

02:36:08.541 --> 02:36:10.241
and in the end, you might do some merging

02:36:10.741 --> 02:36:12.851
you might do some selection or

02:36:12.851 --> 02:36:13.866
majority voting.

02:36:15.321 --> 02:36:17.431
And so in this work, we're actually gonna be

02:36:17.431 --> 02:36:19.681
focusing mostly on parallel scaling. Here.

02:36:20.240 --> 02:36:22.481
And how we can improve that. For,

02:36:22.731 --> 02:36:24.731
reasons. Models. So

02:36:25.610 --> 02:36:27.951
To sort of introduce this, to

02:36:28.521 --> 02:36:30.731
three parallel scaling is,

02:36:30.971 --> 02:36:33.341
you're generating multiple responses. For

02:36:33.341 --> 02:36:35.481
one prompt. One question. And

02:36:35.481 --> 02:36:36.951
this is done usually independent

02:36:37.631 --> 02:36:39.741
What we mean by independence is

02:36:40.711 --> 02:36:43.182
every single sequence that you're generating, for

02:36:43.182 --> 02:36:45.501
this prompt. It's going to be independent of each other.

02:36:45.661 --> 02:36:47.881
So there's not gonna be any sort information transfer.

02:36:48.201 --> 02:36:50.441
So that means something like information that is derived

02:36:50.441 --> 02:36:52.501
in one thread is going to

02:36:52.501 --> 02:36:54.201
be inaccessible in another thread.

02:36:55.521 --> 02:36:57.711
So there's no information flow across these

02:36:57.711 --> 02:36:58.571
different threads.

02:37:00.171 --> 02:37:02.411
The and also for independent sampling,

02:37:02.411 --> 02:37:04.431
we have end to end. Which what we mean is that

02:37:04.432 --> 02:37:06.711
for end threads, we're getting end different answers.

02:37:08.151 --> 02:37:10.631
Another sort of, method of parallel

02:37:10.631 --> 02:37:12.721
scaling that has gone some

02:37:12.721 --> 02:37:14.961
recent traction this notion of, like, decomposition.

02:37:15.581 --> 02:37:17.761
So for a particular question,

02:37:17.921 --> 02:37:19.741
you might break it down into

02:37:20.222 --> 02:37:22.370
parallel subtasks. And each thread

02:37:22.370 --> 02:37:24.471
is kinda tackling each subtask independently

02:37:24.530 --> 02:37:26.711
of each other, And then at certain points, you might do

02:37:26.711 --> 02:37:28.651
some merging. And you might fan out again.

02:37:30.281 --> 02:37:32.171
Keep doing this until you get the final answer.

02:37:32.841 --> 02:37:35.081
There's going to be information flow throughout these threads.

02:37:35.961 --> 02:37:37.721
But in these cases, you're gonna have to

02:37:38.041 --> 02:37:40.182
you're using multiple threads, but you're producing

02:37:40.241 --> 02:37:42.071
one final answer rather than an

02:37:42.481 --> 02:37:44.501
different answers. In the

02:37:44.501 --> 02:37:46.522
extreme case, we also have interdependence.

02:37:48.302 --> 02:37:50.331
Where information flow is going to be

02:37:50.571 --> 02:37:52.351
consistently flowing between threads.

02:37:52.831 --> 02:37:54.631
And also, you're gonna have end

02:37:54.951 --> 02:37:56.991
separate responses. As well. So this is

02:37:56.991 --> 02:37:59.191
going to be something independent

02:37:59.191 --> 02:38:00.311
sampling where we have

02:38:01.472 --> 02:38:02.861
threads and responses.

02:38:03.901 --> 02:38:06.052
But here, the threads are sort of gonna be communicating

02:38:06.052 --> 02:38:07.601
with each other at regular

02:38:08.661 --> 02:38:10.701
and so this is gonna be our focus

02:38:10.701 --> 02:38:12.711
is how can we actually do this, in an efficient

02:38:12.711 --> 02:38:14.772
manner? And the way we do

02:38:14.772 --> 02:38:17.051
this is that we first take a look into

02:38:17.051 --> 02:38:18.911
the hidden states of these LOMs.

02:38:20.431 --> 02:38:22.731
And so, you know, these layers in an

02:38:22.731 --> 02:38:24.761
LLM where it's transformer based LLMs,

02:38:25.161 --> 02:38:26.421
they're gonna be mixing and

02:38:27.781 --> 02:38:29.791
to tensor. And so what

02:38:29.791 --> 02:38:31.981
the hidden states tensor sort of looks like, it's

02:38:32.301 --> 02:38:34.501
this three d sort of structure. Where we have

02:38:34.501 --> 02:38:36.671
the responses on one axis the tokens

02:38:36.671 --> 02:38:38.841
on one axis, and then the features on

02:38:39.081 --> 02:38:41.070
third access. And typically,

02:38:41.131 --> 02:38:43.251
what's gonna happen is that for attention,

02:38:43.251 --> 02:38:45.491
it's going to be mixing information along this

02:38:45.491 --> 02:38:47.691
token access. And then you

02:38:47.691 --> 02:38:50.091
have the feed forward layers that's gonna be sharing information

02:38:50.091 --> 02:38:51.841
between the different features.

02:38:53.441 --> 02:38:55.601
Usually, you know, there's nothing gonna be flowing

02:38:55.601 --> 02:38:57.671
in the response access, in

02:38:57.671 --> 02:38:59.790
batch inference, you're not really making

02:38:59.790 --> 02:39:02.070
any assumption there, in what's gonna

02:39:02.070 --> 02:39:04.081
be in that batch. So you wanna

02:39:04.081 --> 02:39:06.181
keep everything sort of independent with each other.

02:39:06.860 --> 02:39:09.021
But this is actually not the case in parallel scaling because

02:39:09.021 --> 02:39:11.181
in parallel scaling, these different responses

02:39:11.181 --> 02:39:13.201
are actually gonna stem from the same

02:39:13.556 --> 02:39:15.451
prompt. Right? So these sequences

02:39:15.591 --> 02:39:17.732
are no longer sort of completely independent They're

02:39:17.732 --> 02:39:19.831
gonna be highly correlated each other because they're

02:39:19.831 --> 02:39:21.861
they're trying to answer the same question.

02:39:23.816 --> 02:39:25.841
So what that means is that related sequences

02:39:25.841 --> 02:39:27.691
due to parallel scaling allows us to

02:39:28.091 --> 02:39:30.176
view hidden states. As a holistic

02:39:30.176 --> 02:39:32.451
tensor rather than just isolated matrix slices.

02:39:32.691 --> 02:39:34.772
So you can treat this as sort of one structure rather

02:39:34.772 --> 02:39:37.182
than n independent. Substructures.

02:39:38.701 --> 02:39:40.861
And so that's what our method is gonna be

02:39:40.861 --> 02:39:42.771
doing, is we're gonna be adding a third module.

02:39:43.570 --> 02:39:45.401
Specifically for parallel scaling.

02:39:45.642 --> 02:39:47.581
That is going to operate along this

02:39:47.802 --> 02:39:49.311
response or batch access here.

02:39:51.070 --> 02:39:53.141
So So,

02:39:53.141 --> 02:39:55.531
in a nutshell, our method

02:39:55.531 --> 02:39:57.601
is just an

02:39:57.601 --> 02:39:59.941
unmasked attention like operation along the dash axis.

02:40:00.871 --> 02:40:02.871
For all the sequences that send from the same prompt.

02:40:02.951 --> 02:40:04.191
So this is what it kinda looks like.

02:40:05.150 --> 02:40:07.221
So typical transformer block is gonna have

02:40:07.221 --> 02:40:09.241
the attention and the feed forward blocks. Just

02:40:09.241 --> 02:40:11.271
adding a third one, which we

02:40:11.271 --> 02:40:13.281
call bridge. And within that

02:40:13.281 --> 02:40:15.631
module, what gonna look like

02:40:15.641 --> 02:40:17.461
is that for a particular

02:40:17.761 --> 02:40:19.921
prompt so here in this diagram, we have prompt

02:40:20.321 --> 02:40:21.921
q and q prime, so you have two different prompts.

02:40:22.481 --> 02:40:24.761
So between different prompts, still,

02:40:25.540 --> 02:40:27.561
making them independent of each other. There's

02:40:27.561 --> 02:40:29.721
no communication. But, for

02:40:29.721 --> 02:40:31.731
all the rollouts for a particular

02:40:31.731 --> 02:40:33.881
prompt, they're going to be communicating

02:40:34.191 --> 02:40:35.401
each other every time step.

02:40:37.001 --> 02:40:39.400
And so this allows us to share latent information

02:40:39.400 --> 02:40:41.851
to other sequences, at each

02:40:41.990 --> 02:40:44.221
generation step. Our method is also

02:40:44.221 --> 02:40:46.411
fairly low cost. It's going to be adding around

02:40:46.812 --> 02:40:48.881
three to 5% new parameters. And we don't have

02:40:48.881 --> 02:40:50.961
to train from scratch. We can just actually add this

02:40:50.961 --> 02:40:53.101
to a pre trained model. And then just sort

02:40:53.101 --> 02:40:55.171
of fine tune do RL

02:40:55.171 --> 02:40:57.511
on top of that. It's also

02:40:57.511 --> 02:40:59.562
fairly, versatile.

02:40:59.991 --> 02:41:02.232
In the sense that we actually can train at

02:41:02.232 --> 02:41:04.271
a know, parallel width.

02:41:04.511 --> 02:41:06.530
And it can actually generalize to

02:41:07.171 --> 02:41:09.290
different widths both above and under what it was

02:41:09.290 --> 02:41:11.552
trained on. So for example, for

02:41:11.552 --> 02:41:13.570
some models, we trained on a width of four, and it can

02:41:13.570 --> 02:41:15.701
generalize all the way up to 16, or you can also

02:41:16.491 --> 02:41:18.531
use for, with a one, which is

02:41:18.531 --> 02:41:19.441
independent. Sample.

02:41:20.571 --> 02:41:22.410
So To give a more,

02:41:22.650 --> 02:41:24.781
deeper visualization of what's going on,

02:41:25.341 --> 02:41:27.010
so for self attention, what

02:41:27.381 --> 02:41:29.661
typically do is for a particular

02:41:29.661 --> 02:41:31.201
token, we're looking at the historical

02:41:31.716 --> 02:41:33.641
tokens. This particular sequence.

02:41:34.841 --> 02:41:36.801
So that's why I have kind of showed here in in

02:41:37.201 --> 02:41:38.541
this, matrix here.

02:41:39.341 --> 02:41:41.404
And for a self attention, you're gonna have to maintain a cubic

02:41:41.404 --> 02:41:43.661
cache because you have store all this historical information,

02:41:43.661 --> 02:41:45.711
and you also have to keep track of this notion of, like,

02:41:45.711 --> 02:41:47.201
time. With positional encoding.

02:41:48.301 --> 02:41:50.461
In Bridge, we what we're

02:41:50.461 --> 02:41:52.631
doing is we're actually going along that other

02:41:52.631 --> 02:41:54.570
axis along the sequence,

02:41:54.650 --> 02:41:56.341
not the sequence the batch axis.

02:41:57.761 --> 02:41:59.841
And because we're not really storing any historical

02:41:59.841 --> 02:42:01.851
information, there's not sort of a

02:42:01.851 --> 02:42:04.051
memory cost associated with that. So don't have to

02:42:04.051 --> 02:42:06.091
store some sort of KB cache. There's

02:42:06.091 --> 02:42:08.251
also position invariant, and what we mean by that is

02:42:08.251 --> 02:42:10.370
that if you permute the items in

02:42:10.370 --> 02:42:12.482
this batch you're gonna get the same

02:42:12.482 --> 02:42:14.222
answer. So there's no sort of,

02:42:15.022 --> 02:42:17.182
positional information there. And so when

02:42:17.182 --> 02:42:19.240
we combine this together, in an,

02:42:19.561 --> 02:42:21.631
an LLM, what we see is that

02:42:21.631 --> 02:42:23.982
this sort of token is going to be attending to

02:42:23.982 --> 02:42:26.081
the all the other tokens and

02:42:26.081 --> 02:42:27.222
that share this access.

02:42:29.001 --> 02:42:30.771
The act the axis being the

02:42:31.811 --> 02:42:32.761
dimension. Dimension.

02:42:34.610 --> 02:42:36.892
We this also changes the next token

02:42:36.892 --> 02:42:38.971
distributions a bit. So, typically, what's gonna

02:42:40.791 --> 02:42:42.951
next token is gonna be dependent on the prompt and then

02:42:42.951 --> 02:42:45.296
also all the previous tokens that the SQL has

02:42:45.296 --> 02:42:47.340
generated. But

02:42:47.341 --> 02:42:49.501
in our case, it's also gonna be dependent

02:42:49.641 --> 02:42:51.691
on this tokens. That were

02:42:51.691 --> 02:42:53.871
generated previously by all the other sequences.

02:42:54.821 --> 02:42:57.031
And so what this kinda looks like is that you

02:42:57.031 --> 02:42:59.111
think of this, you know, matrix of batch

02:42:59.111 --> 02:43:01.580
by, token, matrix,

02:43:01.771 --> 02:43:04.181
generating sort of one, takes a lot of time,

02:43:04.491 --> 02:43:06.642
but in Bridge, what we're doing is

02:43:06.642 --> 02:43:08.941
we're doing one column at a time. So each column

02:43:08.941 --> 02:43:10.781
is gonna be dependent on the previous column.

02:43:14.732 --> 02:43:16.950
Okay. So now, I'll get

02:43:16.950 --> 02:43:18.801
into some results really briefly.

02:43:19.700 --> 02:43:21.921
So so we're gonna

02:43:21.921 --> 02:43:24.181
show some math results. So

02:43:24.181 --> 02:43:26.095
just a brief overview how we trained it.

02:43:26.281 --> 02:43:28.501
We're using RLVR, and

02:43:28.501 --> 02:43:30.581
we're training with and without Bridge, using

02:43:30.581 --> 02:43:32.121
a dataset of math. Problems.

02:43:33.931 --> 02:43:36.410
And so I'll walk you a little bit through what, these baselines

02:43:36.410 --> 02:43:38.411
are. So we have the original sort

02:43:38.411 --> 02:43:40.602
of yeah, these are all, like, deep deep distilled models.

02:43:41.262 --> 02:43:43.352
We have RL VR only, which is just,

02:43:43.352 --> 02:43:45.431
you know, applying RL VR on top of these

02:43:45.431 --> 02:43:47.490
models. And then we also have this

02:43:47.490 --> 02:43:49.621
baseline called p match. Which just stands

02:43:49.621 --> 02:43:51.728
for matching the number of parameters and the number

02:43:51.728 --> 02:43:53.900
of layers that we induce adding these

02:43:53.900 --> 02:43:56.051
bridge layers. So in a sense,

02:43:56.051 --> 02:43:57.861
this is, like, the compute match baseline.

02:43:58.581 --> 02:44:00.821
And, what we actually end up seeing is that for

02:44:00.821 --> 02:44:01.371
all these

02:44:03.441 --> 02:44:05.621
all these different math tasks, a lot of these are competition

02:44:05.761 --> 02:44:07.631
math problems. We see that Bridge

02:44:07.772 --> 02:44:09.961
actually extends been we get from

02:44:09.961 --> 02:44:12.301
our old. So it extends the sort of headroom

02:44:12.301 --> 02:44:12.751
that we get

02:44:14.711 --> 02:44:16.871
Even though this was trained only on math, we can

02:44:16.871 --> 02:44:18.490
also evaluate on non math

02:44:19.852 --> 02:44:21.870
generalized in terms of you know, is

02:44:21.870 --> 02:44:24.030
it just learning to share information for math task,

02:44:24.030 --> 02:44:25.961
or is it also learning

02:44:26.181 --> 02:44:27.791
to share information beyond,

02:44:28.421 --> 02:44:30.801
math? And what we see is that it actually generalizes

02:44:31.182 --> 02:44:33.201
to non math tasks. Despite being only

02:44:33.201 --> 02:44:35.700
trained on math. So here, we have you know, some language

02:44:35.761 --> 02:44:37.472
tasks, stem, and

02:44:38.041 --> 02:44:40.381
puzzles as well. And

02:44:41.501 --> 02:44:43.661
even in the cases where Bridge is not

02:44:43.661 --> 02:44:45.721
doing the best, performance, it's still doing

02:44:45.721 --> 02:44:47.841
the equivalent. To the other

02:44:47.841 --> 02:44:48.341
baselines.

02:44:50.051 --> 02:44:51.910
Way that we can sort of evaluate,

02:44:52.131 --> 02:44:54.185
our method is at the set level. Right? Because we

02:44:54.185 --> 02:44:56.021
are generating n different responses,

02:44:56.820 --> 02:44:59.040
rather than compressing it all down to one

02:44:59.040 --> 02:45:01.101
response. So we can see how good this

02:45:01.101 --> 02:45:03.276
setup and responses are.

02:45:03.812 --> 02:45:05.892
And for this, we're looking at sort of three

02:45:05.892 --> 02:45:08.161
different things. We're looking at a simple

02:45:08.301 --> 02:45:10.432
spectrum. So from left to right. What

02:45:10.432 --> 02:45:12.681
I'm showing is the accuracy when we're only

02:45:12.681 --> 02:45:14.931
looking at one out of eight is correct.

02:45:14.961 --> 02:45:16.841
That's, like, equivalent to coverage.

02:45:17.161 --> 02:45:19.191
But we can be a little bit more strict

02:45:19.431 --> 02:45:21.481
And so the center is going to be

02:45:21.481 --> 02:45:23.531
out of eight correct in this set. And

02:45:23.911 --> 02:45:26.061
then the, rightmost is eight out of

02:45:26.461 --> 02:45:28.556
eight. So unanimously unanimously all

02:45:28.556 --> 02:45:30.731
correct that's a lot

02:45:30.731 --> 02:45:32.911
more strict. Of notion of accuracy. And

02:45:32.911 --> 02:45:35.191
what we can see is that, our method

02:45:35.191 --> 02:45:37.271
most consistently produces the most

02:45:37.271 --> 02:45:38.961
correct answers for difficult tasks.

02:45:40.641 --> 02:45:42.841
So and now, finally, I'll

02:45:42.841 --> 02:45:45.132
talk a little bit about some growth investments.

02:45:45.232 --> 02:45:47.171
What you mean by width is, you know, the parallelism

02:45:47.791 --> 02:45:50.021
sort of width. So we

02:45:50.081 --> 02:45:52.161
trained at four and, we can

02:45:52.161 --> 02:45:53.961
evaluate at different width.

02:45:54.421 --> 02:45:56.570
So here, I'm showing widths

02:45:56.570 --> 02:45:58.591
from one to 16, where one is essentially

02:45:58.731 --> 02:45:59.941
just independent generation.

02:46:00.981 --> 02:46:03.012
And then we actually end up seeing

02:46:03.012 --> 02:46:05.062
is that though it's only been trained out

02:46:05.062 --> 02:46:07.121
with four Bridge actually outperforms,

02:46:11.451 --> 02:46:13.660
any scenario, including, the

02:46:13.660 --> 02:46:15.501
baselines and also when bridges with

02:46:16.041 --> 02:46:17.061
a certain equal to one.

02:46:19.301 --> 02:46:21.441
So Okay. So, just to wrap

02:46:21.441 --> 02:46:23.741
up. So Bridge allows information

02:46:23.741 --> 02:46:26.051
flow between sequences, without Docker

02:46:26.291 --> 02:46:28.751
parallelism. It's a composable

02:46:28.812 --> 02:46:31.211
block that you can just kinda add into your LLM.

02:46:31.851 --> 02:46:33.931
And it's also fairly robust to different tasks and

02:46:33.931 --> 02:46:34.941
also generation with

02:46:36.081 --> 02:46:38.241
the second thing that we want to emphasize is that, you

02:46:38.241 --> 02:46:40.631
know, bridges only one way to sort

02:46:41.011 --> 02:46:43.221
of leverage this. Untapped

02:46:43.221 --> 02:46:45.161
structure that's in parallel scaling.

02:46:45.431 --> 02:46:47.591
All these sequences are correlated with each other. You

02:46:47.591 --> 02:46:49.781
know, certainly, this sort of

02:46:49.781 --> 02:46:51.991
observation can be used, for other methods as

02:46:51.991 --> 02:46:54.001
well. Some future directions that we think

02:46:54.001 --> 02:46:55.891
would be interesting to explore would be

02:46:56.401 --> 02:46:58.211
know, including this in, you know,

02:46:58.691 --> 02:47:00.631
early on training, say, in mid training or even

02:47:00.951 --> 02:47:03.211
in pre training, everything that we've shown here

02:47:03.211 --> 02:47:05.191
is all done in during post training.

02:47:05.351 --> 02:47:07.511
And then also, it might be pretty interesting

02:47:07.511 --> 02:47:09.571
to look at vision test just because in vision, there's

02:47:09.571 --> 02:47:11.781
gonna be a lot more correlations going on.

02:47:13.052 --> 02:47:14.851
Than a language. So.

02:47:15.246 --> 02:47:16.991
Yeah. That's it. And thank you so much.

02:47:29.691 --> 02:47:31.771
Yeah. Thanks for such a

02:47:31.771 --> 02:47:33.751
remarkable award and thank Harry for

02:47:34.581 --> 02:47:36.740
here as an oral presenter. And then

02:47:36.740 --> 02:47:39.131
we will move to the next part of

02:47:39.476 --> 02:47:41.281
the professor

02:47:41.821 --> 02:47:43.241
Wei Shi's talk. Yeah.

02:47:53.881 --> 02:47:54.381
Yeah.

02:47:55.901 --> 02:47:57.441
Let me give a brief introduction.

02:47:58.271 --> 02:48:00.211
Yeah. Wei Shi is the professor

02:48:00.511 --> 02:48:02.581
and the faculty member of

02:48:02.581 --> 02:48:04.541
School of Interactive

02:48:04.602 --> 02:48:06.222
Computer and Machine Learning Center,

02:48:07.261 --> 02:48:09.461
at Georgia Tech. Which

02:48:09.601 --> 02:48:11.791
is research lies on the interest of

02:48:12.081 --> 02:48:14.321
machine learning, neural language processing,

02:48:14.321 --> 02:48:16.701
and social media. We should

02:48:16.701 --> 02:48:18.781
also address the neuropathy x

02:48:18.781 --> 02:48:20.361
lab with

02:48:21.161 --> 02:48:23.411
focus on large language model here.

02:48:23.651 --> 02:48:25.812
Let's give our time to professor Wei

02:48:25.812 --> 02:48:25.881
Shi.

02:48:30.621 --> 02:48:32.241
Alright. Thank you, everyone.

02:48:32.671 --> 02:48:33.671
For coming.

02:48:34.791 --> 02:48:37.031
I'm going to talk a little bit

02:48:37.031 --> 02:48:39.421
different perspective of efficient

02:48:39.821 --> 02:48:41.921
reasoning. It's not about computational

02:48:43.671 --> 02:48:45.522
efficiency but rather how effectively,

02:48:45.982 --> 02:48:48.221
as a field, a field of

02:48:49.101 --> 02:48:51.411
100,000 tons of thousand

02:48:51.472 --> 02:48:53.411
researchers that we can think differently

02:48:54.240 --> 02:48:56.110
and come out of creative ideas,

02:48:56.522 --> 02:48:58.602
in order to move the field

02:48:58.602 --> 02:49:00.751
efficiently forward other than

02:49:01.871 --> 02:49:03.991
doing similar things, of

02:49:03.991 --> 02:49:04.651
each other.

02:49:06.511 --> 02:49:08.191
So my talk will be,

02:49:08.671 --> 02:49:10.691
about probability reasoning

02:49:10.691 --> 02:49:12.391
for real world decision making.

02:49:12.772 --> 02:49:14.711
Which is beyond the logical

02:49:14.932 --> 02:49:17.102
and mathematic reasonings that you

02:49:17.102 --> 02:49:19.351
see a lot of benchmark are our

02:49:19.411 --> 02:49:21.491
related to So I

02:49:21.491 --> 02:49:23.591
want to, ask you what

02:49:24.400 --> 02:49:26.021
did you reason about today?

02:49:26.870 --> 02:49:28.921
Or maybe you will be reason

02:49:28.921 --> 02:49:29.981
about later today.

02:49:35.241 --> 02:49:37.321
So likely, if you

02:49:37.321 --> 02:49:39.601
haven't think about this, you will

02:49:39.601 --> 02:49:40.481
think about this

02:49:42.240 --> 02:49:44.290
later today or tomorrow. Is when

02:49:44.290 --> 02:49:45.681
should I leave my hotel

02:49:46.665 --> 02:49:48.750
room tomorrow

02:49:48.750 --> 02:49:50.331
morning or That's

02:49:51.110 --> 02:49:53.031
my flight. For the airport. Right?

02:49:57.701 --> 02:50:00.201
So the reasoning will be something like this.

02:50:01.361 --> 02:50:03.142
There's fifteen minutes drive.

02:50:03.941 --> 02:50:06.021
To the airport from about here,

02:50:06.261 --> 02:50:07.241
without traffic.

02:50:09.131 --> 02:50:11.211
It's now the major holiday, but there's a

02:50:11.211 --> 02:50:13.291
lot of attendance, of New Year's might

02:50:13.291 --> 02:50:15.171
be going to the airport at the same time.

02:50:15.331 --> 02:50:17.031
Is this easy to get a rideshare?

02:50:17.411 --> 02:50:18.641
Are you renting a car?

02:50:20.320 --> 02:50:22.391
Yeah. And how long it will take for

02:50:22.551 --> 02:50:24.791
wait in the line to check out the hotel if you

02:50:24.791 --> 02:50:26.451
really need you need to do that.

02:50:28.232 --> 02:50:30.472
Will look up at the terminal map to figure

02:50:30.472 --> 02:50:32.642
out how large is the the airport,

02:50:33.182 --> 02:50:34.971
because some of my airport from the

02:50:35.371 --> 02:50:37.451
security check to the gate is kind of

02:50:37.451 --> 02:50:39.171
a hack for more than a mile.

02:50:40.191 --> 02:50:42.291
With your luggage. So

02:50:42.291 --> 02:50:44.450
this determine, what I

02:50:44.450 --> 02:50:46.642
will do the day I head to to

02:50:46.642 --> 02:50:48.102
the airport after a conference.

02:50:49.461 --> 02:50:51.780
So this is what we read on daily

02:50:51.780 --> 02:50:53.941
basis. But

02:50:53.941 --> 02:50:56.361
what does our large number model reasoning benchmark

02:50:56.501 --> 02:50:58.151
of are working on?

02:51:00.071 --> 02:51:02.331
There is something more similar to this.

02:51:02.671 --> 02:51:04.541
Beside those mathematical questions,

02:51:04.881 --> 02:51:07.191
we may have some logical reasoning on

02:51:07.191 --> 02:51:09.351
on tasks like this from the

02:51:09.351 --> 02:51:10.991
big bunch actual hard release

02:51:11.472 --> 02:51:13.531
by Google Research. That used

02:51:13.531 --> 02:51:15.501
in, almost three recent release

02:51:16.381 --> 02:51:18.611
evaluation. So it's going

02:51:18.991 --> 02:51:19.791
to add some,

02:51:21.541 --> 02:51:23.820
region. It's a very little question,

02:51:23.820 --> 02:51:25.601
but these are on the toy exam.

02:51:27.941 --> 02:51:29.881
And the the top first

02:51:30.021 --> 02:51:32.432
dataset on benchmarked our

02:51:32.432 --> 02:51:34.651
Genesys Ray, in their release, they

02:51:34.711 --> 02:51:36.841
evaluated on, is this humanity's life

02:51:36.841 --> 02:51:38.931
exam. Has a lot of very

02:51:38.931 --> 02:51:41.011
interesting questions like this.

02:51:42.131 --> 02:51:44.400
That probably there's a a

02:51:44.400 --> 02:51:46.481
tiny little number of people in the world

02:51:46.481 --> 02:51:48.101
ever will be able to answer.

02:51:53.141 --> 02:51:55.461
authors on the paper that invited

02:51:55.482 --> 02:51:57.601
all the world expert of very,

02:51:57.601 --> 02:51:59.120
very niche domains in

02:51:59.681 --> 02:52:01.301
this very, very hard question.

02:52:02.421 --> 02:52:04.681
But this is not what we are reasoning

02:52:04.900 --> 02:52:05.881
on daily basis.

02:52:09.331 --> 02:52:11.571
So you can already see the difference of

02:52:11.571 --> 02:52:13.431
what I want to focus on today

02:52:14.120 --> 02:52:16.250
on this what you might really listen on a

02:52:16.250 --> 02:52:18.272
daily basis. What is,

02:52:18.671 --> 02:52:19.892
what are in the benchmark?

02:52:20.946 --> 02:52:22.602
Currently? So

02:52:23.062 --> 02:52:25.302
so the example I presented is more

02:52:25.302 --> 02:52:27.681
on the numerical reasoning and with some

02:52:27.681 --> 02:52:29.991
logical reasoning. But with a lot of

02:52:30.291 --> 02:52:32.320
uncertainty. This is

02:52:32.320 --> 02:52:34.641
what we we do reason as a human

02:52:34.641 --> 02:52:36.642
being. Even not as

02:52:36.642 --> 02:52:38.901
an expert of some of Honeywell.

02:52:41.602 --> 02:52:43.861
So this type of reasoning will require

02:52:44.371 --> 02:52:46.512
reasoning, so that you need

02:52:46.512 --> 02:52:48.371
to estimate some of the likelihood

02:52:48.592 --> 02:52:50.121
and have to make some

02:52:51.461 --> 02:52:53.861
This is what people do to make

02:52:53.861 --> 02:52:56.101
decisions. It's

02:52:56.161 --> 02:52:56.661
undeterministic.

02:52:58.681 --> 02:53:01.081
The most common places you will see

02:53:01.081 --> 02:53:03.031
in in the real world

02:53:03.432 --> 02:53:05.371
for some important decisions

02:53:05.591 --> 02:53:08.012
will be like legal decisions. If

02:53:08.012 --> 02:53:09.721
you want to convince a jury,

02:53:10.421 --> 02:53:12.562
you each jury will make different

02:53:12.562 --> 02:53:14.661
decisions based on the partial evidence

02:53:14.801 --> 02:53:16.001
presented to them.

02:53:18.482 --> 02:53:20.851
And if you work in marketing, or

02:53:20.931 --> 02:53:22.951
HR, you will do this on on

02:53:23.431 --> 02:53:25.646
at your job all the time. Is

02:53:25.646 --> 02:53:27.721
that what kind of product

02:53:28.101 --> 02:53:30.341
I manufacture, make sure design, and

02:53:30.400 --> 02:53:32.640
what color should we manufacture for

02:53:32.641 --> 02:53:34.421
these different colors of process.

02:53:37.182 --> 02:53:39.271
And more recently, as think for a lot

02:53:39.271 --> 02:53:41.771
of long mode, the most successful and more

02:53:42.551 --> 02:53:44.791
in comparison, more studied domain will

02:53:44.791 --> 02:53:46.171
be medical, diagnosences.

02:53:47.641 --> 02:53:49.900
That is always a probabilistic reasoning.

02:53:50.421 --> 02:53:52.501
Although the normally, the cause

02:53:52.501 --> 02:53:54.811
of a disease will be only one, but

02:53:55.211 --> 02:53:57.080
the similar syndrome and test result, there's

02:53:57.352 --> 02:53:59.441
could be a more like a distribution over

02:53:59.441 --> 02:54:00.871
market diseases.

02:54:04.051 --> 02:54:06.231
May may mount to a question, why so far

02:54:06.291 --> 02:54:08.361
of the current large

02:54:08.361 --> 02:54:10.071
number of reasoning benchmark is

02:54:10.431 --> 02:54:12.711
not the probability reasoning task

02:54:12.711 --> 02:54:13.921
that I'm talking about.

02:54:15.681 --> 02:54:17.721
A standby of a

02:54:17.860 --> 02:54:20.031
few related reasons. It's very

02:54:20.031 --> 02:54:22.531
hard to collect ground truth. I think medical

02:54:23.450 --> 02:54:25.500
field is easier to advance is because there's

02:54:25.500 --> 02:54:27.481
a lot of patient

02:54:27.541 --> 02:54:29.671
and a lot of medical that

02:54:29.991 --> 02:54:31.861
was able to have this more intensive

02:54:32.860 --> 02:54:34.321
evidences, and

02:54:34.961 --> 02:54:37.201
decisions being kind of our predictions being

02:54:37.201 --> 02:54:39.291
collected to evaluate. To

02:54:39.291 --> 02:54:39.801
work with.

02:54:41.320 --> 02:54:43.481
But what about whether we want to open,

02:54:43.641 --> 02:54:46.070
I want to open a business. I want to roast a

02:54:46.070 --> 02:54:48.091
certain restaurant with the multiple choices

02:54:48.091 --> 02:54:50.221
of location I have. It's

02:54:50.221 --> 02:54:52.251
hard. Right? You can't do AB test

02:54:52.571 --> 02:54:55.051
and location may be only one factors

02:54:55.051 --> 02:54:57.411
of the business success.

02:54:58.741 --> 02:55:01.081
And, of course, these kind of things are important.

02:55:01.441 --> 02:55:03.001
So a lot of our companies and

02:55:04.041 --> 02:55:06.131
private individuals, you have to

02:55:06.131 --> 02:55:08.071
do research, marketing research analysis.

02:55:08.531 --> 02:55:11.031
In order to so those are data are potentially

02:55:11.251 --> 02:55:13.360
of available, but it's normally in private

02:55:13.360 --> 02:55:15.571
holding. And

02:55:15.571 --> 02:55:17.610
it's incomplete. And

02:55:17.610 --> 02:55:20.110
that's and as a consequently considerably

02:55:20.691 --> 02:55:22.791
evaluation will be inherently challenging

02:55:24.030 --> 02:55:26.530
you may or may not actually know the right answer.

02:55:28.441 --> 02:55:30.610
One of the best attempt we

02:55:30.610 --> 02:55:32.711
have seen this year from at IKEA

02:55:32.771 --> 02:55:35.000
is from Dan Ross group at UIUC.

02:55:35.301 --> 02:55:36.901
So Dan Ross has a long

02:55:37.461 --> 02:55:39.092
term machine learning researcher.

02:55:40.070 --> 02:55:42.311
He has done this since probably

02:55:42.311 --> 02:55:44.470
twenty years ago. Before I even

02:55:44.471 --> 02:55:46.481
entered the field. This is his

02:55:46.481 --> 02:55:48.421
latest attempt with his student.

02:55:49.181 --> 02:55:51.201
It's still a little bit kind

02:55:51.261 --> 02:55:53.341
of a toy examples, makes some

02:55:53.341 --> 02:55:54.891
assumptions, it's kind of

02:55:55.851 --> 02:55:57.900
artificially created dataset. But this is

02:55:57.900 --> 02:55:59.981
one of the best attempts in order

02:55:59.981 --> 02:56:02.040
to make this type of task possible

02:56:02.040 --> 02:56:04.281
to study. For

02:56:04.281 --> 02:56:06.381
AI researchers, need quantitative

02:56:07.245 --> 02:56:09.261
requirements and, evaluation

02:56:12.111 --> 02:56:14.341
So what we can do?

02:56:15.701 --> 02:56:17.721
We we definitely need some real

02:56:17.781 --> 02:56:19.921
data also called benchmark in order to work

02:56:19.921 --> 02:56:22.312
on So what do we really

02:56:22.312 --> 02:56:24.581
looking for? Right? We want to look for something

02:56:24.581 --> 02:56:26.641
more realistic other than, just

02:56:27.041 --> 02:56:29.120
toy examples that no one knows the answer.

02:56:30.401 --> 02:56:32.562
Or the answer is kind of you don't know whether

02:56:32.562 --> 02:56:34.700
if you loosened it or it's it's been

02:56:34.700 --> 02:56:36.891
cracked or not. Right? So for example,

02:56:36.891 --> 02:56:37.971
we want to predict

02:56:38.981 --> 02:56:41.411
how to weigh you know, how likely

02:56:41.711 --> 02:56:43.871
a World War three will happen, which country or

02:56:43.871 --> 02:56:45.905
which region that will be a start of

02:56:45.905 --> 02:56:47.981
it. So it's hard

02:56:47.981 --> 02:56:50.221
to kind of, have the data to

02:56:50.221 --> 02:56:52.401
to to work with us. But

02:56:52.621 --> 02:56:54.831
we can bounce this type of problem

02:56:54.892 --> 02:56:56.602
based reasoning in everyday scenarios.

02:56:58.201 --> 02:57:00.281
But, another thing is,

02:57:00.601 --> 02:57:02.741
when do need to evaluate. We want the

02:57:02.741 --> 02:57:03.802
data to be public.

02:57:04.991 --> 02:57:07.071
That every researchers, you know, on the world on the

02:57:07.071 --> 02:57:09.531
Earth can can can get hands

02:57:09.671 --> 02:57:12.066
on and work with it. With reliable

02:57:12.552 --> 02:57:14.621
or at least efficiently reliable

02:57:14.621 --> 02:57:15.131
ground truth.

02:57:17.081 --> 02:57:19.581
For my own group or myself, personally,

02:57:20.561 --> 02:57:22.651
I want to work on something socially

02:57:22.651 --> 02:57:24.771
impact I don't want to just work on

02:57:24.771 --> 02:57:26.631
things because I can publish papers

02:57:27.131 --> 02:57:29.150
but rather I wanted something useful

02:57:29.211 --> 02:57:30.661
I can make It's

02:57:31.221 --> 02:57:33.251
different for everyday every part is,

02:57:33.571 --> 02:57:35.761
every user's life. In

02:57:35.761 --> 02:57:36.761
on Earth.

02:57:37.801 --> 02:57:39.981
So luckily, we did find

02:57:40.121 --> 02:57:42.431
one such use cases that fit

02:57:42.431 --> 02:57:44.501
all above three criteria.

02:57:45.701 --> 02:57:47.561
So this is the work my PhD

02:57:47.941 --> 02:57:50.021
student, presented here

02:57:50.022 --> 02:57:52.281
a few days ago at, NewRePS.

02:57:53.110 --> 02:57:55.120
So he's the only student

02:57:55.120 --> 02:57:57.280
officer, did the work and did all

02:57:57.280 --> 02:57:59.321
the work because all the rest of three of

02:57:59.321 --> 02:58:01.570
us, Alan Rita, Solvig,

02:58:01.791 --> 02:58:04.131
my collaborators, in

02:58:04.831 --> 02:58:06.991
machine learning field or and privacy

02:58:06.991 --> 02:58:09.071
field. They are all three of us

02:58:09.071 --> 02:58:11.181
are all professors. So Jonathan

02:58:11.181 --> 02:58:12.731
did 100 percentage of the

02:58:13.546 --> 02:58:15.881
work. So,

02:58:16.602 --> 02:58:18.821
the of course, you are curious what

02:58:19.481 --> 02:58:21.482
the use case would be, like, allowing us to

02:58:21.482 --> 02:58:22.101
study this.

02:58:24.272 --> 02:58:26.371
It's just actually, you probably encounter

02:58:26.432 --> 02:58:27.812
all the time as well.

02:58:29.342 --> 02:58:31.271
Reddit or social media, if you use it,

02:58:31.351 --> 02:58:33.671
you will post on it. You are very likely

02:58:33.671 --> 02:58:35.771
to talk about yourself or someone else.

02:58:36.811 --> 02:58:38.751
So Reddit is a pseudo anonymous

02:58:40.151 --> 02:58:42.490
web forum. So people quite

02:58:42.490 --> 02:58:44.751
often talk about themselves or their coworkers

02:58:44.811 --> 02:58:46.900
and so on and so forth. For example, this

02:58:46.900 --> 02:58:48.901
is one of them. But how

02:58:48.901 --> 02:58:51.221
risky it is actually to post

02:58:51.221 --> 02:58:53.351
a post already like this.

02:58:55.671 --> 02:58:57.151
So we had to detect

02:58:58.257 --> 02:59:00.401
later in

02:59:00.401 --> 02:59:02.722
ten, fifteen minutes, is to detect

02:59:02.722 --> 02:59:05.081
where part of the the tags

02:59:06.121 --> 02:59:07.661
people reviewed some

02:59:08.221 --> 02:59:10.151
details about themselves. So

02:59:10.772 --> 02:59:12.632
and, of course, what type of information

02:59:12.911 --> 02:59:15.187
what is related, for example, the location of them,

02:59:15.187 --> 02:59:16.611
the the job of them.

02:59:17.331 --> 02:59:18.631
So these are prior

02:59:19.681 --> 02:59:21.511
detected by a fine toned model.

02:59:22.791 --> 02:59:24.591
Then the question they ask is just

02:59:24.831 --> 02:59:26.911
how how many people, you know, are on the

02:59:26.911 --> 02:59:29.081
earth would fit this description.

02:59:29.081 --> 02:59:31.321
Of course, the smaller the number, the more risky

02:59:31.321 --> 02:59:33.331
it is. I will give you a second

02:59:33.331 --> 02:59:35.731
to read this post because I'm going to show the reasoning

02:59:35.731 --> 02:59:37.861
of it. In a second.

02:59:38.691 --> 02:59:40.851
So there's a few key, key point.

02:59:40.851 --> 02:59:42.781
It's like my son's take care

02:59:43.501 --> 02:59:45.581
also recently increased their rate. I only

02:59:45.581 --> 02:59:47.421
had four months of maturity

02:59:47.796 --> 02:59:50.111
leave. So that

02:59:50.111 --> 02:59:52.471
didn't say this is a mother, but it's very

02:59:52.530 --> 02:59:54.521
likely a mother of a newborn son.

02:59:56.171 --> 02:59:58.251
Presumably much less under the

02:59:58.251 --> 03:00:00.400
age of one. You can't enforce

03:00:00.461 --> 03:00:00.961
those.

03:00:04.551 --> 03:00:07.051
Of course, this is not only for social media.

03:00:07.110 --> 03:00:09.191
I need data you send you

03:00:09.191 --> 03:00:11.511
know, converts with or other

03:00:11.511 --> 03:00:13.621
kind of larger LAN memory or

03:00:13.621 --> 03:00:15.241
APIs depending on your configuration.

03:00:15.631 --> 03:00:17.971
You might give him a a way your, data,

03:00:18.431 --> 03:00:20.331
as well. And also, there's always a

03:00:20.676 --> 03:00:22.481
leak. Concerns.

03:00:23.711 --> 03:00:25.851
So, so for human

03:00:25.851 --> 03:00:27.591
and the OAM changes,

03:00:28.320 --> 03:00:30.411
there's also privacy concern like this.

03:00:31.211 --> 03:00:33.371
So this is what channel of sales will will

03:00:33.371 --> 03:00:35.421
give you. So this is a give

03:00:35.421 --> 03:00:37.501
the ready post and, of course, a lot of prompt

03:00:37.501 --> 03:00:39.511
on engineer as well as the detection

03:00:39.511 --> 03:00:41.527
of which part of the kind

03:00:41.527 --> 03:00:43.921
of the text I showed you, the highlight are provided

03:00:44.111 --> 03:00:45.871
in order to help the model,

03:00:46.121 --> 03:00:48.161
reason better. Of course, you can

03:00:48.161 --> 03:00:50.371
just throw the Reddit just

03:00:50.931 --> 03:00:52.940
zero shot prompted. That's also kind of

03:00:52.941 --> 03:00:54.961
will work a

03:00:54.961 --> 03:00:57.421
lot of cases and especially the simpler cases.

03:00:59.031 --> 03:01:01.110
So But there are just two

03:01:01.331 --> 03:01:02.951
arrows, in this reasoning.

03:01:04.871 --> 03:01:06.951
So here is one. I hope you're in this very

03:01:06.951 --> 03:01:09.411
short time on the slides, you will be able

03:01:10.000 --> 03:01:12.080
to recognize that. That's a little bit like a

03:01:12.080 --> 03:01:14.251
challenging task. But on a

03:01:14.251 --> 03:01:16.330
high level, the idea is that the

03:01:16.490 --> 03:01:18.601
this particular reasoning trend

03:01:19.081 --> 03:01:21.081
mentioned there's a the woman will be five

03:01:21.320 --> 03:01:23.541
50% of the population

03:01:23.681 --> 03:01:25.852
of this, city. And the working

03:01:25.852 --> 03:01:27.552
in the tag, how many percentage

03:01:28.012 --> 03:01:30.121
that point

03:01:30.121 --> 03:01:32.570
one percentage of the population working tag

03:01:32.710 --> 03:01:35.161
in Australia. And then they multiply this

03:01:35.181 --> 03:01:37.221
two things together. But

03:01:37.221 --> 03:01:38.681
that's not very accurate.

03:01:39.821 --> 03:01:41.781
Because we all know that there's

03:01:41.841 --> 03:01:43.391
a gender imbalance in

03:01:43.931 --> 03:01:46.146
the tech, field. So

03:01:46.146 --> 03:01:47.331
is woman well not

03:01:48.371 --> 03:01:50.201
percent. Percentage of that.

03:01:50.881 --> 03:01:53.306
Population group. So

03:01:53.741 --> 03:01:55.821
is another, estimation that,

03:01:56.141 --> 03:01:58.401
in this case, GPT model

03:01:58.401 --> 03:02:00.491
made. This is how many

03:02:00.732 --> 03:02:03.221
they do realize this is like a a

03:02:03.251 --> 03:02:05.461
a child in daycare.

03:02:05.961 --> 03:02:08.281
So their estimation is based on how likely

03:02:08.281 --> 03:02:10.601
you have a toddler. Under

03:02:10.601 --> 03:02:12.681
three year old. But the woman is talking

03:02:12.681 --> 03:02:14.411
about maturity leave. So it's

03:02:14.732 --> 03:02:16.831
unlikely this mother

03:02:17.481 --> 03:02:19.281
more likely the mother have a is

03:02:19.762 --> 03:02:21.281
son not under a year old.

03:02:22.001 --> 03:02:24.022
So this overestimate underestimated

03:02:24.722 --> 03:02:26.511
the risk. Of the poster.

03:02:28.191 --> 03:02:30.511
So in order to do this reasoning more

03:02:30.511 --> 03:02:32.351
accurately, we

03:02:32.761 --> 03:02:34.841
Jonathan devised this, base

03:02:34.841 --> 03:02:37.232
based previous

03:02:37.232 --> 03:02:39.491
reasoning network with large enough model as a back

03:02:39.881 --> 03:02:42.330
bone. So the key idea is that

03:02:42.751 --> 03:02:44.761
as you all know about, base net,

03:02:45.161 --> 03:02:47.451
you have to model the strong

03:02:47.451 --> 03:02:49.691
probabilities in certain orders of a

03:02:49.691 --> 03:02:51.971
composition of the conditional probabilities.

03:02:52.211 --> 03:02:54.562
Of course, there are some assumption you can independent

03:02:54.562 --> 03:02:56.181
of assumptions you can made and

03:02:56.581 --> 03:02:58.421
some you can't. So for example,

03:02:58.961 --> 03:03:00.982
estimating how many

03:03:02.351 --> 03:03:04.431
women in the group of

03:03:04.431 --> 03:03:06.461
people working tech is a little bit

03:03:07.021 --> 03:03:09.320
easier than estimating among

03:03:09.320 --> 03:03:11.621
all the women how many are actually working

03:03:11.621 --> 03:03:13.811
the tag. So this ordering matters,

03:03:13.811 --> 03:03:15.351
and the decomposition matters.

03:03:17.051 --> 03:03:19.131
So there's numerous different ways you

03:03:19.131 --> 03:03:20.981
can construct a BSNAD

03:03:21.221 --> 03:03:23.261
based on the original post.

03:03:23.421 --> 03:03:25.561
This is the one parameter of the better

03:03:25.561 --> 03:03:27.731
ones that will in this

03:03:27.791 --> 03:03:29.811
better order that allow you to estimate

03:03:30.441 --> 03:03:32.601
more accurately and with a little bit

03:03:32.601 --> 03:03:34.900
more evidence you can collect. So

03:03:34.900 --> 03:03:37.061
this is would be one of the many

03:03:37.061 --> 03:03:39.201
constructions of BS9, but

03:03:39.522 --> 03:03:42.011
you do need your user model to predict to

03:03:42.070 --> 03:03:43.530
construct an graph.

03:03:44.261 --> 03:03:46.291
And also pick the one that will work better.

03:03:48.451 --> 03:03:49.941
And this will allow you to

03:03:50.500 --> 03:03:52.820
turn into sub questions that large

03:03:52.820 --> 03:03:55.031
number more likely can answer

03:03:55.031 --> 03:03:56.871
with evidence more accurately.

03:03:57.352 --> 03:03:59.361
So this is going to be

03:03:59.361 --> 03:04:01.521
a the questions that we are

03:04:01.521 --> 03:04:03.821
trying to answer. So this will allow

03:04:03.821 --> 03:04:05.941
you to estimate among the percentage

03:04:05.941 --> 03:04:07.861
of the world population that fit this

03:04:08.262 --> 03:04:08.721
description.

03:04:10.641 --> 03:04:12.801
Even with this decomposition to this

03:04:12.801 --> 03:04:15.131
level of sub questions, some of the

03:04:15.131 --> 03:04:17.511
question still will be harder to answer than others.

03:04:17.961 --> 03:04:20.280
So for example, how many percentage of the people

03:04:20.280 --> 03:04:22.321
in tech in this city in

03:04:22.562 --> 03:04:24.961
Australia have no landlord? This is what mentioned

03:04:24.961 --> 03:04:26.182
in the original post.

03:04:27.131 --> 03:04:29.211
There's normally there are no census data about

03:04:29.211 --> 03:04:31.251
this. Normally. Maybe there is, but

03:04:31.251 --> 03:04:33.531
it's very hard to to dig this out.

03:04:37.290 --> 03:04:39.370
So, we can further break

03:04:39.370 --> 03:04:41.371
down this question into whether

03:04:41.371 --> 03:04:43.791
this person on the their own property

03:04:43.791 --> 03:04:45.811
or they are leaving their visitor parents?

03:04:47.211 --> 03:04:49.311
But still, the model will have fairly

03:04:49.450 --> 03:04:51.531
low confidence or certainty to

03:04:51.531 --> 03:04:53.631
estimate this. Then you'll have to generalize

03:04:53.631 --> 03:04:55.711
it. And also, based on the model's own

03:04:55.711 --> 03:04:57.740
confidence, in order

03:04:58.041 --> 03:05:00.360
to to decide what to abstract

03:05:00.360 --> 03:05:02.771
this generalizes question to be.

03:05:03.011 --> 03:05:05.091
Until you can answer it with some level of

03:05:05.091 --> 03:05:07.211
certainty. So eventually,

03:05:07.271 --> 03:05:09.441
you will kind of two type

03:05:09.441 --> 03:05:11.620
of most common scenario, and then

03:05:11.620 --> 03:05:13.611
you'll sum them together, become this as

03:05:15.711 --> 03:05:17.671
Some other questions are supposed to

03:05:17.911 --> 03:05:20.331
be easier to answer is, for example,

03:05:20.831 --> 03:05:22.991
how many percent of the women in the city,

03:05:23.231 --> 03:05:25.091
that actually have a newborn sign?

03:05:27.331 --> 03:05:29.651
So if you send to a larger

03:05:29.951 --> 03:05:32.031
Frontier larger model, they actually do quite a bit

03:05:32.031 --> 03:05:33.991
thinking just in order to answer this.

03:05:34.472 --> 03:05:36.021
Single simple sub question.

03:05:36.660 --> 03:05:38.740
Based a lot of retrieval, searching

03:05:38.740 --> 03:05:40.767
through all the whole entire Internet,

03:05:40.767 --> 03:05:42.921
and I was in order to answer

03:05:43.301 --> 03:05:45.360
this accurately. And there's a lot of things they

03:05:45.360 --> 03:05:47.201
cannot talk about, but the most

03:05:47.682 --> 03:05:50.001
relevant one is the annual or number of annual

03:05:50.001 --> 03:05:51.901
courses. In this city.

03:05:52.062 --> 03:05:54.222
So the information is fairly accurate

03:05:54.222 --> 03:05:56.301
here, but it's mentioned as one

03:05:56.301 --> 03:05:58.642
of its largest public hospital in

03:05:58.642 --> 03:06:00.941
that city, which is one of the major

03:06:00.941 --> 03:06:02.631
one that's most the newborn

03:06:03.171 --> 03:06:05.241
son, up children come from.

03:06:05.732 --> 03:06:08.081
The twenty twenty three of they have

03:06:08.141 --> 03:06:10.171
a knowledge cutoff they don't they

03:06:10.171 --> 03:06:12.030
have only twenty twenty three data.

03:06:12.591 --> 03:06:14.711
However, if you really can

03:06:14.711 --> 03:06:16.891
actually have two uses and might accurately

03:06:17.110 --> 03:06:18.221
kind of have more structure

03:06:19.405 --> 03:06:21.581
data. Funded in the right places,

03:06:21.641 --> 03:06:23.740
you will realize that although the majority

03:06:23.961 --> 03:06:25.990
are children born in this city

03:06:25.990 --> 03:06:28.121
are from this public but there's a

03:06:28.121 --> 03:06:29.091
lot of private

03:06:30.271 --> 03:06:31.606
facilities also

03:06:33.221 --> 03:06:35.160
get take care of the newborn babies.

03:06:35.951 --> 03:06:37.961
So the actual number is larger than this.

03:06:38.761 --> 03:06:40.481
The number of the model estimated.

03:06:41.120 --> 03:06:43.321
So with all this, you are able

03:06:43.321 --> 03:06:45.581
to make an estimation. And eventually,

03:06:45.722 --> 03:06:47.941
you'll to put it into the right message

03:06:48.456 --> 03:06:50.911
equations. Are condition or independent

03:06:51.131 --> 03:06:53.341
or not. Do a little bit reduction

03:06:53.482 --> 03:06:55.642
or else in order to aggregate the

03:06:55.642 --> 03:06:57.660
number. So number is striking.

03:06:58.471 --> 03:07:00.791
Just just some kind of pretty normal

03:07:00.791 --> 03:07:02.861
sense of your world Internet. I don't

03:07:02.861 --> 03:07:05.031
feel that the original post is so

03:07:05.159 --> 03:07:06.913
kind of revealing.

03:07:07.201 --> 03:07:09.271
But if you actually do it, it's

03:07:09.271 --> 03:07:11.351
come down to 25 women in

03:07:11.351 --> 03:07:13.211
our city. It will be narrowed down

03:07:13.511 --> 03:07:15.562
to that level. If it is they

03:07:15.562 --> 03:07:17.921
talk about something else, for example,

03:07:17.921 --> 03:07:19.461
they engage in some political

03:07:20.521 --> 03:07:22.581
activist, In some other Reddit post,

03:07:22.581 --> 03:07:24.751
they reviewed that. They they

03:07:24.751 --> 03:07:27.111
can be narrowed down, if need

03:07:27.651 --> 03:07:29.881
possible. By people inspired

03:07:29.881 --> 03:07:31.921
who know a of them

03:07:32.235 --> 03:07:32.551
personal.

03:07:34.990 --> 03:07:37.231
So we we did some kind of evaluation

03:07:37.231 --> 03:07:39.676
using this new technical

03:07:39.761 --> 03:07:41.231
kind of, framework.

03:07:41.950 --> 03:07:43.091
For basic listening

03:07:44.021 --> 03:07:46.071
to evaluate this. So compared to

03:07:46.071 --> 03:07:48.211
your off the shelf channel of Salt, which

03:07:48.211 --> 03:07:50.296
a lot of prompt engineering, of

03:07:50.296 --> 03:07:52.571
course. This is kind of the result

03:07:54.621 --> 03:07:56.701
The different model will, actually,

03:07:56.701 --> 03:07:59.081
the model are doing pretty well

03:07:59.081 --> 03:08:00.851
in estimating these numbers.

03:08:01.490 --> 03:08:03.551
And, there's still a lot of

03:08:03.552 --> 03:08:05.491
space to further improve.

03:08:06.381 --> 03:08:08.601
Of course, the goal of this is now to

03:08:09.001 --> 03:08:11.160
try to identify that anonymous users, but

03:08:11.160 --> 03:08:12.021
rather to

03:08:15.131 --> 03:08:17.351
which part of the text they mentioned

03:08:17.351 --> 03:08:19.571
is indoors for themselves. So

03:08:19.711 --> 03:08:21.761
that they can change

03:08:21.761 --> 03:08:23.932
it or modify it in

03:08:23.932 --> 03:08:25.881
order to decide what

03:08:26.121 --> 03:08:28.231
they actually share. Also,

03:08:28.231 --> 03:08:29.961
for as we don't want to make

03:08:30.281 --> 03:08:32.441
Internet user to be more paranoid, what

03:08:32.441 --> 03:08:34.671
what about what they share on the Internet.

03:08:34.951 --> 03:08:37.312
So this project partially myself,

03:08:37.562 --> 03:08:39.642
personally, is I actually want to go on

03:08:39.642 --> 03:08:41.701
ready to help people to answer their

03:08:42.102 --> 03:08:44.231
questions. That I know very

03:08:44.231 --> 03:08:46.461
good answers for. But I don't want to be,

03:08:46.621 --> 03:08:47.360
de identified

03:08:48.851 --> 03:08:51.251
so I want to be able

03:08:52.191 --> 03:08:54.351
to enable a lot of people who are

03:08:54.511 --> 03:08:56.650
will be waiting to help people on

03:08:56.780 --> 03:08:58.761
the Internet to feel more comfortable to

03:08:58.921 --> 03:09:01.181
to do that. With

03:09:01.240 --> 03:09:01.981
some safeguard.

03:09:03.311 --> 03:09:05.811
But, of course, that's that's my personal

03:09:05.870 --> 03:09:07.081
interest in this product.

03:09:08.101 --> 03:09:10.040
So this is a little bit, analysis

03:09:10.181 --> 03:09:12.521
of the of the result. Of course, all the technical

03:09:13.071 --> 03:09:14.861
of my sorta evaluation are

03:09:15.431 --> 03:09:17.371
are in the paper.

03:09:17.651 --> 03:09:19.732
But this is a little bit on a high level

03:09:19.732 --> 03:09:21.961
idea. This is

03:09:21.961 --> 03:09:24.021
comparing branch, what's a chain of thought. Of

03:09:24.021 --> 03:09:26.441
course, if you're just doing dealing with the simple cases,

03:09:27.012 --> 03:09:29.171
there's several only three things the user

03:09:29.171 --> 03:09:31.101
mentioned about themselves. All the model

03:09:31.501 --> 03:09:33.521
work more or less as the same because

03:09:33.521 --> 03:09:35.541
they are easier, so the turn offs are

03:09:36.521 --> 03:09:38.841
any frontier model can do fairly well. But

03:09:38.841 --> 03:09:41.081
once you have a for example,

03:09:41.301 --> 03:09:43.240
a posting history, you'll have a lot of changes

03:09:43.381 --> 03:09:45.541
with larger model over time. Or

03:09:45.541 --> 03:09:47.871
you post things over the time. Those

03:09:48.131 --> 03:09:50.431
become much more complicated.

03:09:51.141 --> 03:09:53.371
In those cases, probably it's a reasonably

03:09:53.772 --> 03:09:55.931
framework as I just presented will make a huge

03:09:55.931 --> 03:09:57.821
difference from just simple

03:09:57.982 --> 03:09:59.121
prompting my sword.

03:10:00.161 --> 03:10:02.171
So So model do

03:10:02.171 --> 03:10:04.111
pretty well, mostly predicting,

03:10:04.732 --> 03:10:06.581
the x axis is the model

03:10:06.982 --> 03:10:09.191
predicting number of the k, how many number of

03:10:09.191 --> 03:10:11.251
people in the on the

03:10:11.251 --> 03:10:13.351
earth that fit that description, and the

03:10:14.080 --> 03:10:16.101
y axis is the ground truth,

03:10:17.141 --> 03:10:19.541
On the diagnosis are the pink colors. Ones

03:10:19.541 --> 03:10:21.602
are the one pretty clear

03:10:21.602 --> 03:10:23.691
very closely to the ground truth.

03:10:24.570 --> 03:10:26.431
With only half of the magnitude

03:10:27.240 --> 03:10:27.691
differences.

03:10:30.811 --> 03:10:32.911
What really make a difference on

03:10:32.911 --> 03:10:35.341
those much more complex cases this

03:10:35.360 --> 03:10:37.111
analysis is on a

03:10:37.592 --> 03:10:39.911
random sample of Rydays. So there are simple cases

03:10:39.911 --> 03:10:42.000
that both Maser worked really well

03:10:42.000 --> 03:10:44.056
and also have cases

03:10:44.056 --> 03:10:46.061
that more complicated at

03:10:46.061 --> 03:10:47.851
our master work a lot better.

03:10:48.331 --> 03:10:50.570
What's concerning is, like, the typical churn

03:10:50.570 --> 03:10:52.701
of salt or TPO from tier lateral

03:10:52.701 --> 03:10:54.400
model if you just do simple prompting.

03:10:55.961 --> 03:10:58.121
Just do prompting, even a very

03:10:58.121 --> 03:11:00.401
simple or more engineered complex.

03:11:00.741 --> 03:11:02.861
Prompts, is always underestimate,

03:11:03.241 --> 03:11:05.461
more likely than not. So that meaning

03:11:05.461 --> 03:11:07.281
they cannot give the k

03:11:07.921 --> 03:11:09.291
much larger k,

03:11:10.651 --> 03:11:12.791
than it actually is. So they

03:11:12.791 --> 03:11:15.111
would have it will indicate that, oh, there's

03:11:15.111 --> 03:11:17.150
a 3,000 people fit this

03:11:17.150 --> 03:11:19.091
description, and there are maybe just 30.

03:11:20.881 --> 03:11:23.031
So it's a little bit so

03:11:23.301 --> 03:11:25.700
being able to not avoid underestimation

03:11:25.700 --> 03:11:27.011
is crucial.

03:11:28.731 --> 03:11:30.891
So here is a a arrow case

03:11:30.891 --> 03:11:32.981
of where that happened. So

03:11:32.981 --> 03:11:35.220
this is actually a real data that

03:11:35.221 --> 03:11:37.312
of user data

03:11:37.312 --> 03:11:39.551
in, changes with the strategy PD

03:11:39.551 --> 03:11:41.721
that the data released by shared

03:11:41.905 --> 03:11:44.031
GPD, You may heard this story

03:11:44.031 --> 03:11:46.441
that the original trade GPT release

03:11:47.581 --> 03:11:49.841
actually included some of the journalists

03:11:49.900 --> 03:11:52.091
using it to go

03:11:52.091 --> 03:11:54.330
through or summarize some transcript they

03:11:54.330 --> 03:11:55.891
had with some

03:11:56.446 --> 03:11:58.511
interviewees, but, actually,

03:11:58.570 --> 03:12:00.990
you revealed anonymous, people's

03:12:01.101 --> 03:12:03.261
name that there's a be

03:12:03.900 --> 03:12:06.001
keep as a voice of the Lord or

03:12:06.301 --> 03:12:08.641
cannot people interviewed

03:12:09.021 --> 03:12:11.181
for a press release, privately.

03:12:12.951 --> 03:12:15.191
So these are real. We had

03:12:15.191 --> 03:12:17.211
to modify this post in

03:12:17.211 --> 03:12:19.331
order to show us here on the

03:12:19.331 --> 03:12:21.391
slides to This is kind of a

03:12:22.351 --> 03:12:24.571
semi fake and

03:12:24.571 --> 03:12:26.391
real, user conversation in GP.

03:12:26.950 --> 03:12:29.351
So it does a person talk about starting a business,

03:12:29.591 --> 03:12:31.341
and want to decide on which company

03:12:32.301 --> 03:12:34.450
name for the startup. And eventually, one of

03:12:34.450 --> 03:12:36.731
the company become the real company, and then

03:12:37.110 --> 03:12:39.511
it's not that hard to figure out who is the founder

03:12:39.732 --> 03:12:41.581
of it. So his case actually won.

03:12:41.821 --> 03:12:44.281
There's a single identifiable individual

03:12:44.941 --> 03:12:46.721
who made this type of changes.

03:12:48.891 --> 03:12:51.110
Okay. Just to wrap a little

03:12:51.110 --> 03:12:53.191
bit here, before I move on

03:12:53.191 --> 03:12:55.320
to something else, I need

03:12:55.320 --> 03:12:57.371
to do this quickly. What

03:12:57.371 --> 03:12:59.451
I want to make at a point is propriety reasoning

03:12:59.451 --> 03:13:01.801
is challenging for both AI and humans.

03:13:02.102 --> 03:13:03.261
We are very interested

03:13:04.171 --> 03:13:05.251
path to study.

03:13:06.691 --> 03:13:09.031
Very quick. I need to wrap up. So another

03:13:09.091 --> 03:13:11.341
work we did a already preceding

03:13:11.341 --> 03:13:13.681
this Work Is To Actually identify

03:13:13.820 --> 03:13:16.210
this this disclosures which

03:13:16.210 --> 03:13:18.431
actually crucial for both channel of

03:13:18.431 --> 03:13:20.741
thought type reasoning as well as our method to

03:13:20.821 --> 03:13:23.062
work for this. Task. This is

03:13:23.062 --> 03:13:25.381
actually hard. Detecting this

03:13:25.381 --> 03:13:25.881
accurately

03:13:30.802 --> 03:13:32.661
or even 90% accurate.

03:13:33.431 --> 03:13:35.411
As well. So thought it would

03:13:35.892 --> 03:13:38.061
with phantom beam. So, yeah, if you're interested

03:13:38.120 --> 03:13:40.141
to improve this, we encourage

03:13:40.141 --> 03:13:42.211
you to do that. And there are some

03:13:42.211 --> 03:13:44.231
other people who follow our work and make this

03:13:45.030 --> 03:13:47.450
to other languages other than English

03:13:47.511 --> 03:13:49.801
as well. So we did actually

03:13:49.801 --> 03:13:52.061
conduct a real user

03:13:52.061 --> 03:13:54.221
study with actual Reddit users, and that's

03:13:54.461 --> 03:13:55.911
them whether they like this thing.

03:13:57.062 --> 03:13:59.150
They they do. Some people have

03:13:59.150 --> 03:14:01.181
found it's not so so far. There are two

03:14:01.181 --> 03:14:03.301
out of the 12 21 people, but they were like,

03:14:03.461 --> 03:14:05.541
I'm already very careful. I don't need this to

03:14:05.541 --> 03:14:07.551
help me to make my calls. I I'm

03:14:07.551 --> 03:14:09.251
not talking much about myself.

03:14:09.762 --> 03:14:12.161
Reddit. But for my cousin for

03:14:12.161 --> 03:14:14.421
my, for my that is a tunator,

03:14:14.421 --> 03:14:16.561
this will be very very useful since

03:14:16.561 --> 03:14:18.650
that So they gave a lot of, good feedback

03:14:18.650 --> 03:14:20.891
and motivated us to continue working

03:14:20.891 --> 03:14:21.211
on this.

03:14:23.051 --> 03:14:25.171
My second to quickly mention actually, once

03:14:25.171 --> 03:14:27.361
you identify this this disclosures,

03:14:27.842 --> 03:14:30.102
the larger model are really good at at

03:14:30.161 --> 03:14:32.182
tracking it to make it safer to everyone.

03:14:32.321 --> 03:14:34.642
I don't know whether you use Grammarly before

03:14:34.642 --> 03:14:36.831
to correct your grandma errors. But

03:14:36.831 --> 03:14:38.701
if you simply have a Grammarly for

03:14:39.341 --> 03:14:41.421
privacy reasons, only text you send out or

03:14:41.421 --> 03:14:43.811
post on the Internet can be just

03:14:44.131 --> 03:14:46.370
modified to make it safer for everyone. That

03:14:46.370 --> 03:14:48.521
will be really nice. Then you can take

03:14:48.521 --> 03:14:50.801
control of your own data other than rely

03:14:50.801 --> 03:14:52.851
on registration to protect. Users.

03:14:52.851 --> 03:14:53.331
Privacy.

03:14:55.331 --> 03:14:57.041
This is going to be my last

03:14:57.361 --> 03:14:59.681
second slide. There are other things

03:14:59.681 --> 03:15:01.871
we are thinking about and we are working on.

03:15:02.431 --> 03:15:04.191
About reasoning, is that,

03:15:04.941 --> 03:15:07.051
cross lingo, cross culture

03:15:07.111 --> 03:15:09.231
scenarios, social scenarios, that

03:15:09.231 --> 03:15:11.571
require quite a bit of reasoning to understand

03:15:11.711 --> 03:15:13.141
social norms and understand

03:15:14.400 --> 03:15:16.561
not only the language, but also kind

03:15:16.561 --> 03:15:18.261
of some unwritten rules.

03:15:19.451 --> 03:15:21.671
There are other, paper way or

03:15:22.071 --> 03:15:24.171
are published, and we are working on this direction.

03:15:24.602 --> 03:15:27.022
Forward is also kind of a geolocation

03:15:27.722 --> 03:15:30.081
of the multimodal

03:15:30.221 --> 03:15:32.251
case scenario. If you

03:15:32.251 --> 03:15:34.311
post a have a image, can

03:15:34.311 --> 03:15:36.651
actually the frontier models can

03:15:36.711 --> 03:15:38.671
actually narrow down to the GPS

03:15:39.211 --> 03:15:41.671
coordinate that probably even at

03:15:41.971 --> 03:15:44.131
the the location to be close enough on

03:15:44.131 --> 03:15:46.471
the block or something like, accurate

03:15:46.691 --> 03:15:48.890
to that level. You're welcome

03:15:48.891 --> 03:15:50.841
to check our paper on that.

03:15:51.841 --> 03:15:54.061
To make it even kind of more

03:15:54.061 --> 03:15:56.141
accurately locate you is to have actual

03:15:56.141 --> 03:15:58.321
user to converge with the

03:15:58.321 --> 03:16:00.341
frontier model in market term conversations.

03:16:01.151 --> 03:16:03.171
That will even narrow down that accuracy

03:16:03.232 --> 03:16:05.091
of geolocation of the image.

03:16:06.171 --> 03:16:07.721
Towards a estimation accurate

03:16:08.366 --> 03:16:10.501
level. Of course, again, they want

03:16:10.581 --> 03:16:12.842
we don't really want to locate people,

03:16:12.842 --> 03:16:14.811
but we want to raise awareness.

03:16:14.871 --> 03:16:16.971
This is is possible. And

03:16:17.211 --> 03:16:19.451
we want to try to prevent this from

03:16:19.451 --> 03:16:21.611
happening. For example, if the user

03:16:21.611 --> 03:16:23.493
post a photo in front

03:16:23.700 --> 03:16:25.971
of tower, then that's perfectly fine.

03:16:25.971 --> 03:16:28.211
Right? Because the user know what they are doing.

03:16:28.371 --> 03:16:30.601
But if the user just have someone else,

03:16:30.601 --> 03:16:32.650
you take a photo with you and you

03:16:32.650 --> 03:16:34.121
are just on the other people.

03:16:34.921 --> 03:16:37.102
Background of someone on a random street.

03:16:37.272 --> 03:16:39.291
You don't want to be located necessarily

03:16:39.432 --> 03:16:41.461
to the level of the same the block.

03:16:41.861 --> 03:16:43.941
And the large and small actually do that. So

03:16:43.941 --> 03:16:46.211
how to kind of moderate

03:16:46.431 --> 03:16:48.610
a larger model to not answer questions

03:16:48.991 --> 03:16:51.151
certain questions about certain photo

03:16:51.151 --> 03:16:51.241
is

03:16:53.301 --> 03:16:55.641
So what I want to encourage everyone to think

03:16:55.781 --> 03:16:57.881
about is do we always need to reason in

03:16:57.881 --> 03:17:00.041
high resource language, mostly like in English

03:17:00.041 --> 03:17:02.091
or Chinese for our frontier models? I think

03:17:02.091 --> 03:17:04.181
the answer is no. Depending on what

03:17:04.181 --> 03:17:05.631
question it is about.

03:17:06.271 --> 03:17:08.061
And do do we want to

03:17:08.360 --> 03:17:10.501
consider to kind of human and AI

03:17:10.501 --> 03:17:12.660
to be a collaborative process in reasoning?

03:17:12.660 --> 03:17:14.761
I think the answer is yes.

03:17:15.511 --> 03:17:17.772
I hope to see more work in these areas,

03:17:18.151 --> 03:17:19.211
other than just

03:17:20.261 --> 03:17:21.841
the large number model benchmarks that

03:17:22.421 --> 03:17:24.711
we we are trying to climb that in the number four.

03:17:25.591 --> 03:17:27.561
So This is, just my group.

03:17:27.721 --> 03:17:30.141
Just, some two money. My student contributed

03:17:30.201 --> 03:17:32.211
to this talk, and I also work

03:17:32.211 --> 03:17:34.421
on a lot of other topics in

03:17:34.421 --> 03:17:36.431
the lab. Yes. Thank you,

03:17:36.431 --> 03:17:38.831
everyone, for for your attention, and, I'm

03:17:38.831 --> 03:17:41.131
happy will be around and answer any questions.

03:17:41.131 --> 03:17:42.001
I'm happy discussion.

03:17:43.921 --> 03:17:44.741
With you.

03:17:50.721 --> 03:17:52.961
Yes. Thanks for the talk. I have one

03:17:52.961 --> 03:17:54.991
question about Asian. You

03:17:54.991 --> 03:17:57.016
talk about a lot of privacy,

03:17:57.016 --> 03:17:59.501
yeah, and how I'm conveying

03:17:59.641 --> 03:18:02.131
the overall privacy for

03:18:02.791 --> 03:18:04.751
document. Yes. And how

03:18:04.991 --> 03:18:07.131
that be for Asia? You

03:18:07.131 --> 03:18:09.571
know, the agent, have capability

03:18:09.941 --> 03:18:12.330
tool for tool calling,

03:18:12.570 --> 03:18:14.271
agent has a capability

03:18:14.642 --> 03:18:16.971
maybe control your computer Would

03:18:17.110 --> 03:18:19.370
the privacy problem be more important in

03:18:20.311 --> 03:18:22.331
if you're using recent model

03:18:22.391 --> 03:18:24.660
agent? Yeah. That's

03:18:24.801 --> 03:18:26.960
a super question. It's how this kind of in two

03:18:26.960 --> 03:18:28.972
use cases and you'll have company

03:18:28.972 --> 03:18:31.121
have private data to do this. I think

03:18:32.241 --> 03:18:34.481
generally, there are two part of my answer.

03:18:34.481 --> 03:18:36.581
So one area we're actively working

03:18:36.581 --> 03:18:38.740
on in my lab is actually doing synthetic data

03:18:38.740 --> 03:18:40.761
generation. So in order to

03:18:40.761 --> 03:18:41.691
do just

03:18:43.321 --> 03:18:45.561
we our release data is

03:18:45.561 --> 03:18:47.751
actually synthetically generated that

03:18:47.751 --> 03:18:49.891
look really like the ready data,

03:18:49.891 --> 03:18:51.711
but they are all fake. Just like some

03:18:52.201 --> 03:18:54.481
the example showed on my slide are those.

03:18:54.721 --> 03:18:56.601
So you don't you couldn't tell there are

03:18:57.001 --> 03:18:59.261
fake ready calls. Right? So they are actually fake

03:18:59.521 --> 03:19:01.711
Reddit post. So so

03:19:01.711 --> 03:19:03.871
that's one thing I felt like, people can do, and

03:19:03.871 --> 03:19:06.232
they said, could be useful for pretraining

03:19:06.371 --> 03:19:08.621
and, in any stage of the model

03:19:08.621 --> 03:19:10.711
being trained. And the other thing

03:19:10.711 --> 03:19:12.761
is a little bit kind of afterwards, I

03:19:13.001 --> 03:19:15.421
after south. Right? That's a little bit like a geolocation

03:19:15.801 --> 03:19:17.810
situation that we want to know the

03:19:17.811 --> 03:19:19.001
context of this

03:19:19.851 --> 03:19:21.776
people whether this

03:19:22.410 --> 03:19:24.650
answer can be answered. Right? Depending on the context of that

03:19:24.650 --> 03:19:26.671
image and what the intention

03:19:26.671 --> 03:19:28.531
of the malicious users uses.

03:19:29.022 --> 03:19:31.211
Or who they are, and why why they

03:19:31.611 --> 03:19:33.451
the model may be need to abstain off certain

03:19:34.011 --> 03:19:35.951
questions. Yeah. Thanks.

03:19:36.456 --> 03:19:38.901
And due to time limit,

03:19:38.961 --> 03:19:41.391
we can only have one question.

03:19:41.772 --> 03:19:44.191
Thanks so much for professor Wichi's

03:19:44.331 --> 03:19:44.681
talk.

03:20:02.751 --> 03:20:04.762
Yeah. Thanks so much. We

03:20:04.762 --> 03:20:06.391
will have our next talk.

03:20:07.351 --> 03:20:09.411
From Baidu's Will team. Yeah.

03:20:09.411 --> 03:20:11.166
Wall is well known

03:20:11.740 --> 03:20:14.061
technology for reinforcement learning and recently

03:20:14.061 --> 03:20:16.081
model training. And Wang

03:20:16.081 --> 03:20:18.431
Zhang is the co teacher of

03:20:19.691 --> 03:20:21.701
world project. Especially

03:20:21.701 --> 03:20:23.641
promoting the system single control,

03:20:23.941 --> 03:20:26.101
etcetera. And he's the machine

03:20:26.101 --> 03:20:27.961
learning system engineer from Baiduancy,

03:20:29.421 --> 03:20:31.661
on building scalable infrastructure

03:20:31.661 --> 03:20:33.301
for reinforcement learning. Yeah.

03:20:35.471 --> 03:20:37.551
Yeah. Let's give our time to Quan

03:20:37.551 --> 03:20:37.951
Chung.

03:20:49.491 --> 03:20:51.811
Hello everyone. My name is Wong Zhang.

03:20:51.811 --> 03:20:54.071
I worked and ByteDance SE as

03:20:54.071 --> 03:20:56.191
a machine learning system architecture

03:20:56.191 --> 03:20:58.592
engineer. I'm also the co initiator

03:20:58.732 --> 03:21:00.801
of the VERO project and prototype

03:21:00.831 --> 03:21:02.881
this single controller architecture.

03:21:03.921 --> 03:21:06.342
Today, I would like to talk about the past the

03:21:06.342 --> 03:21:08.461
current status, the future of the VERA.

03:21:09.661 --> 03:21:11.891
Here's the outline for today's talk.

03:21:13.650 --> 03:21:15.901
So everyone attending this talk

03:21:16.461 --> 03:21:18.491
is likely well aware of the

03:21:18.491 --> 03:21:20.512
importance of reinforcement learning

03:21:21.110 --> 03:21:23.351
especially at the large scale. So I will quickly

03:21:23.351 --> 03:21:25.597
move through this background section.

03:21:26.732 --> 03:21:28.352
Like, a large scale

03:21:28.812 --> 03:21:30.461
reinforcement learning shine because

03:21:30.861 --> 03:21:33.201
pushed the model to reason significantly

03:21:33.421 --> 03:21:35.551
better. For example, without

03:21:35.551 --> 03:21:37.741
reinforcement learning, model like GPD

03:21:38.062 --> 03:21:40.261
four o hits the ceiling on

03:21:40.261 --> 03:21:42.501
math and logic task. But

03:21:42.501 --> 03:21:44.801
with large scale IO, like

03:21:44.801 --> 03:21:46.466
the o one,

03:21:47.280 --> 03:21:48.501
performance jumps dramatically

03:21:50.491 --> 03:21:52.650
Almost doubling on the benchmark like

03:21:52.650 --> 03:21:54.911
Amy and to reach close to ninety five

03:21:55.871 --> 03:21:58.062
on the math. So the takeaway is

03:21:58.062 --> 03:21:59.910
simple. Reinforcement

03:22:00.131 --> 03:22:02.321
learning and scale is a key in

03:22:02.321 --> 03:22:03.411
to that

03:22:04.381 --> 03:22:06.650
unlocks strong reasoning. And it is

03:22:06.951 --> 03:22:09.316
becoming increasingly important and

03:22:09.411 --> 03:22:11.571
for a gen tech task.

03:22:12.531 --> 03:22:14.972
However, running this reinforcement

03:22:15.031 --> 03:22:17.432
learning task and the skill is never an easy

03:22:17.432 --> 03:22:18.735
job. It is

03:22:19.701 --> 03:22:21.901
especially true for architecture

03:22:21.901 --> 03:22:24.101
engineers want to provide the

03:22:24.101 --> 03:22:25.311
flexibility for large

03:22:26.271 --> 03:22:28.361
language model researchers while

03:22:28.361 --> 03:22:30.391
maintaining hyper during the computation.

03:22:32.450 --> 03:22:34.870
The main reason for this difficulty

03:22:35.171 --> 03:22:37.321
is that IOW isn't just a

03:22:37.802 --> 03:22:39.901
single model training loop. It is a complex

03:22:40.041 --> 03:22:42.081
data flow connecting Mont

03:22:43.121 --> 03:22:45.251
models simultaneously. In the age of

03:22:45.251 --> 03:22:47.461
the agents, we are

03:22:47.461 --> 03:22:49.721
often managing hundreds or

03:22:49.941 --> 03:22:51.651
thousands of independent agents.

03:22:51.986 --> 03:22:54.161
Running alongside the model training loop.

03:22:54.721 --> 03:22:56.931
On top of that, we have

03:22:57.391 --> 03:22:59.361
multiple workloads happening at once.

03:22:59.602 --> 03:23:00.901
Generation, inference,

03:23:01.851 --> 03:23:04.141
training, and synchronization. All

03:23:04.141 --> 03:23:04.721
tightly coupled.

03:23:06.191 --> 03:23:08.272
So meanwhile, when we

03:23:08.272 --> 03:23:10.081
scale up this up to

03:23:10.321 --> 03:23:12.721
the large language model, each of those

03:23:12.781 --> 03:23:15.181
steps like the after generation or the reward

03:23:15.181 --> 03:23:17.571
model inference become a massive distributed

03:23:18.211 --> 03:23:20.461
job on its own. So instead

03:23:20.461 --> 03:23:22.041
of a simple loop with

03:23:23.071 --> 03:23:25.571
multiple models, we are actually orchestrating

03:23:25.711 --> 03:23:28.071
a set of large scale distributed

03:23:28.071 --> 03:23:30.240
workloads that all need to

03:23:30.240 --> 03:23:31.381
run smoothly together.

03:23:35.291 --> 03:23:37.301
We face the exact problem about two

03:23:37.301 --> 03:23:39.351
years ago. And I would like to

03:23:39.352 --> 03:23:41.361
share the story of how the very beginning to

03:23:41.361 --> 03:23:43.682
life. The

03:23:43.901 --> 03:23:46.201
story starts with my boss, One day,

03:23:46.201 --> 03:23:48.381
he came to me with two seemingly

03:23:48.841 --> 03:23:51.031
contradictory demands. Like,

03:23:51.821 --> 03:23:53.852
bosses often do. First,

03:23:53.852 --> 03:23:55.711
he wanted a reinforcement learning

03:23:56.171 --> 03:23:58.591
library that could run on

03:23:59.221 --> 03:24:01.461
can run an entire experiment on

03:24:01.461 --> 03:24:03.631
a single machine. With, you

03:24:03.631 --> 03:24:06.110
know, relatively small model and, limited

03:24:06.110 --> 03:24:08.341
data set. Second, once

03:24:08.341 --> 03:24:10.200
he proved his idea worked,

03:24:10.421 --> 03:24:12.601
he want to scale it up to the

03:24:12.661 --> 03:24:15.021
multi model cluster effortlessly.

03:24:16.650 --> 03:24:18.811
So to achieve the first goal, the

03:24:18.811 --> 03:24:20.401
main hurdle was to

03:24:20.831 --> 03:24:22.972
was the limit the GPU resources on

03:24:22.972 --> 03:24:25.271
a single machine. To solve this,

03:24:25.271 --> 03:24:27.431
we combine the training and

03:24:27.431 --> 03:24:29.761
the inference engine into a hybrid engine.

03:24:30.641 --> 03:24:32.151
Using the GPU time sharing.

03:24:32.791 --> 03:24:34.871
In this in the first part of each

03:24:34.871 --> 03:24:36.892
step, the hybrid engine work

03:24:36.892 --> 03:24:38.781
as an inference server. It

03:24:39.391 --> 03:24:41.571
performs a restarting process, optimize the

03:24:41.892 --> 03:24:43.751
for generation and training parallelism.

03:24:45.221 --> 03:24:47.320
Loading model weights from the previous iteration.

03:24:47.982 --> 03:24:49.681
And after the generation,

03:24:50.011 --> 03:24:51.871
the hybrid engine morphs

03:24:52.671 --> 03:24:54.751
into the training mode. The

03:24:54.751 --> 03:24:56.991
second demand fostered the hybrid

03:24:56.991 --> 03:24:58.851
controller. It

03:24:59.891 --> 03:25:01.861
allows OEM researchers, including

03:25:02.001 --> 03:25:04.211
my boss, to you know, use

03:25:04.211 --> 03:25:06.621
single controller mode to write their

03:25:07.401 --> 03:25:09.501
and the trials. They can then scale

03:25:09.501 --> 03:25:11.631
this to multiple nodes

03:25:11.631 --> 03:25:13.711
just like writing a single thread

03:25:13.711 --> 03:25:15.731
script. Meanwhile, the

03:25:15.731 --> 03:25:17.191
multi controller mode

03:25:18.001 --> 03:25:20.501
still allows system engineers like me

03:25:21.101 --> 03:25:23.111
to implement high performance

03:25:23.331 --> 03:25:25.651
computation and maintain the bugability for

03:25:25.651 --> 03:25:28.051
everyone. If

03:25:28.051 --> 03:25:29.511
you, okay.

03:25:30.071 --> 03:25:32.231
So if it's it's your first time here in terms

03:25:32.231 --> 03:25:34.311
like controller or multi controller,

03:25:34.311 --> 03:25:36.351
let me quickly revisit the

03:25:36.351 --> 03:25:38.581
classic design. Choices. In

03:25:38.581 --> 03:25:40.001
a single controller system,

03:25:40.881 --> 03:25:43.171
one centralized controller managed all work

03:25:43.171 --> 03:25:45.371
workers running different

03:25:45.511 --> 03:25:47.391
programs. It is very flexible.

03:25:47.791 --> 03:25:49.181
Examples including like

03:25:49.806 --> 03:25:51.891
TensorFlow, one or race

03:25:51.891 --> 03:25:53.281
Isle like On

03:25:54.301 --> 03:25:56.722
the other hand, a multi controller assistant

03:25:57.931 --> 03:25:59.461
has each worker running its

03:26:00.101 --> 03:26:02.501
own controller with same program

03:26:02.641 --> 03:26:04.771
on different data. This is

03:26:04.771 --> 03:26:06.851
much efficient, and it is

03:26:07.331 --> 03:26:09.610
designed for designed to seeing in

03:26:09.610 --> 03:26:11.471
frameworks like PyTorch and JAX.

03:26:11.901 --> 03:26:14.349
Whenever. This SPMD mode make

03:26:14.471 --> 03:26:15.861
it a difficult to

03:26:16.621 --> 03:26:19.071
data. Flow. Involving many

03:26:19.131 --> 03:26:20.432
interacting models.

03:26:21.591 --> 03:26:23.751
In VERA, we proposed a new paradigm

03:26:23.751 --> 03:26:25.501
called the high grid controller.

03:26:25.982 --> 03:26:28.441
The idea is quite simple. Combines

03:26:28.441 --> 03:26:30.671
the best of both a central

03:26:31.575 --> 03:26:33.881
controller just high level

03:26:33.881 --> 03:26:35.900
algorithm logic while the multi controller

03:26:35.961 --> 03:26:37.881
handles large scale parallelism.

03:26:38.761 --> 03:26:41.051
Acquisition. In this way, we get both flex

03:26:41.771 --> 03:26:42.511
and efficiency.

03:26:45.182 --> 03:26:46.801
From a researcher's perspective,

03:26:47.711 --> 03:26:49.811
we keep the interface simple.

03:26:49.931 --> 03:26:51.950
It looks like a single controller.

03:26:52.421 --> 03:26:54.851
You can write the entire IO loop

03:26:54.931 --> 03:26:56.851
in just a few lines of code

03:26:57.331 --> 03:26:58.881
while the Verint handles the

03:27:00.181 --> 03:27:02.601
heavy lifting underneath. As

03:27:02.601 --> 03:27:04.991
shown in the this example, a developer

03:27:05.131 --> 03:27:06.991
can simply write a core algorithm

03:27:07.211 --> 03:27:09.211
logic in a single process. And

03:27:09.357 --> 03:27:11.141
come the

03:27:11.441 --> 03:27:13.691
distributing training is obstructed away.

03:27:14.011 --> 03:27:16.160
On top of that, it is a

03:27:16.160 --> 03:27:18.261
wide range of algorithm from PPO

03:27:18.892 --> 03:27:21.211
and GRPO to newer methods like Prime

03:27:21.211 --> 03:27:21.871
and DAPO.

03:27:24.522 --> 03:27:26.581
For the efficiency, we

03:27:27.221 --> 03:27:29.241
fully leverage the multi control

03:27:30.411 --> 03:27:31.681
paradigm under the hook.

03:27:34.001 --> 03:27:36.371
And since release,

03:27:36.511 --> 03:27:38.530
very hands received tremendous

03:27:38.911 --> 03:27:41.171
support from developers in both ByteDance

03:27:41.271 --> 03:27:42.811
seed and, the community.

03:27:43.361 --> 03:27:45.521
I would like to highlight some recent

03:27:45.521 --> 03:27:47.591
updates and features

03:27:47.650 --> 03:27:48.551
recently in progress.

03:27:50.341 --> 03:27:52.421
So recently, there has been

03:27:52.421 --> 03:27:53.711
growing interest in

03:27:54.642 --> 03:27:56.802
reinforcement learning for agent take

03:27:56.802 --> 03:27:59.000
tasks. Traditionally, the rollout

03:27:59.000 --> 03:28:01.141
means lot language model generates

03:28:01.191 --> 03:28:02.731
a text token that

03:28:03.570 --> 03:28:05.650
went straight into the reward and the

03:28:05.650 --> 03:28:07.671
training modules. But

03:28:07.671 --> 03:28:09.721
the agent take scenarios the

03:28:09.721 --> 03:28:11.831
model generates a special tokens.

03:28:12.232 --> 03:28:14.501
Like code. And then get executed

03:28:14.641 --> 03:28:15.621
in a sandbox.

03:28:16.711 --> 03:28:18.791
The feedback might include a unit

03:28:18.791 --> 03:28:20.651
test results or compile errors.

03:28:21.041 --> 03:28:23.541
And the model may interact with the environment

03:28:24.081 --> 03:28:26.141
over several turns before the

03:28:26.591 --> 03:28:28.301
lead trajectory is training.

03:28:29.261 --> 03:28:30.891
The rich tool using Paradigm is

03:28:31.611 --> 03:28:33.371
powerful. Training large model

03:28:34.331 --> 03:28:36.611
with reasoning capabilities. But

03:28:36.611 --> 03:28:38.721
this raise raises a question.

03:28:40.561 --> 03:28:42.671
Should we provide a mechanism that

03:28:43.231 --> 03:28:45.361
continuously integrate all

03:28:45.361 --> 03:28:46.781
plug ins and tools.

03:28:47.421 --> 03:28:49.441
Into the interim system Or should

03:28:49.441 --> 03:28:51.621
we mimic how the application

03:28:51.841 --> 03:28:52.811
engineers build as a agent?

03:28:54.031 --> 03:28:56.391
Calling the tools within the application

03:28:56.391 --> 03:28:58.591
logic in And after the

03:28:58.591 --> 03:29:00.311
discussion internally with this

03:29:00.950 --> 03:29:03.280
we chose later. The client

03:29:03.371 --> 03:29:05.411
side choice. Which we call the agent

03:29:05.711 --> 03:29:07.811
loop. We believe this

03:29:08.031 --> 03:29:10.221
design offers clean

03:29:10.441 --> 03:29:12.941
intuitive mechanism for both application engineers

03:29:13.881 --> 03:29:15.791
and algorithm researchers to simulate

03:29:15.921 --> 03:29:18.400
and implement RM

03:29:18.541 --> 03:29:20.551
agent flows. And

03:29:20.551 --> 03:29:22.171
here is a code example

03:29:22.631 --> 03:29:24.011
showing how there is

03:29:25.021 --> 03:29:26.181
this tool calling program.

03:29:27.541 --> 03:29:29.650
So roll out is hand delivered by

03:29:29.681 --> 03:29:31.791
Vero in a server based manner.

03:29:32.272 --> 03:29:34.541
User can leverage the generate

03:29:34.541 --> 03:29:35.761
HCI to

03:29:36.620 --> 03:29:38.860
use underlay engines like

03:29:38.860 --> 03:29:40.821
VOM or SPEAKER two. Geelong. For

03:29:41.041 --> 03:29:43.211
token generation. They also have

03:29:43.211 --> 03:29:45.450
the flexibility to define tools,

03:29:45.450 --> 03:29:47.731
send a box, and the timeline logics.

03:29:48.370 --> 03:29:50.731
And our async IO APIs

03:29:50.905 --> 03:29:52.961
allows allow for high profile

03:29:52.961 --> 03:29:54.901
high concurrency, ensuring

03:29:55.632 --> 03:29:57.601
efficient use of the hardware resources.

03:30:05.761 --> 03:30:08.181
In addition to new features,

03:30:09.030 --> 03:30:11.371
we are reflecting better to improve

03:30:11.821 --> 03:30:14.301
interface and establish it as a

03:30:14.301 --> 03:30:15.521
standard library.

03:30:17.421 --> 03:30:19.581
I will skip this the

03:30:19.581 --> 03:30:21.851
deep dive into this slide for the second.

03:30:21.851 --> 03:30:24.171
Time. But please check the

03:30:24.571 --> 03:30:26.521
issue tracker if you are interested in this

03:30:26.821 --> 03:30:27.121
details.

03:30:32.120 --> 03:30:34.181
Our reason to road maps folks

03:30:34.421 --> 03:30:36.371
also focus on modular design with

03:30:36.831 --> 03:30:39.301
cleaner obstruction back

03:30:39.521 --> 03:30:41.701
ends like FSTP two and Metro.

03:30:42.191 --> 03:30:44.391
We are building partial and

03:30:44.711 --> 03:30:46.731
full async rollout pipelines and

03:30:46.791 --> 03:30:47.861
releasing new

03:30:49.302 --> 03:30:51.310
recipes such as we bench. We are

03:30:51.311 --> 03:30:53.341
also working on more efficient

03:30:53.451 --> 03:30:55.881
multi model. Data

03:30:56.276 --> 03:30:58.371
transfer. All of this is

03:30:58.371 --> 03:31:00.441
tracked on our GitHub road

03:31:00.820 --> 03:31:02.641
maps. And we'd love to for you to

03:31:03.151 --> 03:31:04.802
follow along this contribute.

03:31:06.841 --> 03:31:08.921
So with the support of

03:31:08.921 --> 03:31:11.111
internal and external contributors,

03:31:11.111 --> 03:31:13.291
our community has reached over

03:31:13.541 --> 03:31:14.841
15,000 stores.

03:31:15.740 --> 03:31:17.501
2,400 folks,

03:31:18.061 --> 03:31:20.241
and, almost 400 contributors.

03:31:21.181 --> 03:31:23.211
This has been this has far

03:31:23.211 --> 03:31:24.911
exceeded our initial expectation.

03:31:26.591 --> 03:31:28.610
As a co initiator of the VERA,

03:31:28.812 --> 03:31:31.131
the best way I can thank our

03:31:31.131 --> 03:31:32.971
community is to check those up

03:31:33.530 --> 03:31:35.820
upcoming challenges and new possibilities

03:31:35.961 --> 03:31:37.451
for the two thousand sixteen.

03:31:41.222 --> 03:31:43.302
I would like to focus on

03:31:43.302 --> 03:31:45.562
four areas. We'll believe

03:31:45.701 --> 03:31:48.102
will be the greatest challenge for Verint

03:31:48.102 --> 03:31:50.321
in 2000 Please

03:31:50.321 --> 03:31:52.391
note that this is not a roadmap. This

03:31:53.011 --> 03:31:55.141
there are explorations We

03:31:55.141 --> 03:31:57.472
want to see if this these directions

03:31:57.531 --> 03:31:59.781
can take a very to the next

03:32:00.501 --> 03:32:02.571
stage in coping with rapid evolution

03:32:02.571 --> 03:32:03.981
of RLO in

03:32:04.620 --> 03:32:05.301
large language model.

03:32:07.061 --> 03:32:08.921
The first challenge you may

03:32:09.401 --> 03:32:11.661
many user face come from the limitation

03:32:11.721 --> 03:32:13.450
of rate. Two

03:32:13.751 --> 03:32:15.910
major issue prevents us from scaling

03:32:15.910 --> 03:32:18.261
up aisles task further. The

03:32:18.581 --> 03:32:21.081
first is the single head architecture.

03:32:21.111 --> 03:32:23.261
Of the Ray cluster.

03:32:24.301 --> 03:32:26.320
The head node bears a heavy burden

03:32:26.761 --> 03:32:28.901
man managing logging and the meta

03:32:29.276 --> 03:32:31.671
data without high available

03:32:33.841 --> 03:32:36.141
backups. If the

03:32:36.200 --> 03:32:37.820
header node fails, the entire

03:32:38.421 --> 03:32:40.552
cluster and all the running jobs goes

03:32:40.552 --> 03:32:42.682
down. The second one

03:32:42.741 --> 03:32:44.671
is a built in gRPC protocol.

03:32:45.312 --> 03:32:47.392
It requires serialization and

03:32:47.392 --> 03:32:49.311
deserialization for every

03:32:49.476 --> 03:32:51.501
message. This

03:32:51.501 --> 03:32:53.650
gets worse when we trying to

03:32:53.650 --> 03:32:55.740
transfer a CUDA tensor The

03:32:55.740 --> 03:32:57.891
lack of GPU direct RDMA

03:32:58.371 --> 03:33:00.721
forces our device to host a copy

03:33:00.721 --> 03:33:02.061
should slowing down the

03:33:03.181 --> 03:33:05.291
training loop and increase the risk of

03:33:06.261 --> 03:33:07.891
out of memory errors on the driver obsessed.

03:33:09.091 --> 03:33:11.081
So to address this, the

03:33:11.306 --> 03:33:13.472
Monarch project from

03:33:13.472 --> 03:33:15.621
Meta PyTorch looks like a strong candidate.

03:33:16.261 --> 03:33:17.562
Monarch is a distribute.

03:33:18.530 --> 03:33:20.711
To the programming framework based on the scalability

03:33:21.091 --> 03:33:22.937
scalable actual messaging.

03:33:23.711 --> 03:33:25.011
It's RDMA native

03:33:25.892 --> 03:33:27.881
communication serves us from

03:33:28.361 --> 03:33:30.521
saves us from the serialization and

03:33:30.521 --> 03:33:32.461
the host device copy overheads.

03:33:32.976 --> 03:33:34.731
It also avoids

03:33:35.511 --> 03:33:37.911
rigid cluster architecture. Using

03:33:37.911 --> 03:33:38.661
our custom

03:33:40.142 --> 03:33:42.521
customizable service discovery mechanism.

03:33:42.841 --> 03:33:44.911
So in 2016, we

03:33:44.911 --> 03:33:47.331
plan to explore Monarch as an alternative

03:33:47.791 --> 03:33:49.881
back end to provide a better

03:33:54.591 --> 03:33:56.381
The second exploration in

03:33:56.941 --> 03:33:58.681
involves the dual engine architecture.

03:33:59.641 --> 03:34:01.721
Currently, most reinforcement learning

03:34:01.721 --> 03:34:03.101
framework use heterogeneous

03:34:03.771 --> 03:34:05.991
engines. One for training like

03:34:05.991 --> 03:34:08.091
the PyTorch, and the other for

03:34:08.091 --> 03:34:10.221
inference like VOM and g log.

03:34:10.860 --> 03:34:12.721
This is done to maximize

03:34:12.961 --> 03:34:15.201
the throughput and utilize the existing

03:34:15.201 --> 03:34:17.321
optimizations. Whenever.

03:34:17.511 --> 03:34:19.591
We want to see if a unified engine

03:34:20.150 --> 03:34:21.971
perhaps starting with the JAX is

03:34:22.371 --> 03:34:24.661
possible. The

03:34:24.722 --> 03:34:26.821
issue with a separate engine is

03:34:27.222 --> 03:34:28.441
implementation divergence.

03:34:29.861 --> 03:34:31.401
Our policy training can suffer

03:34:32.201 --> 03:34:34.231
when the training probability deviates

03:34:34.231 --> 03:34:35.701
from the inference probability.

03:34:36.421 --> 03:34:38.761
We currently handle this by recomputed

03:34:39.221 --> 03:34:41.381
log probabilities but and

03:34:42.331 --> 03:34:44.471
unified engine may eliminate this

03:34:45.431 --> 03:34:47.831
recomputation phase. While

03:34:47.831 --> 03:34:49.991
the computation graph for the training and

03:34:50.312 --> 03:34:51.932
inference might still suffer

03:34:52.721 --> 03:34:54.801
a unified implementation would

03:34:54.801 --> 03:34:57.290
help to tackle to tackle

03:34:57.431 --> 03:34:59.601
and forward tackle the

03:34:59.601 --> 03:35:01.381
forward path and reduce the numerical

03:35:02.261 --> 03:35:02.761
deviation.

03:35:05.071 --> 03:35:07.411
And in 2025,

03:35:08.841 --> 03:35:11.011
we we saw a surge in

03:35:11.011 --> 03:35:13.361
a synchronized solution like StreamElements,

03:35:13.656 --> 03:35:15.751
laminar aerial, etcetera.

03:35:16.501 --> 03:35:18.961
These solutions vary in their rollout

03:35:19.296 --> 03:35:21.471
policies, wait synchronization, and

03:35:21.471 --> 03:35:22.931
the replay buffer strategies.

03:35:23.771 --> 03:35:26.111
However, most of them can strike

03:35:26.571 --> 03:35:28.960
truly similar Most of

03:35:28.960 --> 03:35:31.091
them are structurally similar and can be

03:35:31.331 --> 03:35:33.061
transformed into one another with

03:35:33.700 --> 03:35:35.971
minor policy tricks. So this

03:35:35.971 --> 03:35:37.031
suggests that there

03:35:39.331 --> 03:35:41.031
this suggest universal infrastructure

03:35:41.491 --> 03:35:43.272
for async is possible.

03:35:45.311 --> 03:35:47.811
To build this, we need to

03:35:48.342 --> 03:35:50.121
we need the right program interface.

03:35:51.161 --> 03:35:52.711
A a purely decorative

03:35:53.206 --> 03:35:55.400
approach is clean. But the limit

03:35:55.400 --> 03:35:57.181
is the flexibility for researchers.

03:35:57.711 --> 03:35:59.411
So instead, after the discussion,

03:36:00.341 --> 03:36:02.360
we prefer a programming approach.

03:36:02.682 --> 03:36:04.461
Extended single controller API

03:36:04.741 --> 03:36:07.062
to include components like

03:36:07.062 --> 03:36:09.161
replay buffers and the version, the ways

03:36:09.696 --> 03:36:11.851
synchronization. Well, working

03:36:11.851 --> 03:36:13.531
with the volcano engine team,

03:36:13.932 --> 03:36:16.251
on this and hope to offer a solution

03:36:16.251 --> 03:36:18.011
to our researchers too.

03:36:19.530 --> 03:36:21.890
And finally, in 2026,

03:36:22.851 --> 03:36:24.981
we want to preserve the

03:36:25.080 --> 03:36:27.421
original vision of the wearer. Tall

03:36:29.062 --> 03:36:31.401
experiment locally and scale effortlessly.

03:36:32.481 --> 03:36:34.721
As the code base grows, we don't want

03:36:34.721 --> 03:36:37.071
it to become bloated.

03:36:37.691 --> 03:36:39.801
We plan to separate the valuable recipe

03:36:41.861 --> 03:36:43.411
while keeping the

03:36:44.051 --> 03:36:45.651
core functions in the main

03:36:46.132 --> 03:36:47.966
variable code base. This is

03:36:48.221 --> 03:36:50.240
this ensures the core remains

03:36:50.301 --> 03:36:52.521
stable. For production use while

03:36:52.521 --> 03:36:54.141
the ecosystem still grows.

03:36:56.801 --> 03:36:58.901
I would like to thank all

03:36:58.916 --> 03:37:01.131
our our contributors again.

03:37:01.772 --> 03:37:03.311
And, if you are interested

03:37:04.181 --> 03:37:06.281
contributing especially to the Unified

03:37:06.761 --> 03:37:08.791
exploration, please contact us.

03:37:08.951 --> 03:37:10.721
And thank you very much for your attention.

03:37:23.110 --> 03:37:25.370
Yes. Thanks so much for the

03:37:26.081 --> 03:37:28.320
remarkable talk. Yeah, we have

03:37:28.320 --> 03:37:30.682
several Yeah. We have much

03:37:30.821 --> 03:37:33.241
more time for the question mark

03:37:33.791 --> 03:37:36.281
you have. Question, want to ask

03:37:36.341 --> 03:37:37.321
the whole project,

03:37:42.141 --> 03:37:44.221
Hello. Yeah. This was a great talk. I had a,

03:37:44.541 --> 03:37:46.731
just a quick question. I've been seeing a

03:37:46.731 --> 03:37:48.811
lot of people talking about, like,

03:37:49.931 --> 03:37:52.110
training inference log prop mismatch.

03:37:52.631 --> 03:37:54.601
And I was curious your thoughts, like,

03:37:54.761 --> 03:37:56.941
how big of a deal is this in the RL

03:37:57.001 --> 03:37:59.351
training procedure? And is this something that you're

03:37:59.591 --> 03:38:01.900
team is working on? So

03:38:01.900 --> 03:38:04.061
far, we find the deviation from the

03:38:04.061 --> 03:38:05.341
training and the inference

03:38:06.541 --> 03:38:08.391
mainly come from the different node.

03:38:08.871 --> 03:38:11.201
Implementation. So far. And,

03:38:12.161 --> 03:38:14.271
the first one first approach, we're

03:38:14.271 --> 03:38:16.601
trying to fix it is not know,

03:38:16.601 --> 03:38:18.796
fix the kernel difference, but

03:38:19.211 --> 03:38:21.481
to recompute the log probabilities

03:38:21.491 --> 03:38:23.381
when training. So

03:38:23.541 --> 03:38:25.321
that offers one choice,

03:38:25.621 --> 03:38:27.631
but, sometimes it doesn't work.

03:38:28.351 --> 03:38:30.301
And, the performance may be

03:38:30.881 --> 03:38:32.971
reduced. Due to other reasons. So

03:38:33.530 --> 03:38:35.711
we are also working, you know, to

03:38:35.711 --> 03:38:37.731
figure out how to

03:38:38.191 --> 03:38:40.291
align the performance of each kernels

03:38:40.352 --> 03:38:42.831
and And even

03:38:42.831 --> 03:38:45.001
with the know, different

03:38:45.001 --> 03:38:47.049
engines on training and in But,

03:38:47.231 --> 03:38:48.211
I think it

03:38:49.421 --> 03:38:51.211
what we're trying to do in this

03:38:51.771 --> 03:38:54.061
exploration is to find another fundamental change

03:38:54.061 --> 03:38:55.541
to the dual

03:38:56.581 --> 03:38:58.601
engine architecture. And if a single

03:39:00.771 --> 03:39:02.900
engine layer can help. But we have

03:39:02.900 --> 03:39:04.763
to admit is there is a possible to,

03:39:05.021 --> 03:39:07.131
you know, to reduce

03:39:07.512 --> 03:39:09.691
throughput or the latency for the inference

03:39:10.071 --> 03:39:12.107
engine? Both are implemented with Jacks

03:39:12.107 --> 03:39:14.111
or PyTorch. But

03:39:14.482 --> 03:39:16.911
that's also we want to Okay.

03:39:17.562 --> 03:39:18.561
Thank you. Cool. Hello.

03:39:19.900 --> 03:39:22.061
Hi. I do inference. I'm kinda

03:39:22.061 --> 03:39:24.142
new to IO. I have a basic question.

03:39:24.142 --> 03:39:25.451
How do you know the

03:39:26.892 --> 03:39:29.091
performance gap is due to the

03:39:29.731 --> 03:39:32.071
differences between training and inference

03:39:32.291 --> 03:39:33.851
engine. The

03:39:34.490 --> 03:39:36.731
performance gap. Like like like if you

03:39:36.731 --> 03:39:38.921
have like a mismatch between training and

03:39:38.921 --> 03:39:40.891
and jade. We'll cause some issues. So

03:39:41.610 --> 03:39:43.851
Why why are you so sure that it come it

03:39:43.851 --> 03:39:46.151
comes from this mismatch? I see.

03:39:46.632 --> 03:39:48.861
So the issue caused

03:39:48.861 --> 03:39:50.711
by the difference inference and

03:39:51.091 --> 03:39:52.971
training loop training engine is

03:39:53.616 --> 03:39:55.641
mainly reflected

03:39:55.641 --> 03:39:56.111
on the

03:39:58.831 --> 03:40:00.291
the the the convergence

03:40:01.271 --> 03:40:03.351
So, you know, the score for the

03:40:03.351 --> 03:40:05.461
training loop the models policy model

03:40:05.461 --> 03:40:07.421
is not stoop. Is not

03:40:07.981 --> 03:40:10.041
growing as expected. So that

03:40:10.041 --> 03:40:11.031
is a major

03:40:12.101 --> 03:40:13.991
issue we found when you know,

03:40:14.211 --> 03:40:16.331
there's mismatch from the

03:40:17.511 --> 03:40:19.681
the training under the inference. But

03:40:19.681 --> 03:40:21.732
not the performance one. So so,

03:40:21.812 --> 03:40:23.581
yeah, by performance, I mean that.

03:40:24.381 --> 03:40:26.461
Oh, I see. I see. Yeah. Yeah. Yeah. So

03:40:26.701 --> 03:40:28.861
are seeing that you have some scenario where it's

03:40:28.861 --> 03:40:30.941
matched and doesn't collapse. And

03:40:30.941 --> 03:40:33.051
the versus this doesn't match and

03:40:33.051 --> 03:40:35.360
it has some issue with like, the loss.

03:40:35.360 --> 03:40:37.441
You know, a quite easy way is

03:40:37.441 --> 03:40:38.751
to, you know, to

03:40:39.551 --> 03:40:41.791
see we have already, adopted the

03:40:41.791 --> 03:40:43.911
reconfiguration phase for,

03:40:43.911 --> 03:40:46.120
you know, like many, open source

03:40:47.581 --> 03:40:49.391
open source implementation

03:40:50.030 --> 03:40:52.171
we also have a re computation

03:40:52.311 --> 03:40:54.091
in the project. And

03:40:54.741 --> 03:40:56.641
our you can, you know, skip

03:40:56.841 --> 03:40:59.001
you can disable the recomputation and

03:40:59.001 --> 03:41:00.981
see how the performance

03:41:01.280 --> 03:41:02.842
improves or this improves.

03:41:03.541 --> 03:41:05.482
So that that could be, you know, a reason

03:41:06.232 --> 03:41:08.091
approach to see if there is

03:41:08.331 --> 03:41:10.651
if it is caused by, you know, the mismatch

03:41:10.651 --> 03:41:12.761
between the two things. Yeah. That makes a

03:41:12.761 --> 03:41:14.661
lot of sense. Thank you. Thank

03:41:15.121 --> 03:41:15.621
you.

03:41:17.661 --> 03:41:19.621
Yeah. I have a question. I

03:41:19.861 --> 03:41:22.341
we know that Waw is a open source project.

03:41:22.341 --> 03:41:23.671
Yeah. And a lot of

03:41:24.551 --> 03:41:26.711
code are open source. Yeah. I'm

03:41:26.771 --> 03:41:29.011
wondering, what's your plan for the,

03:41:29.011 --> 03:41:30.981
like, large scale

03:41:31.041 --> 03:41:33.201
training, like, digital training? Like, what is

03:41:33.201 --> 03:41:35.300
the I do remember

03:41:35.301 --> 03:41:37.316
it only supports limit

03:41:37.316 --> 03:41:39.661
of digital turning skin. Do you

03:41:39.901 --> 03:41:42.301
have plan to, like, support MOE

03:41:42.301 --> 03:41:44.551
better? Yes. Support like,

03:41:46.001 --> 03:41:47.111
export better,

03:41:48.831 --> 03:41:51.070
So, for the exact

03:41:51.070 --> 03:41:53.301
plan, I probably cannot

03:41:53.301 --> 03:41:55.695
give you the, you know, exact deadline when

03:41:55.771 --> 03:41:58.097
we also what kind of model by

03:41:58.097 --> 03:41:59.491
what date. But,

03:42:02.852 --> 03:42:04.791
from from my side because I

03:42:05.011 --> 03:42:07.071
put up a high single controller part,

03:42:07.472 --> 03:42:09.791
what I'm trying to do is to solve

03:42:09.791 --> 03:42:11.301
the obstacles

03:42:12.155 --> 03:42:14.651
preventing researchers and engineers

03:42:14.711 --> 03:42:16.331
to adopt larger models.

03:42:17.061 --> 03:42:18.751
That is, you know, the limited

03:42:19.166 --> 03:42:20.901
of from the rate. And,

03:42:21.921 --> 03:42:23.931
we were trying to, you know, get rid of

03:42:23.931 --> 03:42:25.371
the single head

03:42:26.331 --> 03:42:28.481
architecture and see we can scale it

03:42:28.481 --> 03:42:30.201
up to, you know, more

03:42:30.830 --> 03:42:32.781
nodes and, provide

03:42:32.911 --> 03:42:34.151
a safe pass for, you know, up

03:42:34.961 --> 03:42:37.120
upgrading their model size to

03:42:37.120 --> 03:42:38.361
much larger one.

03:42:56.941 --> 03:42:58.860
Yeah. Thanks for

03:42:59.241 --> 03:43:01.481
waiting. Interesting talk. Yeah. We would like

03:43:01.481 --> 03:43:03.891
to thanks all our

03:43:03.891 --> 03:43:05.991
speaker in front of us, like,

03:43:06.261 --> 03:43:06.541
yeah.

03:43:15.191 --> 03:43:17.272
And now we are moving into our

03:43:17.272 --> 03:43:19.660
poster session. The poster session

03:43:19.960 --> 03:43:21.421
start from twelve

03:43:22.141 --> 03:43:24.321
and end on thirteen.

03:43:25.410 --> 03:43:27.650
Yes. And also, a good news is

03:43:27.650 --> 03:43:29.581
that we will also provide a

03:43:29.966 --> 03:43:32.291
lunch yeah, lunch taco lunch

03:43:32.431 --> 03:43:34.511
here. The lunch will

03:43:34.511 --> 03:43:35.570
arrive at, like,

03:43:37.311 --> 03:43:39.731
fifty, twelve. Yeah. We all call everyone

03:43:39.870 --> 03:43:41.711
to like, keep

03:43:42.281 --> 03:43:44.381
keep here and join our

03:43:44.381 --> 03:43:46.321
procession and also

03:43:46.711 --> 03:43:48.410
all interesting car in the afternoon.

03:43:48.871 --> 03:43:50.951
Yeah. Thanks, everyone. Let's start our post

03:43:50.951 --> 03:43:51.451
session.

04:43:05.841 --> 04:43:07.781
We'll start at 1PM.

04:43:09.001 --> 04:43:11.031
And, yeah, Everyone

04:43:11.031 --> 04:43:13.151
is welcome to take a seat

04:43:13.151 --> 04:43:15.361
and Professor Wei

04:43:15.361 --> 04:43:17.851
will give us remarkable

04:43:18.230 --> 04:43:18.970
talk. Thanks.

04:46:08.841 --> 04:46:09.741
Hello. Hello.

04:46:11.931 --> 04:46:14.091
Hello, everyone. Yeah. Let's start

04:46:14.091 --> 04:46:15.711
our afternoon session.

04:46:16.171 --> 04:46:18.501
Yeah. Our afternoon session

04:46:18.501 --> 04:46:20.601
will start, where we

04:46:21.462 --> 04:46:22.911
talk today

04:46:23.561 --> 04:46:25.451
and give by professor Yu

04:46:26.650 --> 04:46:28.641
Yu is associate

04:46:28.861 --> 04:46:30.161
professor and

04:46:31.462 --> 04:46:33.481
Institute of Interdisciplinary Information

04:46:33.621 --> 04:46:35.171
Science. Tsinghua

04:46:35.791 --> 04:46:37.811
University, obtained his

04:46:38.031 --> 04:46:40.401
PhD PhD from UC Berkeley

04:46:41.061 --> 04:46:43.221
and he was a researcher at

04:46:43.221 --> 04:46:45.273
OpenAI from

04:46:45.273 --> 04:46:47.212
2090 to 2020.

04:46:47.831 --> 04:46:50.171
Yeah. His research focus around

04:46:50.311 --> 04:46:52.550
reinforcement learning, multi

04:46:52.550 --> 04:46:54.681
agent learning, and agent.

04:46:54.901 --> 04:46:57.019
Yeah. His representative

04:46:57.019 --> 04:46:59.319
work, including the value iteration

04:46:59.380 --> 04:47:01.121
network, the

04:47:01.390 --> 04:47:03.241
MAD DPG,

04:47:03.881 --> 04:47:05.101
MAPPO algorithm.

04:47:07.091 --> 04:47:09.331
Open eyes, hide and seek pojae, and

04:47:09.331 --> 04:47:11.353
aerial pojae. Yeah.

04:47:11.353 --> 04:47:13.591
He received the best paper reward

04:47:13.591 --> 04:47:15.720
at NeurIPS twenty

04:47:15.720 --> 04:47:17.951
sixteen, best

04:47:17.951 --> 04:47:19.551
demo reward at ICIA

04:47:20.171 --> 04:47:22.292
twenty twenty four MIT

04:47:22.751 --> 04:47:25.650
TR certified Asian pack, 2025

04:47:25.650 --> 04:47:27.641
reward. Yeah. Let's welcome,

04:47:27.701 --> 04:47:29.800
professor Yu Wu, for today's afternoon

04:47:29.861 --> 04:47:30.361
talk.

04:47:33.872 --> 04:47:36.112
Okay. Thanks for the introduction. And,

04:47:36.192 --> 04:47:38.591
I'm Yi from Xin University. And,

04:47:38.831 --> 04:47:40.933
yeah, I know everyone is still enjoyed

04:47:40.933 --> 04:47:42.761
the poster, so I'll have some background part

04:47:43.401 --> 04:47:45.561
so we can probably still enjoy the posters,

04:47:45.561 --> 04:47:46.441
but there are some

04:47:47.581 --> 04:47:49.741
content afterwards. So

04:47:49.741 --> 04:47:51.901
today, I will talk about, the aerial, talk

04:47:51.901 --> 04:47:54.060
about efficient, how can

04:47:54.060 --> 04:47:55.891
we do flexible and

04:47:56.466 --> 04:47:58.521
efficient agentic

04:47:58.579 --> 04:48:00.661
reasoning for with reinforcement learning for

04:48:00.661 --> 04:48:03.131
large and with broader agents. So

04:48:03.431 --> 04:48:03.961
this is

04:48:06.751 --> 04:48:09.001
joint project with Tsinghua University and Ant

04:48:09.001 --> 04:48:11.491
Group Yeah.

04:48:11.551 --> 04:48:13.641
So and in

04:48:13.641 --> 04:48:15.881
this talk, I will talk about a few

04:48:15.881 --> 04:48:17.771
content, we'll talk about

04:48:18.011 --> 04:48:20.031
first, what is agent degree enforcement

04:48:20.251 --> 04:48:22.541
learning? And second, we'll

04:48:22.541 --> 04:48:24.791
talk about why do we really need reinforcement

04:48:25.097 --> 04:48:26.951
learning. For agents. Well,

04:48:27.431 --> 04:48:29.451
discuss that with a concrete

04:48:29.511 --> 04:48:31.130
example, a search agent.

04:48:32.141 --> 04:48:34.461
It's perhaps the simplest agent, but it's still

04:48:34.461 --> 04:48:36.701
very challenging. And also talk about,

04:48:36.861 --> 04:48:39.091
with particular example, talk about

04:48:39.091 --> 04:48:41.011
the challenges for efficient agent

04:48:41.151 --> 04:48:43.191
to reinforcement learning, and then we talk about

04:48:43.191 --> 04:48:45.331
how we tackle these

04:48:45.451 --> 04:48:47.462
all these challenges with the aerial

04:48:47.462 --> 04:48:49.782
system, which is designed the

04:48:49.782 --> 04:48:51.612
principle of make

04:48:51.921 --> 04:48:53.461
agentic reinforcement efficient

04:48:54.444 --> 04:48:56.341
and flexible. K.

04:48:56.821 --> 04:48:57.721
There's content.

04:48:59.751 --> 04:49:01.911
So first, let's start with what is agent

04:49:01.911 --> 04:49:04.011
triggering learning? We know,

04:49:04.651 --> 04:49:06.511
there are a lot of milestones with

04:49:07.091 --> 04:49:09.300
models. And most of the milestones

04:49:09.300 --> 04:49:11.372
are driven by the

04:49:11.591 --> 04:49:14.111
reinforcement techniques. For example, in 2022,

04:49:14.111 --> 04:49:16.130
we have JAG GPT which is

04:49:16.511 --> 04:49:18.591
actually driven by the technique called

04:49:18.591 --> 04:49:20.241
reinforcement from human feedback.

04:49:21.061 --> 04:49:22.981
'24, we have reasoning models

04:49:23.221 --> 04:49:25.273
which the model can think and

04:49:25.273 --> 04:49:27.372
that's produced by the technical reasoning

04:49:27.671 --> 04:49:29.513
ring or reinforcement learning with

04:49:29.971 --> 04:49:31.731
verifiable reward or something like that.

04:49:32.421 --> 04:49:34.291
And more recently, we have

04:49:34.451 --> 04:49:36.720
agents and we also have some

04:49:36.720 --> 04:49:38.501
techniques called agentic reinforcement learning.

04:49:38.792 --> 04:49:41.021
We want that to be efficient, to be

04:49:41.021 --> 04:49:43.101
flexible. We want really want the technique of reinforcement

04:49:43.101 --> 04:49:45.201
learning to make our agent products more powerful.

04:49:45.721 --> 04:49:48.034
So today, we'll focus on the very latest part,

04:49:48.345 --> 04:49:50.366
reinforce more in for agents.

04:49:51.489 --> 04:49:53.351
So So

04:49:53.591 --> 04:49:55.371
one is agents. Right? So perhaps

04:49:55.671 --> 04:49:57.811
this year, the the first and

04:49:57.811 --> 04:49:59.851
most successful agent

04:49:59.851 --> 04:50:01.991
product is deep research. Right?

04:50:02.273 --> 04:50:03.571
Basically, you can ask,

04:50:04.321 --> 04:50:06.481
the chatbot deep research product to help

04:50:06.481 --> 04:50:08.611
you do a lot of very deep search

04:50:08.931 --> 04:50:11.091
multi turn reasoning with the final very

04:50:11.091 --> 04:50:13.161
concrete report. Also,

04:50:13.161 --> 04:50:14.651
we have MANUS. Well, right.

04:50:15.251 --> 04:50:17.131
So the difference basically in

04:50:17.452 --> 04:50:19.471
deep research, the agent is first the

04:50:19.471 --> 04:50:21.901
first time can really call tools, to

04:50:21.905 --> 04:50:24.251
multiple search queries

04:50:24.251 --> 04:50:26.702
and call actual tools to

04:50:26.702 --> 04:50:29.103
put a lot of extra ex external content

04:50:29.103 --> 04:50:31.122
in the content window. So

04:50:31.122 --> 04:50:33.141
the window can leverage a lot of

04:50:33.381 --> 04:50:35.471
external resources to perform reasoning.

04:50:35.471 --> 04:50:37.521
Right? And for Menace,

04:50:37.741 --> 04:50:40.122
it's it's it's beyond that because the agent

04:50:40.181 --> 04:50:42.201
has a sandbox. So we can

04:50:42.201 --> 04:50:44.341
do a lot of extra works.

04:50:44.341 --> 04:50:46.391
You can create a file Right? Can

04:50:46.391 --> 04:50:48.712
do a lot of terminal operations because

04:50:48.712 --> 04:50:50.821
you have a sandbox. So at the LM, have a laptop.

04:50:51.411 --> 04:50:53.661
So the agent is more powerful to

04:50:54.221 --> 04:50:56.671
much more things beyond. A classical

04:50:57.371 --> 04:50:58.851
chatbot LM. So,

04:51:01.069 --> 04:51:03.470
we do believe, right, this is really a trend

04:51:03.470 --> 04:51:05.511
that you

04:51:05.511 --> 04:51:07.591
know, when you talk about AI, it's really talk

04:51:07.591 --> 04:51:08.810
about agent applications.

04:51:09.761 --> 04:51:12.051
So a lot of

04:51:12.612 --> 04:51:14.861
concepts for building agent products.

04:51:14.861 --> 04:51:16.941
Right? You have a really complex environment. Right? You

04:51:16.941 --> 04:51:18.771
have a sandbox. Basically

04:51:19.310 --> 04:51:21.341
a laptop. Right? You can have browsers, you have

04:51:21.481 --> 04:51:23.821
a lot of operating systems.

04:51:23.821 --> 04:51:25.981
You have multiple reasoning, right, to

04:51:25.981 --> 04:51:28.351
do a lot of operations, a lot of actions

04:51:28.351 --> 04:51:30.731
in the environment, and every action require

04:51:30.951 --> 04:51:33.151
reasoning process. And you can do

04:51:33.151 --> 04:51:35.231
a lot of tool use. And also, it's

04:51:35.231 --> 04:51:37.311
multi agent system. Right? You need a lot of

04:51:37.311 --> 04:51:39.341
different agents but

04:51:39.341 --> 04:51:41.601
sometimes different models to do a very complex

04:51:43.251 --> 04:51:45.151
to complex drops. So

04:51:45.372 --> 04:51:47.712
very complex. So agent is pretty much complex.

04:51:48.501 --> 04:51:50.691
Right? So what is a gentle ring force modeling?

04:51:50.931 --> 04:51:52.261
Right? So a genteel reinforcement learning

04:51:53.001 --> 04:51:55.001
AKA. Alright. This is analog

04:51:55.380 --> 04:51:57.701
to the classical video

04:51:57.761 --> 04:51:59.782
game reinforcement learning. Pretty much

04:51:59.782 --> 04:52:01.811
the same. You have an environment In the video

04:52:01.811 --> 04:52:03.811
game, reinforcement learning, you have a

04:52:04.372 --> 04:52:06.591
environment which you can be a sandbox

04:52:07.881 --> 04:52:09.861
browser tools, Right? And you can

04:52:10.021 --> 04:52:12.101
you need an agent workflow to wrap the

04:52:12.101 --> 04:52:14.531
LMs with some tool called API

04:52:14.531 --> 04:52:16.711
so the LMs can interact with the environment.

04:52:17.061 --> 04:52:19.301
Right? You need some agent framework

04:52:19.301 --> 04:52:21.411
like open AI agent

04:52:21.411 --> 04:52:23.192
SDK, land gen, whatever things.

04:52:23.681 --> 04:52:26.171
Alright. Developer agent workflow, and then

04:52:26.230 --> 04:52:28.310
you want to run reinforcement learning to train

04:52:28.310 --> 04:52:30.311
this agent workflow to scale it up, to

04:52:30.311 --> 04:52:32.391
make it more powerful. So

04:52:32.391 --> 04:52:34.471
that's basically the workflow of reinforcement

04:52:34.631 --> 04:52:36.806
agent take reinforcement learning. Right? And

04:52:36.806 --> 04:52:38.841
the first question is, why do

04:52:38.841 --> 04:52:40.942
we really need reinforcement learning? Right? Because

04:52:40.942 --> 04:52:43.042
we already have an ALM. We

04:52:43.042 --> 04:52:45.282
already have a very, you know, powerful things. And

04:52:45.282 --> 04:52:47.351
why do we still need reinforcement learning to

04:52:47.351 --> 04:52:48.401
make it scale up?

04:52:49.452 --> 04:52:51.532
Well, a short answer to

04:52:51.532 --> 04:52:53.843
this question is you

04:52:53.843 --> 04:52:56.091
have two important things. You can

04:52:56.091 --> 04:52:58.331
if you can really run reinforcement to

04:52:58.891 --> 04:53:00.911
scale this agent take workflow really well,

04:53:01.551 --> 04:53:03.631
You have two advantages, very big advantages.

04:53:03.631 --> 04:53:05.721
One is simplicity. While simplicity

04:53:05.781 --> 04:53:07.981
is this simplicity means a lot of things.

04:53:08.141 --> 04:53:09.921
It typically means efficiency.

04:53:10.301 --> 04:53:12.391
Means easy to deploy, it means

04:53:12.631 --> 04:53:14.861
is cheap. Right? So simplest is good. Everyone

04:53:14.861 --> 04:53:16.941
likes simplicity. It can make your agent

04:53:16.941 --> 04:53:18.661
work for really, really some

04:53:19.061 --> 04:53:21.321
The second thing, it can really give you generalization

04:53:21.622 --> 04:53:23.631
capabilities since a lot of time you can

04:53:24.531 --> 04:53:26.761
base model to learn a lot of new capabilities.

04:53:27.880 --> 04:53:30.191
Okay. So this is very high level, you know,

04:53:30.671 --> 04:53:31.981
a in a very high level words.

04:53:32.862 --> 04:53:35.261
So here we'll make

04:53:35.261 --> 04:53:37.461
this high level concept more concrete.

04:53:37.701 --> 04:53:39.941
We'll talk about a very con a very concrete

04:53:39.941 --> 04:53:41.952
example, a search agent example.

04:53:41.952 --> 04:53:44.411
So search agents passed the simplest

04:53:44.470 --> 04:53:46.532
the agent you can ever imagine. Right? Just give you

04:53:46.532 --> 04:53:48.773
a query. It's even simpler than deep

04:53:48.773 --> 04:53:50.881
research. It simply give you

04:53:51.122 --> 04:53:53.282
give the agent an inquiry, and agent search online

04:53:53.282 --> 04:53:55.442
give you the answer. May think

04:53:55.442 --> 04:53:57.611
this search agent scenario is super, super

04:53:57.951 --> 04:54:00.021
I already have Google. Already

04:54:00.021 --> 04:54:01.971
have chat g p t. We have a lot of things. Right?

04:54:02.131 --> 04:54:04.212
But still, this scenario, a lot of the

04:54:04.212 --> 04:54:06.403
questions or settings is much more complicated

04:54:06.403 --> 04:54:07.251
than an exam.

04:54:09.081 --> 04:54:11.091
Here is a very concrete example. Right?

04:54:11.331 --> 04:54:12.951
Actually from from XBench.

04:54:14.112 --> 04:54:16.431
So the question is how many gold

04:54:16.431 --> 04:54:18.890
medals does China team

04:54:19.191 --> 04:54:21.122
win at 2020,

04:54:21.281 --> 04:54:23.331
twenty twelve London Olympics? Right? You

04:54:23.331 --> 04:54:25.611
will think this question is super super simple.

04:54:25.611 --> 04:54:27.720
Right? You can Google, you can do whatever

04:54:27.720 --> 04:54:29.881
thing. So I tried that last night, with

04:54:30.202 --> 04:54:32.241
Chagibidi. Laser so

04:54:32.241 --> 04:54:34.211
it basically says 38 gold medals.

04:54:34.691 --> 04:54:36.630
I I I chat with, Germany,

04:54:36.851 --> 04:54:39.310
30 gig eight gold medals. I chat with Groc,

04:54:39.712 --> 04:54:41.890
38 gold medals. Super

04:54:41.890 --> 04:54:43.891
simple, super fast. But actually,

04:54:43.891 --> 04:54:45.171
this is the correct answer.

04:54:46.291 --> 04:54:48.491
If you are familiar with the background in

04:54:48.491 --> 04:54:50.111
history and particularly sports,

04:54:51.212 --> 04:54:53.281
actually, the at this moment, the

04:54:53.281 --> 04:54:55.361
correct answer is 39

04:54:55.361 --> 04:54:57.511
gold medal, not 38. Why?

04:54:58.071 --> 04:55:00.201
Because if you're really

04:55:00.201 --> 04:55:01.661
familiar with that actually,

04:55:02.711 --> 04:55:04.941
the the the

04:55:04.941 --> 04:55:07.121
the woman's race working

04:55:07.171 --> 04:55:09.282
gold medal. And the civil medal

04:55:09.341 --> 04:55:11.640
was disqualified after, like,

04:55:11.640 --> 04:55:13.411
eleven years. For doping.

04:55:14.051 --> 04:55:16.081
Right? That's basically the result. So

04:55:16.081 --> 04:55:18.321
you can see like two Russian players was

04:55:18.321 --> 04:55:20.329
actually disqualified for doping.

04:55:20.329 --> 04:55:22.531
So at that time, at during

04:55:22.531 --> 04:55:24.691
the twenty twenty, year, right, China

04:55:24.691 --> 04:55:26.911
team got a bronze medal. But actually,

04:55:27.231 --> 04:55:29.362
eventually, the the the player got a gold

04:55:29.362 --> 04:55:31.442
medal after eleven years. So if you ask

04:55:31.442 --> 04:55:33.731
the question now, the correct answer

04:55:33.871 --> 04:55:36.061
should be 39. Right? Think

04:55:36.061 --> 04:55:38.103
about this question. Right? If you do

04:55:38.103 --> 04:55:40.183
some search online, you have a lot of the official

04:55:40.183 --> 04:55:42.141
resources to say 38.

04:55:42.381 --> 04:55:44.221
But actually, sometimes you can find some

04:55:44.782 --> 04:55:46.962
misleading or, like like, or conflicting

04:55:47.023 --> 04:55:49.161
information. Say, okay, sometimes 39

04:55:49.801 --> 04:55:51.951
Right? So which is correct? 38 or

04:55:51.951 --> 04:55:53.921
39? You need to do some deep

04:55:54.060 --> 04:55:56.391
research, some deep reasoning to say, okay. Probably

04:55:56.391 --> 04:55:58.811
there's some doping disqualification event

04:55:58.966 --> 04:56:00.980
happened. Okay. And you check the

04:56:00.980 --> 04:56:02.981
rule of IOC and really,

04:56:02.981 --> 04:56:05.141
it's it's a rule about doping. You want

04:56:05.141 --> 04:56:07.301
to make the conclusion that during

04:56:07.301 --> 04:56:09.461
twenty twenty year, right, the China team got

04:56:09.461 --> 04:56:11.731
38 gold medal, but eventually at this year,

04:56:11.731 --> 04:56:13.773
of doping event, China team

04:56:13.773 --> 04:56:15.811
got 39 gold medal. That's the correct

04:56:15.811 --> 04:56:18.281
answer. So there's a much it's like

04:56:18.631 --> 04:56:20.880
about it. There are so many partial information.

04:56:20.880 --> 04:56:22.901
There's so many conflicting information online.

04:56:23.261 --> 04:56:24.890
You really need the agent to

04:56:25.471 --> 04:56:27.731
efficiently reason about these uncertainties

04:56:28.031 --> 04:56:30.091
and partial informations to make the final result.

04:56:30.491 --> 04:56:32.571
So this even simple is even

04:56:32.571 --> 04:56:34.641
search is such a simple question, it's much more

04:56:34.641 --> 04:56:36.691
complicated than X Men. Than

04:56:36.691 --> 04:56:38.851
anything imagine. Then, okay, think about if you

04:56:38.851 --> 04:56:41.021
really want to do a purely

04:56:41.319 --> 04:56:43.372
workflow based agent. How can you do

04:56:43.372 --> 04:56:45.421
this? Right? Think about

04:56:45.421 --> 04:56:47.470
this. Right? Probably the answer

04:56:47.470 --> 04:56:49.952
is you need a multigent system. Right? You need multiple

04:56:50.251 --> 04:56:52.671
modules. You need to agent. You

04:56:52.970 --> 04:56:54.741
need a agent to check the uncertainty. You need

04:56:54.981 --> 04:56:57.051
agent to check conflict information. You

04:56:57.051 --> 04:56:58.521
need a lot of modules. Right?

04:56:59.880 --> 04:57:01.891
And Okay. If you only if you are not allowed to

04:57:01.891 --> 04:57:02.541
train. Right?

04:57:03.900 --> 04:57:06.028
The a lot of agents. And but

04:57:06.028 --> 04:57:07.821
but what about a reinforcement based agent?

04:57:08.061 --> 04:57:10.221
Right? So we did a project which we released a

04:57:10.221 --> 04:57:12.081
few months ago. Called Assructure.

04:57:13.171 --> 04:57:15.489
It's basically we we use, like, end to end

04:57:15.489 --> 04:57:17.371
reinforcement military and a simple search agent.

04:57:18.411 --> 04:57:20.261
If you can do reinforcement pretty

04:57:21.380 --> 04:57:23.861
right in the right way, I would say, then

04:57:23.861 --> 04:57:26.351
the agent architecture can be super super simple.

04:57:26.911 --> 04:57:29.231
It's basically just a 32

04:57:29.231 --> 04:57:31.251
billing it's just just an open source

04:57:31.251 --> 04:57:33.341
model with 32 billing parameters. It's a

04:57:33.341 --> 04:57:35.781
reasoning model. And I only have two tools,

04:57:36.081 --> 04:57:38.542
just a web browsing and search

04:57:39.001 --> 04:57:41.241
just two tools. So it's super, super simple.

04:57:41.241 --> 04:57:43.276
And the reasoning process can be very because it

04:57:43.276 --> 04:57:45.221
only have two models. Only two tools,

04:57:45.361 --> 04:57:47.151
a simple model. So it's super, super simple.

04:57:47.471 --> 04:57:49.631
Just do that and do wrong reinforcement

04:57:49.631 --> 04:57:51.761
training and well, this question is

04:57:51.761 --> 04:57:54.101
from the test set. It's not from training set. And eventually,

04:57:54.671 --> 04:57:57.071
can give you the final answer. So basically,

04:57:57.071 --> 04:57:58.841
the behavior is something like this. Like,

04:57:59.561 --> 04:58:01.901
the agent, like, search for, like, 60 steps

04:58:02.071 --> 04:58:04.371
and like doing a lot of cross checking

04:58:04.511 --> 04:58:06.831
type of verification, eventually give you the final

04:58:06.831 --> 04:58:09.061
answer. Like, after like

04:58:09.061 --> 04:58:10.861
60 steps. 60

04:58:11.161 --> 04:58:13.371
actions, Yeah. So really, the agent

04:58:14.181 --> 04:58:16.361
by reinforcement learns a lot of this

04:58:16.421 --> 04:58:18.581
kind of emergent double checking and cross

04:58:18.581 --> 04:58:20.791
checking behaviors. Make sure

04:58:21.031 --> 04:58:22.371
final answer is correct or not.

04:58:23.161 --> 04:58:25.021
Finally, we do some evaluations

04:58:26.101 --> 04:58:28.261
on very challenging benchmarks and and I think when

04:58:28.261 --> 04:58:30.041
we release this project, I think

04:58:30.991 --> 04:58:33.121
until, like, October like, really,

04:58:33.681 --> 04:58:36.081
until, like, the latest, the Gemini, and the latest,

04:58:36.081 --> 04:58:38.191
the cloud model released, the still this day of

04:58:38.191 --> 04:58:39.991
the art result, Just a 32 billing

04:58:41.031 --> 04:58:43.220
parameters. With reinforcement

04:58:43.220 --> 04:58:44.411
learning and with some

04:58:45.451 --> 04:58:47.663
test time enhancement. And you can achieve

04:58:47.663 --> 04:58:50.112
a STLVR result on this kind of search benchmarks,

04:58:50.171 --> 04:58:51.711
most of the challenging search benchmarks.

04:58:52.751 --> 04:58:55.101
Okay. So again, go back to the question,

04:58:55.101 --> 04:58:56.721
why do we need reinforcement learning?

04:58:57.202 --> 04:58:58.661
For agents? Right? If you can do reinforcement

04:58:59.521 --> 04:59:01.911
learning correctly, for agents, two advantages,

04:59:02.130 --> 04:59:03.991
you have simplicity, you have generalization

04:59:05.161 --> 04:59:07.181
It's can make everything

04:59:07.261 --> 04:59:09.351
much efficient and make

04:59:09.351 --> 04:59:11.591
your life easier. Okay.

04:59:11.591 --> 04:59:13.661
So that's the good part. And what are the

04:59:13.661 --> 04:59:15.611
challenges? What are the challenges

04:59:15.667 --> 04:59:17.683
that prevent you from applying

04:59:17.741 --> 04:59:19.442
reinforcement learning to this scenario?

04:59:20.091 --> 04:59:22.111
Well, first

04:59:22.331 --> 04:59:24.441
thing, is we have a very good observation.

04:59:24.781 --> 04:59:26.261
From this project is that

04:59:27.481 --> 04:59:29.631
so here is experiment, a test time experiment

04:59:29.631 --> 04:59:31.880
evaluation curve. So x axis

04:59:31.880 --> 04:59:33.891
is the number of terms. Or number of

04:59:33.891 --> 04:59:36.231
actions allowed to perform within an episode.

04:59:37.021 --> 04:59:39.521
Or trajectory or roll out. The y axis is the accuracy.

04:59:39.971 --> 04:59:41.271
And basically you find that

04:59:42.042 --> 04:59:44.282
for a simple model, if we can increase the number

04:59:44.282 --> 04:59:46.291
of turns, or number of actions,

04:59:46.291 --> 04:59:48.371
you have a clear trend that

04:59:48.371 --> 04:59:50.461
accuracy will improve.

04:59:50.461 --> 04:59:52.720
And that's the same thing for training. Right? If you really

04:59:52.721 --> 04:59:54.891
want the training, your reinforcement

04:59:54.891 --> 04:59:56.970
can incentivize reinforcement learning to learn some

04:59:56.970 --> 04:59:59.191
new capabilities, you need to allow the agent

04:59:59.191 --> 05:00:01.415
to explore sufficient That means

05:00:01.701 --> 05:00:03.981
during training process, need to allow

05:00:03.981 --> 05:00:05.721
the agent to do a lot of

05:00:06.362 --> 05:00:08.630
action turns. In order to

05:00:08.630 --> 05:00:10.773
make sure the agent can, if explore to

05:00:10.773 --> 05:00:11.911
discover new capability.

05:00:12.952 --> 05:00:15.112
So we do allow the agent to do

05:00:15.112 --> 05:00:17.241
a lot of actions. Actually, in eSearch

05:00:17.241 --> 05:00:19.561
project, we allow the agent to do a 128

05:00:19.561 --> 05:00:22.011
actions. Then you have a

05:00:22.011 --> 05:00:24.261
more severe problem. That is

05:00:24.421 --> 05:00:25.871
you can you can observe a very

05:00:26.431 --> 05:00:28.671
severe long tailed distribution. So so here

05:00:28.671 --> 05:00:30.792
is the statistics. So y axis

05:00:30.792 --> 05:00:32.862
is the counts, and axis access is

05:00:32.862 --> 05:00:33.931
the number of ad tokens

05:00:36.801 --> 05:00:38.951
So the average search time well,

05:00:39.191 --> 05:00:41.441
average trajectory takes some tools

05:00:41.581 --> 05:00:43.681
to search online. So

05:00:43.819 --> 05:00:45.900
every the average time for every trajectory

05:00:45.900 --> 05:00:48.141
is like ten to twenty minutes It's pretty

05:00:48.141 --> 05:00:50.271
pretty much okay. Right? But, if

05:00:50.271 --> 05:00:52.411
you really check the long tail tail distribution,

05:00:52.651 --> 05:00:54.301
or some a lot like you can

05:00:54.862 --> 05:00:56.241
almost like, for every batch,

05:00:56.921 --> 05:00:59.101
of trajectory, it's like almost surely

05:00:59.101 --> 05:01:01.362
you will have some very long trajectory

05:01:01.761 --> 05:01:04.071
that will take, like, 100 actions, and that will take

05:01:04.471 --> 05:01:06.501
one to two hour. Right? One to two hours.

05:01:06.501 --> 05:01:07.681
So it's super expensive

05:01:08.591 --> 05:01:10.751
for some particular long tail distribution.

05:01:10.751 --> 05:01:12.791
So that means imagine. Right? So

05:01:12.791 --> 05:01:14.801
you only have twenty four hour a day.

05:01:14.801 --> 05:01:17.121
Only have twenty hours. A day. And

05:01:17.121 --> 05:01:19.221
every batch, there exist

05:01:19.281 --> 05:01:21.542
a roll out that will one to two hours.

05:01:22.433 --> 05:01:24.532
That means, you know, if you

05:01:24.532 --> 05:01:26.701
collect a batch to train, then, you know,

05:01:26.941 --> 05:01:29.101
for a day, you probably only can take like 20

05:01:29.581 --> 05:01:31.641
batches, and that's super super slow. So

05:01:31.641 --> 05:01:33.673
this is kind of an agentic. Right? Because

05:01:33.673 --> 05:01:35.862
the the environment is super slow. You can't

05:01:35.862 --> 05:01:38.122
really do much things. You need large exploration.

05:01:38.561 --> 05:01:40.531
And everything will be super, super slow.

05:01:41.421 --> 05:01:43.501
So you will really have a slow training. Right?

05:01:43.501 --> 05:01:45.730
Because some prompt give you

05:01:45.730 --> 05:01:47.876
two hours some prompts give you

05:01:47.876 --> 05:01:49.871
like lot of prompts give you, like,

05:01:50.221 --> 05:01:52.331
the It's it's kind of like okay, but

05:01:52.331 --> 05:01:54.331
you have this kind of all wired prompt

05:01:54.491 --> 05:01:56.921
give you super slow. So whenever

05:01:56.921 --> 05:01:58.961
you collect a batch, every batch

05:01:58.961 --> 05:02:00.731
will have extremely long just a

05:02:01.131 --> 05:02:03.452
almost surely will have an extremely long

05:02:03.452 --> 05:02:05.661
allot. And, you know, then

05:02:05.661 --> 05:02:07.991
most of the GPU will be idle time. Right?

05:02:08.071 --> 05:02:10.091
And you're basically wasting a lot of computations.

05:02:11.691 --> 05:02:13.791
So, is kind of severe.

05:02:13.791 --> 05:02:16.031
One thing. So it's slow training. The training

05:02:16.031 --> 05:02:18.112
is very inefficient. I

05:02:18.112 --> 05:02:19.951
really want the training to be speed up.

05:02:20.351 --> 05:02:22.531
We want the reasoning process to be efficient.

05:02:22.861 --> 05:02:25.042
Right? The first thing. Is slow training.

05:02:25.261 --> 05:02:27.612
Training is inefficient. Second

05:02:27.671 --> 05:02:29.831
challenge is the workflow can be

05:02:29.831 --> 05:02:31.441
really, really, really complex. So

05:02:32.081 --> 05:02:33.201
I like, basically, quote

05:02:34.319 --> 05:02:36.560
a a tutorial. Like, a

05:02:36.560 --> 05:02:38.880
survey plot. Survey

05:02:38.880 --> 05:02:40.891
plot. Basically say, okay. There can be a lot of

05:02:40.891 --> 05:02:43.361
different modules of agents. Right? You need, like, planning

05:02:43.441 --> 05:02:45.781
module, You need tools. You need actions.

05:02:45.781 --> 05:02:47.571
You need, like, a lot of memory

05:02:48.292 --> 05:02:50.431
modules. Imagine when you want

05:02:50.431 --> 05:02:52.507
to train this complex agents,

05:02:53.112 --> 05:02:55.101
Right? There are a lot of modules. And every module

05:02:55.421 --> 05:02:57.581
probably need to train. Right? You

05:02:57.581 --> 05:02:59.542
probably need to adopt different models.

05:02:59.681 --> 05:03:01.991
Right? Need, very complex workflows.

05:03:02.212 --> 05:03:04.390
And many nested parts

05:03:05.622 --> 05:03:06.821
will be need to be trained.

05:03:07.853 --> 05:03:10.091
And so I want to say, training

05:03:10.091 --> 05:03:12.191
agents is much more complicated

05:03:12.521 --> 05:03:14.511
by simply training a reasoning model.

05:03:14.911 --> 05:03:17.171
Right? It's much beyond a simple

05:03:17.981 --> 05:03:19.981
single model for rollout, a single model to train.

05:03:20.141 --> 05:03:22.351
It can be a nested model. Some

05:03:22.351 --> 05:03:24.569
models are not to train, Some models need

05:03:24.569 --> 05:03:26.763
to train. You have a lot of models, and

05:03:27.341 --> 05:03:29.361
even for a rollout, it's a very complex

05:03:29.501 --> 05:03:31.771
process. So we have a cartoon,

05:03:32.071 --> 05:03:34.311
for this. Right? Basically, two challenges, low

05:03:34.311 --> 05:03:36.771
efficiency very slow, Right?

05:03:37.069 --> 05:03:39.089
You have a lot of your players. It's just

05:03:39.089 --> 05:03:41.201
like all of our friends just like playing cards

05:03:41.201 --> 05:03:43.391
and why use it. While you are

05:03:43.391 --> 05:03:45.671
like working hard. And sometimes, you

05:03:45.671 --> 05:03:47.751
know, when you want to train, because of agent,

05:03:47.751 --> 05:03:49.971
it's so complicated. So

05:03:49.971 --> 05:03:52.051
a lot of time, you'll develop you'll need

05:03:52.051 --> 05:03:54.112
a lot of engineering efforts It'll be

05:03:54.112 --> 05:03:56.381
very complicated. So,

05:03:56.601 --> 05:03:57.841
you know, train

05:03:58.621 --> 05:04:00.942
deployment gap, it's very it's very

05:04:01.001 --> 05:04:03.202
inefficient for development. Training

05:04:03.202 --> 05:04:04.301
is very inefficient

05:04:05.611 --> 05:04:07.390
because of this kind of long tail distribution.

05:04:08.192 --> 05:04:10.391
And that's what we really want to address. So we can

05:04:10.391 --> 05:04:12.481
have that. Have the idea of developing

05:04:12.481 --> 05:04:14.661
new system called iReal, So

05:04:14.661 --> 05:04:16.821
the Aerial, we really want to design

05:04:16.821 --> 05:04:18.581
for agent to reinforce more learning

05:04:19.301 --> 05:04:21.491
with sufficient and flexible in mind. So

05:04:21.491 --> 05:04:23.651
the key idea is we really want to decouple

05:04:23.651 --> 05:04:25.691
everything Really want to when you

05:04:25.691 --> 05:04:27.739
change one thing, the other

05:04:27.739 --> 05:04:29.900
part of the system or your code, you don't need to

05:04:29.900 --> 05:04:31.931
change anything. And that's what we want to

05:04:31.931 --> 05:04:34.101
present. And we believe that's the key

05:04:34.741 --> 05:04:36.981
for efficient agent development and efficient

05:04:36.981 --> 05:04:39.093
agent training. You need to

05:04:39.093 --> 05:04:41.481
decouple everything as much as possible.

05:04:42.061 --> 05:04:43.441
So the first thing is,

05:04:44.141 --> 05:04:46.011
we have an idea of asynchronous

05:04:46.231 --> 05:04:48.691
reinforcement learning. So we basically decouple training

05:04:49.831 --> 05:04:50.489
and inference.

05:04:52.122 --> 05:04:53.991
And the reason for this is

05:04:54.462 --> 05:04:56.491
for the training, you don't need to wait. So the

05:04:56.491 --> 05:04:58.651
trainers, the training resources

05:04:58.651 --> 05:05:00.831
just keep running on. Keep running

05:05:00.831 --> 05:05:02.989
on. And when you finish your rollout, just send to

05:05:02.989 --> 05:05:05.131
the trainer nodes. And the trainer nodes have

05:05:05.131 --> 05:05:07.211
a data buffer. Just keep whenever you have a

05:05:07.211 --> 05:05:09.523
batch, just train that. So

05:05:09.523 --> 05:05:11.591
in this way, you know, don't

05:05:12.231 --> 05:05:14.551
inference engine doesn't need to wait. Just continue generating

05:05:14.551 --> 05:05:16.560
new batch. Whenever you have

05:05:16.560 --> 05:05:18.521
a new set of parameters, you

05:05:18.841 --> 05:05:21.081
interrupt all the ongoing rollouts and

05:05:21.081 --> 05:05:23.146
sync them with the latest weights. And

05:05:23.146 --> 05:05:25.216
then, you know, you are there

05:05:25.216 --> 05:05:27.271
were there will be some trajectories

05:05:27.271 --> 05:05:29.511
that partially is generated

05:05:29.511 --> 05:05:31.761
by a old version and partially generated

05:05:31.901 --> 05:05:33.891
by a new version. It's kind of okay.

05:05:34.051 --> 05:05:35.911
So just continue doing this as

05:05:36.601 --> 05:05:38.841
continue training, continue to sync up the weights.

05:05:38.841 --> 05:05:41.241
In this way, the generation will have

05:05:41.801 --> 05:05:43.941
no idle time. The generation knows we'll

05:05:43.941 --> 05:05:45.851
have no idle time. So basically,

05:05:46.267 --> 05:05:48.561
break the long tail distribution in a particular

05:05:48.561 --> 05:05:50.862
batch because there's no batch

05:05:50.862 --> 05:05:53.081
generation anymore. It's continued generation

05:05:53.081 --> 05:05:55.121
whenever it's there's a new way to

05:05:55.121 --> 05:05:57.171
sync up with weights, and continue to

05:05:57.171 --> 05:05:59.221
generate continue to generate. So there's no

05:05:59.221 --> 05:06:01.103
gap. So with a proper separation,

05:06:01.861 --> 05:06:03.941
our decoupling with the training resources

05:06:03.941 --> 05:06:06.021
and generation resources, you can achieve no

05:06:06.021 --> 05:06:08.241
way to reinforce malaria. With fully

05:06:08.241 --> 05:06:10.611
utilized. GPU resources. And in in the

05:06:10.611 --> 05:06:12.731
e search example particularly, we

05:06:12.731 --> 05:06:14.501
have more than five x speed up

05:06:14.741 --> 05:06:17.171
in the training time. This is super, super

05:06:17.171 --> 05:06:19.311
fast. But every coin has

05:06:19.311 --> 05:06:19.811
two sides.

05:06:21.531 --> 05:06:23.630
So, you know, in this change, you

05:06:23.630 --> 05:06:25.909
have some rule out of a across different model

05:06:25.909 --> 05:06:27.711
versions. Some of the parts of the model

05:06:28.271 --> 05:06:30.431
is generated by the old version, and some are part

05:06:30.431 --> 05:06:32.341
of the model is generated by another version.

05:06:32.501 --> 05:06:34.661
So we have this kind of trajectories work across

05:06:34.661 --> 05:06:36.761
different model versions. So,

05:06:37.220 --> 05:06:38.841
we have a very classical

05:06:39.300 --> 05:06:41.071
important concept on idea or term

05:06:41.311 --> 05:06:43.011
in reinforcement called stillness.

05:06:43.601 --> 05:06:45.380
So we call a sample stillness

05:06:45.681 --> 05:06:46.900
with two is because

05:06:47.751 --> 05:06:49.970
the the the oldest, the model version generated

05:06:49.970 --> 05:06:52.461
by this sample The different the model difference

05:06:52.521 --> 05:06:54.211
between the oldest the model generate

05:06:54.691 --> 05:06:55.881
for this trajectory

05:06:56.843 --> 05:06:59.031
between that and latest model.

05:06:59.271 --> 05:07:01.659
So for this particular red trajectory,

05:07:01.659 --> 05:07:04.050
the stillness too. So you

05:07:04.331 --> 05:07:06.651
you know, in order to be fast, you need to really

05:07:06.651 --> 05:07:08.351
careful about

05:07:08.781 --> 05:07:10.851
staining the sample. Because

05:07:10.851 --> 05:07:13.091
if you don't do anything for the stilted sample, we

05:07:13.091 --> 05:07:15.481
have some experiment result basically

05:07:15.542 --> 05:07:17.811
saying that if you if

05:07:18.032 --> 05:07:20.191
the if the data stillness increases, then

05:07:20.191 --> 05:07:22.251
the Right? So

05:07:22.251 --> 05:07:24.301
that's what that's something you don't want.

05:07:24.782 --> 05:07:27.023
So you do we have some idea called system

05:07:27.023 --> 05:07:29.141
and algorithm co design. We design

05:07:29.141 --> 05:07:31.220
a system that can do efficient generation,

05:07:31.220 --> 05:07:33.081
streaming generation. It's pretty fast.

05:07:33.321 --> 05:07:35.398
Also need to do some tiny changes to

05:07:35.398 --> 05:07:37.751
the algorithm side. To make sure,

05:07:38.211 --> 05:07:40.101
the algorithm can tolerant

05:07:40.319 --> 05:07:42.380
such a still samples. Okay. So

05:07:42.380 --> 05:07:44.711
we do some simple changes. On the system side,

05:07:44.871 --> 05:07:46.942
we have a centralized controller. We'll

05:07:46.942 --> 05:07:48.712
basically check whether

05:07:49.011 --> 05:07:51.231
there's some very stilled sample. If they are

05:07:51.231 --> 05:07:53.542
really a very stale sample, we'll

05:07:53.542 --> 05:07:55.702
pass the generation process to wait until you have

05:07:55.702 --> 05:07:57.851
the longest sample to finish. Well,

05:07:57.851 --> 05:08:00.011
why we want to wait in instead of, like, just to

05:08:00.011 --> 05:08:02.041
throw it out? You need to wait. You

05:08:02.041 --> 05:08:04.300
can't throw out a very stale sample

05:08:04.361 --> 05:08:06.380
because you're throwing out a still

05:08:06.380 --> 05:08:08.532
sample, you are essentially putting

05:08:08.532 --> 05:08:10.952
a very bad bias the model. You're

05:08:10.952 --> 05:08:13.271
running a rejective sampling. So whenever

05:08:13.271 --> 05:08:15.431
you throw out the sample, you are basically telling the

05:08:15.431 --> 05:08:16.781
model never to generate this

05:08:17.661 --> 05:08:20.161
So never throw out any sample for a reinforcement.

05:08:20.711 --> 05:08:22.951
So but, you know, you don't really

05:08:22.951 --> 05:08:24.811
want a very still sample braking

05:08:24.961 --> 05:08:26.991
system So we have centralized

05:08:27.050 --> 05:08:29.081
control to monitor the still stillness of

05:08:29.081 --> 05:08:31.300
the samples. If you really have an off

05:08:31.300 --> 05:08:33.341
layer there, you have to wait. But kind of an

05:08:33.341 --> 05:08:35.171
off layer is very rare, so it's kind of okay.

05:08:35.411 --> 05:08:37.571
So the system side. The Argon side,

05:08:38.130 --> 05:08:40.550
I want go to the details, but basically

05:08:40.611 --> 05:08:42.641
we do a very simple change. A

05:08:42.641 --> 05:08:44.121
very simple change just like

05:08:44.761 --> 05:08:46.861
we basically decouple the

05:08:49.071 --> 05:08:50.531
sampling term and

05:08:51.091 --> 05:08:53.251
the trust of region term. If you are interested, you can

05:08:53.251 --> 05:08:55.401
check our paper. It's a very tiny

05:08:55.401 --> 05:08:57.872
one line of the code change. And then

05:08:57.872 --> 05:09:00.061
you can basically can

05:09:00.061 --> 05:09:02.461
ensure also basically ensure the model can tolerant

05:09:03.181 --> 05:09:04.841
of stillness, like, up to

05:09:05.241 --> 05:09:07.111
eight. 16. Stillness.

05:09:07.681 --> 05:09:09.861
So with that, everything right?

05:09:10.101 --> 05:09:12.421
You can basically have a very good training

05:09:12.421 --> 05:09:14.201
curve that can even observe

05:09:15.511 --> 05:09:17.371
emergent behavior in a agentic

05:09:17.702 --> 05:09:19.937
in a for the search agent. So the middle part

05:09:19.937 --> 05:09:22.001
is basically the average The

05:09:22.001 --> 05:09:24.221
purple curve is average of actions. Per

05:09:24.221 --> 05:09:26.532
episode. And the and the blue

05:09:26.532 --> 05:09:28.659
curve is the maximum actions per episode.

05:09:28.659 --> 05:09:30.900
You can see a very clear increase of

05:09:30.900 --> 05:09:31.961
emergent behaviors

05:09:32.933 --> 05:09:35.112
just like the the r one zero project.

05:09:36.023 --> 05:09:38.181
Okay. And also, the other part is we

05:09:38.181 --> 05:09:40.333
do a lot of we do a lot of optimization on

05:09:40.333 --> 05:09:42.380
the aerial system. So in

05:09:42.380 --> 05:09:44.471
our latest version, we can

05:09:44.712 --> 05:09:46.720
like, only support training

05:09:46.720 --> 05:09:48.961
of a a channel one two hundred and thirty

05:09:48.961 --> 05:09:51.231
five billion model. Was only

05:09:51.631 --> 05:09:53.841
six nodes. Which is, like, 48

05:09:54.561 --> 05:09:56.791
GPUs. Okay. So that's really

05:09:57.221 --> 05:09:59.381
also resource efficient. So we really want

05:09:59.381 --> 05:10:01.692
everyone to have a chance to train

05:10:01.692 --> 05:10:03.800
large models. Because large models are the most

05:10:03.800 --> 05:10:06.050
powerful models. So this kind of a training curve.

05:10:06.050 --> 05:10:07.999
So release this feature in

05:10:08.060 --> 05:10:09.111
the next a few days.

05:10:10.230 --> 05:10:12.310
So, that's the algorithm part,

05:10:12.310 --> 05:10:14.341
but also we want to reduce the

05:10:14.401 --> 05:10:16.581
engineering efforts. Right? So we want to make

05:10:16.581 --> 05:10:18.721
everyone that can do

05:10:19.362 --> 05:10:21.603
minimum out coding change

05:10:21.761 --> 05:10:24.220
engineering change in order to

05:10:24.220 --> 05:10:25.841
run agent take reinforcement training.

05:10:26.551 --> 05:10:28.401
So we have a feature already there

05:10:28.843 --> 05:10:30.971
is you can develop a

05:10:31.452 --> 05:10:33.471
with opening an agent's SDK. And

05:10:33.471 --> 05:10:35.751
you can basically change your code

05:10:35.751 --> 05:10:37.871
with one or two lines, and you can run agent

05:10:37.871 --> 05:10:39.971
reinforcement learning with Ariel. So we can

05:10:39.971 --> 05:10:41.851
just use the open AI agent SDK

05:10:42.011 --> 05:10:44.121
to build an agent. And then you just

05:10:44.121 --> 05:10:46.621
define a reward model, then you can just launch.

05:10:46.941 --> 05:10:49.081
Everything is there. You could don't need to do

05:10:49.081 --> 05:10:51.241
anything. So the key thing is we

05:10:51.241 --> 05:10:53.321
really designed aerial in

05:10:53.321 --> 05:10:55.421
a principle of everything is service.

05:10:55.911 --> 05:10:57.991
Because an agent is an agent service, training is

05:10:57.991 --> 05:10:59.981
a training service. Everything is with some

05:11:00.942 --> 05:11:03.202
with with with kind of this kind of agent service

05:11:03.501 --> 05:11:05.241
interruption. So it's pretty much modular.

05:11:05.561 --> 05:11:07.641
So when you want to change the front end, you don't need

05:11:07.641 --> 05:11:09.718
to touch the back end. One one change the back end, don't

05:11:09.718 --> 05:11:10.771
need to touch the front end.

05:11:11.810 --> 05:11:12.991
And so,

05:11:14.452 --> 05:11:16.792
and more and more recently, we also

05:11:17.921 --> 05:11:20.231
support multi gene system and multi model training.

05:11:20.391 --> 05:11:22.471
So here is an example of TAU bench,

05:11:22.471 --> 05:11:24.251
TAU two bench, TAU square bench.

05:11:24.702 --> 05:11:27.103
So TAU 12 bench is like a customer

05:11:27.103 --> 05:11:29.211
service scenario. So you do need to

05:11:30.391 --> 05:11:32.631
and a service and a database. To

05:11:32.631 --> 05:11:34.712
do a lot of this kind of conversation. So

05:11:34.712 --> 05:11:36.251
you do need a lot of models

05:11:36.792 --> 05:11:38.831
So how can we do this? So we do support

05:11:39.391 --> 05:11:41.041
multi agent training. In

05:11:41.441 --> 05:11:43.630
like, particularly you can support multi

05:11:43.630 --> 05:11:45.691
model training. It's same thing, particularly

05:11:45.691 --> 05:11:47.881
if every model is just a service. You

05:11:47.881 --> 05:11:49.721
just launch service. Different

05:11:50.021 --> 05:11:52.050
models, and it's already there. You don't need to change

05:11:52.050 --> 05:11:54.122
too much code. It's already super simple.

05:11:54.331 --> 05:11:56.211
Have a one line code and launch a service.

05:11:56.371 --> 05:11:58.702
Another line of code to launch another service. Everything

05:11:58.702 --> 05:12:00.711
is there. And also,

05:12:00.711 --> 05:12:02.569
we have some, scripts language

05:12:03.661 --> 05:12:05.761
to specify, different allocation

05:12:05.900 --> 05:12:07.901
mode and parallelizing strategies for different

05:12:07.901 --> 05:12:10.011
mode. Different models. It's very,

05:12:10.011 --> 05:12:11.391
very convenient.

05:12:13.112 --> 05:12:15.292
So this multi agent training and

05:12:15.351 --> 05:12:15.711
also

05:12:17.630 --> 05:12:19.431
is all. End of this month.

05:12:19.911 --> 05:12:22.171
We'll even support a bit, like,

05:12:22.201 --> 05:12:24.391
all the agent framework beyond

05:12:24.391 --> 05:12:25.991
just the OpenAI agent SDK.

05:12:26.872 --> 05:12:28.591
So you can do whatever

05:12:28.911 --> 05:12:31.400
agent framework you want. We'll basically use

05:12:31.621 --> 05:12:33.853
URLs, HTTP URLs

05:12:33.853 --> 05:12:35.631
to redirect your messages

05:12:36.651 --> 05:12:39.050
to the inference models to our training engines. So we can base

05:12:39.050 --> 05:12:41.111
the with this, design, we

05:12:41.111 --> 05:12:43.051
can achieve, like, zero code change.

05:12:43.351 --> 05:12:45.771
To run agent t reinforcement learning. So we just

05:12:45.911 --> 05:12:48.261
defined whatever, agent workflow

05:12:48.401 --> 05:12:50.571
you want. And we will do everything

05:12:50.571 --> 05:12:52.640
in the back end. To ensure all the

05:12:52.640 --> 05:12:54.491
data will send it to the trainer engine

05:12:54.811 --> 05:12:57.221
and will update weights. So this

05:12:57.361 --> 05:12:59.601
kind of zero code reinforcement lanes, like,

05:12:59.601 --> 05:13:01.731
in experimental animal probably release

05:13:01.731 --> 05:13:03.542
that in the dismounts.

05:13:04.362 --> 05:13:06.380
Later this month. And also we can

05:13:06.380 --> 05:13:08.321
do multitask trainings very straightforward.

05:13:09.282 --> 05:13:11.361
And more importantly, because

05:13:11.361 --> 05:13:13.641
the back end is really decoupled,

05:13:13.702 --> 05:13:16.061
is very flexible, So

05:13:16.201 --> 05:13:18.301
then some people will think, you know, we

05:13:18.301 --> 05:13:20.462
have different GPUs. Right? We have inference

05:13:20.462 --> 05:13:21.921
GPUs. We have training GPUs.

05:13:22.481 --> 05:13:24.301
So, you know, inference GPUs is

05:13:24.542 --> 05:13:26.702
better for token by token generation, and training

05:13:26.702 --> 05:13:28.741
GPUs better than forward

05:13:28.741 --> 05:13:31.011
and backward. Right? And inference GPU is much cheaper.

05:13:31.811 --> 05:13:34.050
So we have a project called Aerial

05:13:34.050 --> 05:13:36.130
Hacks. So we are basically, you

05:13:36.130 --> 05:13:38.150
know, because we have such a flexible

05:13:38.211 --> 05:13:40.483
back end so we can use inference GPU

05:13:40.630 --> 05:13:42.141
to do generation.

05:13:42.631 --> 05:13:44.121
Training GPU to do

05:13:44.681 --> 05:13:46.691
training. Or profiling. So So

05:13:46.691 --> 05:13:48.931
if you can do a very well balanced, you can

05:13:48.931 --> 05:13:50.981
reduce the cost like

05:13:50.981 --> 05:13:53.021
40%, the cost. So

05:13:53.341 --> 05:13:56.001
so, for example, for the, 14,000,000,000

05:13:56.060 --> 05:13:58.211
model, 14,000,000,000 parameter model,

05:13:58.211 --> 05:14:00.481
if you use all the edge h

05:14:00.481 --> 05:14:02.591
hunt h the training GPUs,

05:14:02.591 --> 05:14:04.991
you may need, like, a $100 per year per

05:14:04.991 --> 05:14:07.001
per per like,

05:14:07.001 --> 05:14:09.401
per hour. But if you can use a mix using

05:14:09.401 --> 05:14:11.381
a real HEX, then you're only,

05:14:11.521 --> 05:14:13.681
like, $80 point. Power. It's

05:14:13.681 --> 05:14:15.641
basically, like, it's

05:14:15.701 --> 05:14:17.141
basically 40%

05:14:17.702 --> 05:14:19.771
cost reduction. Super, super cost

05:14:19.771 --> 05:14:21.781
efficient. And also we have some

05:14:21.781 --> 05:14:23.941
more, things to release. So in the

05:14:23.941 --> 05:14:26.192
following week, we have a lot of components.

05:14:26.251 --> 05:14:27.911
Well, will be open sourced.

05:14:29.051 --> 05:14:31.212
With our end, the engineers in

05:14:31.212 --> 05:14:33.310
in the end group. So to wrap

05:14:33.310 --> 05:14:35.390
up, so basically, it's

05:14:35.390 --> 05:14:37.491
a real So the design principle of ARRU

05:14:37.491 --> 05:14:39.630
is really to decouple everything.

05:14:39.630 --> 05:14:41.951
So we really want the engine so the users

05:14:41.951 --> 05:14:43.971
can change as fewer code

05:14:43.971 --> 05:14:45.971
as possible. So do what you want.

05:14:46.131 --> 05:14:47.771
So it's basically, we want to change

05:14:48.171 --> 05:14:49.701
from a sandwich to a tapas.

05:14:50.181 --> 05:14:52.261
So hope you enjoy tapas, particularly

05:14:52.261 --> 05:14:54.400
in San Diego. Right? Okay.

05:14:54.560 --> 05:14:56.720
And that's it. If you are welcome to

05:14:56.720 --> 05:14:58.911
use our project and we thank a lot of

05:14:58.911 --> 05:15:01.001
the team members for

05:15:01.141 --> 05:15:03.211
developing this project. And thank you. That's all.

05:15:03.931 --> 05:15:06.281
And, I think welcome to questions.

05:15:11.411 --> 05:15:13.571
Hello? Yeah. Thank you. Thank

05:15:13.571 --> 05:15:15.241
you so much for the amazing talk.

05:15:15.721 --> 05:15:17.501
So, I think you mentioned

05:15:18.712 --> 05:15:20.651
agent take r l is simple

05:15:20.792 --> 05:15:22.831
at the deployment time compared

05:15:22.831 --> 05:15:24.191
with multi agent system.

05:15:25.151 --> 05:15:27.230
At the very beginning of the talk. Yeah. So if

05:15:27.230 --> 05:15:29.630
you can run ring training, then you can

05:15:29.630 --> 05:15:31.691
simplify your architecture.

05:15:32.661 --> 05:15:34.741
But but late later of your talk,

05:15:34.741 --> 05:15:36.081
you also talk about

05:15:37.622 --> 05:15:39.761
like, a agentic RL system.

05:15:40.241 --> 05:15:42.481
Is complicated. So I kind of

05:15:42.481 --> 05:15:44.261
see the contradiction here.

05:15:44.761 --> 05:15:45.891
Oh, yeah. So

05:15:47.571 --> 05:15:49.282
definitely, I mean, so

05:15:50.292 --> 05:15:51.921
so the first of all,

05:15:52.561 --> 05:15:54.591
agents agent take a agent

05:15:54.591 --> 05:15:57.011
take system is already very complex.

05:15:57.011 --> 05:15:58.501
For example, Yeah. In

05:15:59.061 --> 05:16:01.421
in a lot of, like, example, like, the customer

05:16:01.431 --> 05:16:03.691
service scenario, you always need,

05:16:03.691 --> 05:16:05.781
like, different modules. You need different module during

05:16:05.781 --> 05:16:08.011
training. You need that model to simulate user,

05:16:08.011 --> 05:16:09.671
one model to simulate the customer.

05:16:10.151 --> 05:16:12.292
Sorry. That that service and the someone

05:16:12.292 --> 05:16:14.362
to maintain the database. So

05:16:14.362 --> 05:16:16.101
you always need a very complex

05:16:16.581 --> 05:16:18.701
But if you don't have reinforced learning,

05:16:18.701 --> 05:16:20.781
then the comp system can be more

05:16:20.781 --> 05:16:23.091
complex. I see. I see. I see. So it's basically

05:16:23.091 --> 05:16:25.353
make a life easier but life is already

05:16:25.353 --> 05:16:27.421
tough. Yeah. And I think yeah. I think

05:16:27.421 --> 05:16:29.462
that's a very good clarification that My

05:16:29.462 --> 05:16:31.603
second question is that I guess it's a follow-up

05:16:31.603 --> 05:16:33.691
question. Then how do

05:16:33.831 --> 05:16:35.911
you like, what are the criteria

05:16:35.911 --> 05:16:37.886
to to determine

05:16:38.251 --> 05:16:40.331
if I should use a multi agent system?

05:16:40.331 --> 05:16:42.311
Because I do believe that there's still some

05:16:42.792 --> 05:16:44.571
good use cases for the multi agent.

05:16:45.101 --> 05:16:47.140
System. And probably

05:16:47.361 --> 05:16:49.581
this like, agentic RL is only

05:16:49.581 --> 05:16:51.631
good for a subset of it. Can you give

05:16:51.631 --> 05:16:53.810
us a framework? Well, so

05:16:53.810 --> 05:16:55.751
my suggestion is always

05:16:56.391 --> 05:16:58.571
try to do as simple as possible.

05:16:59.641 --> 05:17:01.421
So, like, when only when

05:17:02.181 --> 05:17:04.141
add a new agent

05:17:04.301 --> 05:17:06.702
when you really need that. So

05:17:07.001 --> 05:17:09.091
because already a very complex system, so try to

05:17:09.091 --> 05:17:11.171
reduce the complexity as much as you you can.

05:17:11.571 --> 05:17:13.811
So that's my suggestion. Based on, like like,

05:17:13.811 --> 05:17:15.831
the customer service, scenario,

05:17:15.831 --> 05:17:17.862
you you at least needed two. Right? This can't

05:17:17.862 --> 05:17:19.891
be simplified. Yeah. Yeah. And and from

05:17:19.891 --> 05:17:22.011
that perspective, I do see that

05:17:22.471 --> 05:17:23.931
maybe for any

05:17:24.970 --> 05:17:26.531
real use cases when we

05:17:27.171 --> 05:17:29.411
need to deploy something let's

05:17:29.411 --> 05:17:31.601
say very fast to the production, Uh-huh.

05:17:31.601 --> 05:17:33.761
Always start from a multi Yes. Agent

05:17:33.761 --> 05:17:35.781
system. And if it doesn't work,

05:17:35.781 --> 05:17:38.091
or when the system complexity gets

05:17:38.091 --> 05:17:40.331
to some threshold, that is very

05:17:40.331 --> 05:17:42.442
hard to manage. Then consider

05:17:42.442 --> 05:17:44.251
migrate to the agentic

05:17:44.630 --> 05:17:46.711
URL which actually

05:17:47.011 --> 05:17:49.202
as another layer of complexity

05:17:49.341 --> 05:17:51.561
of, you know, training, curating your own data.

05:17:51.801 --> 05:17:53.261
So it it's still a cost

05:17:53.831 --> 05:17:55.611
you have to manage. Okay.

05:17:55.991 --> 05:17:57.761
My my suggestion is the opposite.

05:17:58.146 --> 05:18:00.235
Because adding things are always easier

05:18:00.235 --> 05:18:02.671
than removing things. If you really

05:18:02.671 --> 05:18:04.441
deploy something. So trust

05:18:04.900 --> 05:18:06.761
me. So it's like from system development,

05:18:06.991 --> 05:18:09.231
I think for system design principles, always

05:18:09.231 --> 05:18:11.675
suggest suggesting called never

05:18:11.841 --> 05:18:14.001
optimize too early or, like, or never do,

05:18:14.001 --> 05:18:16.071
like, never make your system too

05:18:16.071 --> 05:18:18.201
complex. Right? So

05:18:18.201 --> 05:18:20.390
it's it's I think start with a simple

05:18:20.390 --> 05:18:22.111
thing. Right? And

05:18:22.571 --> 05:18:24.800
then think I mean,

05:18:25.181 --> 05:18:27.301
look, I I agree that

05:18:28.661 --> 05:18:30.901
deploying an agentic

05:18:31.042 --> 05:18:33.513
RL training system it's simple at

05:18:33.571 --> 05:18:35.711
test time. Because you don't have a lot of

05:18:35.711 --> 05:18:38.011
modules. But developing

05:18:38.071 --> 05:18:40.521
that system is actually more complicated.

05:18:40.601 --> 05:18:42.661
Than a multi agent system. Because

05:18:42.661 --> 05:18:44.781
think about it. For the multisagent system, you don't have to

05:18:45.101 --> 05:18:46.381
fine tune your model at all.

05:18:47.263 --> 05:18:49.301
You just you

05:18:49.301 --> 05:18:51.611
know, have a bunch of agents

05:18:51.611 --> 05:18:53.980
which to in many cases just

05:18:53.980 --> 05:18:55.911
wrappers of the closed source model.

05:18:56.311 --> 05:18:58.101
It's very easy to deploy and

05:18:58.361 --> 05:19:00.851
if in some use cases, it solves the problem,

05:19:01.241 --> 05:19:03.401
I actually think that should be the first thing

05:19:03.401 --> 05:19:05.521
you try. Sure. Yeah. This

05:19:05.521 --> 05:19:07.711
this is something I totally agree is that

05:19:09.131 --> 05:19:11.371
I will always suggest

05:19:11.371 --> 05:19:13.601
to start without IRL. That's

05:19:13.601 --> 05:19:16.001
always true. Yeah. Because I always do, like, this

05:19:16.001 --> 05:19:18.081
just a complete layer. So I totally agree.

05:19:18.081 --> 05:19:19.951
I see. Oh, then then I think we're on the

05:19:20.191 --> 05:19:22.231
same page. If I have time, can I ask the

05:19:22.231 --> 05:19:24.081
second question? Yeah.

05:19:24.341 --> 05:19:26.441
The second question is that any support

05:19:26.501 --> 05:19:28.511
for the process reward

05:19:29.311 --> 05:19:31.731
when you do this, agent take our orcas?

05:19:31.981 --> 05:19:34.481
It's it's just support. Alright? Basically,

05:19:34.622 --> 05:19:36.691
rewards always is also a

05:19:36.691 --> 05:19:38.771
service in a real, so we can implement whatever

05:19:38.771 --> 05:19:40.911
you want. Process reward is I

05:19:40.911 --> 05:19:42.991
think we also have papers using Arial

05:19:42.991 --> 05:19:44.651
to do some process reward stuff.

05:19:45.051 --> 05:19:46.181
Okay. Sounds good. Thank you.

05:19:58.641 --> 05:20:00.821
Yeah. Thanks professor Wei for

05:20:00.881 --> 05:20:01.381
his

05:20:03.071 --> 05:20:05.151
Yeah. And then let's move on next

05:20:05.151 --> 05:20:07.381
to next century. Next,

05:20:07.523 --> 05:20:09.462
it will be host professor

05:20:10.292 --> 05:20:11.071
Yuan Dun Tian.

05:20:12.641 --> 05:20:14.741
Is ex research scientist adviser

05:20:15.201 --> 05:20:17.362
and His

05:20:17.821 --> 05:20:19.442
research direction cover multiple

05:20:20.061 --> 05:20:21.871
aspect of decision making

05:20:22.351 --> 05:20:24.771
including learning, planning and efficiency

05:20:25.631 --> 05:20:28.051
as well as the theoretical understanding

05:20:28.351 --> 05:20:30.601
of LR. Yeah. That's

05:20:30.721 --> 05:20:31.921
welcome, professor. Yandu.

05:20:53.001 --> 05:20:53.711
Not going

05:20:58.282 --> 05:21:00.681
Thanks for the introduction. I'm

05:21:00.741 --> 05:21:03.032
still setting up. And hopefully, we done

05:21:03.032 --> 05:21:03.361
soon.

05:21:28.551 --> 05:21:31.051
Okay. Okay. So I think I guess everyone can see the,

05:21:32.111 --> 05:21:34.591
see the presentation. Yeah. So good morning.

05:21:34.591 --> 05:21:36.701
Good afternoon, everyone. My name Nguyen Dong Tian.

05:21:36.941 --> 05:21:39.021
Today, I'm going to talk about efficient reasoning

05:21:39.021 --> 05:21:41.361
from in-depth understanding of large and more behaviors.

05:21:42.521 --> 05:21:44.921
So we all know that larger model has been, leveraged

05:21:44.921 --> 05:21:46.140
in many different scenarios.

05:21:47.281 --> 05:21:49.361
And, our previous group, Ian, Meta,

05:21:49.361 --> 05:21:51.441
has been working on reasoning and planning, for a

05:21:51.441 --> 05:21:53.480
while. And, of course, like,

05:21:53.560 --> 05:21:55.720
we probably wanna think about, like, what's the next

05:21:55.720 --> 05:21:57.741
step for reasoning and planning. Given

05:21:57.741 --> 05:21:59.521
that there's so much progress, in

05:21:59.821 --> 05:22:01.273
the last few years.

05:22:01.891 --> 05:22:04.051
So one thing one image I I really

05:22:04.051 --> 05:22:06.140
like is the image from Apple AI that

05:22:06.140 --> 05:22:07.701
shows the progress of large models.

05:22:08.661 --> 05:22:11.001
Year over year, you see, like, the training

05:22:11.140 --> 05:22:12.751
compute is exponentially increasing.

05:22:13.801 --> 05:22:15.951
Given the linear rate of

05:22:15.951 --> 05:22:17.962
the publication date. And,

05:22:17.962 --> 05:22:20.361
also, you will see the same behavior for data

05:22:20.601 --> 05:22:22.681
Right? So, year over year, you'll

05:22:22.681 --> 05:22:24.792
see the data use it also

05:22:24.931 --> 05:22:27.069
exponentially increasing. Of course, there's

05:22:27.069 --> 05:22:28.801
no enough data at some point.

05:22:29.921 --> 05:22:32.161
And the the the data that's being created

05:22:32.161 --> 05:22:33.901
by human is now obviously

05:22:34.281 --> 05:22:35.980
growing exponentially. It's growing,

05:22:36.461 --> 05:22:38.380
in a linear rate, so you appear

05:22:38.581 --> 05:22:40.400
if appear to be a logarithm,

05:22:41.621 --> 05:22:43.801
curve. So at some point, these two definitely

05:22:43.941 --> 05:22:45.801
will, come other.

05:22:46.282 --> 05:22:48.442
Right? So if we have been using all

05:22:48.442 --> 05:22:50.771
the possible data, in the Internet,

05:22:51.171 --> 05:22:53.212
what should we do? That's actually

05:22:53.212 --> 05:22:55.031
one of a question. But even if we

05:22:55.271 --> 05:22:57.231
already combine all the data,

05:22:58.032 --> 05:23:00.271
and all the compute and to train our

05:23:00.271 --> 05:23:02.431
AIs, And we have a very strong AIs

05:23:02.431 --> 05:23:04.891
in many different scenarios. And

05:23:04.952 --> 05:23:06.872
we have, GPT five. We have,

05:23:07.671 --> 05:23:09.821
Gemini three. Have lots of

05:23:09.821 --> 05:23:11.901
strong AIs, but still, if you

05:23:11.901 --> 05:23:14.001
compare different axis, the current AI

05:23:14.001 --> 05:23:15.941
is still not as strong as humans.

05:23:16.452 --> 05:23:18.773
So, for example, in the training

05:23:18.773 --> 05:23:20.911
data efficiency, we actually

05:23:20.911 --> 05:23:23.391
see, I mean, a thousand,

05:23:23.851 --> 05:23:25.531
a thousand x gap.

05:23:26.569 --> 05:23:28.890
Between SOTA, large models, and human brand.

05:23:28.890 --> 05:23:30.351
Human brand only need

05:23:30.981 --> 05:23:33.061
maybe, like, less than a two 10,000,000,000 text tokens.

05:23:33.061 --> 05:23:35.141
Maybe if you keep reading books

05:23:35.521 --> 05:23:37.681
for the entire for, like, for all

05:23:37.681 --> 05:23:39.773
of your seventy years, and

05:23:39.773 --> 05:23:42.171
you you you may be able to get to, like, ten

05:23:42.171 --> 05:23:43.751
ten billion tokens, in ten

05:23:44.881 --> 05:23:47.042
But if if you compare this with the SOTA

05:23:47.042 --> 05:23:49.351
IRMs training, it's actually a

05:23:49.411 --> 05:23:51.111
thousand three magnitude,

05:23:51.491 --> 05:23:53.551
three hour magnitude difference. And the

05:23:53.551 --> 05:23:55.571
power consumption is also a huge difference.

05:23:55.952 --> 05:23:58.273
And, not only that, but also the human

05:23:58.273 --> 05:23:59.971
AI system is actually super

05:24:00.801 --> 05:24:02.933
efficient. It will be you will

05:24:02.933 --> 05:24:05.011
see human can get to the points of

05:24:05.011 --> 05:24:07.060
the knowledge with just

05:24:07.060 --> 05:24:09.140
maybe a few samples. Maybe even like one

05:24:09.140 --> 05:24:11.220
or two samples. Right? But

05:24:11.220 --> 05:24:13.401
for the sort of other large models,

05:24:13.781 --> 05:24:15.872
you actually see the case that

05:24:15.872 --> 05:24:18.212
the model typically needs, like, hundreds of thousands

05:24:18.261 --> 05:24:20.480
of data points. And it still may fail

05:24:20.480 --> 05:24:22.521
to generate. In the cases that

05:24:22.521 --> 05:24:24.282
human think it's easy to generalize.

05:24:25.212 --> 05:24:27.451
So these are actually the big gap between the two.

05:24:27.851 --> 05:24:29.931
So I mean, of course, one question will be,

05:24:29.931 --> 05:24:31.640
how is human learning so efficiently?

05:24:32.622 --> 05:24:34.741
Right? So, human can generalize things very well.

05:24:34.981 --> 05:24:37.201
And the model does not do that. Right?

05:24:37.681 --> 05:24:39.681
So, I would imagine that at the beginning,

05:24:39.921 --> 05:24:41.441
we have GPUs. And,

05:24:41.980 --> 05:24:43.751
we use GPUs to,

05:24:44.071 --> 05:24:45.221
generate lots of data.

05:24:46.341 --> 05:24:48.581
In the exponential search space. Okay. For

05:24:48.581 --> 05:24:50.122
this given all these points,

05:24:50.841 --> 05:24:52.060
may come with some interpolation.

05:24:53.021 --> 05:24:54.800
To understand what's going on.

05:24:55.819 --> 05:24:57.861
But going forward, we're

05:24:57.861 --> 05:25:00.021
not everyone will become GPU poor.

05:25:00.021 --> 05:25:01.891
Everyone we want to,

05:25:02.131 --> 05:25:04.071
everyone want to explore so much

05:25:04.691 --> 05:25:06.111
possibilities in terms of the

05:25:07.911 --> 05:25:09.909
empirical results.

05:25:10.351 --> 05:25:12.511
And it's going to be very hard to

05:25:12.511 --> 05:25:14.433
use number of the the exponential

05:25:14.571 --> 05:25:16.711
number GPUs to fill in the entire space.

05:25:17.311 --> 05:25:19.551
Not possible. We probably don't cannot afford

05:25:19.551 --> 05:25:21.511
to use all the electricity,

05:25:23.651 --> 05:25:25.831
to fuel the GPUs in a planet.

05:25:26.561 --> 05:25:28.721
So instead, what we should do is we should think

05:25:28.721 --> 05:25:30.691
about open a black box,

05:25:31.171 --> 05:25:33.281
find marginalizable rules. Marginalizeable

05:25:33.471 --> 05:25:35.521
laws, so that given the

05:25:35.521 --> 05:25:37.569
few points provided by TPUs, we

05:25:37.569 --> 05:25:39.400
can do better in the air extrapolation.

05:25:40.091 --> 05:25:42.071
To cover larger subspace.

05:25:42.231 --> 05:25:44.311
But in order to do that, we really want to

05:25:44.311 --> 05:25:46.499
open a black box. So by opening a black box,

05:25:46.499 --> 05:25:47.941
what we are trying to do

05:25:48.581 --> 05:25:50.661
is that there are different level of it. Alright. So

05:25:50.821 --> 05:25:53.181
for example, instead of only staring

05:25:53.265 --> 05:25:55.501
at last term

05:25:55.560 --> 05:25:57.692
performance number, We should be able to

05:25:58.141 --> 05:26:00.081
check the patterns, output trained models,

05:26:00.761 --> 05:26:02.621
So we're gonna check weights, check activations,

05:26:03.111 --> 05:26:05.141
check attentions, check all

05:26:05.141 --> 05:26:07.211
the structures then maybe check, like,

05:26:07.211 --> 05:26:09.291
a pattern of layouts. Check all

05:26:09.291 --> 05:26:10.951
these, then you actually obtain

05:26:11.411 --> 05:26:13.601
more information from a single data

05:26:13.601 --> 05:26:15.701
point. Right? So given when

05:26:16.671 --> 05:26:18.751
GPU hour or several GPU hours, you actually

05:26:18.751 --> 05:26:20.952
get a lot of information compared

05:26:20.952 --> 05:26:23.111
to the case that you only care about the final numbers.

05:26:24.161 --> 05:26:26.141
So of course, these two are basically like a

05:26:26.321 --> 05:26:28.531
in kind of like a rudimentary way of understanding

05:26:29.171 --> 05:26:31.212
understand the mock how the model works. More

05:26:31.212 --> 05:26:33.292
advanced, going forward, what

05:26:33.292 --> 05:26:35.381
we can do is, we probably want

05:26:35.381 --> 05:26:37.591
to come with some mathematical

05:26:38.071 --> 05:26:40.551
theory and, some kind of framework that can

05:26:40.551 --> 05:26:42.611
understand how the model get

05:26:42.611 --> 05:26:44.691
learned, the dynamics of it, and use

05:26:44.691 --> 05:26:46.712
that dynamics to come

05:26:46.712 --> 05:26:48.761
with a better way explain

05:26:48.761 --> 05:26:50.821
how the model behaves. And finally,

05:26:50.821 --> 05:26:52.900
it can come up with a theory that govern, the

05:26:52.980 --> 05:26:54.201
how the pattern emerges.

05:26:54.981 --> 05:26:56.991
That's the final goal of it. So in

05:26:56.991 --> 05:26:59.331
this talk, I'm going to talk a little bit about examples,

05:26:59.791 --> 05:27:01.801
of this philosophy,

05:27:02.341 --> 05:27:04.821
and how this philosophy will give you, practical

05:27:04.821 --> 05:27:06.859
algorithms. There are short

05:27:06.859 --> 05:27:08.361
term and long term, access.

05:27:08.861 --> 05:27:11.361
The short term access is that, oh, we have already

05:27:11.501 --> 05:27:13.761
checked the patterns

05:27:14.060 --> 05:27:16.141
that you see in your experiments, and we

05:27:16.141 --> 05:27:18.161
want to find, for example, nice structures.

05:27:18.751 --> 05:27:21.071
Given nice structures, maybe you can develop

05:27:21.071 --> 05:27:23.011
algorithms that is very useful

05:27:23.361 --> 05:27:25.481
and can improve efficiency of current models.

05:27:26.521 --> 05:27:28.721
And the long term, would

05:27:28.721 --> 05:27:30.431
say, we want to actually understand

05:27:31.101 --> 05:27:32.931
what kind of representation the model learns,

05:27:33.491 --> 05:27:35.731
so that, the representation, can be learned

05:27:35.731 --> 05:27:37.271
better, and this will

05:27:37.751 --> 05:27:39.792
to fundamental changes algorithms in the

05:27:39.792 --> 05:27:42.111
future. So I will cover mostly

05:27:42.111 --> 05:27:43.470
short term, but also long term,

05:27:44.521 --> 05:27:46.692
in this talk. So,

05:27:46.851 --> 05:27:48.712
for short term, I'll give you some examples.

05:27:49.071 --> 05:27:50.931
The first example I tell you,

05:27:51.551 --> 05:27:53.931
I will give you is a is a very classic

05:27:53.931 --> 05:27:56.111
example. You leverage attention sparsity.

05:27:56.641 --> 05:27:59.091
So this attention sparsity structure

05:28:00.291 --> 05:28:02.151
is basically, like, being discovered

05:28:03.202 --> 05:28:05.622
in several of our previous works, shows

05:28:05.681 --> 05:28:08.011
that the attention will be sparse

05:28:08.071 --> 05:28:09.691
and over time during training.

05:28:10.712 --> 05:28:12.952
And, the the attention pattern will

05:28:12.952 --> 05:28:14.831
be dependent on the query. So,

05:28:15.611 --> 05:28:17.851
and, given these kind of findings,

05:28:17.851 --> 05:28:19.962
we actually have a previous work called

05:28:20.101 --> 05:28:22.331
Attention Sync. In that paper, we actually discovered

05:28:22.331 --> 05:28:23.851
that, the first few tokens,

05:28:24.561 --> 05:28:26.601
attract lots of attentions. And,

05:28:26.841 --> 05:28:28.942
by fixing them, we are

05:28:28.942 --> 05:28:31.103
able to find an algorithm, that

05:28:31.103 --> 05:28:33.150
can take only the first few

05:28:33.150 --> 05:28:35.300
tokens and the sliding window of the

05:28:35.300 --> 05:28:37.461
recent tokens, put them together, and

05:28:37.461 --> 05:28:39.751
this combination of kvCash

05:28:41.032 --> 05:28:42.331
is able to give you

05:28:43.292 --> 05:28:44.511
super smooth generations.

05:28:45.511 --> 05:28:47.511
And without memory

05:28:47.810 --> 05:28:50.310
outbound and without, capital tokens.

05:28:51.171 --> 05:28:53.271
And at the same time, we actually achieve

05:28:53.331 --> 05:28:55.651
constant memory usage and

05:28:56.031 --> 05:28:58.191
stable capacity at the same time, and you can

05:28:58.191 --> 05:29:00.442
basically extrapolate the slide down

05:29:00.581 --> 05:29:02.821
of the window to a very long window, and it's also

05:29:02.821 --> 05:29:05.171
faster. So this approach

05:29:05.171 --> 05:29:07.231
is actually showing that

05:29:07.231 --> 05:29:09.601
it's very useful, after

05:29:09.601 --> 05:29:11.681
two years, since it's released. So we

05:29:11.681 --> 05:29:13.712
have already accumulated more

05:29:13.712 --> 05:29:15.831
than a thousand citations, and, this

05:29:15.831 --> 05:29:17.909
approach and its extensions have been

05:29:17.909 --> 05:29:20.151
used in GPU models in pretraining.

05:29:20.771 --> 05:29:23.191
By basically, like, having a trainable attention

05:29:23.251 --> 05:29:25.183
sync tokens, and we are able to

05:29:25.341 --> 05:29:27.523
if the model is able to train smoothly,

05:29:28.081 --> 05:29:29.981
without the spikes, And,

05:29:30.301 --> 05:29:32.381
we also see that recently, QN's

05:29:32.381 --> 05:29:34.111
Getty Attention paper, which you'll see

05:29:34.431 --> 05:29:36.511
received, this year's in Europe's 2025,

05:29:36.671 --> 05:29:38.771
best paper award. Also,

05:29:39.751 --> 05:29:41.261
stem its own study

05:29:41.881 --> 05:29:44.361
from the attention

05:29:44.361 --> 05:29:46.421
sink phenomena. So basically,

05:29:46.800 --> 05:29:49.220
like, once we have a gated

05:29:49.811 --> 05:29:52.051
approach, we are able to,

05:29:52.131 --> 05:29:53.591
we'll be able to, like,

05:29:54.372 --> 05:29:56.521
get rid of the attention sink. And at

05:29:56.521 --> 05:29:58.841
the same time make the entire training much smoother

05:29:58.841 --> 05:30:00.962
and get a better results. So

05:30:00.962 --> 05:30:03.121
you can actually see that, with this kind

05:30:03.121 --> 05:30:04.671
of, sinking process,

05:30:05.292 --> 05:30:06.941
we are able to identify

05:30:07.341 --> 05:30:09.351
important patterns, during

05:30:09.351 --> 05:30:11.521
the training. And use that

05:30:11.521 --> 05:30:13.191
to come with a more efficient

05:30:13.751 --> 05:30:16.091
efficient, efficient reasoning algorithms.

05:30:16.942 --> 05:30:19.023
And at the same time, in the long run, improve the

05:30:19.023 --> 05:30:19.811
training process.

05:30:21.292 --> 05:30:23.231
So, we here, I'll give you another

05:30:23.371 --> 05:30:25.401
example which is called the DeepSync with

05:30:25.401 --> 05:30:27.741
Convenience. This is a very recent paper.

05:30:28.341 --> 05:30:30.351
That we published. So in that

05:30:30.351 --> 05:30:32.301
paper, what we do here is that

05:30:32.942 --> 05:30:34.962
when we do multiple

05:30:35.261 --> 05:30:37.362
rollouts in Resiliente, usually,

05:30:37.951 --> 05:30:40.031
we first do lots of robots, and then, we

05:30:40.031 --> 05:30:42.111
do majority of all, and then we get the final

05:30:42.111 --> 05:30:44.511
results. But, we find

05:30:44.811 --> 05:30:47.001
that it's interesting to see, doing

05:30:47.001 --> 05:30:49.451
the rollouts of these, thinking

05:30:49.991 --> 05:30:52.091
traces we don't even need any supervision

05:30:53.081 --> 05:30:55.501
to decide whether each of these robots are

05:30:56.151 --> 05:30:58.311
useful or not for the final, majority

05:30:58.311 --> 05:31:00.591
vote. We find that there actually exists,

05:31:00.591 --> 05:31:01.811
like, a very simple

05:31:03.431 --> 05:31:05.691
very simple criteria called confidence.

05:31:05.751 --> 05:31:07.941
And use that confidence, we can get rid of

05:31:07.941 --> 05:31:10.181
some of the thinking traces that are

05:31:10.181 --> 05:31:11.321
known to be non confident.

05:31:12.212 --> 05:31:14.631
And use the convenant syncing traces

05:31:14.971 --> 05:31:17.042
use them, in majority vote. And

05:31:17.042 --> 05:31:19.091
this actually give you better performance. It's

05:31:19.091 --> 05:31:21.331
a very simple approach, and, it actually

05:31:21.331 --> 05:31:23.640
works quite well. So, in AIM

05:31:23.640 --> 05:31:25.880
twenty twenty five, what you actually see

05:31:25.880 --> 05:31:28.230
is that the model that, coupled

05:31:28.230 --> 05:31:30.321
with the GPU assist one twenty b,

05:31:30.481 --> 05:31:33.481
models. It can achieve 99.9%

05:31:34.441 --> 05:31:36.499
of the accuracy, in AM

05:31:36.499 --> 05:31:38.681
2025, with a majority

05:31:38.900 --> 05:31:40.121
of the load of five

05:31:40.901 --> 05:31:43.321
five hundred and twelve after the deep confidence

05:31:43.461 --> 05:31:45.501
filtering. At the same time, the

05:31:45.501 --> 05:31:46.641
usage of tokens

05:31:47.702 --> 05:31:50.061
is actually much fewer. Because

05:31:50.202 --> 05:31:52.231
most of the routes was being killed, and

05:31:55.641 --> 05:31:57.849
So we're able to queue up to,

05:31:58.265 --> 05:31:59.901
eighty four point seven

05:32:01.021 --> 05:32:03.181
of the tokens, and at the same time, you actually

05:32:03.181 --> 05:32:05.212
preapproved performance. So

05:32:05.212 --> 05:32:07.611
you can try the same thing for other models

05:32:07.611 --> 05:32:09.821
like DeepCAB, NVAC

05:32:09.821 --> 05:32:12.061
could give you also a pretty strong performance,

05:32:12.061 --> 05:32:14.101
87.4, At the

05:32:14.101 --> 05:32:16.201
same time, you see, substantial

05:32:17.991 --> 05:32:20.091
drop in terms of token being generated.

05:32:20.101 --> 05:32:22.181
So that's, very good.

05:32:22.181 --> 05:32:24.400
You get basically better both worlds. So you get

05:32:24.400 --> 05:32:26.476
a better accuracy, and you get,

05:32:26.791 --> 05:32:28.751
an elastic token usage.

05:32:30.431 --> 05:32:32.548
So, what's the secret The secret

05:32:32.548 --> 05:32:34.761
is actually, very simple. We

05:32:34.761 --> 05:32:37.001
just check, okay, the confidence from the property

05:32:37.001 --> 05:32:39.091
distribution. So if we're to define

05:32:39.091 --> 05:32:41.431
confidence being, the sum of the log probability

05:32:42.151 --> 05:32:44.390
of all tokens, and we have average

05:32:44.390 --> 05:32:46.421
over the number of tokens. So it's

05:32:46.421 --> 05:32:48.581
very simple. So once you have that,

05:32:48.819 --> 05:32:51.061
then you can compute

05:32:51.411 --> 05:32:53.831
the confidence score for each of the routes.

05:32:54.831 --> 05:32:56.909
So, you see that, oh, if the route is

05:32:56.909 --> 05:32:59.171
correct, then the confidence tend

05:32:59.171 --> 05:33:01.310
to be high. And the loss is not

05:33:01.310 --> 05:33:03.021
correct, the confidence tends to be low.

05:33:03.341 --> 05:33:05.501
So this is happened this has seemed to be true

05:33:05.501 --> 05:33:07.281
for multiple definition of contents.

05:33:08.771 --> 05:33:10.952
So we would because we have confidence at token

05:33:11.091 --> 05:33:12.921
level, we can define multiple,

05:33:13.462 --> 05:33:15.271
different confidence for the sequence level.

05:33:15.511 --> 05:33:17.851
So there's different ways of defining those confidence.

05:33:18.292 --> 05:33:20.212
And we can basically filter out,

05:33:21.251 --> 05:33:23.531
the the the rollouts with different

05:33:23.531 --> 05:33:25.651
combinators. So

05:33:25.651 --> 05:33:27.811
these are basically the other criteria. You see that for all

05:33:27.811 --> 05:33:29.831
these three criteria, you see

05:33:30.321 --> 05:33:32.401
a clear separation between correct and

05:33:32.401 --> 05:33:32.901
incorrect.

05:33:34.470 --> 05:33:36.361
Robots. So this actually,

05:33:36.601 --> 05:33:38.841
give us a hope. Right? So then of course,

05:33:38.841 --> 05:33:41.341
we can leverage the confidence to filter out

05:33:41.891 --> 05:33:43.991
the the incorrect samples.

05:33:44.601 --> 05:33:46.761
Use only the correct raws to do majority of

05:33:46.761 --> 05:33:48.980
it. So, given the question,

05:33:48.980 --> 05:33:51.060
we first can do online generation, which is

05:33:51.060 --> 05:33:53.273
parallel parallizable, and

05:33:53.273 --> 05:33:55.671
then we check, okay, if the confidence is consistent and

05:33:55.671 --> 05:33:57.771
higher. Then we accept. Otherwise,

05:33:57.771 --> 05:33:59.951
we just reject these, generations.

05:34:00.601 --> 05:34:02.220
And then we only use

05:34:04.650 --> 05:34:06.721
the accepted draws to do majority vote.

05:34:07.282 --> 05:34:09.442
So that's, and we actually have

05:34:09.442 --> 05:34:10.661
very efficient implementations

05:34:12.640 --> 05:34:14.511
in VMs to implement that. So

05:34:15.691 --> 05:34:17.751
and we see that in both the offline and

05:34:18.231 --> 05:34:20.391
settings, the performance is actually, pretty

05:34:20.391 --> 05:34:22.470
good. Here, offlineonline basically

05:34:22.470 --> 05:34:24.541
say, we have to set

05:34:24.681 --> 05:34:27.112
some threshold for contents either in offline

05:34:27.171 --> 05:34:28.571
manner or in online manner.

05:34:29.251 --> 05:34:31.651
But in both kind of settings, we actually

05:34:31.651 --> 05:34:33.730
see pretty good performance

05:34:33.951 --> 05:34:35.811
compared to the baselines.

05:34:36.611 --> 05:34:38.771
It's a very simple approach, and it gives

05:34:38.771 --> 05:34:40.361
you a pretty strong boost.

05:34:41.001 --> 05:34:43.031
Yeah. Okay. So that's the

05:34:43.031 --> 05:34:45.291
example of, leveraging these short term

05:34:46.511 --> 05:34:48.591
nice structures of the, of

05:34:48.591 --> 05:34:50.641
the of the output. Of the

05:34:50.641 --> 05:34:52.706
neural network or the main representation

05:34:52.706 --> 05:34:55.051
of neural network, to improve

05:34:55.111 --> 05:34:56.931
your reading systems. And this actually

05:34:57.171 --> 05:34:58.921
actually give you pretty strong impact.

05:34:59.321 --> 05:35:01.161
So now I'm talking about long term

05:35:01.641 --> 05:35:03.721
So in the long term, we probably want to understand, like,

05:35:03.721 --> 05:35:05.792
what reputation we learned, during

05:35:05.792 --> 05:35:07.821
the training, so that we can leverage them.

05:35:08.386 --> 05:35:10.411
To develop new algorithms.

05:35:10.711 --> 05:35:13.211
So one paper I want to mention is

05:35:13.591 --> 05:35:15.831
a paper called the pass note taken. We

05:35:15.831 --> 05:35:17.971
study the delta

05:35:18.031 --> 05:35:20.371
w, the weight difference between

05:35:20.431 --> 05:35:22.041
reinforcement learning and So

05:35:22.841 --> 05:35:25.001
So this is actually a paper that's also

05:35:25.001 --> 05:35:27.321
present, in this

05:35:27.321 --> 05:35:29.601
workshop. So it's

05:35:29.601 --> 05:35:31.991
interesting to see that, if you actually check

05:35:32.231 --> 05:35:34.391
the, data weights of STT and

05:35:34.391 --> 05:35:36.441
data weights of reinforcement learning, they

05:35:36.441 --> 05:35:38.761
are actually having very nice, very different

05:35:38.761 --> 05:35:41.091
structures, and they learn different things. So

05:35:41.331 --> 05:35:43.431
of all, you basically check, okay, how much

05:35:43.891 --> 05:35:46.011
degree you have the weights have

05:35:46.011 --> 05:35:47.071
rotated after,

05:35:48.091 --> 05:35:49.631
this either eye or SFT.

05:35:50.581 --> 05:35:53.001
Training, you see that there's a huge difference between

05:35:53.220 --> 05:35:54.819
the two. For SFT, usually,

05:35:55.521 --> 05:35:57.211
the weights the singular

05:35:57.741 --> 05:35:59.909
space. In each of these weights of matrices.

05:36:00.221 --> 05:36:01.421
They rotate a lot.

05:36:02.851 --> 05:36:04.941
Especially, physically like a four

05:36:04.941 --> 05:36:07.101
four like, smaller eigenvectors,

05:36:07.321 --> 05:36:09.361
I notice even more for

05:36:09.521 --> 05:36:11.681
a large eigen eigen eigen eigen space,

05:36:11.681 --> 05:36:13.721
it should look at less, but

05:36:13.821 --> 05:36:15.981
it's still much larger than the reinforcement learning setting.

05:36:16.301 --> 05:36:18.641
So that's actually a very interesting observation.

05:36:20.640 --> 05:36:22.720
And, what happens is that, we

05:36:22.720 --> 05:36:24.991
actually check okay, for the data

05:36:24.991 --> 05:36:27.361
weights that reinforcement learning has already been

05:36:28.400 --> 05:36:30.521
changed, and you check what's the

05:36:30.819 --> 05:36:32.761
mask that correspond to the update.

05:36:33.531 --> 05:36:35.781
You see that the updated mask is

05:36:35.781 --> 05:36:37.871
actually correlated a lot with the

05:36:37.871 --> 05:36:40.031
low magnitude mask rather than the

05:36:40.031 --> 05:36:41.951
principal mask. Alright. So,

05:36:42.111 --> 05:36:44.511
by principal mask, it doesn't mean that, we first

05:36:44.511 --> 05:36:46.800
do we keep only

05:36:46.800 --> 05:36:48.900
the first few, singular vectors,

05:36:48.961 --> 05:36:50.971
and then we reconstruct

05:36:51.111 --> 05:36:53.101
back to the waste space. And

05:36:53.241 --> 05:36:55.481
check what's what's the magnitude of each

05:36:55.481 --> 05:36:56.941
of the, locations.

05:36:57.651 --> 05:36:59.601
So you see that the update mask is

05:37:00.640 --> 05:37:02.999
very well with a lower

05:37:03.140 --> 05:37:04.961
singular vector space.

05:37:05.361 --> 05:37:07.521
So that's actually very interesting. So IOA is

05:37:07.521 --> 05:37:09.900
actually take a detour. It actually

05:37:09.961 --> 05:37:12.391
optimize the low singular space,

05:37:13.032 --> 05:37:14.901
and keep the high singular space intact.

05:37:15.801 --> 05:37:17.851
That's different from SFT. SFT

05:37:17.851 --> 05:37:19.861
is basically changing the principle of the pixel

05:37:19.861 --> 05:37:21.631
mask. So So,

05:37:21.872 --> 05:37:23.891
we also do some experiments. So

05:37:24.191 --> 05:37:25.731
since we now know that

05:37:26.372 --> 05:37:28.311
the the the weights that IR has changed,

05:37:29.611 --> 05:37:31.691
Then what we can do is that, okay, how

05:37:31.691 --> 05:37:34.111
about we train and enforce money

05:37:34.872 --> 05:37:36.952
training and enforcement on ARMs, but

05:37:36.952 --> 05:37:39.032
we only ask the IR to change the

05:37:39.032 --> 05:37:41.191
principal weights. But not the

05:37:41.191 --> 05:37:43.531
low singular, vector weights.

05:37:43.881 --> 05:37:46.301
So it seems like if you ask

05:37:46.442 --> 05:37:48.451
the IR to do that, then it doesn't work.

05:37:49.011 --> 05:37:51.331
Very well. We actually tried LoRa

05:37:51.331 --> 05:37:53.401
versus Pizza. PISA is

05:37:53.401 --> 05:37:55.291
the the this kind of LoRa version that

05:37:55.531 --> 05:37:57.971
only focus on the first few,

05:37:58.191 --> 05:37:59.731
singular vectors of the weights.

05:38:00.431 --> 05:38:02.511
And you'll see that if you, run a piece,

05:38:02.511 --> 05:38:04.611
I always reinforce learning, and performance

05:38:04.671 --> 05:38:06.192
actually drops substantially.

05:38:07.181 --> 05:38:09.341
Sometimes, it will collapse. To

05:38:09.341 --> 05:38:11.661
zero. But, if you run LoRa,

05:38:11.819 --> 05:38:14.051
run higher with LoRa, which is

05:38:14.051 --> 05:38:16.551
allowed to change any, Wirespace,

05:38:17.171 --> 05:38:19.101
the performance is actually stable.

05:38:20.801 --> 05:38:23.041
And we actually see, like, if you actually

05:38:23.041 --> 05:38:25.221
check the KL divergence, KL

05:38:25.521 --> 05:38:27.531
loss, between, between the

05:38:27.531 --> 05:38:29.991
the model and the original

05:38:30.050 --> 05:38:32.161
model. And what you see is that the

05:38:33.542 --> 05:38:35.861
our if you train

05:38:36.691 --> 05:38:38.771
in this small space, this small

05:38:38.771 --> 05:38:40.981
single space, the the

05:38:40.981 --> 05:38:42.751
behavior is very similar. To

05:38:43.212 --> 05:38:45.551
the case that if you train with the entire space.

05:38:46.321 --> 05:38:48.401
But if you constrain that, the higher

05:38:48.401 --> 05:38:50.871
training is only focused on the principal weights,

05:38:50.871 --> 05:38:52.511
then the behavior is very different.

05:38:53.901 --> 05:38:56.141
Yeah. So that's actually interesting. So basically, that's

05:38:56.141 --> 05:38:58.001
why we call the the path not taken.

05:38:58.191 --> 05:38:59.890
So it seems as it's it's possible

05:39:00.271 --> 05:39:02.621
that we the ire

05:39:02.621 --> 05:39:04.781
is trained on a very different

05:39:04.781 --> 05:39:07.101
space, and that particular space is very

05:39:07.101 --> 05:39:08.942
useful. To keep generalization

05:39:09.551 --> 05:39:11.792
to keep the model not forgetting, any

05:39:11.792 --> 05:39:13.933
other previous things. And to make

05:39:13.933 --> 05:39:16.023
the entire training stable. So but on the

05:39:16.023 --> 05:39:17.561
other hand, for SFT,

05:39:18.101 --> 05:39:20.181
things that turn out sink sink in principal weights,

05:39:20.181 --> 05:39:22.631
it might destroy the backbone of the models,

05:39:23.171 --> 05:39:25.291
and it might give you, issues in my

05:39:25.531 --> 05:39:27.951
lead to overfitting, etcetera. Of course,

05:39:28.650 --> 05:39:30.872
we basically give you an

05:39:30.872 --> 05:39:32.811
idea about examples and observations.

05:39:33.031 --> 05:39:35.011
Right? So what's insight,

05:39:35.251 --> 05:39:36.441
can be further explored?

05:39:37.800 --> 05:39:40.111
So in this paper, we also

05:39:40.671 --> 05:39:42.771
give demystify why

05:39:43.150 --> 05:39:44.531
the gradient

05:39:45.481 --> 05:39:47.681
provide the weights updates can be sparse in

05:39:47.681 --> 05:39:49.900
reinforcement learning. This mostly

05:39:49.900 --> 05:39:52.140
is because of there's a DPF 16 position

05:39:52.140 --> 05:39:54.541
underflow. So because in reinforcement

05:39:54.541 --> 05:39:56.481
learning, lots of ways to basically change

05:39:57.042 --> 05:39:58.681
very small in a very small manner.

05:39:59.241 --> 05:40:01.401
So if you use b f 16, then

05:40:01.401 --> 05:40:03.261
the the amount of change won't

05:40:03.441 --> 05:40:04.851
change the quantization level.

05:40:05.731 --> 05:40:07.890
So, basically, all these dis weights appear

05:40:07.890 --> 05:40:09.921
not to change. So, and this is

05:40:09.921 --> 05:40:11.501
actually perceived

05:40:12.361 --> 05:40:14.831
very noticeably if you compare

05:40:14.890 --> 05:40:16.970
the base model and fine tune our reinforcement

05:40:16.970 --> 05:40:19.161
models. All the reinforcement models

05:40:19.161 --> 05:40:21.542
being reinforcement tuned, trend,

05:40:21.542 --> 05:40:24.042
for example, trained with JPO, with reinforcement

05:40:24.181 --> 05:40:26.311
plus plus or DPO, you

05:40:26.311 --> 05:40:28.421
see a huge sparsity. But

05:40:28.421 --> 05:40:29.841
if you train with SFT,

05:40:30.481 --> 05:40:32.561
you actually see very low sparsity, which means

05:40:32.561 --> 05:40:34.891
that many of the weights have been changed.

05:40:36.091 --> 05:40:38.461
So that's actually very interesting. I mean, how reinforcement

05:40:38.681 --> 05:40:40.821
learning can achieve much better results with

05:40:40.821 --> 05:40:42.872
much fewer changes

05:40:42.872 --> 05:40:45.013
of the weights, remains a mystery. And it can

05:40:45.013 --> 05:40:47.122
be a and it can be a further walk But

05:40:47.122 --> 05:40:49.341
the observation itself is quite

05:40:49.341 --> 05:40:51.461
interesting. Okay. So,

05:40:51.701 --> 05:40:54.051
finally, I want to also cover our previous

05:40:54.051 --> 05:40:56.311
work at the latent space reasoning directions.

05:40:56.741 --> 05:40:59.091
Which is also, true

05:40:59.641 --> 05:41:00.781
focus on, like,

05:41:02.091 --> 05:41:04.221
basically, using the representation

05:41:04.521 --> 05:41:06.060
to do reasoning.

05:41:06.721 --> 05:41:08.801
So if we understand the representation better,

05:41:08.801 --> 05:41:10.721
then maybe reasoning can be much more efficient.

05:41:11.282 --> 05:41:13.603
So, the latest based reasoning, is,

05:41:13.841 --> 05:41:15.872
one idea I try to

05:41:15.872 --> 05:41:18.031
replace the output tokens in the

05:41:18.031 --> 05:41:20.251
reasoning traces, with

05:41:20.470 --> 05:41:22.611
a continuous vector. So So if

05:41:22.611 --> 05:41:24.851
you use continuous vector, which is typically other

05:41:24.851 --> 05:41:26.801
vector, right before softmax,

05:41:27.622 --> 05:41:30.122
And then the hope here is that we can basically

05:41:30.181 --> 05:41:32.183
use more we can

05:41:32.183 --> 05:41:34.263
have more information for each output, and

05:41:34.263 --> 05:41:36.301
this information will be fed back

05:41:36.301 --> 05:41:38.401
to the large models. Without

05:41:38.401 --> 05:41:40.491
quantile without, like, sampling. So

05:41:40.491 --> 05:41:42.661
if you do softmax sampling,

05:41:43.141 --> 05:41:45.561
then, of course, you have to basically

05:41:45.622 --> 05:41:47.651
force the model to

05:41:47.651 --> 05:41:49.739
collapse into representation that is discrete.

05:41:50.371 --> 05:41:52.069
But if you use continuous factor,

05:41:52.451 --> 05:41:54.630
then there could be much more things inside

05:41:55.231 --> 05:41:56.931
a discontinuous representation.

05:41:57.391 --> 05:41:59.630
That is the idea. And the idea actually

05:41:59.630 --> 05:42:01.651
works to some extent So

05:42:01.651 --> 05:42:03.971
we actually have, initial results

05:42:03.971 --> 05:42:05.801
on this original coconut paper.

05:42:06.442 --> 05:42:08.221
That shows that it can actually worked quite well,

05:42:08.701 --> 05:42:10.621
for some of the datasets that

05:42:11.151 --> 05:42:13.441
contain tricky structures.

05:42:13.881 --> 05:42:15.962
So, basically, that that dataset

05:42:15.962 --> 05:42:18.319
has this graph reachability

05:42:18.319 --> 05:42:20.239
problem that asks the model to see,

05:42:20.480 --> 05:42:21.961
which destination can

05:42:22.491 --> 05:42:24.941
be reached. Given the starting

05:42:25.081 --> 05:42:27.241
point. In that particular tricky cases,

05:42:27.702 --> 05:42:29.801
we actually show that this approach

05:42:30.131 --> 05:42:32.231
can do very well compared to the,

05:42:32.773 --> 05:42:34.331
channel sorts. So

05:42:35.801 --> 05:42:38.221
so, and we actually have a theory basically tell you that

05:42:38.501 --> 05:42:40.201
for the the distributability

05:42:40.741 --> 05:42:42.911
problems, the channel faults, If

05:42:42.911 --> 05:42:45.151
you use the squid version of it, then

05:42:45.151 --> 05:42:47.241
you need, like, o n square. N being, like,

05:42:47.241 --> 05:42:49.206
number of nodes. Being the

05:42:49.271 --> 05:42:51.371
directed graph. In order to find,

05:42:51.611 --> 05:42:52.991
the reachability problems.

05:42:53.952 --> 05:42:56.372
But if you use continuous chain of thoughts,

05:42:57.202 --> 05:42:58.341
then, you actually

05:43:00.050 --> 05:43:02.271
can use order n o

05:43:02.271 --> 05:43:04.355
o d o n, number of

05:43:04.355 --> 05:43:06.411
steps. With o n embedded in dimensions

05:43:06.411 --> 05:43:08.692
to solve the problem. The reason

05:43:08.692 --> 05:43:10.792
why it can work is because

05:43:12.131 --> 05:43:14.551
continuous channel source, will store

05:43:15.381 --> 05:43:17.491
superposition of the embeddings.

05:43:17.911 --> 05:43:20.151
Right? So while the discrete channel source can

05:43:20.151 --> 05:43:22.273
only store one of the, node

05:43:22.273 --> 05:43:24.191
at a time. That's

05:43:24.810 --> 05:43:26.901
the key difference. So

05:43:26.901 --> 05:43:29.241
if you have a vector that can store multiple,

05:43:30.126 --> 05:43:32.211
possible frontier

05:43:32.211 --> 05:43:34.398
of your search, then

05:43:34.398 --> 05:43:36.501
what happens is that every

05:43:36.501 --> 05:43:38.911
time you do one step search, it basically

05:43:38.971 --> 05:43:40.911
expands its frontier to

05:43:41.911 --> 05:43:44.371
to the larger frontier in the graph.

05:43:44.962 --> 05:43:47.042
And then you don't need to worry too much

05:43:47.042 --> 05:43:49.451
about which syncing traces is the correct

05:43:49.931 --> 05:43:52.111
syncing traces, because we have already simultaneously

05:43:52.251 --> 05:43:54.191
enumerated all the possible syncing traces.

05:43:55.151 --> 05:43:57.571
With the superposition of all the node embeddings.

05:43:58.161 --> 05:44:00.421
And once, if you do this iteratively

05:44:01.201 --> 05:44:03.281
until the frontier of the

05:44:03.281 --> 05:44:05.183
node touch the

05:44:05.321 --> 05:44:07.640
final target, which is basically the target

05:44:07.640 --> 05:44:10.079
that you want to reason about, then the model

05:44:10.079 --> 05:44:12.239
realizes that, the model has reached

05:44:12.239 --> 05:44:14.411
the destination. And once the model know

05:44:14.411 --> 05:44:16.171
it reached destination, then it can

05:44:17.131 --> 05:44:19.140
back trace find what's the best

05:44:19.140 --> 05:44:20.041
syncing traces.

05:44:21.231 --> 05:44:23.251
That you can the the syncing

05:44:23.311 --> 05:44:25.391
should be able to follow. Then,

05:44:25.550 --> 05:44:27.411
the model the the problem has been solved.

05:44:27.981 --> 05:44:30.381
So I think, as a human, usually

05:44:30.381 --> 05:44:32.462
people also have this kind of experience

05:44:32.462 --> 05:44:34.531
that we first come

05:44:34.531 --> 05:44:36.771
with an idea and then know how to how it works, and

05:44:36.771 --> 05:44:38.211
then try explain

05:44:39.249 --> 05:44:41.271
how these things should work.

05:44:41.911 --> 05:44:44.011
With discrete, thinking traces,

05:44:44.310 --> 05:44:46.451
with language. Right?

05:44:46.591 --> 05:44:48.781
So but the the syncing tracer itself

05:44:49.561 --> 05:44:51.801
seems to be a black box. So we don't know how we think

05:44:51.801 --> 05:44:53.651
about that. Once we think about that,

05:44:54.031 --> 05:44:56.121
we can back trace and we give an

05:44:56.121 --> 05:44:58.111
explanation to other people who don't understand.

05:44:59.071 --> 05:45:01.061
So this, basically,

05:45:01.521 --> 05:45:03.221
tell you how this works.

05:45:03.681 --> 05:45:05.841
This gives you a mechanism to how

05:45:05.841 --> 05:45:08.042
this can work. Alright. The latent vector can

05:45:08.042 --> 05:45:09.622
store a superposition.

05:45:10.561 --> 05:45:12.792
For multiple possible sinking

05:45:12.792 --> 05:45:14.571
traces and multiple front terriers.

05:45:15.051 --> 05:45:17.081
Once you hit some hit the

05:45:17.081 --> 05:45:19.331
target, and then everything

05:45:19.331 --> 05:45:21.452
can be explained. So this this

05:45:21.452 --> 05:45:23.471
is much more efficient than if you

05:45:23.532 --> 05:45:25.569
do discrete thinking traces once

05:45:25.621 --> 05:45:27.640
at a time. If you do that, then

05:45:27.640 --> 05:45:29.792
you probably will hit that end,

05:45:29.792 --> 05:45:31.952
and you need to back trace, and hit that again

05:45:31.952 --> 05:45:33.991
again to back trace. But with

05:45:33.991 --> 05:45:36.221
this representation, the the signature

05:45:36.462 --> 05:45:37.951
the search becomes much more efficient.

05:45:38.989 --> 05:45:41.130
So So we actually do experiments to show

05:45:41.130 --> 05:45:43.310
that this is indeed the case, by

05:45:44.271 --> 05:45:46.441
checking the inner product between the can you source vector,

05:45:46.441 --> 05:45:48.811
and and the nullity values.

05:45:48.971 --> 05:45:51.292
You see that if the nodes are not reachable

05:45:51.292 --> 05:45:53.622
from a starting point, the attention

05:45:53.781 --> 05:45:55.821
the the inner product is actually almost

05:45:55.821 --> 05:45:57.981
zero. So very spread very spread and

05:45:57.981 --> 05:46:00.221
about central zero. But for

05:46:00.221 --> 05:46:02.241
nodes that are promising, they're frontiers,

05:46:02.621 --> 05:46:04.511
and they're optimal, then,

05:46:04.909 --> 05:46:07.032
the inner product actually

05:46:07.091 --> 05:46:09.571
is higher, which means that they can use sort of vector

05:46:09.571 --> 05:46:11.640
as a container key information.

05:46:11.901 --> 05:46:14.141
And the key information, over time, will become

05:46:14.141 --> 05:46:16.150
more and more salient that's

05:46:16.150 --> 05:46:18.171
that's interesting. Of course,

05:46:18.171 --> 05:46:20.471
people may ask, okay, why there's a difference between

05:46:20.471 --> 05:46:22.631
frontier and optimum? Right? Given the

05:46:22.631 --> 05:46:23.292
previous explanation,

05:46:24.763 --> 05:46:26.901
all the frontier nodes should have the same weights.

05:46:27.141 --> 05:46:29.251
Rather than optimal being

05:46:30.212 --> 05:46:32.041
being, like, having, like, a higher weight.

05:46:33.112 --> 05:46:35.112
So, interestingly that,

05:46:35.351 --> 05:46:37.831
you can actually think about other possible

05:46:37.970 --> 05:46:40.071
scenarios that the model

05:46:40.712 --> 05:46:42.971
can, find the channel source,

05:46:43.032 --> 05:46:44.939
to find answer of the reasoning,

05:46:45.050 --> 05:46:46.761
even without syncing traces. Right? So,

05:46:47.351 --> 05:46:49.511
we actually have a paper, already

05:46:49.511 --> 05:46:51.731
this year that explore these

05:46:51.731 --> 05:46:53.971
possibilities. This this is the same thing

05:46:53.971 --> 05:46:56.091
as setting as the previous settings.

05:46:56.091 --> 05:46:58.101
So that, you have all the you have

05:46:58.101 --> 05:47:00.171
the graph, and then you want the model

05:47:00.171 --> 05:47:01.691
to predict the shortest paths.

05:47:02.491 --> 05:47:04.621
But the shortest path, but we don't want

05:47:04.621 --> 05:47:06.081
the model to do any thinking.

05:47:06.761 --> 05:47:08.872
So there's no CoT. So

05:47:08.872 --> 05:47:10.991
in that case, it's interesting to see that

05:47:11.151 --> 05:47:13.011
the model is able to

05:47:13.231 --> 05:47:15.251
give you pretty good results

05:47:15.821 --> 05:47:17.761
Not only that, you can actually

05:47:18.061 --> 05:47:20.091
see there's

05:47:20.091 --> 05:47:22.371
actually a interesting correspondence

05:47:22.371 --> 05:47:24.462
between the representation,

05:47:25.161 --> 05:47:26.782
after the model had trend,

05:47:27.331 --> 05:47:29.321
and the representation as it computed

05:47:30.361 --> 05:47:32.462
from the graphing balance. So since

05:47:32.462 --> 05:47:34.481
there's a graph, a natural way to compute

05:47:35.271 --> 05:47:37.411
representation without doing any

05:47:37.411 --> 05:47:39.013
deep learning is to use,

05:47:39.891 --> 05:47:42.191
graphing bindings. So you have this graph,

05:47:42.191 --> 05:47:43.971
and then you do some transformations,

05:47:44.781 --> 05:47:46.971
you compute its Laplace, and you

05:47:46.971 --> 05:47:49.131
can get an embedding vectors. Now

05:47:49.131 --> 05:47:50.831
this is embedding vectors. This is embedding vectors.

05:47:51.231 --> 05:47:53.551
Vector that usually people will use to analyze the

05:47:53.551 --> 05:47:54.361
graph structures.

05:47:55.712 --> 05:47:57.792
But, but this the the top part

05:47:57.792 --> 05:47:59.872
has nothing to do with the neural torques. So

05:47:59.872 --> 05:48:02.032
this is some analysis people come up with

05:48:02.032 --> 05:48:04.141
as a mathematicians. But

05:48:04.141 --> 05:48:06.141
interestingly, when we actually train the models,

05:48:06.301 --> 05:48:08.372
without channel thoughts, the model

05:48:08.372 --> 05:48:10.452
actually figure out the representation that is

05:48:10.452 --> 05:48:12.591
very, very similar to

05:48:12.591 --> 05:48:14.691
the edge emitting lightings that we actually get.

05:48:14.691 --> 05:48:15.792
From the

05:48:16.931 --> 05:48:18.391
from the graph and operations.

05:48:19.111 --> 05:48:21.351
We if you actually check correlations, I mean, there's

05:48:21.351 --> 05:48:23.731
actually very high correlations between,

05:48:24.271 --> 05:48:26.511
the graphene runnings computed by graph La

05:48:26.511 --> 05:48:28.471
Plasian and

05:48:28.771 --> 05:48:30.311
the embedding that is, computed

05:48:30.831 --> 05:48:32.601
after you have trained this model.

05:48:33.001 --> 05:48:35.201
For long. So it's actually

05:48:35.361 --> 05:48:37.851
we can actually, basically, like, find a

05:48:37.991 --> 05:48:40.066
way to, basically,

05:48:40.066 --> 05:48:41.911
get dump these algorithms,

05:48:42.371 --> 05:48:44.751
used by the transformers. So from transformers,

05:48:44.751 --> 05:48:47.069
we can actually come with a new algorithm, and find

05:48:47.069 --> 05:48:49.121
shortcuts pass. So algorithm is very simple.

05:48:49.121 --> 05:48:51.621
Right? So you first compute the embeddings, and then you just

05:48:51.841 --> 05:48:53.560
do a greedy approach.

05:48:53.871 --> 05:48:55.951
So this this is an algorithm people never heard about.

05:48:56.111 --> 05:48:58.461
So this is a of course, it has no

05:48:58.461 --> 05:49:00.861
guarantees, for shortest paths, but

05:49:00.861 --> 05:49:03.130
interestingly, you can actually

05:49:03.130 --> 05:49:05.291
find optimal paths for a very small

05:49:05.291 --> 05:49:07.400
graph. To 99% of the time. So

05:49:07.400 --> 05:49:08.861
this algorithm is actually legit.

05:49:09.671 --> 05:49:12.091
If you do you apply this to a graph

05:49:12.150 --> 05:49:13.611
of size 200,

05:49:14.212 --> 05:49:16.311
100, you get maybe 70%

05:49:16.372 --> 05:49:18.720
accuracies. So, then

05:49:18.861 --> 05:49:20.841
this basically tell you that the

05:49:21.081 --> 05:49:22.741
maybe the neural networks actually

05:49:23.221 --> 05:49:25.161
can do, some kind of reasoning,

05:49:25.622 --> 05:49:27.831
for this structured path. Even without

05:49:27.831 --> 05:49:29.051
using any channel source.

05:49:29.933 --> 05:49:32.013
So without using a channel of sorts, the model already

05:49:32.013 --> 05:49:34.122
come with some graph representations then

05:49:34.122 --> 05:49:36.122
use a representation. To,

05:49:36.601 --> 05:49:38.601
have a few candidates. For

05:49:38.890 --> 05:49:40.721
the shortest paths. Then

05:49:41.131 --> 05:49:43.300
can do a chain of thoughts to further

05:49:43.300 --> 05:49:45.541
refine them so that the final results

05:49:45.541 --> 05:49:47.702
become better. That could be one possibility.

05:49:48.071 --> 05:49:50.151
Yeah. So and the reason why

05:49:50.151 --> 05:49:51.851
you see, the optimal

05:49:52.851 --> 05:49:54.942
and the frontier are different.

05:49:54.942 --> 05:49:56.962
It's probably because the model additional

05:49:57.101 --> 05:49:59.111
learn these representations over time.

05:49:59.111 --> 05:50:01.461
So So it's actually very

05:50:01.521 --> 05:50:03.841
interesting, and we're going to continue to exploring,

05:50:04.380 --> 05:50:06.612
whether there is any principles

05:50:06.911 --> 05:50:08.853
for the channel source ideas.

05:50:10.192 --> 05:50:12.251
And how and when the model

05:50:12.331 --> 05:50:14.491
get the answer beforehand or when the

05:50:14.491 --> 05:50:16.431
model really needs the channel source.

05:50:16.721 --> 05:50:18.161
To give you the final solutions.

05:50:19.042 --> 05:50:21.042
Okay. So, it's the time.

05:50:21.361 --> 05:50:23.241
So summary is that first of all,

05:50:23.801 --> 05:50:25.962
if you want to explore exponential search

05:50:25.962 --> 05:50:28.241
space, everyone is GPU poor. Everyone

05:50:28.241 --> 05:50:30.301
is equal. It's it's very hard to

05:50:30.462 --> 05:50:32.702
use all the CPUs to even all the possible

05:50:32.702 --> 05:50:34.900
situations. So people

05:50:34.900 --> 05:50:36.941
have to think about how to

05:50:36.941 --> 05:50:39.021
find a way to generalize, how to

05:50:39.021 --> 05:50:41.230
find generalizable loss in

05:50:41.230 --> 05:50:42.341
order to

05:50:43.612 --> 05:50:45.231
get the right ideas with

05:50:45.692 --> 05:50:47.751
fewer samples. One

05:50:47.751 --> 05:50:49.831
way of doing that is to open a black box

05:50:49.831 --> 05:50:51.611
to get more examples, more informations.

05:50:52.381 --> 05:50:53.681
And how the model works.

05:50:54.452 --> 05:50:56.851
And the data algorithms, from these different

05:50:56.851 --> 05:50:58.941
sites. Right? So and

05:50:58.941 --> 05:51:01.361
this basically give us, like, a unique mode

05:51:01.841 --> 05:51:04.001
if we have unique understanding. Right? So in the

05:51:04.001 --> 05:51:06.021
future, if we don't really

05:51:06.351 --> 05:51:08.452
a lot of compute, we don't have lots

05:51:08.452 --> 05:51:10.511
of resources, maybe

05:51:10.511 --> 05:51:12.372
we should aim for more

05:51:12.881 --> 05:51:14.900
understandings. Then everyone will

05:51:14.900 --> 05:51:17.031
have their own ideas and come up

05:51:17.031 --> 05:51:19.091
with a newer solutions. So

05:51:19.091 --> 05:51:21.171
in the future, hopefully, instead of, like,

05:51:21.171 --> 05:51:23.161
printing front of computer, to

05:51:23.221 --> 05:51:25.381
say it works, we should open a black box

05:51:25.381 --> 05:51:27.511
of the neural networks and then,

05:51:27.970 --> 05:51:30.241
find something better. Thanks.

05:51:37.042 --> 05:51:37.821
Any questions?

05:51:43.721 --> 05:51:45.730
Hi. Yeah. I think you'll find

05:51:45.791 --> 05:51:47.811
good patterns on the attention sink and

05:51:48.051 --> 05:51:50.212
many other findings. I'm just wondering, can

05:51:50.212 --> 05:51:52.480
we design it's just like

05:51:52.480 --> 05:51:54.800
a pattern recognition Right? Can we design

05:51:54.800 --> 05:51:56.951
some of workflow or let AI to do

05:51:57.431 --> 05:51:59.661
some of this work? Okay.

05:51:59.661 --> 05:52:00.800
That's a great question.

05:52:01.782 --> 05:52:03.962
Yeah. I think in the future, definitely, it's possible.

05:52:04.721 --> 05:52:06.721
We can it's possible to use AI to,

05:52:07.041 --> 05:52:09.221
to to work on AI itself. So

05:52:09.221 --> 05:52:11.401
that, we actually have

05:52:11.401 --> 05:52:12.791
automatic pattern matching

05:52:13.622 --> 05:52:15.701
way to find find new discoveries.

05:52:16.021 --> 05:52:18.101
Yeah. So I think the future is possible. But I

05:52:18.101 --> 05:52:19.640
will also say it may be constrained

05:52:21.281 --> 05:52:23.372
by how current

05:52:23.372 --> 05:52:25.481
AI system can can find the patterns.

05:52:25.962 --> 05:52:28.202
Yeah. Because these patterns are very subtle,

05:52:28.202 --> 05:52:30.391
and the human has insights human. Human

05:52:30.872 --> 05:52:33.032
is human is super efficient on learning, so

05:52:33.032 --> 05:52:35.131
it can come with a new insights with

05:52:35.131 --> 05:52:37.181
only a few samples. Yeah. I feel

05:52:37.181 --> 05:52:39.042
like you kind of have a methodology

05:52:39.261 --> 05:52:41.292
of how to I also sir,

05:52:41.292 --> 05:52:43.532
listened to your podcast. You said use

05:52:43.532 --> 05:52:45.731
fewer data points to guess

05:52:45.891 --> 05:52:47.991
identify that pattern. Yeah. So maybe

05:52:48.201 --> 05:52:50.140
a good summary of these methodologies

05:52:50.681 --> 05:52:52.702
will be very helpful. Okay. For,

05:52:52.702 --> 05:52:54.801
like, us young people.

05:52:55.641 --> 05:52:57.951
I see. I see. Okay. Thank you. Thanks.

05:52:58.221 --> 05:53:00.381
I have one very short question. I know

05:53:00.381 --> 05:53:02.581
we're out of time. We

05:53:02.581 --> 05:53:04.671
tried coconut, We think that

05:53:04.831 --> 05:53:06.861
you know, it's it's it excels at

05:53:07.501 --> 05:53:09.741
many areas, but also

05:53:09.741 --> 05:53:11.952
struggle compared to the like

05:53:11.952 --> 05:53:13.651
a token space chain or T on others.

05:53:14.171 --> 05:53:15.871
In another word, it's very unstable.

05:53:16.251 --> 05:53:18.431
So in order to make it work and

05:53:18.631 --> 05:53:20.872
like, mass deploy into production, what do you think

05:53:20.872 --> 05:53:22.831
is the path forward? Somebody

05:53:23.130 --> 05:53:25.561
argued that we need to do that. At the pretraining

05:53:25.701 --> 05:53:27.712
stage. Do you agree? Yeah. I

05:53:27.712 --> 05:53:30.051
think how these a way of seeing papers that

05:53:30.111 --> 05:53:31.801
can can can do this in the pre training stage.

05:53:32.721 --> 05:53:34.841
And, for example, people propose

05:53:34.841 --> 05:53:36.141
to use, like, the interleaving

05:53:36.921 --> 05:53:38.841
channel of thoughts, continuous and,

05:53:39.161 --> 05:53:41.261
discretion of thoughts. In order

05:53:41.261 --> 05:53:43.292
to get this to work. I

05:53:43.433 --> 05:53:45.521
think one big problem is that we

05:53:45.521 --> 05:53:47.601
don't know how to supervise these latent

05:53:47.601 --> 05:53:49.862
vectors. Yep. Right. So and

05:53:50.001 --> 05:53:52.041
right now, people usually are using

05:53:52.761 --> 05:53:54.621
for example, a mixture of

05:53:54.841 --> 05:53:56.859
sub tokens. We have tokens, each

05:53:56.859 --> 05:53:58.171
have embeddings and mixture of embeddings.

05:53:58.891 --> 05:54:00.861
That that might be one of the way of doing that.

05:54:01.101 --> 05:54:03.261
This is also consistent with the self fulfilling

05:54:03.261 --> 05:54:05.501
prophecy position picture. That you can

05:54:05.501 --> 05:54:07.630
do. I think going forward, I mean, the long

05:54:07.630 --> 05:54:09.821
term run of course will be okay, we want

05:54:09.821 --> 05:54:12.061
the latent vectors, and we want latent vector

05:54:12.061 --> 05:54:14.121
to be super creative and to find

05:54:14.121 --> 05:54:16.211
the right representation so that the reason it

05:54:16.211 --> 05:54:18.461
become very efficient. So I think

05:54:18.461 --> 05:54:20.451
we want to explore

05:54:20.611 --> 05:54:22.701
many different kind of variations so that

05:54:22.861 --> 05:54:24.862
some of them can work better than the others. And I see. I

05:54:24.862 --> 05:54:26.971
think soft talk can do Can we work? Yeah. And

05:54:26.971 --> 05:54:29.011
it seems it seems that you do believe that a

05:54:29.011 --> 05:54:30.470
mixture of the token

05:54:31.601 --> 05:54:33.351
reasoning and the latent reasoning

05:54:33.831 --> 05:54:35.911
Combining them is the path forward. Yeah.

05:54:35.911 --> 05:54:38.051
That's combining them definitely will give

05:54:38.051 --> 05:54:40.071
you more efficiency. Because

05:54:40.071 --> 05:54:42.230
then you have partially autogressive and

05:54:42.230 --> 05:54:44.241
also partially fulfill. In

05:54:44.241 --> 05:54:46.401
this kind of stage. Got it. Yeah. This will definitely

05:54:46.401 --> 05:54:48.481
one way of doing that. Yeah. Thank

05:54:48.481 --> 05:54:48.751
you.

05:54:53.361 --> 05:54:55.741
Okay. So if you don't

05:54:55.741 --> 05:54:57.121
have more questions, then

05:54:57.791 --> 05:55:00.011
we can start this

05:55:00.071 --> 05:55:00.701
panel stage.

05:55:06.991 --> 05:55:07.491
Thanks.

05:55:43.601 --> 05:55:44.061
Yes.

05:55:46.985 --> 05:55:49.351
Test. Yeah. Thanks for

05:55:49.351 --> 05:55:50.711
your interest. Remark.

05:55:51.671 --> 05:55:54.036
We all know a lot from to

05:55:54.036 --> 05:55:56.300
make more reasonable

05:55:56.801 --> 05:55:58.890
explainable. So, yeah, and we

05:55:58.890 --> 05:56:00.971
will continue as

05:56:00.971 --> 05:56:03.051
our panel session. And Yuan Dong is

05:56:03.051 --> 05:56:03.931
also one of

05:56:05.192 --> 05:56:07.201
some of her most important panel session.

05:56:07.601 --> 05:56:10.023
Yes. And I will also introduce

05:56:10.321 --> 05:56:12.481
another panel session, and left one

05:56:12.481 --> 05:56:13.622
is Zhong Wei.

05:56:14.491 --> 05:56:16.591
He is a Baidu seed researcher.

05:56:16.731 --> 05:56:18.671
Many folks are on the post training

05:56:19.221 --> 05:56:21.353
Yeah. And the later

05:56:21.353 --> 05:56:23.261
one is the Nidiang. Yeah.

05:56:23.401 --> 05:56:25.011
He's flown Microsoft.

05:56:25.801 --> 05:56:28.042
Research of Microsoft M AI and

05:56:28.042 --> 05:56:30.001
doing most of

05:56:30.161 --> 05:56:31.731
also the post training work.

05:56:32.292 --> 05:56:34.071
And there's also.

05:56:34.771 --> 05:56:36.898
Yeah. We all know

05:56:36.898 --> 05:56:39.281
him. He he was the speaker.

05:56:39.831 --> 05:56:42.011
And Tao Jiawe, he's the

05:56:42.231 --> 05:56:44.411
meta researcher Many

05:56:44.711 --> 05:56:46.261
folks doing efficient reasoning.

05:56:47.273 --> 05:56:49.353
Yeah. I'm all about to host this

05:56:49.353 --> 05:56:51.456
session. And to

05:56:51.917 --> 05:56:53.961
ask question for all the

05:56:53.961 --> 05:56:55.841
panel speaker. Yeah.

05:57:01.011 --> 05:57:03.331
Okay. Now let's start our

05:57:03.331 --> 05:57:05.581
panel session. The first

05:57:05.880 --> 05:57:08.091
question will be continue with

05:57:08.091 --> 05:57:09.306
previous talk.

05:57:10.640 --> 05:57:12.471
Large language model remain

05:57:12.931 --> 05:57:15.282
fundamentally black ball system Yeah.

05:57:15.901 --> 05:57:18.081
As Randall's talk said, recently

05:57:18.081 --> 05:57:19.441
there are a lot of

05:57:20.560 --> 05:57:22.341
work trying to open a

05:57:22.801 --> 05:57:24.311
black box For example,

05:57:24.851 --> 05:57:25.991
security level analysis,

05:57:27.311 --> 05:57:29.411
Providence Life have began to reveal

05:57:29.951 --> 05:57:32.441
how the model store

05:57:32.661 --> 05:57:35.081
language, how the model doing listening,

05:57:36.171 --> 05:57:37.791
and how the model implement

05:57:38.251 --> 05:57:39.711
represent our task.

05:57:40.571 --> 05:57:42.991
So the question is, how far

05:57:43.050 --> 05:57:45.041
can opening the black box

05:57:46.079 --> 05:57:48.101
actually guide the design of

05:57:49.329 --> 05:57:49.909
more efficient

05:57:51.601 --> 05:57:53.841
reasoning system. Or in other

05:57:53.841 --> 05:57:56.321
word, can this insight translate

05:57:56.542 --> 05:57:58.611
to like, loyal

05:57:58.611 --> 05:58:00.631
design such as more efficient

05:58:01.011 --> 05:58:03.021
model with structured

05:58:03.021 --> 05:58:05.130
memory, like efficient k v cache

05:58:05.130 --> 05:58:06.651
management, or

05:58:07.212 --> 05:58:09.411
better attention magazine. Yeah.

05:58:09.471 --> 05:58:11.011
Let's give time to panelist.

05:58:15.181 --> 05:58:16.249
Thanks for the

05:58:17.231 --> 05:58:19.421
thanks for the question. So I think

05:58:19.421 --> 05:58:21.261
right now, we already have a lot of,

05:58:21.541 --> 05:58:23.281
work that, try to explore

05:58:24.541 --> 05:58:26.941
how the reasoning actually happens.

05:58:27.421 --> 05:58:29.501
In the models to understand what's going on

05:58:29.501 --> 05:58:31.691
and leverage that to improve

05:58:32.069 --> 05:58:33.611
our our model efficiencies.

05:58:34.271 --> 05:58:36.431
So I think, in my talk, I already mentioned

05:58:36.431 --> 05:58:38.551
some examples. We have more examples,

05:58:38.851 --> 05:58:40.911
due to the time limit, so we don't really have

05:58:40.911 --> 05:58:42.991
time and effort to to time to

05:58:42.991 --> 05:58:45.051
to explain. But I think

05:58:45.051 --> 05:58:46.751
a lot of work has already,

05:58:47.452 --> 05:58:49.471
been going along

05:58:49.471 --> 05:58:51.651
in different directions. Right? So

05:58:51.711 --> 05:58:53.251
I think many of the existing

05:58:54.107 --> 05:58:56.431
study existing, like, a mainstream

05:58:57.741 --> 05:58:59.901
architect design are also following that

05:58:59.901 --> 05:59:02.061
path. Right? So for example, the

05:59:02.061 --> 05:59:04.461
the reason why people use MOE is

05:59:04.499 --> 05:59:06.650
because FFN layer is

05:59:06.650 --> 05:59:08.681
huge. Right? So you have a lot of

05:59:08.681 --> 05:59:10.781
parameters, and you have to use

05:59:11.131 --> 05:59:13.452
a way to set specific parameter into different,

05:59:13.612 --> 05:59:15.151
GPUs to different machines.

05:59:15.861 --> 05:59:18.021
Right? So then that's why you people have tensor

05:59:18.021 --> 05:59:20.091
parison that's why people have

05:59:20.470 --> 05:59:22.561
MOEs, have routers, etcetera. So

05:59:22.561 --> 05:59:24.801
these designs are all constrained by that and

05:59:24.801 --> 05:59:26.581
constrained by other and insights.

05:59:26.962 --> 05:59:29.201
Of our models. And we find that, oh,

05:59:30.081 --> 05:59:32.341
doing the feed forward only a very fast

05:59:32.981 --> 05:59:34.921
number of neurons that get activated.

05:59:35.581 --> 05:59:37.741
Alright. So then, naturally, people will think,

05:59:37.901 --> 05:59:40.161
maybe should group them together into

05:59:40.161 --> 05:59:42.581
multiple Express. That's why m o e comes.

05:59:42.931 --> 05:59:44.991
So, basically, we find spot

05:59:44.991 --> 05:59:47.201
patterns and then we go to the

05:59:47.761 --> 05:59:49.701
MOS stage. So this is examples

05:59:50.161 --> 05:59:51.951
into how we leverage the

05:59:52.351 --> 05:59:54.691
insights to develop new acquisition approaches.

05:59:55.311 --> 05:59:57.391
So, and as to the more principle

05:59:57.391 --> 05:59:59.811
way of understanding the neural networks,

06:00:00.292 --> 06:00:01.921
I think it's a very hard problem.

06:00:02.437 --> 06:00:04.411
And but I would say, eventually,

06:00:04.550 --> 06:00:06.390
this will happen. I have

06:00:06.556 --> 06:00:08.581
I'm being very optimistic about this.

06:00:09.051 --> 06:00:11.471
Right? So and some people may not have the same optimism

06:00:11.612 --> 06:00:13.621
that as me. They will say, oh,

06:00:13.861 --> 06:00:15.421
model cannot be understood.

06:00:15.981 --> 06:00:18.241
But I would say I am optimistic.

06:00:18.301 --> 06:00:20.561
I think, eventually, it will happen.

06:00:20.871 --> 06:00:22.171
This is based on my

06:00:23.381 --> 06:00:25.542
of my simple reasoning. Right? So here's my

06:00:25.542 --> 06:00:27.631
reasoning. If AJI

06:00:27.773 --> 06:00:30.021
has achieved and the human

06:00:30.341 --> 06:00:31.501
they have nothing to do,

06:00:32.462 --> 06:00:34.651
then they will find they will

06:00:34.651 --> 06:00:36.792
try to understand what's going to make

06:00:36.792 --> 06:00:38.111
sure that he must be available.

06:00:39.071 --> 06:00:41.251
If the model cannot achieve AGI,

06:00:41.571 --> 06:00:43.591
then we get stuck and we know

06:00:43.591 --> 06:00:45.692
most CPU doesn't help. Then we have

06:00:45.692 --> 06:00:47.311
to sit down and understand what's going

06:00:48.181 --> 06:00:50.341
So in either cases, we need

06:00:50.341 --> 06:00:52.571
to understand what's going on. So that's

06:00:52.571 --> 06:00:54.461
the path that everyone need to go through.

06:00:55.181 --> 06:00:56.851
So I'm optimistic.

06:01:04.661 --> 06:01:07.161
Is there any other one, two,

06:01:07.691 --> 06:01:09.069
Yeah. Maybe I can continue.

06:01:09.921 --> 06:01:12.171
Yeah. I totally agree with agree with that.

06:01:12.171 --> 06:01:14.241
I think model improvement is

06:01:14.641 --> 06:01:16.981
definitely a good direction to go. But

06:01:16.981 --> 06:01:19.061
I'm not quite sure whether it is too long

06:01:19.061 --> 06:01:21.183
or too short. Maybe you need to take

06:01:21.183 --> 06:01:23.471
a long time to to find a

06:01:23.471 --> 06:01:25.550
new, component, but maybe just the

06:01:25.550 --> 06:01:27.701
single moment we come up with a new

06:01:27.701 --> 06:01:29.741
component, and that works very well. So

06:01:30.061 --> 06:01:31.681
But, from poster training

06:01:32.141 --> 06:01:34.181
perspective, it's better. I'm

06:01:34.181 --> 06:01:36.261
doing the RA infrastructure. So

06:01:36.261 --> 06:01:38.041
I feel like, when the model

06:01:38.821 --> 06:01:41.321
changes, RL system is quite

06:01:41.771 --> 06:01:44.031
sensitive to that model model

06:01:44.971 --> 06:01:46.991
modification. So

06:01:47.231 --> 06:01:49.391
so currently, some of the model people,

06:01:49.391 --> 06:01:50.971
they are they want to do some

06:01:51.452 --> 06:01:53.792
pre training exploration and

06:01:55.650 --> 06:01:58.081
just, in just, you know, maybe few days. They want to

06:01:58.140 --> 06:01:59.371
do the whole stack.

06:02:00.271 --> 06:02:02.511
Don't want to just focus on pre training, and

06:02:02.511 --> 06:02:04.751
then we try the post train and post train that

06:02:05.292 --> 06:02:07.491
So because for

06:02:07.491 --> 06:02:09.861
RL, I think yeah,

06:02:09.861 --> 06:02:11.941
if if we can memorize something and make

06:02:11.941 --> 06:02:13.731
the model smaller, that definitely work.

06:02:14.131 --> 06:02:16.532
But for RL, sometimes maybe negative

06:02:16.532 --> 06:02:18.321
samples is also important.

06:02:18.641 --> 06:02:20.782
Because the RL can if we

06:02:20.782 --> 06:02:22.862
can generate the negative samples, and RO

06:02:22.862 --> 06:02:25.001
will use that negative sample to

06:02:25.001 --> 06:02:27.331
avoid generating the

06:02:27.331 --> 06:02:29.390
same patterns. So So,

06:02:29.550 --> 06:02:31.901
from my current belief, I feel like

06:02:32.542 --> 06:02:34.451
making the smaller models like

06:02:35.091 --> 06:02:37.231
is more suitable

06:02:37.291 --> 06:02:39.501
for, like, we distill our

06:02:39.501 --> 06:02:41.661
large models to our smaller models, and

06:02:41.661 --> 06:02:44.001
use smaller model for inference or serving.

06:02:44.291 --> 06:02:45.456
But for

06:02:46.341 --> 06:02:48.361
audio training, I I guess

06:02:48.361 --> 06:02:49.771
we probably need more research.

06:02:50.810 --> 06:02:51.310
Yeah.

06:02:52.931 --> 06:02:54.871
Yeah. So I basically do

06:02:55.001 --> 06:02:57.462
search on pretraining and I got capture, and I

06:02:57.761 --> 06:02:59.861
look at example on intention mechanism. Like,

06:03:00.841 --> 06:03:02.981
so people are used to publish

06:03:03.301 --> 06:03:05.381
blog post on, like, a the understanding

06:03:05.655 --> 06:03:07.951
the cost and learning

06:03:07.951 --> 06:03:09.853
and the induction pass. They found that,

06:03:10.641 --> 06:03:12.501
you have two layers of transformer,

06:03:13.131 --> 06:03:15.231
attention will learn to have

06:03:15.372 --> 06:03:17.712
a pre fixed machine and then do a copying.

06:03:18.001 --> 06:03:20.271
But if you only have one layer of attention,

06:03:20.331 --> 06:03:22.569
then you cannot do this kind of induction test,

06:03:22.569 --> 06:03:23.941
which hurts the

06:03:24.801 --> 06:03:26.962
by a lot. So

06:03:26.962 --> 06:03:28.981
so so so so so this kind of a mechanism

06:03:29.201 --> 06:03:31.551
capability by doing some synthetic

06:03:31.771 --> 06:03:33.631
task pretraining actually helps

06:03:34.151 --> 06:03:36.481
the area to develop new mechanism to

06:03:36.481 --> 06:03:37.937
do the

06:03:39.471 --> 06:03:41.631
injection head in one layer so that we can have

06:03:41.631 --> 06:03:43.792
better capacity. So they

06:03:43.792 --> 06:03:45.741
actually use a very simple trick to actually

06:03:46.077 --> 06:03:48.271
to have, like, a short collision

06:03:48.491 --> 06:03:50.701
and to let the key vectors

06:03:50.701 --> 06:03:52.481
to be aware of the neighbor tokens.

06:03:53.081 --> 06:03:54.712
So that the attention can actually

06:03:55.731 --> 06:03:57.851
retrieve the in to

06:03:57.851 --> 06:03:59.931
do the prefix matching in one layer instead

06:03:59.931 --> 06:04:02.201
of two layers. It actually can help, like, to

06:04:02.671 --> 06:04:04.831
model to converge very faster. And it has

06:04:04.831 --> 06:04:06.862
been widely used user for

06:04:06.862 --> 06:04:08.921
letting your attention mechanism and the

06:04:08.921 --> 06:04:10.831
ARNN like a or like a

06:04:11.871 --> 06:04:14.131
a, something like that. And so it's actually

06:04:15.181 --> 06:04:17.411
mechanical interpreter has been

06:04:17.731 --> 06:04:19.891
very useful for architecture development.

06:04:19.891 --> 06:04:22.023
And I think it will be

06:04:22.023 --> 06:04:24.421
useful. But it also

06:04:24.421 --> 06:04:26.511
has some like, on drawbacks,

06:04:26.511 --> 06:04:28.890
like you do experiments on synthetic

06:04:28.890 --> 06:04:31.211
data, so hyperparameter will be very

06:04:31.451 --> 06:04:33.951
sensitive for your performance. So it it's not always

06:04:33.999 --> 06:04:35.951
like a very

06:04:36.421 --> 06:04:38.741
statistically significant correlation

06:04:38.741 --> 06:04:40.711
there. But, actually, I think

06:04:41.431 --> 06:04:43.451
actually, it inspired you to do something like

06:04:43.771 --> 06:04:45.811
to do some modification to architecture.

06:04:45.951 --> 06:04:48.331
So it's still have for even though it may

06:04:48.630 --> 06:04:50.771
not scale in some in some case. So,

06:04:50.771 --> 06:04:53.011
yeah, scaling is still very hard, like, to have some

06:04:53.011 --> 06:04:53.511
scalable

06:04:55.251 --> 06:04:56.961
predictions. With the mechanics and

06:04:57.841 --> 06:04:58.261
interpretive

06:05:03.901 --> 06:05:06.331
Okay. Yeah. Thanks for answering that question.

06:05:06.641 --> 06:05:08.831
Let's move on to the second

06:05:08.831 --> 06:05:11.071
question. Our second question is

06:05:11.071 --> 06:05:13.561
about the scaling door. Yeah.

06:05:13.881 --> 06:05:16.041
As the recent model skill

06:05:16.771 --> 06:05:19.211
are beginning to see some

06:05:20.310 --> 06:05:22.521
phenomena or even a law

06:05:22.579 --> 06:05:24.761
like the scoring or like one.

06:05:25.622 --> 06:05:26.811
For example,

06:05:27.851 --> 06:05:30.273
QA can sample multiple reason in pass,

06:05:31.181 --> 06:05:32.391
and aggregate them

06:05:33.353 --> 06:05:35.421
to majority watering. For

06:05:35.421 --> 06:05:37.581
example, if you test the pass one

06:05:37.581 --> 06:05:39.631
accuracy, only

06:05:39.631 --> 06:05:41.101
about 68%

06:05:42.140 --> 06:05:43.291
by field test pass

06:05:44.451 --> 06:05:46.880
500 the accuracy increase

06:05:47.021 --> 06:05:48.081
to 80%.

06:05:49.103 --> 06:05:51.042
So, does the panelist believe

06:05:51.581 --> 06:05:53.841
that reasoning model

06:05:53.890 --> 06:05:55.521
has such scaling

06:05:56.442 --> 06:05:58.551
that we can eventually formulate it

06:05:59.141 --> 06:06:01.301
And could such scaling know go

06:06:01.462 --> 06:06:03.890
continue to go? And

06:06:03.890 --> 06:06:05.051
guide the design of

06:06:06.013 --> 06:06:08.501
more efficient reasoning model. Yeah.

06:06:11.792 --> 06:06:13.231
So I do agree that,

06:06:13.941 --> 06:06:16.021
for the post training, already inside the

06:06:16.021 --> 06:06:18.051
skin laws are very important, and,

06:06:18.532 --> 06:06:20.631
we should even design a new skin allowed

06:06:21.612 --> 06:06:23.881
for skin test time compute. So

06:06:23.881 --> 06:06:26.282
to to explain this questions,

06:06:26.282 --> 06:06:28.362
we can break into two parts, like, one

06:06:28.362 --> 06:06:30.481
is scaling by weights, and

06:06:30.481 --> 06:06:32.881
scaling by depth. So scaling by

06:06:32.941 --> 06:06:35.400
weight means we can,

06:06:35.901 --> 06:06:37.319
just, have parallel

06:06:37.941 --> 06:06:40.431
scaleings launching

06:06:40.489 --> 06:06:42.971
parallel test launching a a

06:06:43.032 --> 06:06:45.192
few chain of salt at the same time and

06:06:45.192 --> 06:06:47.321
do majority vote to other aggregation

06:06:47.321 --> 06:06:49.581
methodologies to aggregate the final answers.

06:06:50.861 --> 06:06:53.161
To build scaling by weight is

06:06:53.561 --> 06:06:55.821
is more like, like a sampling problems.

06:06:56.471 --> 06:06:57.941
We we it's actually the

06:06:58.851 --> 06:07:01.011
scaling, scaling by which is

06:07:01.011 --> 06:07:02.310
the same as sampling

06:07:03.381 --> 06:07:05.561
from a structured distributions.

06:07:05.791 --> 06:07:08.291
By with these heretical

06:07:08.351 --> 06:07:10.390
modes. If you

06:07:10.390 --> 06:07:12.231
view, if you view the reasoning

06:07:12.712 --> 06:07:14.971
as sampling problems given a fixed

06:07:15.933 --> 06:07:17.952
prompts and a fixed LOMs, we

06:07:18.011 --> 06:07:20.230
always have a sequential distributions

06:07:21.282 --> 06:07:23.523
based on the constructed by the next

06:07:23.523 --> 06:07:25.741
token prediction and distributions.

06:07:26.311 --> 06:07:28.171
And in that case, if we sample

06:07:28.433 --> 06:07:30.091
if we sample more rollouts,

06:07:30.451 --> 06:07:32.601
from the distributions, we have

06:07:32.980 --> 06:07:35.171
more realizations. And understanding this

06:07:35.171 --> 06:07:37.271
distribution better. In

06:07:37.271 --> 06:07:39.591
this case, we can, by

06:07:39.591 --> 06:07:41.891
doing by by launching more

06:07:42.622 --> 06:07:44.782
by launching more samples and more

06:07:44.782 --> 06:07:46.691
rollouts, can have

06:07:46.931 --> 06:07:49.351
better understanding of the underlying distributions,

06:07:49.730 --> 06:07:51.731
and and and doing

06:07:51.731 --> 06:07:53.911
a more accurate majority vote to boost

06:07:54.211 --> 06:07:55.991
the test time performance.

06:07:56.391 --> 06:07:58.551
That's, that's scaling by ways.

06:07:58.551 --> 06:08:00.571
And also, we can do scaling by depths.

06:08:01.011 --> 06:08:02.871
It's very important for

06:08:03.431 --> 06:08:05.421
very complicated reading

06:08:05.476 --> 06:08:07.550
task. Such as some

06:08:07.550 --> 06:08:09.921
some reasoning task, might

06:08:09.921 --> 06:08:11.331
require more compute,

06:08:12.347 --> 06:08:14.581
during test time, like

06:08:14.581 --> 06:08:16.741
scaffoldings and some planning searching

06:08:16.741 --> 06:08:18.841
problems. Those problem,

06:08:19.060 --> 06:08:20.841
usually, if you think harder or longer,

06:08:21.601 --> 06:08:23.851
you can almost, for sure, get a get a

06:08:24.091 --> 06:08:26.251
better results. In such scenarios,

06:08:26.771 --> 06:08:29.011
should definitely scale

06:08:29.011 --> 06:08:31.071
in by the depth. To thinking

06:08:31.071 --> 06:08:33.251
longer, so to to boost

06:08:33.251 --> 06:08:35.611
the performance. But there's a

06:08:36.071 --> 06:08:38.202
but there's a balance between

06:08:38.202 --> 06:08:40.371
both weights and depths.

06:08:40.551 --> 06:08:42.091
That's why I believe we should

06:08:42.651 --> 06:08:45.091
design some like, a like a

06:08:45.091 --> 06:08:46.631
new evaluation paradigms,

06:08:48.171 --> 06:08:49.861
like a new evaluation paradigms, like

06:08:50.501 --> 06:08:51.501
controllable thinking. You

06:08:52.861 --> 06:08:54.911
evaluations or benchmarks, should

06:08:54.911 --> 06:08:57.171
control the thinking budget at the same

06:08:57.171 --> 06:08:59.101
time to to measure

06:08:59.239 --> 06:09:01.480
how much the sinking effort and

06:09:01.480 --> 06:09:03.561
improvement over the depths

06:09:03.561 --> 06:09:05.621
or the weights. But overall, I

06:09:05.621 --> 06:09:07.641
do believe, we the

06:09:08.761 --> 06:09:10.702
the scaling loss exists for the reasoning

06:09:10.921 --> 06:09:12.980
models, and there will be more loss.

06:09:13.241 --> 06:09:15.321
To be studied over these

06:09:15.321 --> 06:09:15.821
paradigms.

06:09:23.531 --> 06:09:25.691
Yeah. Maybe I can share some thoughts from the

06:09:25.691 --> 06:09:27.901
infrared So

06:09:27.901 --> 06:09:28.381
I think,

06:09:30.181 --> 06:09:32.421
so we up when we trained the ARIA system,

06:09:32.421 --> 06:09:33.661
we observed that

06:09:34.781 --> 06:09:36.701
the the sequence is increasing,

06:09:36.841 --> 06:09:38.921
but when we add some like,

06:09:39.161 --> 06:09:41.211
the sequence lens panel

06:09:41.211 --> 06:09:42.737
like syncing budget,

06:09:43.282 --> 06:09:44.941
it decreased. But

06:09:46.111 --> 06:09:48.381
the thing is that the

06:09:48.381 --> 06:09:50.721
rollout is a main bottleneck of the ARIA

06:09:50.721 --> 06:09:52.811
system. And it take, if

06:09:52.811 --> 06:09:54.641
you want to generate just

06:09:55.681 --> 06:09:57.781
one training batch, it can take over one

06:09:57.781 --> 06:09:59.831
hour. So

06:09:59.831 --> 06:10:01.481
that means you can you

06:10:01.862 --> 06:10:04.011
can definitely increase the budget

06:10:04.011 --> 06:10:06.077
to the infinite. Infinite. Right?

06:10:06.425 --> 06:10:08.541
So I think it's a quite interesting

06:10:10.183 --> 06:10:12.261
question because, if you want to you

06:10:12.261 --> 06:10:14.351
want to find such a point

06:10:14.351 --> 06:10:16.671
where you can gain the good model

06:10:16.671 --> 06:10:18.792
performance, but also you can

06:10:19.013 --> 06:10:21.230
decrease the the time of

06:10:21.230 --> 06:10:23.081
the rollout. Yeah. So

06:10:23.821 --> 06:10:25.841
yeah, that's my thought.

06:10:28.862 --> 06:10:31.011
Okay. Yeah. Thanks for

06:10:31.011 --> 06:10:33.031
apparently for answering that question.

06:10:33.962 --> 06:10:36.301
So let's continue to our third question.

06:10:36.561 --> 06:10:38.901
Third question is about the agent.

06:10:39.301 --> 06:10:40.773
We all know that

06:10:41.442 --> 06:10:43.911
the agent workflow introduce new

06:10:44.071 --> 06:10:45.917
dimension of optimizing

06:10:47.181 --> 06:10:48.329
Such an operating is

06:10:49.452 --> 06:10:51.601
NILAR CHINING or

06:10:51.601 --> 06:10:54.081
PURE INFLENCE. For example,

06:10:54.140 --> 06:10:56.411
it require toll tour

06:10:56.446 --> 06:10:58.761
calling it require reasoning

06:10:59.061 --> 06:11:01.192
model planning, It require

06:11:01.491 --> 06:11:02.712
retrieval information.

06:11:03.577 --> 06:11:06.071
And it require a wide strategy

06:11:06.212 --> 06:11:07.751
in the middle of a task.

06:11:08.481 --> 06:11:10.515
So the question is, is

06:11:10.515 --> 06:11:12.532
there a key open

06:11:12.532 --> 06:11:13.841
problem in open night

06:11:14.651 --> 06:11:16.071
such agent system

06:11:16.712 --> 06:11:17.941
for efficient reasoning.

06:11:18.821 --> 06:11:20.880
And how the new

06:11:20.880 --> 06:11:22.501
tool, new plan, new task

06:11:24.351 --> 06:11:26.212
need to do for most efficient agent

06:11:26.751 --> 06:11:28.411
calling and listening. Yeah.

06:11:41.911 --> 06:11:44.130
Two quoting Okay. So the two calling

06:11:44.130 --> 06:11:45.831
channel. Yeah. Yeah. Two calling.

06:11:48.741 --> 06:11:50.881
So when we when we

06:11:50.881 --> 06:11:52.821
build, Viral, the RL framework,

06:11:53.021 --> 06:11:54.741
I think the agent

06:11:55.702 --> 06:11:56.431
the agent one,

06:11:57.800 --> 06:12:00.121
there were two main challenges. The first one

06:12:00.121 --> 06:12:02.161
will be whether the framework is

06:12:02.161 --> 06:12:04.591
holistic to the

06:12:04.612 --> 06:12:06.183
different agent work,

06:12:06.952 --> 06:12:08.421
The other one is

06:12:09.381 --> 06:12:11.351
for the framework part,

06:12:11.451 --> 06:12:13.211
I think so

06:12:13.771 --> 06:12:15.091
currently, the Vowel is using

06:12:16.371 --> 06:12:18.501
the the agent loop, like, single prompt

06:12:18.501 --> 06:12:20.371
is you can you can view

06:12:20.851 --> 06:12:22.361
view it like a process, and

06:12:22.921 --> 06:12:25.261
it will iteratively call the decoding

06:12:25.400 --> 06:12:26.891
and call the agent

06:12:27.612 --> 06:12:29.701
and then, wait when when it

06:12:29.701 --> 06:12:31.962
finish, it will finish that infinite

06:12:32.816 --> 06:12:35.042
that So But

06:12:35.042 --> 06:12:37.421
the agent core is very complex

06:12:37.561 --> 06:12:39.981
because, in the future, you might consider,

06:12:40.042 --> 06:12:42.191
like, different agent will interact with

06:12:42.191 --> 06:12:44.231
each other. And

06:12:44.911 --> 06:12:46.921
it will you

06:12:47.140 --> 06:12:49.081
will construct graph

06:12:49.577 --> 06:12:51.691
that connect

06:12:51.691 --> 06:12:53.071
different agents

06:12:54.441 --> 06:12:56.140
first prompts to the final response.

06:12:57.261 --> 06:12:59.273
So whenever you construct

06:12:59.273 --> 06:13:00.891
that graph, it can be a optimization

06:13:01.511 --> 06:13:03.541
question. Whether you can allocate

06:13:03.541 --> 06:13:05.181
different resources for different

06:13:05.981 --> 06:13:07.761
generation, whether you can allocate

06:13:08.231 --> 06:13:10.241
the resources for the rollout, whether

06:13:10.481 --> 06:13:12.721
for this rollouts, this prompt route, or the other

06:13:12.721 --> 06:13:14.730
rollouts, so you can schedule

06:13:14.730 --> 06:13:16.281
in some prompts, scheduling some

06:13:17.351 --> 06:13:19.501
agent core, and you can overlap

06:13:19.901 --> 06:13:21.741
the agent core with the the other decoding.

06:13:22.462 --> 06:13:24.581
So that's the second challenge, how to

06:13:24.581 --> 06:13:26.961
improve the performance. So I think currently,

06:13:28.201 --> 06:13:30.361
Viral is focusing on the first part and,

06:13:30.601 --> 06:13:32.361
do very well the first part.

06:13:33.161 --> 06:13:35.241
And, in our internal framework, we are

06:13:35.241 --> 06:13:37.341
focusing on the second. Part.

06:13:37.341 --> 06:13:37.801
Yep.

06:13:41.851 --> 06:13:43.871
I think another challenge is probably

06:13:43.931 --> 06:13:45.470
like there's a lot of tours

06:13:46.989 --> 06:13:49.150
in the possible agent behaviors and lots

06:13:49.150 --> 06:13:51.371
of tools Some of them, like, has

06:13:51.371 --> 06:13:53.301
very high latency. Some of them had low

06:13:53.622 --> 06:13:55.862
latencies. So, you probably want

06:13:55.862 --> 06:13:57.911
to design a synchronized

06:13:58.691 --> 06:14:00.711
reinforcement system so that it can cope

06:14:00.711 --> 06:14:02.871
with all the situation and try to hide all the tendencies.

06:14:03.751 --> 06:14:05.591
With sufficient large batch size

06:14:05.991 --> 06:14:08.311
and also diverse tool called

06:14:08.311 --> 06:14:10.431
structures. So I

06:14:10.431 --> 06:14:12.751
think first another issue is that how

06:14:12.751 --> 06:14:14.811
you want the how to

06:14:15.131 --> 06:14:17.321
sure that the model can be aware

06:14:17.321 --> 06:14:19.381
of all the tools It could

06:14:19.381 --> 06:14:21.481
be, like, a 100 a thousand tools. And

06:14:21.481 --> 06:14:23.561
you pull everything to the front and you're going

06:14:23.561 --> 06:14:25.021
to have super long contacts problems.

06:14:25.661 --> 06:14:27.681
And you're going to queue the coherence

06:14:27.741 --> 06:14:29.781
of the models. Usually, the model says

06:14:30.421 --> 06:14:32.683
claim has a million contents

06:14:32.683 --> 06:14:34.881
windows, but what really can be used for is maybe

06:14:35.603 --> 06:14:37.181
a 100 a 100 k or something.

06:14:37.931 --> 06:14:40.091
Right? So, that's actually one of the problem,

06:14:40.331 --> 06:14:42.421
that people will face So the

06:14:42.421 --> 06:14:44.501
longer the content window if you put, like, a lot

06:14:44.501 --> 06:14:46.692
of content in the content window, then

06:14:46.692 --> 06:14:48.773
the reasoning capabilities of the model for this

06:14:48.773 --> 06:14:51.101
long content window will decrease. That's

06:14:51.101 --> 06:14:53.391
actually not great. So

06:14:53.452 --> 06:14:55.631
how we deal with that? That's another

06:14:55.931 --> 06:14:57.761
big problem. Another

06:14:58.021 --> 06:14:59.921
and and finally, the context editing

06:15:00.532 --> 06:15:02.521
the context management is another one.

06:15:03.011 --> 06:15:05.411
You want all the information to be incorporating

06:15:05.411 --> 06:15:07.811
to the context window so that the model you know,

06:15:07.811 --> 06:15:09.941
the agent is aware. But at the same

06:15:09.941 --> 06:15:11.901
time, you have a limited context window.

06:15:12.712 --> 06:15:14.571
So how you prioritize

06:15:14.872 --> 06:15:16.571
which information is most important?

06:15:17.241 --> 06:15:18.701
Is another big challenge.

06:15:21.122 --> 06:15:23.470
Yeah. Thanks for all answering

06:15:23.470 --> 06:15:25.890
question. Now it's the following

06:15:26.191 --> 06:15:28.281
question and maybe the next question

06:15:28.771 --> 06:15:30.881
final question. So what

06:15:30.881 --> 06:15:32.542
do we believe the future

06:15:33.061 --> 06:15:35.191
of Efficient recently? What

06:15:35.191 --> 06:15:36.730
we expect is the

06:15:37.411 --> 06:15:39.121
next major efficient gain.

06:15:39.881 --> 06:15:42.011
Gaining from the future You

06:15:42.011 --> 06:15:43.831
see, the agent?

06:15:44.451 --> 06:15:46.881
Is he just a big model? You

06:15:46.946 --> 06:15:49.050
see the prompt, a

06:15:49.191 --> 06:15:51.371
better prompt. Yes? Or is he

06:15:52.001 --> 06:15:53.661
parallel thinking? Skilling,

06:15:54.231 --> 06:15:56.311
Yeah. It's open question and feel free

06:15:56.311 --> 06:15:58.381
to answer. Question of

06:15:58.381 --> 06:16:00.101
future of efficient listening.

06:16:00.581 --> 06:16:00.921
Yeah.

06:16:08.831 --> 06:16:11.071
Okay. So, I think an interesting

06:16:11.071 --> 06:16:12.130
challenge will be

06:16:13.501 --> 06:16:15.981
if you have a a lot of, small

06:16:15.981 --> 06:16:18.091
models, how are they going to

06:16:18.091 --> 06:16:20.251
collaborate and how are going to form together

06:16:20.251 --> 06:16:22.683
into a big model? So that,

06:16:23.721 --> 06:16:25.901
we can basically leverage all these models

06:16:25.962 --> 06:16:28.061
at the same time and do something bigger.

06:16:28.441 --> 06:16:30.591
That could be a very interesting thing. So one thing that

06:16:30.591 --> 06:16:32.611
I have one kind of crazy idea I

06:16:32.611 --> 06:16:34.751
have is that, oh, you might be able to train

06:16:35.071 --> 06:16:37.122
a 100 small models and if

06:16:37.122 --> 06:16:39.362
they put them together, then they become big model.

06:16:39.362 --> 06:16:41.442
They can do big things. If you

06:16:41.442 --> 06:16:43.021
put them into separate

06:16:43.581 --> 06:16:45.661
places, they become specialized model. They can do

06:16:45.661 --> 06:16:47.671
small things. So it's the

06:16:47.671 --> 06:16:49.131
true transformer. But,

06:16:49.991 --> 06:16:52.151
the it it will be very interesting to see if

06:16:52.151 --> 06:16:54.631
that could happen. So in that case, you might maximize

06:16:54.631 --> 06:16:56.841
our efficiency by using

06:16:56.901 --> 06:16:59.091
your small models at demand at the

06:16:59.091 --> 06:17:01.291
same time. Can put them together into big model

06:17:01.801 --> 06:17:03.641
for very high very high problems.

06:17:04.042 --> 06:17:06.381
So that could be interesting challenge.

06:17:06.881 --> 06:17:09.161
But this is just my crazy

06:17:09.161 --> 06:17:11.319
thoughts. I would love to hear about other people's

06:17:11.319 --> 06:17:11.801
ideas

06:17:14.411 --> 06:17:16.171
Yeah. I do think there's a large

06:17:16.651 --> 06:17:18.591
space for like a new architecture to

06:17:18.751 --> 06:17:20.991
be more efficient on, like, a long

06:17:20.991 --> 06:17:23.001
sequence reading, like, you know

06:17:23.001 --> 06:17:25.141
for when we're generating sort like 32 ks

06:17:25.221 --> 06:17:27.301
if you're using full attention, it will be very

06:17:27.301 --> 06:17:28.442
slow. And

06:17:29.421 --> 06:17:30.681
so we need to cut that

06:17:31.441 --> 06:17:33.441
that kind of cost so that we can have a good

06:17:33.681 --> 06:17:35.819
more effective and agent that

06:17:35.819 --> 06:17:37.521
can sync shorter and quicker?

06:17:38.220 --> 06:17:40.191
And also,

06:17:40.411 --> 06:17:42.480
like, question, like, how we do

06:17:42.480 --> 06:17:44.111
the continued learning, doing, like,

06:17:44.981 --> 06:17:46.966
inference time. So so they also needed, like, a

06:17:47.077 --> 06:17:49.300
an actual innovations like how to update

06:17:49.300 --> 06:17:51.311
your ways, like update

06:17:51.631 --> 06:17:53.311
architectures, like, in the

06:17:53.941 --> 06:17:56.181
during the test time. So I think

06:17:56.181 --> 06:17:58.622
there's still, like, a space, like, on both the

06:17:58.921 --> 06:18:00.301
efficiency side for your architecture

06:18:01.151 --> 06:18:03.551
for long lived and also on the capacity

06:18:03.551 --> 06:18:05.301
side, like, how to define

06:18:06.023 --> 06:18:07.991
develop more capable

06:18:08.231 --> 06:18:10.421
models to do, like, a continued learning at

06:18:10.421 --> 06:18:11.351
a time. Yeah.

06:18:16.041 --> 06:18:18.201
So I also agree that, the better

06:18:18.201 --> 06:18:19.941
agents system

06:18:20.239 --> 06:18:22.521
will be much, more important

06:18:22.521 --> 06:18:24.513
than like,

06:18:24.651 --> 06:18:26.433
just increasing the size of the models.

06:18:26.811 --> 06:18:28.891
We all know that the there

06:18:28.891 --> 06:18:31.051
will be diminishing returns if we

06:18:31.051 --> 06:18:33.271
just scale the model size.

06:18:33.271 --> 06:18:35.601
But on the other hand, if we have

06:18:35.921 --> 06:18:38.101
smaller model but with a better

06:18:38.161 --> 06:18:40.101
agent workflow and environment,

06:18:41.351 --> 06:18:43.511
it will be much easier for us

06:18:43.511 --> 06:18:45.541
to explore the the structure

06:18:45.681 --> 06:18:47.461
of the onlining agent task.

06:18:48.021 --> 06:18:49.841
This will be very useful for

06:18:50.271 --> 06:18:52.391
some coding coding task and

06:18:52.391 --> 06:18:53.952
some other relevant,

06:18:54.581 --> 06:18:55.081
scenarios.

06:19:00.351 --> 06:19:02.051
Thanks. Thanks for every

06:19:03.051 --> 06:19:05.231
panelist for sharing the insight about

06:19:05.751 --> 06:19:07.991
the current state of efficient listening and

06:19:07.991 --> 06:19:09.931
the future of efficient listening.

06:19:10.651 --> 06:19:12.511
Let's thank all our panelists

06:19:13.130 --> 06:19:15.310
for such a wonderful discussion with

06:19:15.310 --> 06:19:16.471
us. Yeah.

06:19:22.031 --> 06:19:24.251
Six. And, yeah,

06:19:24.251 --> 06:19:26.112
now we welcome our

06:19:26.311 --> 06:19:27.626
next speaker

06:19:28.542 --> 06:19:30.331
sorry. Yeah. Don't

06:19:30.551 --> 06:19:30.551
worry.

06:19:34.801 --> 06:19:36.881
Okay. Thanks thanks

06:19:36.941 --> 06:19:39.181
for speaking again, and let's welcome

06:19:39.421 --> 06:19:41.751
our next speaker, Hao

06:19:41.751 --> 06:19:43.611
Zhang. He's from UCSD

06:19:44.241 --> 06:19:46.421
He's a assistant professor at UCED.

06:19:47.122 --> 06:19:49.141
Mainly working on efficient

06:19:49.511 --> 06:19:51.391
listening and and he also

06:19:51.531 --> 06:19:53.661
is the inventor of war, and I will give

06:19:53.901 --> 06:19:56.056
my time to professor Hao Ta. For

06:19:56.056 --> 06:19:58.218
his speech. Okay. Thank you. Yeah. Know

06:19:58.218 --> 06:20:00.531
how to set up? Do I

06:20:46.731 --> 06:20:47.231
Okay?

06:20:57.712 --> 06:20:59.851
Okay. Yeah. Let's get started. Good

06:20:59.851 --> 06:21:02.031
afternoon, everyone. My name is Hong,

06:21:02.031 --> 06:21:04.271
and, I'm currently a faculty at the UC San

06:21:04.271 --> 06:21:06.630
Diego. Local here. I I hope you enjoy the

06:21:06.630 --> 06:21:08.821
weather. Okay? And, really

06:21:08.821 --> 06:21:11.001
excited to share our work on making

06:21:11.151 --> 06:21:13.171
large language models, specifically

06:21:13.311 --> 06:21:15.591
reasoning model, more efficient. Okay?

06:21:17.031 --> 06:21:19.331
So So, since the release of OpenAI

06:21:19.631 --> 06:21:22.111
o one, right, and virtually every

06:21:22.171 --> 06:21:24.291
major language model

06:21:24.291 --> 06:21:26.311
provider, including, like, OpenAI, interpreting,

06:21:26.371 --> 06:21:28.671
Google, They are

06:21:28.671 --> 06:21:30.911
kind of offering a reasonably enhanced language

06:21:30.911 --> 06:21:33.141
model. And compared to the typical,

06:21:33.381 --> 06:21:35.401
language model, Chapels, I think this model

06:21:35.962 --> 06:21:38.101
is extremely good at math. They

06:21:38.101 --> 06:21:40.601
are, like, achieving gold medal level,

06:21:40.900 --> 06:21:43.141
mathematical solving, and they

06:21:43.141 --> 06:21:44.601
are also literally powering

06:21:45.891 --> 06:21:48.051
the most widely called agents today. I think most

06:21:48.051 --> 06:21:50.551
of you, and including me, also use that cursor.

06:21:50.571 --> 06:21:51.761
Cloud code. Okay?

06:21:53.081 --> 06:21:55.050
So the key enabler for

06:21:55.271 --> 06:21:57.391
this is basically a so called test

06:21:57.391 --> 06:21:59.470
time screening law. Which means that you need to allocate

06:21:59.470 --> 06:22:01.321
the extra compute during inference

06:22:01.801 --> 06:22:03.431
in order to yield higher accuracy.

06:22:04.161 --> 06:22:06.241
And, and, roughly speaking, we

06:22:06.241 --> 06:22:08.181
converted into two ways of scaling,

06:22:08.501 --> 06:22:10.640
test and compute. The first way is

06:22:10.640 --> 06:22:12.651
basically channel software. Which

06:22:12.651 --> 06:22:14.641
basically generates, explicit

06:22:15.491 --> 06:22:17.651
longer reasoning steps. And the second way, which

06:22:17.651 --> 06:22:19.671
is also quite popular, is self contingency.

06:22:20.372 --> 06:22:22.532
That, you keep sampling one more accurate, and

06:22:22.532 --> 06:22:24.612
then you perform a majority voting to get the

06:22:24.612 --> 06:22:26.621
answer. While this

06:22:26.621 --> 06:22:28.400
approach is, improved accuracy,

06:22:28.881 --> 06:22:30.991
actually, both measures demand generating

06:22:30.991 --> 06:22:33.151
many, many tokens, which is a major

06:22:33.151 --> 06:22:35.391
bottleneck. Okay? And

06:22:35.391 --> 06:22:37.771
as you probably know, like Arm Inference

06:22:37.771 --> 06:22:40.071
is actually a big field, and and the

06:22:40.071 --> 06:22:42.221
reason that is actually I mean, inference is

06:22:42.542 --> 06:22:44.901
inherently very expensive. And slow.

06:22:45.141 --> 06:22:47.221
And, and reasoning models makes this

06:22:47.221 --> 06:22:49.321
problem, even worse. Because reasoning

06:22:49.321 --> 06:22:50.751
model need to generate more tokens.

06:22:51.551 --> 06:22:53.481
So here's an experiment we,

06:22:53.751 --> 06:22:55.991
We, we we basically performed on the mass

06:22:55.991 --> 06:22:58.631
dataset, compare a standard QIN 2.5

06:22:58.631 --> 06:23:00.461
model to its original variant.

06:23:00.721 --> 06:23:02.862
And, reasoning basically

06:23:02.911 --> 06:23:04.862
achieves higher accuracy. But,

06:23:04.942 --> 06:23:06.961
if you lock the accuracy at 80%,

06:23:07.601 --> 06:23:10.239
the standard model use 78%

06:23:10.239 --> 06:23:12.400
less tokens. And

06:23:12.400 --> 06:23:14.341
a similar problem can be observed in

06:23:14.581 --> 06:23:16.771
another paradigm, self consistency. So

06:23:16.771 --> 06:23:18.301
typically, when we use SC,

06:23:19.421 --> 06:23:21.202
the standard practice is to

06:23:21.581 --> 06:23:23.641
specify fixed number, say, 64 trajectories.

06:23:23.641 --> 06:23:25.721
And then you start using different random states to

06:23:25.721 --> 06:23:27.751
keep sampling. Until you finish it. Right?

06:23:27.751 --> 06:23:29.181
And then you do majority voting.

06:23:29.821 --> 06:23:31.911
But if you go inspect the

06:23:32.151 --> 06:23:33.881
those trajectories, you will find that

06:23:35.021 --> 06:23:37.181
two patterns. So for example, for hard

06:23:37.181 --> 06:23:40.181
problems, 85%

06:23:40.191 --> 06:23:42.361
of the traces are actually wasted.

06:23:42.841 --> 06:23:44.531
Some failed very early.

06:23:45.251 --> 06:23:47.491
And some derailed in the middle way. And

06:23:48.131 --> 06:23:50.292
and and or containing man minor errors,

06:23:50.292 --> 06:23:52.461
which not going to give you the correct answer.

06:23:53.021 --> 06:23:55.101
And for those easy problems, however,

06:23:55.101 --> 06:23:56.880
many solutions are actually redundant.

06:23:57.181 --> 06:23:59.341
Already get that in in your first directory. Okay?

06:23:59.661 --> 06:24:01.796
And this massive token consumption,

06:24:02.251 --> 06:24:04.331
redundant or low quality traces, is

06:24:04.331 --> 06:24:06.001
a also not quite sustainable.

06:24:08.161 --> 06:24:10.321
So basically, both COT and, SC

06:24:10.321 --> 06:24:12.273
actually waste a lot of tokens

06:24:12.571 --> 06:24:14.501
and trajectories. So so

06:24:15.001 --> 06:24:17.161
then here comes to our problem. Right?

06:24:17.161 --> 06:24:19.471
So want to do more efficient reasoning. Okay?

06:24:19.712 --> 06:24:22.141
The main theme of this workshop. And, ideally,

06:24:22.201 --> 06:24:24.381
we want to achieve two outcomes.

06:24:25.381 --> 06:24:27.542
First, we want to generate fewer tokens for

06:24:27.542 --> 06:24:29.691
the same accuracy. Right? That way, I'll lower

06:24:29.691 --> 06:24:32.071
our cost. Second, we want to achieve more accuracy

06:24:32.071 --> 06:24:34.281
if we have a larger budget of general auto tokens.

06:24:34.521 --> 06:24:36.731
Compared to the current status quo. K? And this

06:24:36.731 --> 06:24:38.651
is crucial because, if we can,

06:24:39.051 --> 06:24:40.911
improving this kind of reasoning efficiency

06:24:41.212 --> 06:24:43.221
actually directly reduces, both

06:24:43.221 --> 06:24:45.470
cost and latency. Which is a big part of

06:24:45.470 --> 06:24:47.891
that behind today's language model

06:24:47.891 --> 06:24:49.931
based applications. So the question is, can we

06:24:49.931 --> 06:24:52.091
achieve this? And, and

06:24:52.091 --> 06:24:54.171
the goal of this talk is that I want to give

06:24:54.171 --> 06:24:55.531
you a definitely yes answer

06:24:56.331 --> 06:24:58.771
And our idea in this talk is

06:24:58.876 --> 06:25:00.921
actually we will be able

06:25:00.921 --> 06:25:02.942
to use the model's internal certainty

06:25:03.001 --> 06:25:05.421
okay, to assist reasoning.

06:25:06.481 --> 06:25:08.560
I think in the literature, there are quite a lot of

06:25:08.560 --> 06:25:10.581
approaches make this more efficient. But in

06:25:10.581 --> 06:25:12.821
this talk, we are going to focus more on, uncertainty

06:25:12.821 --> 06:25:14.970
or confidence. So, the

06:25:14.970 --> 06:25:17.122
high level intuition is basically language

06:25:17.122 --> 06:25:19.192
model often knows what they know. Especially

06:25:19.192 --> 06:25:21.640
when they are confident. Okay? And we can try

06:25:21.640 --> 06:25:23.720
to kind of, like, quantify the signal and

06:25:23.720 --> 06:25:25.853
use that signal as a as

06:25:25.853 --> 06:25:28.131
a as a signal to to gather your austere in a reasoning.

06:25:29.331 --> 06:25:31.491
And, and here, we we try to extract

06:25:31.491 --> 06:25:33.819
that confidence signal at either

06:25:33.819 --> 06:25:35.991
or cross screen answer level or

06:25:35.991 --> 06:25:38.112
fine grained the token level. And this signal

06:25:38.112 --> 06:25:40.122
actually allows us to develop a mechanism

06:25:40.122 --> 06:25:42.271
that for example, like SIS

06:25:42.271 --> 06:25:44.701
to language model, you should stop here because

06:25:44.701 --> 06:25:46.721
you are confident. And it's

06:25:46.721 --> 06:25:48.792
unlikely you are changing your answer. Or, like, you

06:25:48.792 --> 06:25:50.872
are confident that you are not going to solve this problem. Why

06:25:50.872 --> 06:25:52.551
don't you stop here? Something like that. Okay?

06:25:53.991 --> 06:25:56.311
So, okay, the first project I want to

06:25:56.311 --> 06:25:58.362
introduce is basically a certain And this is

06:25:58.362 --> 06:26:00.261
a paper that we posted on our archive, actually,

06:26:00.401 --> 06:26:02.821
about almost one year ago, but it was recently accepted

06:26:03.202 --> 06:26:05.211
to to neuro loops and presented

06:26:05.211 --> 06:26:07.451
here. And the TLDR

06:26:07.451 --> 06:26:09.681
is basically we can use a coarse

06:26:09.681 --> 06:26:11.911
grain, the answer level certainty. Metric

06:26:11.911 --> 06:26:14.171
to trigger an early exit from the reasoning

06:26:14.390 --> 06:26:16.391
process. So let's

06:26:16.391 --> 06:26:18.641
take one step back. I to illustrate

06:26:18.781 --> 06:26:20.861
why we, generated so many tokens, we

06:26:20.861 --> 06:26:22.101
can look at this case study.

06:26:22.901 --> 06:26:25.221
For this given question, which is on the upper

06:26:25.221 --> 06:26:27.441
top upper right a

06:26:27.441 --> 06:26:29.681
long reasoning model outputs a correct answer

06:26:29.681 --> 06:26:31.451
in roughly 300 tokens.

06:26:32.042 --> 06:26:34.221
And the reasoning model would generate over

06:26:34.761 --> 06:26:36.341
one k tokens for the same problem,

06:26:36.981 --> 06:26:38.661
and the key observation here is

06:26:39.141 --> 06:26:41.462
the model gets the correct answer. I mean, for the original

06:26:41.462 --> 06:26:43.501
model, it gets the correct answer very

06:26:43.501 --> 06:26:45.192
early on. Which I marked.

06:26:45.921 --> 06:26:48.081
With a red red box But then it

06:26:48.081 --> 06:26:50.321
continues to double check self dot and

06:26:50.321 --> 06:26:52.532
re verify its thought. And it builds

06:26:52.532 --> 06:26:54.811
confidence until it eventually stops. Right?

06:26:55.131 --> 06:26:57.231
And this repeated ray analysis

06:26:57.631 --> 06:26:59.792
actually consumes a massive amount

06:26:59.792 --> 06:27:01.952
of unnatural tooling. And this is we

06:27:01.952 --> 06:27:03.491
believe, the source of the overheads.

06:27:04.372 --> 06:27:06.532
Okay? Want to,

06:27:06.532 --> 06:27:08.771
basically, point out that this original model's long narrative

06:27:08.771 --> 06:27:11.021
is not a backup because when we use to

06:27:11.021 --> 06:27:12.771
incentivize reasoning, we're all basically

06:27:13.241 --> 06:27:15.401
kind of incentivize the model to have this kind

06:27:15.401 --> 06:27:17.441
of behavior. Okay? But

06:27:17.501 --> 06:27:19.361
this is basically the model being comprehensive,

06:27:19.579 --> 06:27:21.782
trying to basically explore

06:27:21.843 --> 06:27:24.241
different solution path and eventually get a more

06:27:24.241 --> 06:27:26.511
common answer. But, here

06:27:26.511 --> 06:27:28.212
the idea is if we could detect

06:27:28.641 --> 06:27:30.891
the moment of early correctness we

06:27:30.891 --> 06:27:33.181
would save substantial compute at inference time.

06:27:34.941 --> 06:27:36.640
So how do we quantify this phenomenon?

06:27:37.821 --> 06:27:40.140
To quantify this kind of, like, self doubting,

06:27:40.140 --> 06:27:42.291
we develop a probing

06:27:42.291 --> 06:27:44.191
technique called a prop in the middle.

06:27:44.591 --> 06:27:46.591
So for the long reasoning trees, after

06:27:46.751 --> 06:27:48.891
for example, after every of your tokens say,

06:27:49.131 --> 06:27:51.141
after every 128

06:27:51.141 --> 06:27:53.221
tokens, we basically append a

06:27:53.221 --> 06:27:55.501
prop like, We just directly prompt

06:27:55.501 --> 06:27:57.631
it to say, what's your answer Okay. Could you

06:27:57.721 --> 06:27:59.901
answer now? And to force the model to reveal

06:27:59.901 --> 06:28:02.181
the current solution, And

06:28:02.181 --> 06:28:04.091
by inserting this kind of prob,

06:28:04.571 --> 06:28:06.811
small prompts, we force the model to always output

06:28:06.811 --> 06:28:08.911
the current answer no matter where the progress

06:28:08.911 --> 06:28:10.931
is currently at. And this allows

06:28:10.931 --> 06:28:13.071
us to accurately, track the reading path.

06:28:14.911 --> 06:28:17.001
And within, connect

06:28:17.141 --> 06:28:19.571
the model's intermediate answers and check them against

06:28:20.122 --> 06:28:22.151
ground truth. So here, I want to point out

06:28:22.151 --> 06:28:24.391
that this is an ideal Oracle driven

06:28:24.391 --> 06:28:26.541
test So in reality, when you do inference, you actually

06:28:26.541 --> 06:28:28.641
don't know the answer of your question. But,

06:28:28.721 --> 06:28:30.881
for some data set, we can do this kind of like a

06:28:30.881 --> 06:28:32.431
study. And we'll try to inspect the

06:28:33.241 --> 06:28:35.051
the port this phenomenon. Okay?

06:28:36.931 --> 06:28:39.011
So, basically, we we insert a lot of pro problems,

06:28:39.011 --> 06:28:41.041
and we try to connect the answer, and then we get all the answers.

06:28:41.121 --> 06:28:42.579
And we check all the answers against

06:28:43.281 --> 06:28:44.161
the ground truth.

06:28:46.241 --> 06:28:47.141
And this experiment,

06:28:48.661 --> 06:28:50.821
actually revealed the results. So I will spend a

06:28:50.821 --> 06:28:53.151
little bit more time on this. So this experiment

06:28:53.451 --> 06:28:54.391
took two minutes to decide.

06:28:55.591 --> 06:28:57.681
Against the the DeepSig r one, our very famous reasoning

06:28:57.681 --> 06:28:59.452
model. And for the baseline,

06:28:59.831 --> 06:29:01.251
the model basically runs

06:29:02.532 --> 06:29:04.692
naturally to finish. We just make it run. We never

06:29:04.692 --> 06:29:06.881
interrupt it. Okay? And for probing,

06:29:07.101 --> 06:29:09.261
we basically interrupt every one twenty eight

06:29:09.261 --> 06:29:11.291
tokens, as I said. And we connect

06:29:11.291 --> 06:29:13.691
the answer and check for correctness, and then we terminate

06:29:13.691 --> 06:29:15.441
it. If the answer is correct,

06:29:15.841 --> 06:29:17.970
Okay? And we record we also record the

06:29:17.970 --> 06:29:20.101
tokens used for both baseline. Lines.

06:29:20.241 --> 06:29:22.511
So let me explain these two figures. So the

06:29:22.511 --> 06:29:24.831
x axis here is basically a number of tokens

06:29:24.831 --> 06:29:26.909
we spend on particular

06:29:26.909 --> 06:29:29.050
question. And the y axis is

06:29:29.050 --> 06:29:31.261
the question ID. And the color of

06:29:31.261 --> 06:29:33.281
each cell, represents how

06:29:33.281 --> 06:29:35.311
many runs. The

06:29:35.311 --> 06:29:37.411
correct result up to how many

06:29:37.411 --> 06:29:39.571
runs has the correct result up to this point?

06:29:39.731 --> 06:29:41.737
For each question, we basically run 10 times in

06:29:41.737 --> 06:29:43.561
order to get a some

06:29:43.941 --> 06:29:46.023
statistical significance. Okay? And the

06:29:46.023 --> 06:29:48.103
color red basically means that all rents

06:29:48.103 --> 06:29:50.161
are wrong. And the the green means

06:29:50.161 --> 06:29:51.941
turn auto turn on correct. And you can

06:29:52.181 --> 06:29:54.211
actually figure out color spectrum.

06:29:55.409 --> 06:29:57.569
And if you compare the left and right one, the left

06:29:57.569 --> 06:29:59.712
is baseline. The right is basically the

06:29:59.712 --> 06:30:01.773
one that we insert all the props. So

06:30:01.773 --> 06:30:03.771
the result is quite striking. Right? So

06:30:04.011 --> 06:30:05.951
the baseline model actually spend

06:30:06.091 --> 06:30:08.271
a medium 2.7

06:30:08.271 --> 06:30:10.421
k token. To get to correctness. And

06:30:10.421 --> 06:30:12.341
the property manager has spent only

06:30:12.821 --> 06:30:15.069
800 tokens. Okay. That is a three

06:30:15.069 --> 06:30:17.462
x gap. Okay? Which means

06:30:17.462 --> 06:30:19.741
three x latency reduction. Right? And

06:30:19.801 --> 06:30:22.041
and this clearly shows that, the reasoning

06:30:22.041 --> 06:30:24.481
model can arrive at the correct answer. Much

06:30:25.021 --> 06:30:26.961
earlier than the default

06:30:27.301 --> 06:30:29.481
termination point. Okay? And

06:30:29.481 --> 06:30:31.261
this actually motivates the need

06:30:32.103 --> 06:30:34.171
for a very automatic and practical

06:30:34.571 --> 06:30:36.811
certainty based early exiting mechanism,

06:30:36.811 --> 06:30:38.721
which I'm going to develop next.

06:30:40.101 --> 06:30:42.310
Okay. So now,

06:30:42.310 --> 06:30:44.470
the practical challenge is, how do

06:30:44.470 --> 06:30:46.661
we apply this reality

06:30:47.201 --> 06:30:49.281
This in reality? Because, like I said,

06:30:49.441 --> 06:30:51.431
the previous experiment that we use Oracle,

06:30:52.071 --> 06:30:54.191
but, in practice, we don't have the granules.

06:30:54.271 --> 06:30:56.521
Tell us when to stop, because we are not able to check our answer

06:30:56.521 --> 06:30:58.550
correctness. Right? So we need some

06:30:58.550 --> 06:31:00.711
kind of signal that other than checking against

06:31:00.711 --> 06:31:02.341
the ground truth, Okay?

06:31:02.971 --> 06:31:05.131
And it turns out that, this

06:31:05.131 --> 06:31:07.151
is where the certainty model certainty,

06:31:07.151 --> 06:31:09.191
becomes useful. Okay? And we can

06:31:09.191 --> 06:31:11.212
basically measure certainty of a sliding

06:31:11.271 --> 06:31:13.071
window of the red

06:31:13.371 --> 06:31:15.041
linear trajectory. So here, we define,

06:31:15.421 --> 06:31:17.601
three concepts here. The first one is

06:31:17.601 --> 06:31:19.603
the search, certainly window w. Which

06:31:19.603 --> 06:31:22.023
is the last w answers we

06:31:22.231 --> 06:31:24.542
prob and collect to evaluate uncertainty.

06:31:25.292 --> 06:31:27.437
Here, apparently, the w equal to

06:31:27.437 --> 06:31:29.581
three. The second metric is

06:31:29.581 --> 06:31:31.571
entropy term, entropy symbol, h.

06:31:32.452 --> 06:31:34.851
Which is roughly measures your certainty.

06:31:34.851 --> 06:31:37.031
Right? So which defines how we calculate the certainty.

06:31:37.031 --> 06:31:39.111
Okay? Here we define it as

06:31:39.111 --> 06:31:41.161
a number of answers that is the same to the

06:31:41.161 --> 06:31:43.371
last. Like, how many times the answer

06:31:43.571 --> 06:31:45.161
the same with the last answer? Okay?

06:31:47.171 --> 06:31:49.251
And, and this is measuring the window.

06:31:49.251 --> 06:31:51.380
So has to be normalized by the window size.

06:31:51.380 --> 06:31:53.521
So the value is always from zero to one.

06:31:53.921 --> 06:31:56.121
Okay? And finally, we have, like,

06:31:56.121 --> 06:31:58.201
a configured certainty threshold.

06:31:58.501 --> 06:32:00.581
To determine when to early access it. For example, if your

06:32:00.581 --> 06:32:02.581
certainty is greater than the threshold, then you'll

06:32:02.981 --> 06:32:04.541
Okay. Very, very simple.

06:32:05.341 --> 06:32:07.739
So let's work through a very, very quick example

06:32:07.739 --> 06:32:09.792
with Windows as a three and

06:32:09.792 --> 06:32:11.961
a threshold of one. Means that you check

06:32:11.961 --> 06:32:14.361
the last three answers, and all these rounds are need to be equal,

06:32:14.361 --> 06:32:16.441
then you exit. Right? So

06:32:16.441 --> 06:32:18.701
at the start, the model answers are inconsistency.

06:32:19.641 --> 06:32:21.651
Inconsistent. So the

06:32:21.651 --> 06:32:23.721
last answer is forty forty two. As you

06:32:23.721 --> 06:32:25.911
can see. So the uncertainty measure is basically one

06:32:25.911 --> 06:32:28.081
third. Okay? And it's below threshold one.

06:32:28.641 --> 06:32:30.571
Okay? Which means that

06:32:30.741 --> 06:32:32.931
it's not searching yet. And, therefore,

06:32:32.931 --> 06:32:34.791
we ask a model to continue. Okay?

06:32:35.431 --> 06:32:35.931
Okay?

06:32:37.841 --> 06:32:40.081
Okay? And then it continues. And after

06:32:40.081 --> 06:32:42.101
another 28 tokens, it generates an

06:32:42.101 --> 06:32:44.181
answer, 42. We check again.

06:32:44.181 --> 06:32:46.251
Right? We connect two answers, which is the same with

06:32:46.251 --> 06:32:48.501
the last But the certainty is two third.

06:32:48.821 --> 06:32:50.861
It's still smaller than the threshold one. Okay?

06:32:51.021 --> 06:32:52.941
Therefore, we generate another one twenty eight tokens.

06:32:55.181 --> 06:32:57.311
Okay? And finally, in the last

06:32:57.311 --> 06:32:58.281
step, see the model

06:32:59.319 --> 06:33:01.751
answer 42. Then we basically

06:33:01.811 --> 06:33:04.211
think about the it actually matches our threshold.

06:33:04.341 --> 06:33:06.622
It actually exists. We stop

06:33:06.622 --> 06:33:08.683
it. Okay? Is a very, very

06:33:08.683 --> 06:33:10.711
easy problem and already active mechanism.

06:33:15.431 --> 06:33:17.261
So why does certainty work in this case?

06:33:18.141 --> 06:33:19.991
We can dive deeper a little bit here. Okay?

06:33:20.151 --> 06:33:22.171
So in this figure, we plot the creation.

06:33:22.291 --> 06:33:23.771
Between steps

06:33:24.411 --> 06:33:25.811
to yield a correct answer.

06:33:26.692 --> 06:33:29.051
And the answer network certainty. So

06:33:29.051 --> 06:33:30.841
the x is basically the

06:33:31.167 --> 06:33:33.431
steps Steps is is basically

06:33:33.431 --> 06:33:35.471
proportional to the compute. Right? To some compute.

06:33:35.792 --> 06:33:37.331
And the y axis is basically,

06:33:38.032 --> 06:33:40.171
like, after you're going this many steps,

06:33:40.171 --> 06:33:42.183
what's your current So you can

06:33:42.183 --> 06:33:44.683
see that, this scatter plot is roughly

06:33:44.911 --> 06:33:47.282
l shaped. And this is what we want,

06:33:47.341 --> 06:33:49.230
because that means when certainty

06:33:49.851 --> 06:33:51.861
is high, the number of compute needed

06:33:51.861 --> 06:33:53.831
to get a correct answer is is low.

06:33:54.231 --> 06:33:56.361
Right? And it means it means

06:33:56.361 --> 06:33:58.201
that we will get to a certain answer very soon.

06:33:59.081 --> 06:34:01.471
Okay? And also, when the certainty is low, we generally

06:34:01.471 --> 06:34:03.191
require more more steps to

06:34:03.511 --> 06:34:05.631
to to get an answer. Okay? And I look

06:34:05.631 --> 06:34:07.853
at more compute, we are likely improve

06:34:07.853 --> 06:34:09.921
the answer for these questions. Okay? And this plot

06:34:09.921 --> 06:34:12.231
actually explains the kind of statistical

06:34:12.531 --> 06:34:13.331
signal behind this.

06:34:15.251 --> 06:34:17.491
And we actually applied this certainty based

06:34:17.491 --> 06:34:19.271
determination to quite a few,

06:34:19.630 --> 06:34:21.810
reasoning models, and the result is quite promising.

06:34:22.452 --> 06:34:24.721
We consistently achieved from

06:34:24.721 --> 06:34:25.781
12% to

06:34:28.021 --> 06:34:30.341
our setups, so we're maintaining the same accuracy.

06:34:30.661 --> 06:34:32.691
Right? And this demonstrates the power

06:34:32.691 --> 06:34:34.831
of this certainty. To improve

06:34:35.372 --> 06:34:37.551
efficiency without actually compressing performance.

06:34:37.831 --> 06:34:40.311
And this approach is quite a welcoming

06:34:40.372 --> 06:34:42.501
because you don't have to train your Okay? It's an

06:34:42.501 --> 06:34:43.541
inference time approach.

06:34:46.261 --> 06:34:48.531
So for so far, we have focused on Chiang Salt.

06:34:48.771 --> 06:34:50.241
But many other

06:34:50.981 --> 06:34:53.131
more sophisticated reasoning or test

06:34:53.131 --> 06:34:55.081
time, skimming method exists. Right?

06:34:55.481 --> 06:34:57.801
One, famous one is the one I mentioned, the self

06:34:57.801 --> 06:35:00.111
consistency. Which samples

06:35:00.171 --> 06:35:02.471
multiple trajectories. And

06:35:02.471 --> 06:35:04.671
there are also some other one, like tree

06:35:04.671 --> 06:35:06.451
based m MCTS. They are slightly

06:35:07.251 --> 06:35:09.411
not that popular today, but IBD will

06:35:09.411 --> 06:35:11.341
reemerge again. Okay?

06:35:11.501 --> 06:35:13.581
And all these advanced paradigms still

06:35:13.581 --> 06:35:15.261
rely on, increasing the

06:35:15.741 --> 06:35:16.891
test time compute to improve accuracy.

06:35:18.720 --> 06:35:21.060
So the good news is the certain

06:35:21.612 --> 06:35:23.031
measure I just defined is

06:35:23.831 --> 06:35:26.271
actually broadly applicable it is agnostic

06:35:26.411 --> 06:35:28.521
to reasoning algorithms. Because as long as you

06:35:28.521 --> 06:35:30.621
are able to probe your customer,

06:35:30.621 --> 06:35:32.741
are able to get an answer. And as long as you can

06:35:32.741 --> 06:35:34.900
get answers, you can basically evaluate the answer

06:35:34.900 --> 06:35:36.601
network certainty. Right?

06:35:37.071 --> 06:35:39.231
So we can generalize our answer network

06:35:39.231 --> 06:35:41.141
certainty to apply across all these

06:35:41.622 --> 06:35:43.721
programs. And for self consistency,

06:35:43.862 --> 06:35:45.880
it will be just like you extract the

06:35:45.880 --> 06:35:47.021
answer for each directory.

06:35:47.891 --> 06:35:50.141
Before you spawn more more directories. Right? And

06:35:50.141 --> 06:35:52.451
you measure your certainty and and try to say if

06:35:52.691 --> 06:35:54.731
it exists your threshold. And for

06:35:54.731 --> 06:35:56.901
MCTS, depends on the algorithm

06:35:56.901 --> 06:35:58.980
you use. But for some MCTS, you also can

06:35:58.980 --> 06:36:01.091
extract answers. But if for those, you

06:36:01.091 --> 06:36:03.202
cannot extract answers, you cannot do

06:36:03.202 --> 06:36:05.702
prob. You can use a reward score, because in many MCTs,

06:36:06.011 --> 06:36:07.861
algorithm, you have a reward model associated.

06:36:08.900 --> 06:36:10.931
Okay? So here, the keynote

06:36:10.931 --> 06:36:12.041
I want to give is basically

06:36:13.159 --> 06:36:15.319
that this certainty actually serves as a

06:36:15.319 --> 06:36:17.021
unified algorithm

06:36:17.640 --> 06:36:19.971
agnostic measure to measure this answer

06:36:19.971 --> 06:36:21.541
confidence. And this answer confidence

06:36:22.181 --> 06:36:24.281
directly correlates with the step to correctness.

06:36:24.281 --> 06:36:26.421
Which can be used to reduce token consumption.

06:36:30.651 --> 06:36:32.811
Okay. And, I talk about

06:36:32.811 --> 06:36:34.751
how how you perform this kind of

06:36:36.211 --> 06:36:38.291
kind of, like, acceleration for single reasoning,

06:36:38.531 --> 06:36:40.641
request. Right? But I think that in practice,

06:36:41.021 --> 06:36:43.091
many of the time you deal with, like, a serving systems, like

06:36:43.091 --> 06:36:45.321
a BOM and kind of system. You have

06:36:45.321 --> 06:36:47.471
to serve a lot of requests. You put

06:36:47.471 --> 06:36:49.612
your model on top of that. So we

06:36:49.612 --> 06:36:51.691
can actually generate generate this answer and have

06:36:51.691 --> 06:36:53.741
a certainty from a

06:36:54.021 --> 06:36:56.101
basically, a single request to many more many

06:36:56.101 --> 06:36:58.441
requests and generate it from a single request inference

06:36:58.661 --> 06:37:00.921
to serving. So this is

06:37:00.921 --> 06:37:02.931
this is what we did. This is a

06:37:02.931 --> 06:37:05.069
certain DAX, because we want to generalize this into different

06:37:05.069 --> 06:37:07.161
algorithms. And multiple requests.

06:37:07.561 --> 06:37:09.291
So this sorting acts basically

06:37:09.611 --> 06:37:11.691
its power actually extends beyond a

06:37:11.691 --> 06:37:14.021
single request to language model serving systems.

06:37:14.421 --> 06:37:16.771
So consider, you know, a batch request. Right?

06:37:17.011 --> 06:37:18.951
You you are given a batch of written queries.

06:37:19.471 --> 06:37:21.651
And and these certain tags can be used for dynamic

06:37:21.792 --> 06:37:23.331
token allocation across requests.

06:37:24.051 --> 06:37:26.212
So now please consider a language model

06:37:26.212 --> 06:37:28.701
serving use case where you to generate tokens

06:37:28.861 --> 06:37:31.171
for for a request I illustrated

06:37:31.550 --> 06:37:33.761
here, p one, p two, p three, p four. And,

06:37:33.761 --> 06:37:35.841
typically, we would set a maximum token

06:37:35.841 --> 06:37:38.161
limit for each query. Right? That's that's some something

06:37:38.161 --> 06:37:40.261
we always do in streaming systems. And

06:37:40.261 --> 06:37:42.421
also consider that, assuming this p one,

06:37:42.421 --> 06:37:44.231
p two, p three, p four, they have different difficulty.

06:37:44.712 --> 06:37:46.801
So p one is a is a request. P

06:37:46.801 --> 06:37:48.431
two is a is an okay request.

06:37:48.861 --> 06:37:51.021
P three is a a little bit difficult,

06:37:51.021 --> 06:37:53.121
but if you spend more tokens, maybe

06:37:53.121 --> 06:37:55.321
the model will survive. But the

06:37:55.621 --> 06:37:57.640
p four essentially lies at the impossible

06:37:57.861 --> 06:37:59.931
region which is beyond this language

06:37:59.931 --> 06:38:00.971
model's capability.

06:38:02.331 --> 06:38:04.561
So so for this easy question p one,

06:38:04.801 --> 06:38:07.042
we if we have a certainty, right, we can early

06:38:07.042 --> 06:38:08.581
terminate and reallocate

06:38:09.962 --> 06:38:12.136
the unused tokens. To a

06:38:12.136 --> 06:38:13.651
harder request p three.

06:38:14.212 --> 06:38:16.411
Okay? Might need to need more compute.

06:38:16.661 --> 06:38:18.821
In your original allocation, like I like I

06:38:18.821 --> 06:38:20.731
said, shared in this part,

06:38:20.971 --> 06:38:23.051
you would allocate a equal amount of tokens for each

06:38:23.051 --> 06:38:25.351
request. And, therefore, you use more tokens

06:38:25.351 --> 06:38:27.501
for p one. You you are able to sort p one

06:38:27.501 --> 06:38:29.659
p two by you you just don't have enough token for

06:38:29.659 --> 06:38:31.701
storing p three. Right? And same

06:38:31.701 --> 06:38:33.901
thing. Sometimes when the language when

06:38:33.901 --> 06:38:36.061
the language model is confident, it is also

06:38:36.061 --> 06:38:37.871
confident in no. That means

06:38:38.111 --> 06:38:40.191
like, you you measure its confidence. The answer is

06:38:40.191 --> 06:38:42.251
always wrong, but the confidence is means the language

06:38:42.251 --> 06:38:44.331
model is trying to demonstrate a signal that I'm not

06:38:44.331 --> 06:38:46.470
able to solve that question. And for this kind

06:38:46.470 --> 06:38:48.361
of thing, you can also calculate it because

06:38:48.601 --> 06:38:50.841
if the modeling model already show a signal that it can answer

06:38:50.841 --> 06:38:51.761
this question, why don't

06:38:53.591 --> 06:38:55.921
Right? That's for p four. Okay? You can reallocate

06:38:55.921 --> 06:38:57.942
this token for p four. Back to

06:38:57.942 --> 06:38:59.971
p three, and that will that will help you actually

06:39:00.131 --> 06:39:01.561
solve it under query. Right?

06:39:02.202 --> 06:39:04.452
Increase accuracy. It's a more efficient

06:39:04.671 --> 06:39:06.941
token usage. Okay. This gives

06:39:06.941 --> 06:39:09.021
you a high high level idea of how you can incorporate

06:39:09.021 --> 06:39:11.201
this kind of, like, certain signal in

06:39:11.201 --> 06:39:13.231
a more complicated, serving pieces.

06:39:14.911 --> 06:39:17.231
So our evaluation, in real serving

06:39:17.231 --> 06:39:19.321
engine comes these benefits. So for

06:39:19.321 --> 06:39:21.343
batch processing, we achieve

06:39:21.343 --> 06:39:23.421
up to 50% of computer

06:39:23.421 --> 06:39:25.122
saving on on some, like, datasets.

06:39:25.591 --> 06:39:27.671
Including the famous Mac mass metal data

06:39:27.671 --> 06:39:29.191
sets. And for other serving,

06:39:29.811 --> 06:39:31.881
using, consistency, we

06:39:31.881 --> 06:39:33.821
can achieve up to 1.6 higher

06:39:34.771 --> 06:39:36.941
like, program rate, per per

06:39:37.980 --> 06:39:40.061
second. Is your throughput. Okay? Compared

06:39:40.181 --> 06:39:42.661
to baseline. You don't do dynamic

06:39:42.880 --> 06:39:45.161
token allocation. Okay.

06:39:45.161 --> 06:39:47.321
If you need to know more details, feel free to check

06:39:47.321 --> 06:39:49.361
our paper. And I hope I convinced

06:39:49.361 --> 06:39:51.381
you that this reasoning model

06:39:51.381 --> 06:39:53.411
and reasoning program actually weighs level tokens.

06:39:53.411 --> 06:39:55.641
And we can do much better. And one possible

06:39:55.641 --> 06:39:57.681
way I I try to allocate here is basically

06:39:58.241 --> 06:39:59.941
using this cross screen

06:40:00.501 --> 06:40:02.821
answer level certainty. Okay. And in fact,

06:40:02.821 --> 06:40:05.221
we are able to push this, answer level certainty

06:40:05.221 --> 06:40:07.720
into one of the leading serving engine,

06:40:08.181 --> 06:40:10.331
TRT Arm by NVIDIA. And if you

06:40:10.331 --> 06:40:12.411
go check their code, there is already some code that

06:40:12.411 --> 06:40:13.341
we committed Okay?

06:40:15.421 --> 06:40:17.493
Okay. That that basically wraps up my first part

06:40:17.493 --> 06:40:19.591
of the talk. And I want to turn to the

06:40:19.591 --> 06:40:21.701
second part, So deep sync with confidence,

06:40:21.701 --> 06:40:23.321
and this is actually a collaboration

06:40:24.071 --> 06:40:25.751
with our panelists here, Yuan Dong, and,

06:40:26.901 --> 06:40:27.901
led by my student.

06:40:29.421 --> 06:40:31.291
And Deepgram is an improvement

06:40:32.511 --> 06:40:33.966
on self consistency.

06:40:34.601 --> 06:40:36.220
Okay? And, it actually leveraged

06:40:36.841 --> 06:40:38.909
Fang Green model level of

06:40:38.909 --> 06:40:41.150
certainty, token level of certainty, to address the massive

06:40:41.150 --> 06:40:43.230
efficiency or parallel thinking,

06:40:43.390 --> 06:40:45.271
with generating hundreds of tokens.

06:40:46.310 --> 06:40:47.701
So So So,

06:40:48.400 --> 06:40:50.421
so in the first part of my talk, I already

06:40:51.282 --> 06:40:53.362
defined one kind of competence, which is

06:40:53.362 --> 06:40:55.411
the entrepreneur We try to extract all the answers,

06:40:55.411 --> 06:40:57.571
and we try to calculate sort of an entropy

06:40:57.951 --> 06:41:00.021
across answers. But I would say that

06:41:00.021 --> 06:41:02.001
certainly is a coarse grain level, because you'll

06:41:02.161 --> 06:41:03.791
have to get answers in order to calculate uncertainty.

06:41:06.405 --> 06:41:08.763
But sometimes

06:41:09.381 --> 06:41:11.611
some application especially in online generation,

06:41:11.671 --> 06:41:13.891
you want to know something like a very fine grained

06:41:14.292 --> 06:41:16.532
signal in order to stop or early access

06:41:16.532 --> 06:41:18.560
a token level. And, and the

06:41:18.560 --> 06:41:20.640
next, we are going to basically develop a more

06:41:20.640 --> 06:41:22.771
fine grained certainty. From the

06:41:22.771 --> 06:41:25.101
model internal itself. So So

06:41:25.101 --> 06:41:27.261
besides this, coarse grain, the answer to

06:41:27.261 --> 06:41:29.400
our uncertainty, in fact, the language

06:41:29.400 --> 06:41:31.341
model also signals their certainty

06:41:32.001 --> 06:41:34.031
at every step. Right? Every time when

06:41:34.031 --> 06:41:35.511
a general token, they are signaled

06:41:36.071 --> 06:41:38.231
presented. Why? Language model is doing

06:41:38.231 --> 06:41:40.351
next token prediction. And before you sample the token,

06:41:40.351 --> 06:41:41.651
you actually first

06:41:42.521 --> 06:41:44.712
across the vocabulary. And that distribution

06:41:45.282 --> 06:41:47.452
is where you can actually extract a token out of

06:41:47.452 --> 06:41:48.801
certainty. Okay?

06:41:49.921 --> 06:41:52.161
So, basically, a sharp distribution means

06:41:52.161 --> 06:41:54.441
that the model is highly confident about

06:41:54.441 --> 06:41:56.551
that token. Okay? And

06:41:56.551 --> 06:41:58.611
it indicates that you already

06:41:58.611 --> 06:42:00.431
maybe cracked a step. Okay?

06:42:00.671 --> 06:42:02.911
A flat distribution is all that. The model is

06:42:02.911 --> 06:42:05.151
fine to output any token. Right? Many token

06:42:05.151 --> 06:42:06.371
have equal probability.

06:42:07.181 --> 06:42:09.031
Therefore, the model is less certain.

06:42:09.431 --> 06:42:11.511
Which often corresponds to reasoning hesitation

06:42:11.511 --> 06:42:13.811
or a potential error. K?

06:42:14.051 --> 06:42:16.292
And we basically try to quantify this using

06:42:16.292 --> 06:42:18.351
token network metrics, like entropy or

06:42:18.351 --> 06:42:20.161
more effectively. Token confidence.

06:42:21.241 --> 06:42:23.470
Which is basically the average log probability of

06:42:23.470 --> 06:42:25.651
of of that. Distribution.

06:42:25.651 --> 06:42:27.081
Okay? At that token position.

06:42:28.841 --> 06:42:31.081
So so how can we develop this kind of token,

06:42:31.081 --> 06:42:33.452
our confidence to help with

06:42:33.831 --> 06:42:35.921
parallel thinking? So we need to first

06:42:36.220 --> 06:42:38.650
develop a a develop a certainty

06:42:38.711 --> 06:42:40.841
metric to tell the confidence of a

06:42:40.841 --> 06:42:42.861
trees inside of a token. Right? We only have the confidence

06:42:42.861 --> 06:42:44.971
of token. We want to generate that into a tree.

06:42:45.612 --> 06:42:47.591
So So to get a quality

06:42:47.811 --> 06:42:50.081
score, for the entire tree, the simplest

06:42:50.081 --> 06:42:52.401
method is basically average trace confidence.

06:42:52.641 --> 06:42:54.800
Okay? Which I define here as the

06:42:54.800 --> 06:42:57.300
main of all token level confidence values,

06:42:57.811 --> 06:42:59.891
actually, this definition has been used in a

06:42:59.891 --> 06:43:02.291
lot of papers published

06:43:02.291 --> 06:43:04.311
earlier. And, people have have been

06:43:04.311 --> 06:43:06.575
finding this quite useful. And

06:43:06.781 --> 06:43:08.941
this is the faculty, but it also has a

06:43:08.941 --> 06:43:10.782
critical limitation. That is,

06:43:11.023 --> 06:43:13.261
it is basically a global averaging of the confidence

06:43:13.261 --> 06:43:15.641
across all tokens in the trees. Which

06:43:15.641 --> 06:43:17.720
means that it is quite prone to

06:43:17.720 --> 06:43:19.771
local failures. Sometimes, the language model

06:43:19.771 --> 06:43:21.791
can go back and forth, and you actually get the answer.

06:43:21.791 --> 06:43:24.220
If you average the confidence occurs entirely accurate,

06:43:24.411 --> 06:43:26.452
it's likely like it's done the align

06:43:26.452 --> 06:43:28.281
with the final answers to confidence.

06:43:28.521 --> 06:43:30.541
Okay? So a single critical

06:43:30.601 --> 06:43:32.461
step with low confidence can actually

06:43:33.051 --> 06:43:34.751
can be diluted and missed when

06:43:35.311 --> 06:43:37.331
average over a very long trees.

06:43:37.491 --> 06:43:39.731
And this motivated with the need for a more granular

06:43:39.731 --> 06:43:40.231
approach.

06:43:42.121 --> 06:43:44.281
So basically, this work, Deepakanth

06:43:44.281 --> 06:43:46.470
introduced a new metric to, better

06:43:46.470 --> 06:43:48.541
capture local

06:43:48.541 --> 06:43:50.720
reasoning behavior. Basically, we use a so called a

06:43:50.720 --> 06:43:53.011
group confidence is a sliding window

06:43:53.011 --> 06:43:54.981
average over nearby tokens? And,

06:43:55.462 --> 06:43:57.541
to smoothify the noise and track

06:43:57.541 --> 06:43:58.761
intermediate step confidence.

06:43:59.611 --> 06:44:01.691
And here, we can even generalize, the

06:44:01.691 --> 06:44:04.091
group confidence to something like a bottom 10%,

06:44:04.091 --> 06:44:06.489
group confidence. Which basically computes

06:44:06.550 --> 06:44:08.791
the average of the lowest scoring confidence

06:44:08.791 --> 06:44:11.031
groups. Okay? And it captures the

06:44:11.031 --> 06:44:13.230
weak points that

06:44:13.230 --> 06:44:15.281
a global that a global average misses.

06:44:15.601 --> 06:44:17.681
And and this load is the group of confidence that

06:44:17.681 --> 06:44:19.792
uses the absolute minimum value. Which

06:44:19.853 --> 06:44:21.999
is particularly useful sometimes in

06:44:22.140 --> 06:44:24.211
certain applications. We can

06:44:24.211 --> 06:44:26.291
also use something like basically, we

06:44:26.291 --> 06:44:28.441
have modified like a trajectory algorithm to

06:44:28.441 --> 06:44:30.681
to basically make it capture a lot of

06:44:30.681 --> 06:44:32.831
locality. We can use a tail confidence,

06:44:32.831 --> 06:44:35.091
which basically focus only on the last few tokens.

06:44:35.571 --> 06:44:37.591
And this local patterns are

06:44:37.591 --> 06:44:39.611
far more effective than global

06:44:39.611 --> 06:44:41.771
average across entire trees. Okay?

06:44:41.771 --> 06:44:43.851
So why I introduced so many terms? Because,

06:44:44.091 --> 06:44:45.671
this slide actually explain why.

06:44:46.151 --> 06:44:48.091
So we can also do a very,

06:44:49.111 --> 06:44:51.341
straightforward correlation plot. Right? So

06:44:51.341 --> 06:44:53.501
this figure virtually confirms, the

06:44:53.501 --> 06:44:55.651
improvement of this new definition of metrics.

06:44:57.011 --> 06:44:58.791
So the mean confidence is a left figure.

06:44:59.191 --> 06:45:01.491
Which is basically global averaging, the token level

06:45:01.491 --> 06:45:03.271
confidence across all tokens in the directory.

06:45:04.701 --> 06:45:06.380
And either basically shows a noisy,

06:45:06.781 --> 06:45:08.862
overlap distribution for correct

06:45:09.301 --> 06:45:11.601
incorrect. Which is green and

06:45:12.175 --> 06:45:14.351
orange. Traces. So here, again,

06:45:14.351 --> 06:45:16.378
we are doing an oracle study. That

06:45:16.378 --> 06:45:18.532
is, we know the answer. And we

06:45:18.532 --> 06:45:20.611
try to classify answer

06:45:20.611 --> 06:45:23.031
into correct and incorrect and try to study speculation.

06:45:23.671 --> 06:45:26.171
Between the actual correctness and the and the confidence

06:45:26.390 --> 06:45:28.441
value. Okay? And and in in

06:45:28.441 --> 06:45:30.681
contrast, if you compare the middle figure to the left

06:45:30.681 --> 06:45:32.801
figure, this bottom 10% of

06:45:32.801 --> 06:45:34.891
confidence is much better. Right? At

06:45:34.891 --> 06:45:37.221
least, like, the first impression is either separates

06:45:37.361 --> 06:45:39.521
the orange and green much better than the left.

06:45:39.841 --> 06:45:41.871
Than the first figure. This is why this

06:45:41.871 --> 06:45:43.751
is why we need to refine this metric. Okay?

06:45:43.991 --> 06:45:46.007
Because our end goal is basically we try

06:45:46.007 --> 06:45:48.161
to map this confidence

06:45:48.220 --> 06:45:49.981
to the uncertain crime means. And we try to use

06:45:50.221 --> 06:45:52.221
this as a signal to help us identify those

06:45:52.782 --> 06:45:54.931
per missing traces that would yield a correct answer.

06:45:57.239 --> 06:45:58.861
Okay? So, so

06:45:59.401 --> 06:46:01.801
then, that that basically

06:46:01.801 --> 06:46:04.281
gives us a few definition about this onshore network trace

06:46:04.281 --> 06:46:06.291
level certainty. So you are

06:46:06.291 --> 06:46:08.532
probably wondering why do we

06:46:08.532 --> 06:46:10.741
care about developing another fine grain of confidence

06:46:10.741 --> 06:46:12.801
at a trajectory error, Here, I

06:46:12.801 --> 06:46:14.401
want to provide a few argument.

06:46:14.881 --> 06:46:17.042
So fine grained uncertainty actually enables two

06:46:17.042 --> 06:46:19.201
key improvements. First, we can do

06:46:19.201 --> 06:46:21.461
offline aggregation. We can use the confidence

06:46:21.461 --> 06:46:23.501
as weights. For voting

06:46:24.491 --> 06:46:26.651
to to basically outperform a

06:46:26.651 --> 06:46:28.890
standard consistency where you don't have a waste of waste,

06:46:29.091 --> 06:46:31.381
have greater majority voting. Right? So every battery is

06:46:31.381 --> 06:46:33.523
equal weight. Okay? Secondly, we

06:46:33.523 --> 06:46:35.671
can also use this, like, a trace level certainty.

06:46:35.991 --> 06:46:38.263
Which is calculated online. To do online

06:46:38.263 --> 06:46:40.191
steering of the region. Okay?

06:46:40.391 --> 06:46:42.792
So we can continuously monitor the competence

06:46:42.792 --> 06:46:45.031
during, generation, Early

06:46:45.031 --> 06:46:47.171
stop those low competence increases, and

06:46:47.171 --> 06:46:49.411
then reallocated the computer, budget to more

06:46:49.411 --> 06:46:51.521
promise increases. This is

06:46:51.579 --> 06:46:53.741
calculated and maintained at a token

06:46:53.741 --> 06:46:55.811
level, it's very, very, like, responsive. You can

06:46:55.811 --> 06:46:57.641
stop at any time. Okay? It's very rare since it's

06:46:59.321 --> 06:47:01.741
And this basically leads us to our recent extension,

06:47:01.962 --> 06:47:02.921
to the first work.

06:47:04.181 --> 06:47:06.261
And I probably will skip a lot of details, but I'll

06:47:06.261 --> 06:47:08.121
just give you a highlight. Of

06:47:08.441 --> 06:47:10.501
of this work. So this is Deepgram.

06:47:10.501 --> 06:47:12.763
And as a highlight, basically, Deepgram achieves

06:47:13.621 --> 06:47:15.781
a a stable accuracy on 2025.

06:47:15.781 --> 06:47:18.781
I think it's the first work that reports 99.9%

06:47:19.281 --> 06:47:21.495
digital accuracy. On '25.

06:47:21.495 --> 06:47:23.931
Okay? And, and using the GPT OSS

06:47:24.411 --> 06:47:26.891
one twenty b. Okay? Which arguably

06:47:27.271 --> 06:47:29.571
is not a wireless phone model today. Okay? And

06:47:29.571 --> 06:47:31.811
while at the same time, reducing token, cost

06:47:31.811 --> 06:47:34.011
by up to 85 percentage.

06:47:34.491 --> 06:47:36.811
And it basically features two modes, offline

06:47:36.811 --> 06:47:38.851
mode, which use confidence weighted

06:47:38.876 --> 06:47:40.921
voting on four traces,

06:47:41.241 --> 06:47:43.111
to achieve better self consistency.

06:47:43.271 --> 06:47:45.351
That is for each recovery, you you,

06:47:45.591 --> 06:47:47.831
you you do the company's

06:47:48.211 --> 06:47:50.310
measurement offline. You try to assign a weight to

06:47:50.310 --> 06:47:52.531
each trajectory. And you

06:47:52.531 --> 06:47:54.622
remove those low confidence

06:47:55.221 --> 06:47:57.481
traces, and you keep those high confidence traces.

06:47:57.671 --> 06:47:59.831
And once you figure out a subset of

06:47:59.831 --> 06:48:01.451
high confidence traces, you use that

06:48:02.011 --> 06:48:04.091
trace, confidence as a weight to weight the combined

06:48:04.091 --> 06:48:06.321
answers and to get your final answer. Okay?

06:48:06.561 --> 06:48:08.561
That is a direct improvement of

06:48:10.671 --> 06:48:12.962
standard self consistency. Right? Another

06:48:12.962 --> 06:48:15.462
way is online mode, which basically monitors confidence

06:48:15.521 --> 06:48:17.800
in real time to early stop,

06:48:17.800 --> 06:48:19.961
no confidence traces, and a safe compute. And this is

06:48:19.961 --> 06:48:22.021
already similar to the first word I just introduced.

06:48:22.021 --> 06:48:24.031
Where you keep monitoring the confidence. And

06:48:24.031 --> 06:48:26.371
whenever the confidence drops to a certain threshold,

06:48:26.811 --> 06:48:29.041
you give up, and you try to spawn under pieces.

06:48:29.041 --> 06:48:29.921
To restart.

06:48:35.601 --> 06:48:37.861
Okay? And, and the result of this

06:48:38.311 --> 06:48:40.551
online deep Deepgram IV are

06:48:40.551 --> 06:48:42.901
the most impressive, which I want to highlight So

06:48:43.042 --> 06:48:44.970
basically, we achieved huge token

06:48:45.191 --> 06:48:48.007
reduction. Between 32 percentage and 85

06:48:48.007 --> 06:48:50.161
percentage fewer tokens across

06:48:50.221 --> 06:48:52.591
various datasets. Such as

06:48:52.591 --> 06:48:54.911
a 76% reduction on the actual

06:48:54.911 --> 06:48:57.151
MIMT 25. And, more

06:48:57.151 --> 06:48:59.151
importantly, this massive,

06:48:59.391 --> 06:49:01.792
computer reduction the accuracy

06:49:01.931 --> 06:49:02.911
remains competitive.

06:49:04.101 --> 06:49:06.041
And sometimes, you can even improve

06:49:06.981 --> 06:49:09.271
the accuracy compared to a standard majority voting

06:49:09.671 --> 06:49:11.991
it is because you, indeed, use this confidence

06:49:11.991 --> 06:49:13.851
to filter somewhat noisy trajectory.

06:49:14.421 --> 06:49:16.581
You only aggregate those high quality answer

06:49:16.581 --> 06:49:17.800
across important trajectories.

06:49:19.821 --> 06:49:22.271
Okay? So in conclusion, for this work,

06:49:23.071 --> 06:49:25.181
we developed another kind of confidence

06:49:25.485 --> 06:49:27.361
metric our parallel generation.

06:49:27.921 --> 06:49:30.081
And we argue that the parallel generation, with

06:49:30.081 --> 06:49:31.621
tokens due to diminishing returns,

06:49:32.773 --> 06:49:35.042
or having more accurate accuracy performing

06:49:35.263 --> 06:49:37.531
majority only. On them. And Deepgram

06:49:37.531 --> 06:49:39.611
basically tried to solve this, and the online mode

06:49:39.611 --> 06:49:41.692
actually receives about computer without

06:49:41.692 --> 06:49:44.101
the extra training. And the offline mode was

06:49:44.101 --> 06:49:45.711
the first that basically

06:49:46.319 --> 06:49:48.271
make a ME twenty twenty five

06:49:48.591 --> 06:49:50.872
benchmark not useful. Okay? You

06:49:50.921 --> 06:49:53.201
basically saturate that. And, I think

06:49:53.341 --> 06:49:55.581
my student and the collaborators, they are also pushing

06:49:55.581 --> 06:49:58.021
pretty hard to put this Deepgram

06:49:58.081 --> 06:50:00.122
into TRT

06:50:00.122 --> 06:50:02.282
ARM and VOM. And we are working

06:50:02.282 --> 06:50:04.480
together to integrate this kind of like a

06:50:04.480 --> 06:50:06.751
test time to steering

06:50:06.776 --> 06:50:08.921
method. Okay? Thank you. That's all my

06:50:08.921 --> 06:50:09.241
talk.

06:50:13.981 --> 06:50:16.261
Thank you. So we will have questions.

06:50:17.271 --> 06:50:19.341
Time for one question. Yes. Thanks.

06:50:28.751 --> 06:50:30.691
Hi, professor. Professor,

06:50:30.831 --> 06:50:32.911
thank you very much. Come to

06:50:32.911 --> 06:50:35.201
hear. My name is Ming. Ming

06:50:35.201 --> 06:50:36.561
Qing come from 08/2008.

06:50:37.601 --> 06:50:39.622
And the first part you mentioned,

06:50:39.681 --> 06:50:41.749
like, my impression is it

06:50:41.810 --> 06:50:43.831
talk about how to

06:50:43.831 --> 06:50:45.292
achieve a reasonable

06:50:46.071 --> 06:50:48.130
accuracy by measure

06:50:49.521 --> 06:50:51.532
and token in the reasoning

06:50:51.532 --> 06:50:53.026
model? Consumed

06:50:54.841 --> 06:50:56.942
I'm not sure how you conduct

06:50:57.081 --> 06:50:59.431
this research I think there

06:50:59.431 --> 06:51:01.131
are lots commercial vendor.

06:51:01.611 --> 06:51:03.991
Mhmm. And my

06:51:04.050 --> 06:51:06.291
working environment, they always charge me,

06:51:06.291 --> 06:51:08.621
like, how many token I

06:51:08.621 --> 06:51:10.202
do the inference.

06:51:10.741 --> 06:51:12.561
So when you do your research

06:51:12.801 --> 06:51:14.962
you really relay on the API

06:51:14.962 --> 06:51:17.241
provider provider from

06:51:17.241 --> 06:51:18.071
the vendor to

06:51:19.390 --> 06:51:20.931
get every benchmark

06:51:21.591 --> 06:51:23.831
how much tokens they give to you?

06:51:23.831 --> 06:51:24.981
Or you have

06:51:26.141 --> 06:51:28.491
some kind of your self module you

06:51:28.491 --> 06:51:30.821
do that kind of research. My

06:51:30.821 --> 06:51:33.221
question is, how I can

06:51:33.531 --> 06:51:35.792
predict like, a confidence

06:51:36.372 --> 06:51:38.731
the vendor provided this reduced

06:51:40.581 --> 06:51:42.681
token is reliable. That Yeah.

06:51:42.681 --> 06:51:44.761
Yeah. So, you know, you know, first method I introduced,

06:51:44.761 --> 06:51:46.831
I think, you need to you do not rely

06:51:46.831 --> 06:51:47.961
on getting the logins.

06:51:48.861 --> 06:51:51.191
Language model. Right? I think ARM providers,

06:51:51.191 --> 06:51:53.271
they are not exposed end point for you to

06:51:53.271 --> 06:51:55.251
get logins. You can only get tokens.

06:51:55.411 --> 06:51:57.571
But in the first one, I introduced you don't need that. You just

06:51:57.571 --> 06:51:59.821
need to probe and get answers. That's a

06:51:59.821 --> 06:52:01.952
pure pure token. Which is fine. In

06:52:01.952 --> 06:52:04.273
the second work, I think, is slightly more

06:52:04.273 --> 06:52:06.751
complicated because the way you

06:52:06.751 --> 06:52:08.911
aggregate a trace level confidence is you need to

06:52:08.911 --> 06:52:11.141
get the log token level distribution. Which

06:52:11.141 --> 06:52:13.161
you are not able to gather from API providers.

06:52:13.401 --> 06:52:15.481
So if you want to apply the second word, you have to serve

06:52:15.481 --> 06:52:17.231
your own language model. Yeah.

06:52:18.431 --> 06:52:20.831
Okay. Yeah. I have a second

06:52:20.970 --> 06:52:23.321
question. That's my peer to proceed.

06:52:24.292 --> 06:52:26.451
First. Hello. I had a question regarding

06:52:26.451 --> 06:52:28.461
Datacom. Does

06:52:28.461 --> 06:52:30.961
it assume that the models are properly calibrated

06:52:31.081 --> 06:52:33.311
As in that

06:52:33.761 --> 06:52:35.701
there won't be underconfidence or overconfidence

06:52:35.921 --> 06:52:38.150
issues with token For example, for some

06:52:38.150 --> 06:52:40.251
erroneous tokens, the models may be overconfident.

06:52:40.761 --> 06:52:42.491
And maybe in that case, choosing the

06:52:43.531 --> 06:52:45.641
highest 10 or the lowest 10% tokens

06:52:45.881 --> 06:52:47.541
may result in, like, a bad

06:52:48.101 --> 06:52:50.261
confidence estimate. Yeah. So in my talk, I I actually simply

06:52:50.261 --> 06:52:52.461
have a lot of details. I tried to present a

06:52:52.461 --> 06:52:54.751
metric as simple as possible.

06:52:54.751 --> 06:52:56.901
But in fact, there are a lot of parameters

06:52:56.901 --> 06:52:58.800
that you need to tune. To make sure that,

06:52:59.050 --> 06:53:01.140
for example, bottom 10% of that 10

06:53:01.140 --> 06:53:03.212
is arguable. Right? And you

06:53:03.212 --> 06:53:05.081
probably want to tailor that

06:53:05.401 --> 06:53:07.171
percentage against your application. Yeah.

06:53:07.411 --> 06:53:09.571
It's a limited data set specific, I have to

06:53:09.731 --> 06:53:12.042
say. Yeah. Yeah. That makes sense. Thank you. Okay.

06:53:12.042 --> 06:53:12.601
Cool. Thank you.

06:53:17.551 --> 06:53:19.091
Just last one question, right.

06:53:20.791 --> 06:53:23.031
So in other people has come here.

06:53:23.191 --> 06:53:24.569
So my last question is,

06:53:25.321 --> 06:53:27.041
I saw your did a research.

06:53:27.761 --> 06:53:29.541
From, like, a commercial perspective.

06:53:29.921 --> 06:53:31.521
Right? And

06:53:32.481 --> 06:53:34.831
organization business user,

06:53:35.411 --> 06:53:36.961
Is there is a way to

06:53:38.011 --> 06:53:40.071
create a real time

06:53:40.131 --> 06:53:41.671
or some kind of, like,

06:53:42.819 --> 06:53:43.741
orchestra. Right?

06:53:46.071 --> 06:53:48.151
Like you have a credit score company

06:53:48.151 --> 06:53:50.211
and give your credit score

06:53:50.211 --> 06:53:52.361
evaluation. You You're zero

06:53:52.361 --> 06:53:54.361
with this moving forward, we can

06:53:54.681 --> 06:53:57.121
have some kind of company merge out? Hey.

06:53:57.281 --> 06:53:59.380
You have different commercial moduling.

06:54:00.212 --> 06:54:02.372
I have this service to measure

06:54:02.372 --> 06:54:04.331
the efficiency even like you

06:54:04.491 --> 06:54:06.671
you give you example, You're forced to

06:54:06.970 --> 06:54:08.853
interrupt get a final result.

06:54:09.441 --> 06:54:11.401
You pro you forcing

06:54:11.541 --> 06:54:13.791
like, hey. Stop zero. Where is this good enough?

06:54:14.271 --> 06:54:16.371
User zero is going to create a platform

06:54:17.230 --> 06:54:19.271
or aux tool. I can dispatch

06:54:19.271 --> 06:54:21.431
my like you some people

06:54:21.431 --> 06:54:23.651
maintain this kind of benchmark. To

06:54:23.712 --> 06:54:26.007
testing, to give the people, say, hey, You

06:54:26.007 --> 06:54:28.441
can use at this moment, maybe

06:54:29.021 --> 06:54:30.501
this vendor is there

06:54:31.301 --> 06:54:33.491
resource consumption is less

06:54:33.811 --> 06:54:35.872
a reasonable like, a this

06:54:35.872 --> 06:54:37.991
standard. So customer will say, hey.

06:54:38.071 --> 06:54:39.751
I don't need to pay too

06:54:40.212 --> 06:54:42.391
much based on this kind of reference.

06:54:42.876 --> 06:54:44.941
So so that's what my crazy idea

06:54:44.941 --> 06:54:47.386
is in Because I think it's long

06:54:47.386 --> 06:54:49.481
term this going to be a society

06:54:49.481 --> 06:54:51.531
resource. Like, how

06:54:51.531 --> 06:54:53.712
we efficiently to use them We

06:54:53.712 --> 06:54:55.971
need a reliable, neutral like,

06:54:55.971 --> 06:54:57.611
aging in ZER. Say, hey,

06:54:58.011 --> 06:55:00.181
I can tell you in this current time

06:55:00.341 --> 06:55:02.431
for this mass, like, a benchmark

06:55:02.811 --> 06:55:04.871
the foundry rich company, you

06:55:04.871 --> 06:55:07.271
can spend less money

06:55:08.273 --> 06:55:10.621
and the the, I think, win win situation for less

06:55:10.841 --> 06:55:12.941
Yeah. Indeed. I think that's reasonable.

06:55:13.031 --> 06:55:15.111
Yeah. And I think people have been approaching

06:55:15.111 --> 06:55:17.431
this from both inference and training. On inference,

06:55:17.431 --> 06:55:19.310
that's basically kind of like my work.

06:55:20.311 --> 06:55:22.611
Where you try to do inference

06:55:22.829 --> 06:55:24.931
time steering. Like I said, we are able to put these

06:55:24.931 --> 06:55:27.031
methods into TRT RMM, and a lot of companies

06:55:27.091 --> 06:55:29.112
are actually using TRT ARM two. Serve

06:55:29.112 --> 06:55:31.211
their models, right, as a infrastructure. On

06:55:31.211 --> 06:55:33.331
the training side, I think, I don't know which company is the best,

06:55:33.331 --> 06:55:35.461
but a lot of people to train this kind

06:55:35.461 --> 06:55:37.471
of confidence based

06:55:38.872 --> 06:55:40.901
early early exit into the model. Okay?

06:55:41.301 --> 06:55:43.462
So basically, the next generation model, which IBD is

06:55:43.462 --> 06:55:45.641
basically adaptive thinking. Right?

06:55:45.962 --> 06:55:48.261
So a lot of commercial

06:55:48.321 --> 06:55:49.811
endpoint already have that option that

06:55:50.372 --> 06:55:52.612
depends on your problem of difficulty. It's

06:55:52.612 --> 06:55:54.431
going to generate more or less tokens.

06:55:54.671 --> 06:55:56.751
Yeah. Yeah. Yeah. Okay. Thank

06:55:56.751 --> 06:55:57.761
you. Thank you.

06:56:08.371 --> 06:56:10.390
Yeah. Thanks, Pu Hao, for

06:56:11.122 --> 06:56:13.462
his remarkable talk, and we will

06:56:13.462 --> 06:56:15.181
walk on to our next

06:56:16.621 --> 06:56:17.121
thanks.

06:56:20.792 --> 06:56:22.971
Yes. And now we welcome

06:56:23.192 --> 06:56:25.371
our next speaker, NEE

06:56:25.371 --> 06:56:26.876
class banner. Class is

06:56:27.542 --> 06:56:29.581
student at Stanford, and he will talk

06:56:30.141 --> 06:56:32.641
about his work on test test sculling

06:56:33.042 --> 06:56:35.442
But due to some yeah. You can see Viglas

06:56:35.442 --> 06:56:37.221
on the screen. Yeah. Hi, Viglas.

06:56:37.731 --> 06:56:39.763
Yeah. Due to some

06:56:39.981 --> 06:56:42.161
personal issue, Viglas cannot join

06:56:42.301 --> 06:56:44.391
us, but today for

06:56:44.881 --> 06:56:47.202
personal talk, but we still kindly

06:56:47.202 --> 06:56:49.249
invite V class for the motor.

06:56:49.249 --> 06:56:51.489
So, yeah, now I will give my time to

06:56:51.489 --> 06:56:53.221
Nicklaus. For his

06:56:53.521 --> 06:56:55.741
presentation. Thanks.

06:56:56.622 --> 06:56:59.042
I'm Nick Closon. I'll I'll talk about test time scaling.

06:56:59.661 --> 06:57:01.801
Wanna thank the organizers for having me, and

06:57:02.202 --> 06:57:04.282
arranging me online. I I'm

06:57:04.282 --> 06:57:06.291
sorry. I injured myself, so I couldn't make it

06:57:07.171 --> 06:57:09.361
person. But I'm excited to talk to you

06:57:09.361 --> 06:57:10.661
about test time scaling.

06:57:12.711 --> 06:57:15.061
And I wanna

06:57:15.061 --> 06:57:17.201
start with this plot from from Jensen

06:57:17.201 --> 06:57:19.111
Huang, the CEO of NVIDIA earlier this year.

06:57:19.351 --> 06:57:21.701
I liked it because it frames

06:57:21.701 --> 06:57:24.021
test on scaling as a continuation of the scaling

06:57:24.021 --> 06:57:26.121
paradigm. So we started off with

06:57:26.121 --> 06:57:28.292
pretraining scaling and then post

06:57:28.591 --> 06:57:30.921
training and now test time scaling.

06:57:31.201 --> 06:57:33.361
And I think what's interesting about test time

06:57:33.361 --> 06:57:35.751
scaling is that it it's quite different

06:57:36.041 --> 06:57:38.060
to the previous two because there's

06:57:38.121 --> 06:57:40.341
two axes of compute that we're scaling.

06:57:41.081 --> 06:57:42.550
For pretraining and posttraining,

06:57:43.301 --> 06:57:45.261
we're scaling training each time.

06:57:45.421 --> 06:57:47.561
It's about training compute. But

06:57:47.561 --> 06:57:50.031
for a test I'm scaling, we

06:57:50.031 --> 06:57:52.071
are scaling training compute as well.

06:57:52.311 --> 06:57:54.391
So this is a plot from from the o one

06:57:54.391 --> 06:57:55.691
release last year.

06:57:56.427 --> 06:57:58.651
And at the same time, we're also

06:57:58.651 --> 06:58:00.730
scaling test time

06:58:00.730 --> 06:58:03.011
compute. So we need to scale train time

06:58:03.011 --> 06:58:05.031
compute first to get the model

06:58:05.031 --> 06:58:07.191
to use a lot of reasoning and

06:58:07.191 --> 06:58:09.650
thinking. And then once it has

06:58:09.650 --> 06:58:10.630
this capability,

06:58:11.890 --> 06:58:14.341
we can vary, test, and compute. So

06:58:14.621 --> 06:58:16.781
there's two axes we're we're scaling, which

06:58:16.781 --> 06:58:18.911
I think is is quite interesting. So there's

06:58:18.911 --> 06:58:21.071
a lot more compute that we need because we need to

06:58:21.551 --> 06:58:23.900
scale both of those unlike for the

06:58:23.900 --> 06:58:26.241
prior paradigm. So we we just scaled

06:58:26.801 --> 06:58:27.531
the training compute.

06:58:29.331 --> 06:58:31.730
And earlier this year, we released a paper

06:58:31.730 --> 06:58:34.161
called s one, which I think

06:58:34.380 --> 06:58:36.541
provides two useful baselines for both of

06:58:36.541 --> 06:58:38.551
those parts. So for the train time

06:58:38.551 --> 06:58:39.901
scaling and

06:58:40.962 --> 06:58:42.251
the the test part.

06:58:44.451 --> 06:58:46.561
So for training, the paper

06:58:46.561 --> 06:58:48.741
showed that you can do simple

06:58:48.741 --> 06:58:50.401
SFT, supervised fine tuning,

06:58:50.781 --> 06:58:52.821
on 1,000 samples to get

06:58:53.221 --> 06:58:55.081
this good reasoning performance comparative

06:58:55.301 --> 06:58:57.631
to o one and some other models.

06:58:57.712 --> 06:58:59.651
So here on the x axis, we have the number of

06:59:00.051 --> 06:59:01.881
of training samples and then reasoning

06:59:02.103 --> 06:59:04.361
performance on the y axis. And it got

06:59:04.361 --> 06:59:06.441
pretty good with just 1,000 samples. So this

06:59:06.441 --> 06:59:07.921
is a useful baseline for

06:59:08.481 --> 06:59:10.721
that training aspect of test time scaling. So how

06:59:10.721 --> 06:59:12.831
you set up the model to be able to reason

06:59:12.962 --> 06:59:15.041
and and think for a long time and

06:59:15.041 --> 06:59:17.130
and use a lot

06:59:17.130 --> 06:59:19.351
of computer test time. And

06:59:19.411 --> 06:59:21.641
then for the testing part, so

06:59:21.641 --> 06:59:23.861
then once you have this trained model, how do you

06:59:24.341 --> 06:59:26.581
make it use varying amounts of computer

06:59:26.581 --> 06:59:28.701
test time? We introduced budget

06:59:28.701 --> 06:59:30.601
forcing which is, again,

06:59:30.739 --> 06:59:32.811
very simple The idea is

06:59:32.811 --> 06:59:34.933
that given some question that you want the

06:59:34.933 --> 06:59:37.281
model to reason about, such as how

06:59:37.341 --> 06:59:39.551
many are you

06:59:40.831 --> 06:59:42.911
we let the model first think about it, and

06:59:42.911 --> 06:59:45.071
then if we want the model to

06:59:45.532 --> 06:59:47.630
to be faster, so reason for a

06:59:47.630 --> 06:59:49.681
short amount of time, then we just cut it off

06:59:49.681 --> 06:59:51.712
and directly force it to go

06:59:51.712 --> 06:59:53.731
into its answer. If you wanted to think

06:59:53.731 --> 06:59:55.991
longer such as in this example, so extrapolating

06:59:57.161 --> 06:59:59.341
then rather than letting it finish,

06:59:59.771 --> 07:00:01.491
we'll have something like

07:00:02.531 --> 07:00:04.771
wait or something. We'll replace this finish token with

07:00:04.771 --> 07:00:06.843
some other token that will lead the model

07:00:06.843 --> 07:00:09.103
to continue reasoning. So here in this example,

07:00:09.542 --> 07:00:11.862
the model actually wanted to finish after

07:00:11.862 --> 07:00:14.031
the sentence, The number of r's

07:00:14.031 --> 07:00:16.111
in Raspberry is two, so it's, like, the eighth or

07:00:16.111 --> 07:00:18.281
ninth line. But then rather than letting

07:00:18.281 --> 07:00:19.611
it finish, we just ignore

07:00:20.390 --> 07:00:22.571
this finishing token and and put a weight

07:00:22.571 --> 07:00:24.712
in there. And then, of course, the

07:00:24.712 --> 07:00:27.031
most logical thing to generate after weight is

07:00:27.431 --> 07:00:29.542
additional reasoning tokens. And so the

07:00:29.542 --> 07:00:30.292
model continues

07:00:31.560 --> 07:00:33.781
reasoning. And then actually, in this instance,

07:00:33.781 --> 07:00:35.861
it even fixes its answer from two to

07:00:35.861 --> 07:00:37.421
three. So in the final answer, then

07:00:37.981 --> 07:00:40.081
gets it correct thanks to

07:00:40.081 --> 07:00:42.226
this additional thinking stage.

07:00:43.311 --> 07:00:45.421
And if we if we

07:00:45.421 --> 07:00:47.579
use this for varying budgets, we get pretty good

07:00:47.579 --> 07:00:49.861
test time scaling plots.

07:00:50.501 --> 07:00:52.761
Like shown here. So for

07:00:52.761 --> 07:00:54.841
for math, PhD

07:00:54.841 --> 07:00:57.001
level science questions, a bunch of different things from from

07:00:57.001 --> 07:00:58.481
benchmarks you probably know about.

07:00:59.161 --> 07:01:00.941
And if we zoom into one of those,

07:01:01.521 --> 07:01:03.621
we so Amy, for

07:01:03.621 --> 07:01:05.861
example, then a bunch of those

07:01:05.861 --> 07:01:08.041
thoughts are cutting the model short.

07:01:08.041 --> 07:01:10.161
So for example here, forcing

07:01:10.300 --> 07:01:12.341
2,048 or 4,096

07:01:12.901 --> 07:01:14.911
maximum thinking tokens. So they're when

07:01:14.911 --> 07:01:16.701
the model reaches this

07:01:17.181 --> 07:01:19.521
budget, then we force it to stop and start answering

07:01:20.271 --> 07:01:21.901
based on what it's got so far. So it'll just

07:01:22.381 --> 07:01:24.241
look at its current reasoning, Chase, and then

07:01:24.721 --> 07:01:26.821
give it the best shot. And

07:01:26.821 --> 07:01:29.061
then toward the right, we let it think longer and longer. And

07:01:29.061 --> 07:01:31.140
then even extrapolating

07:01:31.521 --> 07:01:33.541
by not letting it finish its thinking, but

07:01:33.541 --> 07:01:35.591
rather a pending weight multiple

07:01:35.651 --> 07:01:37.792
times. And here for Amy, this

07:01:37.792 --> 07:01:40.112
competition math benchmark, it it can improve

07:01:40.112 --> 07:01:42.161
performance. This is how this looks in

07:01:42.161 --> 07:01:44.470
in detail. It does give us a good

07:01:44.470 --> 07:01:46.671
and useful baseline for for test

07:01:46.671 --> 07:01:48.921
time scaling. But, obviously, I think there's

07:01:48.921 --> 07:01:51.121
many problems with it, such as if we would go further

07:01:51.121 --> 07:01:53.202
to the right, then it doesn't work forever

07:01:53.202 --> 07:01:55.211
eventually. The model gets gets

07:01:55.211 --> 07:01:57.581
sick of all the weights, and it'll do some not

07:01:57.581 --> 07:01:59.931
very good stuff. Then also the forcing,

07:01:59.931 --> 07:02:02.060
of course, is a bit arbitrary.

07:02:02.060 --> 07:02:04.001
The model might be in the middle of something important,

07:02:05.093 --> 07:02:07.491
So it is very useful as a baseline, but probably

07:02:07.491 --> 07:02:08.631
not the final solution.

07:02:10.300 --> 07:02:12.621
But nonetheless, I think these are two pretty useful baselines,

07:02:12.621 --> 07:02:14.751
so we have the training baseline, SFT

07:02:14.751 --> 07:02:17.091
on few samples, and then the

07:02:17.461 --> 07:02:19.569
testing baseline of of using budget forcing,

07:02:20.021 --> 07:02:22.351
a very simple baseline

07:02:22.351 --> 07:02:24.751
that doesn't require any training. You can just take any existing

07:02:24.751 --> 07:02:26.521
models and and implement it.

07:02:26.841 --> 07:02:29.001
And since this release, it's been very cool

07:02:29.001 --> 07:02:31.211
to see a lot of works of bashing our

07:02:31.211 --> 07:02:33.251
baselines and getting much better things.

07:02:33.761 --> 07:02:35.841
So I wanna briefly go through some of them and then

07:02:35.841 --> 07:02:37.980
think a bit about what is still missing

07:02:37.980 --> 07:02:39.921
and and what we should do next potentially.

07:02:41.281 --> 07:02:43.300
So on the training site, I

07:02:43.300 --> 07:02:44.991
think there was

07:02:45.451 --> 07:02:47.531
this paper Open Thoughts earlier

07:02:47.531 --> 07:02:49.761
this year, and they just

07:02:49.761 --> 07:02:51.872
show that you can scale up as as

07:02:51.872 --> 07:02:54.051
Feet and get much better performance. So, obviously,

07:02:55.071 --> 07:02:57.101
1,000 examples it's

07:02:57.101 --> 07:02:59.112
nice. But it won't

07:02:59.491 --> 07:03:01.761
you can probably do much better with much more. It's it's

07:03:02.001 --> 07:03:04.151
good because it's very easy, But, for example,

07:03:04.151 --> 07:03:06.372
if you look at GPQA, then

07:03:06.372 --> 07:03:07.831
even though at the 1,000

07:03:08.481 --> 07:03:10.561
range on the x axis, the s 1.1

07:03:10.561 --> 07:03:12.911
dataset is is not bad. It

07:03:12.971 --> 07:03:15.131
definitely doesn't scale as far as the dataset

07:03:15.131 --> 07:03:15.791
they created.

07:03:17.351 --> 07:03:19.651
And this is one way, I think,

07:03:19.712 --> 07:03:21.751
to to improve on the training part.

07:03:21.751 --> 07:03:23.452
So just scaling SFT further,

07:03:23.881 --> 07:03:25.962
And, of course, in parallel, we can also scale

07:03:25.962 --> 07:03:27.581
RLs, so there have been some works

07:03:28.282 --> 07:03:30.061
scaling up RL even earlier like

07:03:30.442 --> 07:03:32.079
deepseag and recently for Meta.

07:03:33.730 --> 07:03:35.831
And yeah, I think there's

07:03:36.051 --> 07:03:38.061
also a way more gains to be had if you

07:03:38.061 --> 07:03:40.300
if you scale up our a lot. So

07:03:40.300 --> 07:03:42.721
this is probably also way more than a thousand

07:03:42.721 --> 07:03:45.050
training samples. So the progress

07:03:45.050 --> 07:03:47.511
there has been great. I think what's still missing

07:03:47.511 --> 07:03:49.641
on the training part is we

07:03:49.701 --> 07:03:51.781
can probably still scale further on the RL

07:03:51.781 --> 07:03:53.980
side. But even here at

07:03:53.980 --> 07:03:56.261
the end, think there's a lot of extrapolation happening.

07:03:56.261 --> 07:03:58.271
There's not We haven't they don't didn't

07:03:58.271 --> 07:04:00.560
put all the dots train all the dots

07:04:00.560 --> 07:04:02.781
there. So going further there, seeing what's

07:04:02.781 --> 07:04:04.909
actually the limit, in an open

07:04:04.909 --> 07:04:06.981
setting, it is where very interesting, I think.

07:04:07.462 --> 07:04:09.782
Maybe the some of the labs might have already figured this out but haven't

07:04:09.782 --> 07:04:12.161
shared it. So scaling further

07:04:12.401 --> 07:04:14.631
I think, is is one exciting thing on

07:04:14.631 --> 07:04:16.671
the performance part. And

07:04:16.671 --> 07:04:18.452
to improve the efficiency of training,

07:04:19.471 --> 07:04:21.301
I think probably

07:04:21.921 --> 07:04:23.941
process rewards. Figuring that out

07:04:23.941 --> 07:04:26.013
seems like an important step.

07:04:26.013 --> 07:04:28.101
So Ilya was recently talking about this

07:04:29.061 --> 07:04:31.220
that currently the models will do no

07:04:31.220 --> 07:04:33.319
learning at all. Until you come up with a proposed

07:04:33.319 --> 07:04:35.550
solution. So we only give it the reward at

07:04:35.550 --> 07:04:37.589
the end. Even though there's been some some

07:04:37.589 --> 07:04:39.731
work on process rewards earlier, in

07:04:39.731 --> 07:04:41.711
the past couple of years, I think we haven't quite

07:04:42.031 --> 07:04:43.581
figured out how to use it at scale.

07:04:44.301 --> 07:04:46.462
And so I think these are two aspects that are

07:04:46.462 --> 07:04:47.341
important there.

07:04:48.621 --> 07:04:50.381
And then on the testing part,

07:04:50.622 --> 07:04:52.702
I think there have also been a bunch of pretty

07:04:52.702 --> 07:04:54.811
cool improvements I wanna take a look at.

07:04:56.251 --> 07:04:58.351
So this work from from Salesforce

07:04:58.861 --> 07:05:01.021
they have a very simple idea of rather than just having

07:05:01.021 --> 07:05:03.122
one budget for the the

07:05:03.122 --> 07:05:05.622
reasoning trace, we also have another budget for the response.

07:05:06.542 --> 07:05:08.321
And this gives the model more flexibility

07:05:09.091 --> 07:05:10.981
And so they show in their paper that

07:05:11.942 --> 07:05:13.962
while it's one, it lets you have decent control

07:05:14.241 --> 07:05:16.275
over the tokens used. So tokens used here

07:05:16.275 --> 07:05:18.321
on the x axis, it

07:05:18.381 --> 07:05:19.521
performs worse than their

07:05:20.640 --> 07:05:22.441
separate budgeting method. So the

07:05:23.241 --> 07:05:25.480
the yellow line because you get more

07:05:25.480 --> 07:05:27.201
flexibility for the model by having these

07:05:27.601 --> 07:05:28.661
two two separate budgets.

07:05:31.282 --> 07:05:33.702
And then going even a step further and integrating

07:05:33.761 --> 07:05:35.301
it partly into the training,

07:05:35.911 --> 07:05:37.991
I think there was there were a bunch of papers on

07:05:37.991 --> 07:05:40.251
this. One of them was l one, which I enjoyed.

07:05:40.981 --> 07:05:43.291
Where they show that you can train

07:05:43.291 --> 07:05:45.451
a model with reinforcement learning to adhere to these

07:05:45.451 --> 07:05:47.721
budgets. So rather than just after training it,

07:05:48.122 --> 07:05:50.261
forcing the model to stop after

07:05:50.581 --> 07:05:52.391
a specific token count

07:05:53.032 --> 07:05:55.112
They put those into the prompt and then train the

07:05:55.112 --> 07:05:56.881
model with it, and then

07:05:57.241 --> 07:05:59.561
it turns out that the model learns to adhere

07:05:59.561 --> 07:06:01.712
to these budgets pretty well. So

07:06:01.712 --> 07:06:03.811
in their plot as one, is

07:06:03.811 --> 07:06:05.961
very, very controllable. So the purple line

07:06:06.601 --> 07:06:08.681
very controllable across the tokens used

07:06:08.681 --> 07:06:10.060
and performance improves

07:06:11.103 --> 07:06:13.341
But the performance is not as good as if you tell

07:06:13.341 --> 07:06:15.821
the model in the prompt. And I think

07:06:15.880 --> 07:06:17.891
the reason this is the case is the

07:06:17.891 --> 07:06:20.051
model knows in the prompt already what its budget

07:06:20.051 --> 07:06:22.032
is gonna be, not gonna

07:06:22.091 --> 07:06:24.380
start off and do something wildly

07:06:24.380 --> 07:06:26.471
different. If it knows it has a very very small budget

07:06:26.471 --> 07:06:28.612
for this reasoning chain, then it

07:06:28.612 --> 07:06:29.991
won't start a very long

07:06:30.702 --> 07:06:33.023
a very, very long reasoning effort.

07:06:33.023 --> 07:06:34.941
But it it knows, okay. I have to be quick.

07:06:35.581 --> 07:06:37.521
So it'll jump to the solution much faster.

07:06:38.061 --> 07:06:40.141
Versus for budget forcing just cutting it off in the

07:06:40.141 --> 07:06:41.361
middle of its reasoning chain

07:06:42.321 --> 07:06:44.211
expect it to not be as good, I think.

07:06:44.891 --> 07:06:47.042
I think this is a important improvement,

07:06:47.181 --> 07:06:49.231
letting the model already know what its

07:06:49.231 --> 07:06:50.371
budget is gonna be.

07:06:52.461 --> 07:06:54.511
And then another

07:06:54.511 --> 07:06:56.831
paper recently I think,

07:06:56.831 --> 07:06:59.221
was also interesting where they

07:06:59.221 --> 07:07:00.621
let the model

07:07:02.541 --> 07:07:04.480
check for its budget during inference.

07:07:04.621 --> 07:07:06.798
So you can let the model know at

07:07:06.798 --> 07:07:08.981
the beginning but then counting how many tokens

07:07:08.981 --> 07:07:10.521
it has generated thus far

07:07:11.131 --> 07:07:12.881
is not very easy,

07:07:13.181 --> 07:07:15.161
and I'm not sure it's something that we should

07:07:16.042 --> 07:07:18.321
teach these models to do. And

07:07:18.321 --> 07:07:20.480
so what they do is they simply have a

07:07:20.480 --> 07:07:22.800
tracker where the model

07:07:22.861 --> 07:07:24.720
can check its budget anytime

07:07:25.051 --> 07:07:26.042
during the reasoning.

07:07:27.281 --> 07:07:29.341
And then knows, okay, how man how

07:07:29.341 --> 07:07:31.501
much have I already you have I already used? How much

07:07:31.501 --> 07:07:33.702
do I have left? And this

07:07:33.881 --> 07:07:36.122
kind of saves the model from having to count

07:07:36.122 --> 07:07:38.131
the tokens itself Arguably, in this

07:07:38.131 --> 07:07:40.273
s l one approach, model

07:07:40.273 --> 07:07:41.871
somehow learns during our l to

07:07:42.591 --> 07:07:44.581
count its own tokens. And

07:07:44.801 --> 07:07:46.821
that might take away from other capabilities

07:07:46.881 --> 07:07:49.261
it could learn during RL.

07:07:49.901 --> 07:07:51.981
And so I think this is pretty useful

07:07:53.821 --> 07:07:56.211
and they wanna extrapolate in this work,

07:07:56.451 --> 07:07:58.531
they also use this weight approach,

07:07:58.531 --> 07:07:59.981
but add some additional stuff

07:08:00.942 --> 07:08:02.891
to use some tools or do some other stuff.

07:08:03.561 --> 07:08:05.683
To let the model know that it it can

07:08:05.683 --> 07:08:06.433
think longer.

07:08:07.941 --> 07:08:09.981
And they show that this is better than

07:08:09.981 --> 07:08:12.001
than not having this budget, and

07:08:12.001 --> 07:08:14.161
it can even extrapolate quite well. So earlier

07:08:14.161 --> 07:08:16.251
with the way to approach, we also

07:08:16.251 --> 07:08:18.257
got some extrapolation. But here, I

07:08:18.257 --> 07:08:20.301
think maybe partly because they the model

07:08:20.301 --> 07:08:22.381
used tools and it has a lot more things that it

07:08:22.381 --> 07:08:24.401
can it can play with. They might be

07:08:24.401 --> 07:08:25.511
getting even better

07:08:26.550 --> 07:08:28.601
extrapolation I think the last line the last connection

07:08:28.601 --> 07:08:30.621
here is is extrapolation. So y

07:08:30.621 --> 07:08:32.671
axis is performance, x axis

07:08:32.671 --> 07:08:34.781
is how many how many tools it

07:08:34.781 --> 07:08:35.391
can use.

07:08:36.810 --> 07:08:38.880
So quite some extrapolation there.

07:08:39.121 --> 07:08:41.151
I think this is one obvious improvement.

07:08:41.201 --> 07:08:43.371
So we probably want to combine them.

07:08:43.371 --> 07:08:45.442
So having the model know what its

07:08:45.442 --> 07:08:47.471
budget is already in the prompt

07:08:47.761 --> 07:08:49.841
and sort of training it a little bit with learning to

07:08:49.841 --> 07:08:52.021
use that. But then during

07:08:52.021 --> 07:08:54.421
generation, it can check at any point in time,

07:08:54.421 --> 07:08:56.661
okay, where am I at? How much more do I have

07:08:56.661 --> 07:08:58.711
left? A bit like in exams,

07:08:58.711 --> 07:09:00.741
I think we also are usually

07:09:00.741 --> 07:09:03.111
allowed to check what's the time, how long have you already

07:09:03.271 --> 07:09:05.351
been working on the exam, how much more time do we have

07:09:05.351 --> 07:09:07.151
left. I think that's quite useful.

07:09:08.991 --> 07:09:11.111
I think what's next on the testing side

07:09:11.111 --> 07:09:13.181
for performance we

07:09:13.441 --> 07:09:15.560
probably need to work even more on the

07:09:15.560 --> 07:09:17.661
extrapolation. So here,

07:09:17.661 --> 07:09:19.741
I assume that if we go further to the right,

07:09:19.741 --> 07:09:21.891
so beyond the 200, they probably

07:09:22.111 --> 07:09:24.261
saw it also flattening out. Similar

07:09:24.261 --> 07:09:26.361
to what we had earlier with s one.

07:09:27.273 --> 07:09:29.431
So figuring out how it doesn't flatten

07:09:29.431 --> 07:09:31.581
out is is quite important. And

07:09:31.581 --> 07:09:33.841
especially if you if you train and then test

07:09:34.111 --> 07:09:36.191
and during training, you might not be able

07:09:36.191 --> 07:09:38.281
to let the model reason for very

07:09:38.281 --> 07:09:40.151
long, say, for for a day

07:09:40.372 --> 07:09:42.659
or two days because then you would have very few updates.

07:09:43.321 --> 07:09:45.401
So training with shorter reasoning

07:09:45.401 --> 07:09:47.641
chains and then being able to extrapolate to

07:09:47.641 --> 07:09:49.791
much longer One's a test time, I

07:09:49.791 --> 07:09:52.023
think, is is a quite promising direction.

07:09:52.541 --> 07:09:54.681
And then another pretty obvious

07:09:54.681 --> 07:09:56.782
one is that if we want these models to

07:09:57.131 --> 07:09:58.523
test time scale to

07:09:59.811 --> 07:10:01.891
days or weeks in the future where they think for

07:10:01.891 --> 07:10:03.561
really, really long amounts of time.

07:10:05.001 --> 07:10:07.401
Then we probably have to make it more efficient

07:10:07.401 --> 07:10:09.470
because with the

07:10:09.470 --> 07:10:11.550
current full attention it'll get

07:10:11.550 --> 07:10:13.773
prohibitively expensive for very, very

07:10:13.773 --> 07:10:15.933
long chains because the model needs to keep everything in

07:10:15.933 --> 07:10:18.021
memory. So we need to figure out

07:10:18.021 --> 07:10:20.079
some ways to to do this. And I think

07:10:20.079 --> 07:10:22.171
there have already been some some exciting

07:10:22.171 --> 07:10:23.901
works on this, but I think there's more to do.

07:10:24.381 --> 07:10:26.611
Figuring out how to how to remove

07:10:26.611 --> 07:10:28.781
things or maybe changing it

07:10:28.781 --> 07:10:30.381
to some linear attention approach

07:10:30.862 --> 07:10:32.771
I think a lot of things there.

07:10:34.001 --> 07:10:35.951
And so in summary, think

07:10:36.171 --> 07:10:38.511
what's interesting with test set scaling is that unlike before,

07:10:38.872 --> 07:10:40.841
unlike for pretraining scaling and for

07:10:41.081 --> 07:10:43.241
the brief post training scaling, though you might

07:10:43.241 --> 07:10:45.451
argue this is still some form of post training,

07:10:45.611 --> 07:10:47.741
But unlike those two, there's now

07:10:47.741 --> 07:10:49.871
two scaling axes. So we gotta scale up the

07:10:50.111 --> 07:10:52.531
the training part to teach the model how to reason,

07:10:53.171 --> 07:10:55.361
and to think And then

07:10:55.601 --> 07:10:57.761
the test time part where the model then

07:10:57.761 --> 07:10:59.851
uses this to think for very long and so

07:11:00.251 --> 07:11:02.701
problems. And for training,

07:11:02.941 --> 07:11:05.220
I think the two things

07:11:05.220 --> 07:11:07.560
that we can work on on the performance and efficiency

07:11:07.621 --> 07:11:09.991
sides are scaling for performance,

07:11:10.050 --> 07:11:11.671
figuring out how to scale further,

07:11:12.141 --> 07:11:13.281
And then on efficiency,

07:11:14.431 --> 07:11:16.470
probably letting the model learn

07:11:16.470 --> 07:11:18.841
from its intermediate reasoning steps and not just

07:11:19.321 --> 07:11:20.811
a reward at the very end.

07:11:21.391 --> 07:11:23.471
And on the on the testing part of

07:11:23.471 --> 07:11:25.491
test time scaling, we probably

07:11:25.491 --> 07:11:27.541
need to get better extrapolation

07:11:28.101 --> 07:11:29.991
so the model can the models can think for longer

07:11:30.151 --> 07:11:32.491
potentially longer than than they have been trained for.

07:11:32.491 --> 07:11:34.351
And we also

07:11:35.491 --> 07:11:37.531
for efficiency, we have to eventually solve

07:11:37.531 --> 07:11:39.851
the full attention complexity, I think, if we want

07:11:39.851 --> 07:11:41.791
to test time scale to

07:11:42.111 --> 07:11:43.251
days or weeks.

07:11:44.671 --> 07:11:44.991
Thanks.

07:11:51.561 --> 07:11:53.261
Thanks, Miklas, for,

07:11:53.721 --> 07:11:55.661
such a wonderful task.

07:11:56.271 --> 07:11:58.481
We have time. For question,

07:11:58.701 --> 07:12:00.621
if you can. Question after

07:12:01.591 --> 07:12:03.731
welcome to come here to ask

07:12:03.731 --> 07:12:06.161
the class about Ted Tai's coding. Oh,

07:12:06.161 --> 07:12:07.011
praise, please.

07:12:13.121 --> 07:12:14.501
Hello. Can you hear me?

07:12:15.181 --> 07:12:17.251
Yeah. Thanks. I had

07:12:17.251 --> 07:12:19.391
a question regarding s one

07:12:19.391 --> 07:12:21.501
paper. Did

07:12:21.501 --> 07:12:23.581
you guys have the time to try

07:12:23.581 --> 07:12:26.061
something like a dynamic token budget?

07:12:26.061 --> 07:12:28.220
As in token

07:12:28.220 --> 07:12:30.441
budget, which depends

07:12:30.441 --> 07:12:32.781
on maybe like the difficulty or the confidence

07:12:32.841 --> 07:12:34.641
of recent trace,

07:12:34.861 --> 07:12:37.271
for example. The difficulty of the

07:12:37.411 --> 07:12:39.431
problem. Yeah.

07:12:39.431 --> 07:12:40.571
That's a good point.

07:12:42.501 --> 07:12:44.621
I I think we we didn't try that.

07:12:44.861 --> 07:12:46.941
So one thing is, yeah, you gotta know

07:12:46.941 --> 07:12:48.791
the difficulty ahead of time, which

07:12:49.111 --> 07:12:51.191
I guess is is kind of hard, or you

07:12:51.191 --> 07:12:53.661
use the confidence of the model.

07:12:53.720 --> 07:12:56.061
So then it would be, I guess, during the reasoning,

07:12:57.192 --> 07:12:59.591
you figured the model sort of figures

07:12:59.591 --> 07:13:01.771
out how how difficult it is, how confident

07:13:01.771 --> 07:13:03.831
it is, and then you let it think longer. I think

07:13:03.831 --> 07:13:05.942
that's an interesting direction, but I haven't haven't

07:13:05.942 --> 07:13:08.251
tried it. Thank you.

07:13:16.141 --> 07:13:18.221
Thank you. Maybe I have a

07:13:18.221 --> 07:13:20.523
question for you. Yeah. Do you

07:13:20.523 --> 07:13:22.763
agree for that for the future of test

07:13:22.763 --> 07:13:24.962
type skilling, is it continuous

07:13:25.101 --> 07:13:27.601
scaling? Is it the cloud way or for the scaling

07:13:27.741 --> 07:13:29.861
or it has some limit

07:13:30.341 --> 07:13:32.191
like scaling has limit like

07:13:32.991 --> 07:13:35.401
the training, GPT, GPT four, by

07:13:35.991 --> 07:13:38.391
five, or one may now reach the

07:13:38.391 --> 07:13:40.631
reach the end of scale in all.

07:13:40.631 --> 07:13:42.622
Yeah. Do you have that belief similar

07:13:42.683 --> 07:13:44.841
thing will happen in test time scaling or

07:13:44.841 --> 07:13:46.011
no? Yeah.

07:13:47.551 --> 07:13:50.051
Yeah. It's a good question. I personally think we can scale

07:13:50.111 --> 07:13:52.431
a lot further. So looking at

07:13:52.671 --> 07:13:54.611
some of the current RL plots,

07:13:55.651 --> 07:13:57.831
at least here on the right side, it doesn't seem like it's

07:13:57.831 --> 07:13:59.921
it's flattening out. From other

07:13:59.921 --> 07:14:02.001
people's work. I think on the training side,

07:14:02.721 --> 07:14:04.801
like, training the model to reason, we can

07:14:04.801 --> 07:14:06.781
scale further. But, yeah, maybe that one

07:14:07.301 --> 07:14:09.321
least I think in this paper, they say it approaches

07:14:09.381 --> 07:14:11.801
the sigmoid curve, so it it sort of starts flattening

07:14:11.862 --> 07:14:14.221
out. But I think the evidence

07:14:14.221 --> 07:14:16.331
isn't super clear on that yet. So we should

07:14:16.331 --> 07:14:18.391
test it and there's it might scale

07:14:18.391 --> 07:14:20.621
further. And then on the on the testing part,

07:14:20.621 --> 07:14:22.641
so just letting the model think it after

07:14:23.181 --> 07:14:25.391
it has been trained, I think

07:14:25.550 --> 07:14:27.640
I'm pretty confident we can scale a lot

07:14:27.640 --> 07:14:30.026
further. Than currently the models

07:14:30.841 --> 07:14:32.321
aren't thinking for super long yet.

07:14:33.122 --> 07:14:35.341
And at least personally, solving

07:14:35.341 --> 07:14:37.041
a really hard problem, I think you could

07:14:37.521 --> 07:14:39.701
I could work on it for weeks or something.

07:14:39.841 --> 07:14:41.980
So until we get there, I think

07:14:41.980 --> 07:14:44.042
we can can scale a lot further.

07:14:44.452 --> 07:14:46.581
Yeah. And I think it's it's pretty exciting to to

07:14:46.581 --> 07:14:47.761
try to get us there.

07:14:49.441 --> 07:14:51.521
Yeah. Thanks. And that's the

07:14:51.521 --> 07:14:52.561
next question.

07:14:53.841 --> 07:14:55.542
Hi. Thanks talk.

07:14:56.091 --> 07:14:58.401
Interesting type. Area

07:14:58.721 --> 07:15:00.819
to I had a question

07:15:00.819 --> 07:15:03.079
about essence, in the presentation

07:15:03.140 --> 07:15:04.942
you were talking about budgets right?

07:15:05.341 --> 07:15:06.561
Computer budgets presumably.

07:15:08.811 --> 07:15:10.900
Whether you see the work going

07:15:10.900 --> 07:15:12.931
on in the industry as well as maybe a

07:15:12.931 --> 07:15:15.131
bit in academia on LLM

07:15:15.192 --> 07:15:16.911
routers. Whether that

07:15:17.951 --> 07:15:19.577
complements the scaling

07:15:20.331 --> 07:15:21.841
that you're doing within

07:15:23.181 --> 07:15:24.451
And if so, how?

07:15:25.890 --> 07:15:28.101
Oh, the rudders? So, like,

07:15:28.661 --> 07:15:30.739
GPT five having multiple separate models,

07:15:30.739 --> 07:15:32.591
and then they decide which one to route to.

07:15:33.231 --> 07:15:35.571
So the goal is to given a particular

07:15:35.630 --> 07:15:37.819
compute budget. Right? Try to figure

07:15:37.819 --> 07:15:39.561
out which of

07:15:40.981 --> 07:15:43.421
the various options

07:15:43.421 --> 07:15:45.441
you have would be most effective?

07:15:46.542 --> 07:15:48.801
Using an LLM or much smaller

07:15:49.131 --> 07:15:51.581
model that routes the request

07:15:51.861 --> 07:15:53.880
to whichever model seems most

07:15:53.880 --> 07:15:55.201
appropriate given the budget.

07:15:56.542 --> 07:15:57.921
Yeah. I think that's interesting.

07:15:59.671 --> 07:16:01.451
So I think some of the approaches

07:16:01.751 --> 07:16:03.841
there are OpenAI is doing with g p five, I

07:16:03.841 --> 07:16:06.001
guess, and then also what Google is doing

07:16:06.001 --> 07:16:08.097
with this

07:16:08.981 --> 07:16:10.651
metformar approach, I think.

07:16:12.091 --> 07:16:14.291
And, yeah, I've I've also worked

07:16:14.291 --> 07:16:16.251
a bit on MOEs before. I think there's

07:16:16.411 --> 07:16:18.491
things you can do there. So I'm not sure it has to be separate

07:16:18.491 --> 07:16:20.851
models. It could just be one big MOE and then

07:16:21.171 --> 07:16:23.671
it has some some experts

07:16:23.730 --> 07:16:25.890
that are much faster than others. I guess that's a bit

07:16:25.890 --> 07:16:27.131
closer to Matt Former.

07:16:28.311 --> 07:16:30.551
Yeah. I think that's also promising, but I'm not sure we

07:16:30.551 --> 07:16:31.591
can get a lot of,

07:16:32.821 --> 07:16:34.901
or at least current works, I think, don't promise

07:16:34.901 --> 07:16:37.061
a lot of extrapolation. Which

07:16:37.061 --> 07:16:39.241
is arguably the most exciting part. So how do we

07:16:39.641 --> 07:16:41.881
get the model to think for very long to solve very,

07:16:41.881 --> 07:16:43.900
very hard questions? So for saving

07:16:43.900 --> 07:16:46.042
compute, it's probably great. But

07:16:46.341 --> 07:16:48.501
for thinking longer and solving

07:16:48.501 --> 07:16:49.641
really hard questions,

07:16:50.829 --> 07:16:53.329
it would the architecture would probably need some

07:16:54.112 --> 07:16:56.353
some recursiveness or something to enable

07:16:56.353 --> 07:16:58.571
that. If if it if it's purely

07:16:58.571 --> 07:17:00.773
an architecture based approach, I think. If

07:17:00.773 --> 07:17:03.212
we stick with this sort of token based scaling

07:17:03.212 --> 07:17:05.261
approach, then I think the

07:17:05.261 --> 07:17:07.523
methods we we just talked about makes sense.

07:17:08.141 --> 07:17:10.381
But for for something that scales

07:17:10.381 --> 07:17:12.591
by the architecture, I think these

07:17:12.650 --> 07:17:14.730
MATFORMER MOE's multiple models, there needs to be

07:17:14.730 --> 07:17:16.791
some some recursive component to to

07:17:16.791 --> 07:17:18.011
do that, if that makes sense.

07:17:19.819 --> 07:17:22.081
Might be interesting to look into. Thanks.

07:17:22.471 --> 07:17:24.131
Yeah. Thanks, Satya.

07:17:25.711 --> 07:17:27.951
Hey. Great talk. I may have missed

07:17:27.951 --> 07:17:30.451
the beginning, so you didn't go over this. But

07:17:30.621 --> 07:17:32.701
I really agree with the need for process

07:17:32.701 --> 07:17:34.781
reward models. I'm curious

07:17:34.781 --> 07:17:36.961
if you've seen or aware of like,

07:17:37.521 --> 07:17:39.681
interesting or promising approaches

07:17:39.681 --> 07:17:41.739
to a general process for

07:17:41.739 --> 07:17:43.901
model you know, made for even

07:17:43.901 --> 07:17:44.640
non verifiable

07:17:45.921 --> 07:17:46.361
domains?

07:17:48.681 --> 07:17:50.721
Yeah. I So I've read

07:17:50.721 --> 07:17:52.792
some of the earlier works on it, And I think

07:17:52.792 --> 07:17:54.952
initially, when we worked on this and we're trying to

07:17:54.952 --> 07:17:57.151
reproduce o one, we thought, okay.

07:17:57.151 --> 07:17:59.611
It has to be using process rewards. And so

07:18:00.251 --> 07:18:02.331
I read a lot of these and and tried to play

07:18:02.331 --> 07:18:04.511
with it, but then figured out it it's too

07:18:04.511 --> 07:18:06.542
complex. It probably not using

07:18:06.542 --> 07:18:08.771
that. I think it turns out that a one

07:18:08.771 --> 07:18:10.970
one the the follow ons, they

07:18:10.970 --> 07:18:12.909
didn't use any process rewards, I think.

07:18:13.171 --> 07:18:15.181
At least that's what most people say. So just the

07:18:15.421 --> 07:18:17.071
based on the the final result,

07:18:17.471 --> 07:18:18.032
And so,

07:18:19.481 --> 07:18:21.591
those earlier works be interesting. So I think,

07:18:21.591 --> 07:18:23.891
like, let's verify step by step in

07:18:23.891 --> 07:18:25.831
those papers. And then

07:18:26.291 --> 07:18:28.491
more recently, I think Deepseak

07:18:28.491 --> 07:18:30.521
had a paper on on proving

07:18:31.442 --> 07:18:33.942
on a prover model that used some form of process

07:18:34.241 --> 07:18:36.251
rewards. That could be interesting to

07:18:36.731 --> 07:18:37.951
to read in more detail.

07:18:39.111 --> 07:18:39.611
And

07:18:41.791 --> 07:18:43.871
other than that, I I think I haven't come across

07:18:43.871 --> 07:18:46.141
much and maybe I missed

07:18:46.141 --> 07:18:48.381
some things. But I think there we have still haven't figured

07:18:48.381 --> 07:18:50.601
out a the final recipe here, so I think it's

07:18:50.601 --> 07:18:53.011
exciting to to do more research on this.

07:18:53.821 --> 07:18:55.361
Awesome. Yeah. Thank you.

07:18:56.141 --> 07:18:58.521
Thanks. Hi.

07:18:58.521 --> 07:19:00.622
Thanks. Great call great talk.

07:19:01.081 --> 07:19:03.140
I wanted to ask do you think

07:19:03.140 --> 07:19:05.361
that the that the performance of the model

07:19:05.361 --> 07:19:07.409
that's trained on the

07:19:07.409 --> 07:19:09.607
SFT reasoning traces is upper

07:19:09.607 --> 07:19:11.671
bound by the performance of the

07:19:11.671 --> 07:19:13.801
model that generated the reasoning traces?

07:19:16.362 --> 07:19:18.731
Yeah. That's a great question.

07:19:19.261 --> 07:19:20.319
I think

07:19:26.712 --> 07:19:28.411
I think you can get better.

07:19:29.171 --> 07:19:31.331
So because imagine this

07:19:31.331 --> 07:19:32.681
scenario, you have a model that's

07:19:33.720 --> 07:19:35.819
really, really sometimes really

07:19:35.819 --> 07:19:37.521
good, but sometimes really bad.

07:19:37.831 --> 07:19:39.862
And so if you evaluate it on

07:19:39.862 --> 07:19:42.292
Amy, say it gets 50%, like, 50%

07:19:42.292 --> 07:19:44.380
of times, it's really good, and the other half,

07:19:44.380 --> 07:19:46.581
it's really bad. And then you distill

07:19:46.581 --> 07:19:48.761
from it, so you generate a lot of SFT

07:19:48.901 --> 07:19:51.341
samples from it. But you only take the 50%

07:19:51.341 --> 07:19:53.380
that are really good. And and more

07:19:53.380 --> 07:19:55.391
that look like those. And so now

07:19:55.551 --> 07:19:57.712
you get a distilled model that, say, it gets

07:19:57.712 --> 07:19:59.971
100% on Amy because it hasn't gotten

07:19:59.971 --> 07:20:02.060
any of the the bad samples. So it's always

07:20:02.060 --> 07:20:04.130
really, really good. So I

07:20:04.130 --> 07:20:06.171
think if you if you distill and you have some form

07:20:06.171 --> 07:20:07.751
of rejection sampling to

07:20:08.311 --> 07:20:10.351
only take out the good parts, you

07:20:10.351 --> 07:20:12.611
can get a model that will be better on benchmarks,

07:20:12.751 --> 07:20:14.829
I think. But if if

07:20:15.451 --> 07:20:17.641
it's if it's better in the limit, so in

07:20:17.641 --> 07:20:19.851
terms of say, pass it

07:20:19.851 --> 07:20:21.601
k, probably

07:20:22.541 --> 07:20:24.671
probably not. Right? So if you generate a lot with first

07:20:24.671 --> 07:20:26.811
model with the big model, then eventually,

07:20:26.811 --> 07:20:29.050
it'll get the right solution just that it's not at maybe

07:20:29.050 --> 07:20:31.532
its first solution. And so it's

07:20:31.872 --> 07:20:34.122
it's, pass it k where

07:20:34.122 --> 07:20:35.701
its coverage will still be

07:20:36.261 --> 07:20:38.341
really, really good. Like, it'll it'll solve all of those if you give

07:20:38.341 --> 07:20:40.421
it enough tries. And then the small

07:20:40.421 --> 07:20:42.611
model would be the same.

07:20:42.771 --> 07:20:44.931
So I think in terms of coverage, you probably won't get

07:20:44.931 --> 07:20:46.851
a better model. But in terms of pass it one,

07:20:47.011 --> 07:20:49.071
I think you can distill a better model. If

07:20:49.071 --> 07:20:51.331
that makes sense. That makes sense. Yep. Thanks.

07:20:54.051 --> 07:20:55.671
Hi, Nicholas. Nice to see you.

07:20:56.311 --> 07:20:57.441
So I have like

07:20:58.321 --> 07:21:00.321
two possible directions of scaling that

07:21:01.282 --> 07:21:02.501
might be, related to today's talk.

07:21:03.630 --> 07:21:05.811
One is the scaling of, external

07:21:05.951 --> 07:21:08.391
contacts So for example, Alpha Evolve

07:21:08.391 --> 07:21:10.401
has this, database, external

07:21:10.781 --> 07:21:12.081
database that could expand

07:21:12.911 --> 07:21:14.851
and, evolve during infrastime.

07:21:14.991 --> 07:21:16.911
So that could be, like, one

07:21:17.051 --> 07:21:19.111
direction of scaling, and the other one can

07:21:19.111 --> 07:21:21.331
be like, the scaling of

07:21:21.331 --> 07:21:22.937
recurrent depth where you,

07:21:23.730 --> 07:21:26.021
use the same like, the blocks

07:21:26.021 --> 07:21:28.071
of LLMs and then scale

07:21:28.071 --> 07:21:30.532
the depth of repetitions that it

07:21:30.751 --> 07:21:33.101
goes through. Doing inference. Like, for these

07:21:33.241 --> 07:21:35.521
two directions, what's your take?

07:21:36.720 --> 07:21:38.741
Yeah. Thanks for bringing them up. I should have

07:21:38.800 --> 07:21:40.871
meant I'm mostly focused on sequential scaling here

07:21:40.871 --> 07:21:43.231
and obviously missed those two and

07:21:43.231 --> 07:21:45.391
also missed, I think, parallel scaling, so just

07:21:45.391 --> 07:21:47.763
scaling up majority voting. So I think

07:21:47.763 --> 07:21:50.231
there's other exciting axes. I

07:21:50.291 --> 07:21:52.511
think just my personal

07:21:52.511 --> 07:21:54.601
thoughts, the alpha evolve

07:21:54.601 --> 07:21:56.381
approach, I think it's very exciting.

07:21:57.192 --> 07:21:58.731
But, it does seem

07:21:59.542 --> 07:22:01.481
to me that at some point, the model

07:22:02.077 --> 07:22:04.131
will would need to learn

07:22:04.292 --> 07:22:06.451
like, you need to update the parameters at

07:22:06.451 --> 07:22:08.796
some point. So just

07:22:08.903 --> 07:22:09.403
purely

07:22:10.951 --> 07:22:12.891
scaling that way. I'm not I'm not sure.

07:22:13.301 --> 07:22:15.391
It gets us all the way, but I think it does

07:22:15.631 --> 07:22:17.651
make a lot of sense And

07:22:17.651 --> 07:22:19.721
for recurrent depth, I

07:22:21.681 --> 07:22:23.621
I like solutions that are really simple.

07:22:24.011 --> 07:22:26.251
And so if you just see the model's

07:22:26.251 --> 07:22:28.499
reasoning, in text,

07:22:29.212 --> 07:22:31.591
then that's really nice and simple.

07:22:31.591 --> 07:22:33.372
You can just read it. And and personally,

07:22:34.081 --> 07:22:36.501
could imagine myself just thinking in in text

07:22:36.880 --> 07:22:38.891
and I think I would be would

07:22:39.136 --> 07:22:41.023
be just as

07:22:41.241 --> 07:22:43.391
it could solve similar problems. I'm not sure

07:22:44.431 --> 07:22:46.861
how much a latent reasoning I'm I'm personally

07:22:46.861 --> 07:22:48.989
doing versus just thinking in text in

07:22:48.989 --> 07:22:51.021
my head. So I'm not sure we we need

07:22:51.021 --> 07:22:53.101
the recurrent depth approach. I I think it

07:22:53.101 --> 07:22:55.191
it could be that, like, both of us could lead us

07:22:55.191 --> 07:22:56.980
to the to the final solution.

07:22:58.051 --> 07:23:00.151
Either one is fine. And then maybe the text one

07:23:00.391 --> 07:23:02.431
is a bit nicer because you can read it

07:23:02.431 --> 07:23:04.601
and it's a bit simpler. But maybe

07:23:04.601 --> 07:23:06.681
the work in-depth approach will will win out in the

07:23:06.681 --> 07:23:09.060
end. I think, yeah, both are very promising.

07:23:10.021 --> 07:23:12.271
Okay. Thank you so much. Yeah.

07:23:12.591 --> 07:23:14.671
Thanks. Thanks for all the question, and

07:23:14.671 --> 07:23:15.890
thanks, big SaaS again.

07:23:16.801 --> 07:23:18.821
For his wonderful talk. Yeah. Let's

07:23:19.462 --> 07:23:20.361
sense together.

07:23:24.001 --> 07:23:26.310
Yeah. Thanks, Victor.

07:23:26.371 --> 07:23:28.390
And then let's move on to

07:23:28.681 --> 07:23:30.341
next part. Our next is

07:23:30.901 --> 07:23:33.221
two oral presentation. First one

07:23:33.221 --> 07:23:34.621
is oral paper talk

07:23:35.501 --> 07:23:37.751
when listening meets it slow. We

07:23:38.051 --> 07:23:39.681
welcome our two

07:23:40.411 --> 07:23:41.310
oral speaker.

07:23:46.761 --> 07:23:47.521
I see.

07:23:59.111 --> 07:24:01.451
Okay. Hi, everyone. My name is

07:24:01.511 --> 07:24:03.531
Yijing Yuzhang, and I'm master student

07:24:04.741 --> 07:24:07.001
in computer science at UIUC, and

07:24:07.060 --> 07:24:08.531
also a PhD applicant

07:24:09.491 --> 07:24:11.640
this year. And, also, I would like to introduce Yifan,

07:24:11.640 --> 07:24:13.816
a PhD student at UIUC.

07:24:14.471 --> 07:24:16.551
Today, today, we would like to

07:24:16.551 --> 07:24:18.671
present our paper when

07:24:18.671 --> 07:24:19.431
reasoning Mason's laws.

07:24:21.911 --> 07:24:23.671
And first, I would like to introduce,

07:24:24.171 --> 07:24:26.291
our motivation for studying the Risen

07:24:26.291 --> 07:24:28.343
laws. I'm actually

07:24:28.401 --> 07:24:30.481
inspired by the physical laws in

07:24:30.481 --> 07:24:32.542
our natural world. As we

07:24:32.542 --> 07:24:34.311
know from classical to modern physics,

07:24:34.631 --> 07:24:37.051
physical laws have shaped our world,

07:24:37.112 --> 07:24:39.191
as we know it can allow us to form

07:24:39.191 --> 07:24:41.462
fundamentally understand predict,

07:24:41.462 --> 07:24:43.171
and transform our natural world.

07:24:43.571 --> 07:24:45.751
So maybe a question

07:24:45.811 --> 07:24:47.991
could be, should AI have its

07:24:47.991 --> 07:24:49.631
own laws and, specifically,

07:24:50.411 --> 07:24:51.311
if there exist

07:24:52.421 --> 07:24:54.499
recent laws for LOM so we can

07:24:54.499 --> 07:24:56.462
better understand and

07:24:56.523 --> 07:24:58.061
improve their reasoning capabilities.

07:25:00.071 --> 07:25:02.202
And in

07:25:02.202 --> 07:25:04.362
our paper, we will primarily focus on

07:25:04.362 --> 07:25:05.711
the recent process

07:25:06.591 --> 07:25:08.441
and generation of the large RASING models.

07:25:09.161 --> 07:25:11.401
And despite the stronger performance in

07:25:11.401 --> 07:25:13.101
solving complex problems,

07:25:13.421 --> 07:25:15.661
there exist a a challenge

07:25:15.661 --> 07:25:17.651
that even powerful

07:25:17.711 --> 07:25:19.890
LMs will exhibit abnormal,

07:25:20.201 --> 07:25:22.069
behaviors that deviate from,

07:25:22.310 --> 07:25:24.581
typical human reasoning patterns. And

07:25:24.581 --> 07:25:26.841
here is a illustrative example deep

07:25:26.841 --> 07:25:28.890
seated r one, we can see in

07:25:28.890 --> 07:25:31.361
this example, the DeepSig

07:25:31.421 --> 07:25:33.781
r one will tend to generate like,

07:25:34.021 --> 07:25:35.661
300 more recent tokens.

07:25:36.221 --> 07:25:38.421
Than its competitor question. But

07:25:38.561 --> 07:25:40.101
also with a lower accuracy,

07:25:41.451 --> 07:25:43.061
on this simple sub question.

07:25:43.942 --> 07:25:46.023
So this mismatch with human reasoning

07:25:46.023 --> 07:25:48.001
will reduce an abnormal

07:25:48.060 --> 07:25:50.311
reasoning patterns present in

07:25:50.311 --> 07:25:52.319
current LRM. And

07:25:52.319 --> 07:25:53.771
I also would like to

07:25:54.891 --> 07:25:57.051
explain the potential reasons

07:25:57.051 --> 07:25:58.471
I couldn't account for that.

07:25:59.151 --> 07:26:01.491
Because our researchers will generally

07:26:01.951 --> 07:26:03.561
overlook the high viral

07:26:05.501 --> 07:26:07.593
data during the training phase. So

07:26:07.593 --> 07:26:09.631
it will lead to inefficient allocation of

07:26:09.631 --> 07:26:12.112
computation either underthinking

07:26:12.491 --> 07:26:13.231
or overthinking.

07:26:15.551 --> 07:26:17.411
So in view of this challenge,

07:26:17.591 --> 07:26:19.531
our first research question

07:26:19.831 --> 07:26:22.281
can be can we theoretically formalize

07:26:22.421 --> 07:26:24.241
the model reasoning to ensure,

07:26:25.021 --> 07:26:27.341
desirable behavior? So we present

07:26:28.041 --> 07:26:30.131
Law of Reasoning which is a

07:26:30.131 --> 07:26:32.631
unified framework that systematically formalizes

07:26:32.851 --> 07:26:34.921
the relationship between the complex

07:26:35.321 --> 07:26:37.341
complexity as well as the model reasoning

07:26:37.341 --> 07:26:37.841
behaviors.

07:26:39.771 --> 07:26:41.931
And And in the LOA framework, we will

07:26:41.931 --> 07:26:43.791
focus on two key concepts

07:26:44.692 --> 07:26:46.921
reasoning compute and accuracy. And we

07:26:46.921 --> 07:26:49.141
will present the central compute law

07:26:49.542 --> 07:26:51.001
as as well as the complementary

07:26:51.702 --> 07:26:53.771
accuracy law. So before we

07:26:53.771 --> 07:26:55.804
talk about these two laws, there

07:26:55.804 --> 07:26:57.381
are three key concepts here.

07:26:57.942 --> 07:26:59.962
So complexity, which is ideal

07:27:00.023 --> 07:27:02.112
definition, are the

07:27:02.112 --> 07:27:04.192
compute. And in our paper, we

07:27:04.192 --> 07:27:06.281
use the expecting

07:27:06.281 --> 07:27:07.861
number of token as

07:27:08.792 --> 07:27:10.872
and accuracy, which is intuitive to

07:27:10.872 --> 07:27:11.372
understand.

07:27:13.441 --> 07:27:15.601
So let's come to our first law,

07:27:15.601 --> 07:27:17.630
the compute law. Oh, here

07:27:17.630 --> 07:27:19.941
is a general hypothesis without

07:27:20.400 --> 07:27:22.265
any constraints that I assume

07:27:22.411 --> 07:27:24.558
the reasoning compute has

07:27:24.561 --> 07:27:26.706
has a linear relationship with

07:27:26.706 --> 07:27:28.952
the In another words,

07:27:29.251 --> 07:27:31.561
the reading model allocates its reasoning

07:27:31.861 --> 07:27:34.011
computing efficiently The amount of compute is

07:27:34.391 --> 07:27:36.091
expected to scale proportionally

07:27:36.631 --> 07:27:37.452
with complexity.

07:27:39.441 --> 07:27:41.681
As the complexity is difficult to measure

07:27:41.681 --> 07:27:44.032
in practice, so we introduced

07:27:44.251 --> 07:27:46.191
two trackable properties. Monotonicity

07:27:46.571 --> 07:27:48.730
as well as the compositionality as

07:27:48.730 --> 07:27:50.911
proxy. To studying our

07:27:50.911 --> 07:27:52.970
compute law. And for

07:27:52.970 --> 07:27:55.311
the compositionality, we assume that

07:27:55.551 --> 07:27:57.712
the compute is monotonically non

07:27:57.712 --> 07:27:59.411
decreasing with complexity

07:27:59.891 --> 07:28:01.671
and it should be easy to understand

07:28:01.971 --> 07:28:04.122
because more complex questions

07:28:04.181 --> 07:28:05.601
naturally require more reasoning.

07:28:06.321 --> 07:28:08.661
So here, we also present an example

07:28:08.971 --> 07:28:11.151
For the first question, we need to perform

07:28:11.212 --> 07:28:13.311
one metric operation. And second,

07:28:13.891 --> 07:28:16.051
has one more operation. So ideally,

07:28:16.511 --> 07:28:18.591
t one should, t two should

07:28:18.591 --> 07:28:20.121
be larger than t one.

07:28:22.560 --> 07:28:24.948
And for the compositionality, if

07:28:24.948 --> 07:28:26.995
x one and x two

07:28:26.995 --> 07:28:29.042
are independent, their answer to

07:28:29.042 --> 07:28:30.542
question should exhibit

07:28:31.513 --> 07:28:33.532
the additive compute. And I also,

07:28:34.611 --> 07:28:36.079
have an example here

07:28:36.933 --> 07:28:39.171
For the first for the first question,

07:28:39.171 --> 07:28:41.101
we need to calculate the n and send

07:28:41.421 --> 07:28:43.501
question, the 10 digits, and also the

07:28:43.501 --> 07:28:45.361
concept question. So

07:28:47.401 --> 07:28:49.491
ideally, the thinking token length

07:28:49.491 --> 07:28:51.941
of the computer question should be approximately

07:28:52.321 --> 07:28:54.531
equals to t one plus t two.

07:28:55.890 --> 07:28:58.211
And here is overview of our compute's

07:28:58.211 --> 07:29:00.651
law. A linear relationship

07:29:01.032 --> 07:29:03.121
hypothesis and two trackable properties

07:29:03.501 --> 07:29:05.531
as proxy. Which

07:29:05.531 --> 07:29:07.591
is similar to the accuracy

07:29:07.890 --> 07:29:10.091
law. And for our accuracy law, we also

07:29:10.091 --> 07:29:12.271
have a general hypothesis that

07:29:13.561 --> 07:29:15.581
the overall accuracy is expected to,

07:29:16.921 --> 07:29:19.081
decrease exponentially with complexity.

07:29:19.541 --> 07:29:21.880
And it also, for the optimal

07:29:22.181 --> 07:29:24.212
reasoning model, it will satisfy

07:29:24.271 --> 07:29:25.571
two fundamental properties.

07:29:26.331 --> 07:29:28.191
Monotonicity and the compositionality,

07:29:28.491 --> 07:29:29.791
which is very similar.

07:29:32.231 --> 07:29:34.471
So when we have have our comprehensive

07:29:34.771 --> 07:29:36.792
framework, how can we evaluate

07:29:36.851 --> 07:29:38.853
whether car or whether

07:29:38.911 --> 07:29:40.991
popular ORMs will follow these proposed

07:29:40.991 --> 07:29:43.111
principles. So in response,

07:29:43.111 --> 07:29:45.532
we present a comprehensive

07:29:45.831 --> 07:29:47.831
benchmark that examine

07:29:48.211 --> 07:29:50.511
monotonicity and compositionality in our ends.

07:29:50.591 --> 07:29:52.451
And let's even introduce a benchmark.

07:29:54.331 --> 07:29:56.111
Thank you. So as for

07:29:56.751 --> 07:29:58.991
Lori Mono, we first ask the questions

07:29:58.991 --> 07:30:01.311
from four domains, Mass science,

07:30:01.311 --> 07:30:03.331
language, and code. Then for each

07:30:03.731 --> 07:30:06.231
seed question, we expanded it into a series of variants,

07:30:06.542 --> 07:30:08.962
with strictly increasing complexity. For example,

07:30:09.263 --> 07:30:11.306
variants were here requires to

07:30:11.306 --> 07:30:13.593
do the matrix operation once, variances

07:30:13.731 --> 07:30:15.989
two twice, and variances thirty thirty

07:30:15.989 --> 07:30:18.251
times, we recorded the Spearman correlation coefficient

07:30:18.310 --> 07:30:19.611
between the variant index

07:30:20.341 --> 07:30:22.501
and the quantity we care, namely the reasoning

07:30:22.501 --> 07:30:23.880
compute and the log accuracy.

07:30:25.091 --> 07:30:27.513
As for LORI Compose, it's actually more straightforward.

07:30:27.981 --> 07:30:30.061
Using Math 500, we randomly sample

07:30:30.061 --> 07:30:32.161
two questions and the concatenated them using

07:30:32.161 --> 07:30:34.241
a predefined template, asking the large

07:30:34.241 --> 07:30:36.341
reasoning model to answer them in order.

07:30:36.641 --> 07:30:38.683
Now that we can get to the sub questions and

07:30:38.683 --> 07:30:40.581
the competitive questions, ideally,

07:30:41.161 --> 07:30:42.721
if conversationality holds,

07:30:43.282 --> 07:30:45.622
this absolute difference should be close to zero.

07:30:45.981 --> 07:30:48.001
To account for the unit difference,

07:30:48.041 --> 07:30:50.281
we use the normalized MAD

07:30:50.281 --> 07:30:51.451
as the final metric.

07:30:53.771 --> 07:30:55.711
Now comes to the benchmark results.

07:30:56.191 --> 07:30:58.211
Nearly all large reasoning models exhibit

07:30:58.271 --> 07:31:00.531
a strong positive correlation between reasoning

07:31:01.042 --> 07:31:03.421
compute and the variant index. The only exception

07:31:03.480 --> 07:31:05.640
is DeepSeq. Our one, there's still q

07:31:05.640 --> 07:31:07.721
one one dot five b, which demonstrates

07:31:07.861 --> 07:31:09.881
unexpected patterns in sound domains.

07:31:10.521 --> 07:31:11.681
Namely language and code.

07:31:13.441 --> 07:31:15.671
In If an our reasoning model adheres

07:31:15.671 --> 07:31:17.921
to the compositionality law, most

07:31:17.921 --> 07:31:20.081
points would align closely with the y equals x

07:31:20.081 --> 07:31:22.111
line. In practice, however, the

07:31:22.111 --> 07:31:24.202
majority of points deviate substantially.

07:31:24.901 --> 07:31:26.981
Also, you can observe that for larger models,

07:31:26.981 --> 07:31:29.201
they don't necessarily show better compositionality.

07:31:30.331 --> 07:31:31.961
Suggesting that size is not

07:31:32.521 --> 07:31:33.471
the only factor here.

07:31:34.751 --> 07:31:36.911
So this now that we have the conclusion that

07:31:36.911 --> 07:31:38.351
current large reasoning models

07:31:38.991 --> 07:31:41.311
generally satisfy more than ethnicity, but failed

07:31:41.311 --> 07:31:42.451
to exhibit conversationality,

07:31:43.591 --> 07:31:45.995
So there's enforcing conversationality can

07:31:46.372 --> 07:31:48.551
further improve the general reason capacities.

07:31:48.991 --> 07:31:51.471
So we want to investigate this question as our

07:31:51.471 --> 07:31:53.631
third research question. So we introduced

07:31:54.011 --> 07:31:55.911
SFT Compound, a simple yet effective

07:31:56.391 --> 07:31:58.811
supervised by tuning master to provoke the

07:31:58.872 --> 07:32:00.851
composition conversational behavior in large

07:32:00.991 --> 07:32:03.261
reasoning models. Specifically,

07:32:03.321 --> 07:32:05.581
we just used the same way we construct

07:32:06.141 --> 07:32:08.301
construct our low recompile. We have

07:32:08.301 --> 07:32:10.481
the subcautions and their competitive versions.

07:32:10.991 --> 07:32:12.691
And for each question, we do

07:32:13.071 --> 07:32:15.391
multiple rollouts. Specifically, we do

07:32:15.952 --> 07:32:18.211
k model outputs from a large reasoning model.

07:32:18.371 --> 07:32:20.481
And finally, we select a combination that fit.

07:32:20.641 --> 07:32:22.181
Fast to satisfy the conversationality

07:32:22.801 --> 07:32:25.021
condition. You can understand that this is a very, very

07:32:25.161 --> 07:32:27.263
simple method. And finally, we

07:32:27.263 --> 07:32:29.763
just performed SFT on the selected robots.

07:32:30.021 --> 07:32:31.961
That's our very simple SFT component.

07:32:32.361 --> 07:32:33.749
And our stream should

07:32:34.401 --> 07:32:36.111
introduce the experimental results.

07:32:37.630 --> 07:32:39.650
Yeah. So to

07:32:39.650 --> 07:32:41.909
empirically evaluate our SFT

07:32:41.970 --> 07:32:44.331
Compose, our aims

07:32:44.331 --> 07:32:46.281
to address two research questions.

07:32:47.730 --> 07:32:49.970
And first is, does acetone perform

07:32:49.970 --> 07:32:52.391
effectively in enforce the

07:32:52.391 --> 07:32:54.461
compositionality? So I'm finding

07:32:54.461 --> 07:32:56.282
is the compositionality

07:32:56.741 --> 07:32:58.821
of the recent compute can be improved

07:32:58.821 --> 07:33:01.061
with such simple fine

07:33:01.061 --> 07:33:03.301
tuning approach, which is obvious in

07:33:03.301 --> 07:33:05.319
our in our figure,

07:33:05.319 --> 07:33:06.060
which demonstrates

07:33:07.771 --> 07:33:09.721
deep r one one point five

07:33:10.661 --> 07:33:12.560
b significant significantly improves.

07:33:14.761 --> 07:33:16.841
And the second part, I think, is the

07:33:16.841 --> 07:33:18.441
most important part.

07:33:19.001 --> 07:33:21.341
Thus, that enforcing compositionality

07:33:21.800 --> 07:33:23.771
can lead to general

07:33:23.831 --> 07:33:25.851
reasoning better general reasoning

07:33:25.911 --> 07:33:28.171
capabilities. So we find

07:33:28.171 --> 07:33:29.951
that enforcing it can

07:33:30.751 --> 07:33:32.999
can actually lead to better reasoning

07:33:32.999 --> 07:33:35.130
capabilities. Across different math and

07:33:35.130 --> 07:33:36.191
science benchmarks.

07:33:37.489 --> 07:33:39.681
In we also add a control

07:33:39.681 --> 07:33:41.801
baseline here, SF And

07:33:41.801 --> 07:33:44.121
the only difference between the SFT and

07:33:44.121 --> 07:33:45.741
SFT Compose is performing

07:33:46.231 --> 07:33:48.311
a uniform sample or selecting the

07:33:48.311 --> 07:33:50.551
combinations that best satisfies the

07:33:50.551 --> 07:33:52.452
com. Compositionality.

07:33:53.612 --> 07:33:55.151
So from our result,

07:33:55.991 --> 07:33:58.231
SFT can outperform the SFT in

07:33:58.231 --> 07:34:00.271
all cases. And showing that these

07:34:00.271 --> 07:34:02.281
gains are not just from this from

07:34:02.501 --> 07:34:04.581
a stronger model, but from better

07:34:04.581 --> 07:34:06.621
compliance with compositionality.

07:34:08.230 --> 07:34:09.971
And finally, I will like to

07:34:10.692 --> 07:34:12.471
like to discuss an interesting,

07:34:13.171 --> 07:34:14.791
synergistic effects in our

07:34:15.611 --> 07:34:17.851
experiments. Because we find that

07:34:17.851 --> 07:34:20.220
our model can yield broader improvement

07:34:20.220 --> 07:34:22.161
across different properties and

07:34:22.380 --> 07:34:24.851
laws For example, enforcing compositionality

07:34:25.391 --> 07:34:27.651
in, recent compute can

07:34:27.651 --> 07:34:29.891
improve its monotonicity. And

07:34:30.032 --> 07:34:31.901
also, enforcing compositionality

07:34:32.361 --> 07:34:34.541
in reasoning compute can improve compositionality

07:34:35.841 --> 07:34:36.421
in accuracy.

07:34:39.131 --> 07:34:41.071
So finally, as a comprehensive

07:34:41.692 --> 07:34:42.751
study from theoretical

07:34:43.900 --> 07:34:45.980
hypothesis, we advanced the

07:34:45.980 --> 07:34:48.311
theoretical perspective grounding

07:34:48.421 --> 07:34:50.381
human reasoning for understanding

07:34:50.531 --> 07:34:52.569
and improve the reasoning in

07:34:52.630 --> 07:34:54.581
current ORMs. And it also

07:34:55.542 --> 07:34:57.561
let us to think about how

07:34:57.622 --> 07:34:59.681
structural properties can lead to improved

07:34:59.681 --> 07:35:01.581
general reasoning beyond just

07:35:01.937 --> 07:35:04.050
scaling or scaling data

07:35:04.050 --> 07:35:06.181
or the model size. So we hope

07:35:06.181 --> 07:35:08.442
Lori can inspire more potential strategies

07:35:08.861 --> 07:35:11.101
that guides models towards their

07:35:11.101 --> 07:35:13.241
optimal paradigm of thinking. And

07:35:13.241 --> 07:35:15.273
here is our paper our

07:35:15.273 --> 07:35:17.341
website, and GitHub. And

07:35:17.501 --> 07:35:18.741
thank you for your listening.

07:35:28.911 --> 07:35:30.911
Amazing work. Thank you.

07:35:31.391 --> 07:35:33.891
For the presentation. I have a question on

07:35:34.301 --> 07:35:36.191
the two models you showed that

07:35:36.511 --> 07:35:38.741
doesn't conform this law.

07:35:39.411 --> 07:35:40.901
Any insights on why?

07:35:41.542 --> 07:35:43.622
Oh, or what two models

07:35:43.622 --> 07:35:45.641
didn't? I I think you mentioned

07:35:46.301 --> 07:35:48.542
but that page went through super fast. I'm not

07:35:48.542 --> 07:35:50.291
entirely sure that's what you mean.

07:35:51.952 --> 07:35:54.112
You said there are two, like, one math model,

07:35:54.112 --> 07:35:55.421
one coding model that

07:35:57.101 --> 07:35:59.011
where this law doesn't We're

07:35:59.231 --> 07:36:01.101
Yeah. I think this this page This one?

07:36:02.151 --> 07:36:02.651
Go

07:36:04.239 --> 07:36:06.671
Yes. You bring the monotonicity

07:36:07.691 --> 07:36:09.841
of 1.5 b. There are two

07:36:09.841 --> 07:36:12.061
models that are set in Well, that's probably another page. You you

07:36:12.061 --> 07:36:14.251
you said that

07:36:14.251 --> 07:36:16.491
there are oh, yeah. Current failed

07:36:16.651 --> 07:36:18.532
to exhibit.

07:36:21.622 --> 07:36:22.122
Yeah.

07:36:23.741 --> 07:36:25.511
Oh, any insights on why?

07:36:26.151 --> 07:36:28.311
Oh, so you mean why

07:36:28.311 --> 07:36:30.091
they failed to accept the conversationality?

07:36:30.701 --> 07:36:33.091
Yeah. Actually, it's explained

07:36:33.091 --> 07:36:35.141
a little bit before before

07:36:35.521 --> 07:36:37.401
this before our framework.

07:36:37.561 --> 07:36:39.851
I think it typically can

07:36:39.851 --> 07:36:41.021
be accounts that

07:36:42.773 --> 07:36:44.851
researchers also like, will collect

07:36:44.851 --> 07:36:46.551
data during their training phase.

07:36:47.051 --> 07:36:49.311
Do well or heurously curate

07:36:49.372 --> 07:36:50.751
by the human annotators,

07:36:51.501 --> 07:36:53.951
or generated through online rollouts

07:36:54.041 --> 07:36:55.971
So it really constrained

07:36:56.431 --> 07:36:57.331
by explicit rules.

07:36:58.791 --> 07:37:00.871
So, I think that can come

07:37:00.871 --> 07:37:02.991
for that because for

07:37:02.991 --> 07:37:05.111
simple problems or the different

07:37:05.111 --> 07:37:07.151
problems, human didn't aware that

07:37:07.151 --> 07:37:09.441
the difference and use like, very

07:37:10.191 --> 07:37:12.542
similar like, generation

07:37:12.921 --> 07:37:14.622
forces two different questions.

07:37:15.651 --> 07:37:17.952
So I I think that's why it will

07:37:17.952 --> 07:37:19.411
all fail to exhibit the compositionality.

07:37:20.231 --> 07:37:21.471
Yeah. Got it. Thank you.

07:37:23.231 --> 07:37:25.273
Hi. Great work, by the

07:37:25.273 --> 07:37:27.042
way. So for your SFT

07:37:27.531 --> 07:37:29.691
and SFT combo comparison where

07:37:29.691 --> 07:37:31.962
you show that compositionality

07:37:32.501 --> 07:37:34.711
helps in reasoning. Right? Yep. Can

07:37:34.711 --> 07:37:36.730
you is the data matched?

07:37:37.321 --> 07:37:39.501
Like, is the number of data points matched?

07:37:39.641 --> 07:37:40.581
Yeah. They are

07:37:41.941 --> 07:37:43.991
same. The only difference is the SFT

07:37:43.991 --> 07:37:45.952
is perform the uniform sampling,

07:37:46.692 --> 07:37:48.761
and the SFTP compose selecting

07:37:48.761 --> 07:37:50.921
the data that the best set is the

07:37:51.641 --> 07:37:53.611
condition compositionality. Data

07:37:53.941 --> 07:37:56.031
other set and

07:37:56.031 --> 07:37:58.051
datasets size. Is totally the same.

07:37:58.051 --> 07:38:00.141
Got it. So thousand points. One is

07:38:00.381 --> 07:38:02.542
random selection, and one is selecting

07:38:02.542 --> 07:38:04.451
based on your Yeah. You can see here,

07:38:04.611 --> 07:38:06.763
the rep the

07:38:06.763 --> 07:38:09.081
mock as red box is satisfied

07:38:09.621 --> 07:38:11.980
with the compositionality. But for the

07:38:11.980 --> 07:38:14.131
SFT component, I will uniform sample

07:38:14.131 --> 07:38:16.171
from others. Yeah. Thank

07:38:16.550 --> 07:38:16.981
you.

07:38:25.531 --> 07:38:27.871
Thanks for the oral paper presentation.

07:38:28.741 --> 07:38:31.021
And thanks. Please come.

07:38:31.437 --> 07:38:33.481
For yeah.

07:38:33.481 --> 07:38:35.701
Here we welcome our next oral paper talk.

07:38:36.739 --> 07:38:38.281
Inflow Agent System Optimization

07:38:38.980 --> 07:38:40.581
for efficient planning and

07:38:41.041 --> 07:38:42.001
tool use Yes.

07:38:43.131 --> 07:38:44.381
Let me help you

07:38:52.081 --> 07:38:53.351
Put in for 20

07:39:08.921 --> 07:39:11.013
Alright. First, it would

07:39:11.013 --> 07:39:13.513
be our great honor to be selected as

07:39:14.671 --> 07:39:17.051
So I'm pan I'm Stanford

07:39:17.267 --> 07:39:18.731
University. This is

07:39:19.372 --> 07:39:21.341
Jofo and Houxiang who work with

07:39:21.901 --> 07:39:24.251
us this summer. So today, we

07:39:24.251 --> 07:39:26.273
are very, glad

07:39:26.273 --> 07:39:28.531
to present our recent work agent flow.

07:39:28.851 --> 07:39:30.319
In the flow agent system

07:39:30.911 --> 07:39:33.131
optimization effective planning and to

07:39:33.131 --> 07:39:35.372
use So this work

07:39:35.431 --> 07:39:37.051
was partially supported by

07:39:38.061 --> 07:39:40.101
AI Lab, high renaissance

07:39:42.071 --> 07:39:43.050
officer of Naval

07:39:44.263 --> 07:39:45.121
research and cast.

07:39:46.560 --> 07:39:48.751
So to power empower a Lange

07:39:48.751 --> 07:39:51.171
model with specialized skills, one

07:39:51.231 --> 07:39:52.231
common line of work.

07:39:53.271 --> 07:39:55.532
Was to is to augment language model with internal

07:39:55.591 --> 07:39:57.362
tools like web search,

07:39:57.821 --> 07:40:00.021
Python encoder for knowledge retrieval, and

07:40:00.421 --> 07:40:02.661
precise computation. So basically, there are

07:40:02.661 --> 07:40:04.872
two paradigms So first one is

07:40:04.931 --> 07:40:06.961
a large net model agent.

07:40:07.942 --> 07:40:10.122
So typically, an agent

07:40:10.122 --> 07:40:11.421
consists of a planner,

07:40:12.560 --> 07:40:14.521
for making decisions, the next

07:40:14.835 --> 07:40:17.001
steps. And tool, tool

07:40:17.001 --> 07:40:19.301
set to power

07:40:19.702 --> 07:40:21.401
spatial skills and memory

07:40:21.782 --> 07:40:24.211
to store intermediate results.

07:40:25.091 --> 07:40:27.112
So so recent

07:40:27.112 --> 07:40:29.292
work has extended reinforcement learning

07:40:29.481 --> 07:40:31.501
in this field with rewards

07:40:31.917 --> 07:40:33.911
to to guide the

07:40:33.931 --> 07:40:36.271
learning model, learn how to use tools

07:40:36.361 --> 07:40:38.751
and when to use tools by

07:40:38.871 --> 07:40:40.941
interleaving reasoning and a

07:40:40.941 --> 07:40:43.031
generation. However, there are some

07:40:43.031 --> 07:40:44.810
limitations for this kind of paradigm.

07:40:45.311 --> 07:40:47.561
So first, as you can see, they are

07:40:47.561 --> 07:40:49.901
just one single engine model to do the planning,

07:40:50.601 --> 07:40:52.781
execution, verify and the final solution

07:40:52.861 --> 07:40:54.821
generation So it introduced

07:40:54.880 --> 07:40:56.691
a lot of, overhead

07:40:57.331 --> 07:40:59.361
our single model. So it may perform

07:40:59.361 --> 07:41:00.371
very bad.

07:41:01.409 --> 07:41:03.191
Bad in, downstream task,

07:41:03.611 --> 07:41:05.761
or long context reasoning task

07:41:05.761 --> 07:41:07.751
or, the the

07:41:07.991 --> 07:41:10.271
scenarios that require much more turn

07:41:10.569 --> 07:41:12.691
reasoning or compressed reasoning.

07:41:14.130 --> 07:41:16.560
So with that, so

07:41:16.560 --> 07:41:18.781
built up built out there is another paradigm that

07:41:19.261 --> 07:41:21.591
that is much higher my tech agent assist terms.

07:41:22.231 --> 07:41:23.971
So are usually,

07:41:24.351 --> 07:41:26.611
built on top of a single language model,

07:41:27.097 --> 07:41:29.023
agents. So with that, you can

07:41:29.321 --> 07:41:31.661
see there are multiple agents, and each agent

07:41:32.122 --> 07:41:34.421
may work on just one

07:41:34.421 --> 07:41:36.601
role or one skill for and

07:41:37.401 --> 07:41:39.661
the different agents can collaborate in our

07:41:40.231 --> 07:41:42.391
sequential way, in a hierarchical way,

07:41:42.391 --> 07:41:43.851
or customized way.

07:41:44.661 --> 07:41:45.720
So with so here,

07:41:47.263 --> 07:41:49.372
the structure of these methods. So you

07:41:49.372 --> 07:41:51.872
can see there are different modules. They can work sequentially

07:41:52.251 --> 07:41:54.622
or Yammer. Structured, complex

07:41:55.001 --> 07:41:57.421
way. So with that, you can see that they can compose

07:41:57.901 --> 07:42:00.151
they can compose very contest

07:42:00.151 --> 07:42:02.431
test complex test into multiple

07:42:02.521 --> 07:42:04.550
steps. And they can solve the

07:42:04.550 --> 07:42:06.651
problem step by step. So it's

07:42:06.651 --> 07:42:08.921
more powerful So you can see

07:42:08.921 --> 07:42:11.051
that these systems have shown the mark

07:42:11.051 --> 07:42:12.806
for capabilities in

07:42:13.651 --> 07:42:15.749
many complex tasks, So for example, you

07:42:15.749 --> 07:42:18.041
can see deep search by OpenAI.

07:42:18.261 --> 07:42:20.421
So by integrating different tools

07:42:20.421 --> 07:42:21.411
like web search,

07:42:23.470 --> 07:42:25.331
and other specialized tools, they

07:42:25.811 --> 07:42:27.971
can do can integrate resources

07:42:27.971 --> 07:42:30.069
from different websites and

07:42:30.069 --> 07:42:32.431
have a very comprehensive solutions

07:42:32.751 --> 07:42:34.991
And more recently, you can see these

07:42:34.991 --> 07:42:37.171
paradigms have been applied in scientific

07:42:37.311 --> 07:42:39.421
discovery, For example, you

07:42:39.421 --> 07:42:41.603
can use, developed magic agent system

07:42:41.901 --> 07:42:43.737
to discover new antibodies.

07:42:44.221 --> 07:42:46.271
And more risk like virtual lab. And

07:42:46.271 --> 07:42:48.331
more recently there are BioMini devices

07:42:48.871 --> 07:42:51.216
So it's, general purpose biomedical

07:42:51.216 --> 07:42:52.781
AI agent that can

07:42:53.741 --> 07:42:56.241
support, hundreds of tools, databases,

07:42:56.461 --> 07:42:58.781
and softwares and can do a lot

07:42:58.781 --> 07:43:00.531
of scientific discovery in

07:43:00.851 --> 07:43:02.792
medical domains. But

07:43:03.372 --> 07:43:05.612
although this, agent assist system has

07:43:05.612 --> 07:43:07.781
shown a marked up performance, you can see they

07:43:07.781 --> 07:43:10.202
are still very fragile. In various

07:43:10.421 --> 07:43:12.362
scenarios. For example, they may make mistakes,

07:43:14.183 --> 07:43:16.683
For example, the system might might be very fragile

07:43:17.481 --> 07:43:19.791
and cannot not very reliable

07:43:19.791 --> 07:43:21.171
in different scenarios.

07:43:21.811 --> 07:43:24.011
Scenarios. So and there are sometimes there

07:43:24.011 --> 07:43:25.630
are limitations in the communication

07:43:26.031 --> 07:43:27.140
across different

07:43:28.872 --> 07:43:30.841
modules or systems, Sometimes,

07:43:31.001 --> 07:43:33.161
it's very hard to verify if the

07:43:33.221 --> 07:43:34.761
intermediate results are good or not.

07:43:35.661 --> 07:43:37.841
So with that, we propose agent flow.

07:43:38.626 --> 07:43:40.651
Well first in

07:43:40.651 --> 07:43:42.351
the flow of multi agent systems.

07:43:42.712 --> 07:43:45.032
You can see our, system is

07:43:45.032 --> 07:43:47.131
very complex. But it's very

07:43:47.351 --> 07:43:49.431
well structured. We can support

07:43:49.701 --> 07:43:51.792
diverse tools, like Google

07:43:51.792 --> 07:43:53.811
Search, Python encoder, Wikipedia search,

07:43:53.911 --> 07:43:56.362
and more. There are four

07:43:56.421 --> 07:43:58.451
specialized agents like planner,

07:43:58.591 --> 07:44:00.601
executor, verify, and the generator.

07:44:00.841 --> 07:44:03.321
And each is responsible for one specific

07:44:04.861 --> 07:44:07.021
function. For example, planner, the planner

07:44:07.021 --> 07:44:08.791
can generate the overall plan,

07:44:09.351 --> 07:44:11.771
for the complete task and decide how

07:44:12.069 --> 07:44:14.411
to break down these complete tasks into multiple

07:44:14.411 --> 07:44:16.292
steps. As a curator,

07:44:16.671 --> 07:44:18.372
we convert each step into

07:44:20.631 --> 07:44:22.792
concrete tool calling with the sub goal, and

07:44:22.792 --> 07:44:24.811
we will have to verify to verify

07:44:25.201 --> 07:44:27.281
if the intermediate results are good

07:44:27.281 --> 07:44:29.371
enough or it requires

07:44:29.371 --> 07:44:31.441
additional tool cost to make

07:44:31.441 --> 07:44:33.181
the the task solvable.

07:44:33.661 --> 07:44:35.901
Finally, we have a generator to summarize all

07:44:35.901 --> 07:44:38.041
the trajectory in

07:44:38.041 --> 07:44:40.391
the memory and generate final solution through

07:44:40.391 --> 07:44:42.331
original query. So with that, we have,

07:44:42.411 --> 07:44:44.273
involving memory. It stores

07:44:44.731 --> 07:44:46.782
it stores intermediate results, to

07:44:46.782 --> 07:44:48.942
make the communication across different

07:44:48.942 --> 07:44:50.481
agents more smoothly.

07:44:52.201 --> 07:44:54.601
So we can take, the we can

07:44:54.601 --> 07:44:56.941
take moving into more technical details

07:44:56.941 --> 07:44:59.031
about our agent flow system. So

07:44:59.031 --> 07:45:01.111
you can see there are multiple terms giving the

07:45:01.111 --> 07:45:03.181
one query and to a state

07:45:04.141 --> 07:45:06.111
it may perform much turned

07:45:06.591 --> 07:45:08.841
more turns to solve problems. So for the first term,

07:45:09.241 --> 07:45:11.376
it caused a planner agent to

07:45:11.376 --> 07:45:13.611
generate the action for each

07:45:13.611 --> 07:45:14.911
step. Then the executor

07:45:15.881 --> 07:45:17.611
converts this texture

07:45:18.721 --> 07:45:21.091
action into more executable way

07:45:22.051 --> 07:45:24.091
giving the tool selector tool and the sub

07:45:24.411 --> 07:45:26.491
goal. And then we have to verify to verify

07:45:26.491 --> 07:45:28.491
if the kind of step is good

07:45:28.550 --> 07:45:31.001
or not. If it's complete, we we

07:45:31.201 --> 07:45:33.300
can generate to the final solution. If not,

07:45:33.481 --> 07:45:34.961
it will repeat this loop.

07:45:36.341 --> 07:45:38.513
In multiple turns. For example,

07:45:38.513 --> 07:45:40.431
we have a second turn, we have the

07:45:40.751 --> 07:45:42.831
the t turn. So at this time, the

07:45:42.831 --> 07:45:44.381
verifier is like, very

07:45:45.023 --> 07:45:46.531
verify checks that

07:45:48.151 --> 07:45:50.351
it has got, enough tools to

07:45:50.751 --> 07:45:52.801
solve the problem, and it's a good time to stop

07:45:53.042 --> 07:45:55.151
to generate the final solution. So

07:45:55.292 --> 07:45:57.331
you can see, our agent assist

07:45:57.331 --> 07:45:58.952
10 features multiple tools

07:45:59.536 --> 07:46:01.711
agents, we have the memory

07:46:01.711 --> 07:46:03.391
to make this process

07:46:03.931 --> 07:46:05.231
working very smoothly.

07:46:06.511 --> 07:46:08.591
And and in our system, we fine tune

07:46:08.591 --> 07:46:10.751
the planner because it's the one of the

07:46:10.751 --> 07:46:13.011
most important components in these systems.

07:46:14.501 --> 07:46:16.581
So, yet, I'd like to give more

07:46:16.581 --> 07:46:18.781
details about our system so we have a planner.

07:46:19.523 --> 07:46:21.622
So it takes the the query

07:46:21.691 --> 07:46:23.641
analysis, sub the original

07:46:24.101 --> 07:46:26.281
query, and all the skill all

07:46:26.281 --> 07:46:27.952
the tools in

07:46:28.331 --> 07:46:30.551
the toolset. To make the decision for

07:46:30.551 --> 07:46:32.631
each step. So it will generate the sub

07:46:32.631 --> 07:46:34.921
goal the select tool, along

07:46:34.921 --> 07:46:36.881
with the contest for this sub goal.

07:46:37.523 --> 07:46:39.862
And we and then we have an executor, so it takes

07:46:40.741 --> 07:46:42.821
sub goal and action generated

07:46:42.821 --> 07:46:44.881
by the planner and converts to

07:46:45.362 --> 07:46:47.701
execute way to call external

07:46:48.001 --> 07:46:50.261
tools And we have verified to

07:46:50.401 --> 07:46:52.531
verify if each step is complete or

07:46:52.531 --> 07:46:54.571
if there are any ambiguities needed

07:46:54.571 --> 07:46:56.827
to be addressed with further tool

07:46:57.216 --> 07:46:59.621
calling. And the older

07:46:59.711 --> 07:47:01.211
agent old agents

07:47:02.171 --> 07:47:04.441
have been updated in the memory, So

07:47:04.441 --> 07:47:06.640
information in in different terms

07:47:06.640 --> 07:47:08.811
can be shared So

07:47:09.622 --> 07:47:11.862
with that, we, our system can feature a

07:47:11.862 --> 07:47:13.989
diverse range of

07:47:13.989 --> 07:47:16.150
tools. First, we have base generators, so it

07:47:16.150 --> 07:47:18.371
is a very basic tool

07:47:18.371 --> 07:47:20.561
that can take any search query.

07:47:20.721 --> 07:47:23.001
And answer the questions step by step. We

07:47:23.001 --> 07:47:24.621
have present quarter, so it

07:47:25.480 --> 07:47:27.523
can generate precise

07:47:27.661 --> 07:47:29.551
compute perform very

07:47:29.691 --> 07:47:32.121
precise logistic reasoning or, mathematical

07:47:32.181 --> 07:47:34.271
reasoning. And we have a course

07:47:34.271 --> 07:47:36.361
search. To provide real

07:47:36.361 --> 07:47:38.311
time information from the Internet without

07:47:38.952 --> 07:47:41.380
with some citation support. We

07:47:41.380 --> 07:47:43.571
have Wikipedia search, so it can

07:47:43.971 --> 07:47:46.071
provide very specialized knowledge

07:47:47.231 --> 07:47:49.301
feature in the Wikipedia. This, too,

07:47:49.301 --> 07:47:51.401
might be very helpful if you are working on

07:47:52.042 --> 07:47:53.671
special domain like medical

07:47:54.431 --> 07:47:56.371
domain or scientific domain.

07:47:56.531 --> 07:47:58.702
And finally, we have web search. So

07:47:58.702 --> 07:48:01.042
it's very, well designed, giving the

07:48:01.191 --> 07:48:03.521
website. URL. It will

07:48:03.819 --> 07:48:06.001
understand the whole test and return

07:48:06.441 --> 07:48:08.811
a summarized information given your user query.

07:48:09.612 --> 07:48:12.032
So next, I'd like to take a concrete example

07:48:12.542 --> 07:48:13.921
give you more high level

07:48:14.571 --> 07:48:16.712
high level understanding. Of how

07:48:16.712 --> 07:48:18.961
agent flow works. So here's a

07:48:18.961 --> 07:48:21.361
question, computer, the track digit.

07:48:21.361 --> 07:48:23.417
For this trend

07:48:23.801 --> 07:48:25.581
hippocampus ID for this

07:48:26.141 --> 07:48:28.351
funky, would help if

07:48:28.351 --> 07:48:30.451
it were ISBN

07:48:30.757 --> 07:48:33.212
10 number. So basically,

07:48:33.351 --> 07:48:35.452
there are three steps First,

07:48:35.452 --> 07:48:37.212
you need to, maybe,

07:48:37.872 --> 07:48:40.231
have the Google search to return

07:48:40.231 --> 07:48:42.651
this TID for this funky

07:48:42.711 --> 07:48:44.900
order. Then you need to convert

07:48:45.121 --> 07:48:47.300
it into ISBN

07:48:47.300 --> 07:48:49.611
10 number. And finally, needed

07:48:49.749 --> 07:48:50.911
to compute this digital

07:48:53.391 --> 07:48:55.551
check digit. So here, we we show

07:48:55.551 --> 07:48:57.701
the our system without the hour

07:48:57.701 --> 07:48:59.961
of untuning. You can see, for the first step,

07:48:59.961 --> 07:49:01.971
it caused Wikipedia search and

07:49:02.532 --> 07:49:04.361
unfortunately, it returns the wrong information.

07:49:05.640 --> 07:49:07.031
But our system has

07:49:07.801 --> 07:49:09.471
improved or saved

07:49:10.511 --> 07:49:12.381
ability to refine the steps

07:49:12.862 --> 07:49:14.981
And this time, it turned to turned

07:49:15.050 --> 07:49:17.140
to Google search. And you

07:49:17.140 --> 07:49:19.231
can see it returned useful information

07:49:20.192 --> 07:49:22.202
like a TD. And

07:49:22.202 --> 07:49:24.319
for next step, it will cost PAS and

07:49:24.319 --> 07:49:26.131
Coder. But you can see for this step,

07:49:26.372 --> 07:49:28.671
there are some error for, the

07:49:28.671 --> 07:49:30.485
tool coding. Unfortunately, because

07:49:31.021 --> 07:49:32.951
the system has not been fine tuned,

07:49:33.111 --> 07:49:34.881
so you can see it has limited

07:49:36.001 --> 07:49:37.042
limited, planning,

07:49:38.353 --> 07:49:40.612
and making, the repeated arrows

07:49:40.711 --> 07:49:42.731
and unfortunately, it returns

07:49:43.292 --> 07:49:45.353
wrong solution. So it it

07:49:45.353 --> 07:49:47.513
motivates us to introducing our AI

07:49:47.513 --> 07:49:50.001
fine tuning that we can introduce in

07:49:50.801 --> 07:49:53.141
the rewards from the

07:49:53.141 --> 07:49:55.491
final solutions and encourage the engine model

07:49:55.811 --> 07:49:58.051
to enhance the engine model's abilities in planning

07:49:58.051 --> 07:50:00.140
and to usage. Next, I'd

07:50:00.140 --> 07:50:02.577
like to, hands on. Things to

07:50:02.581 --> 07:50:04.526
Joe Fong. And for

07:50:05.131 --> 07:50:07.451
our IO tuning and more experimental results.

07:50:11.319 --> 07:50:13.421
Yeah. I want to talk

07:50:13.421 --> 07:50:15.581
more about why and how

07:50:15.581 --> 07:50:17.711
to reinforcement

07:50:17.711 --> 07:50:18.931
learning to the planner.

07:50:21.471 --> 07:50:22.771
This technique, you know,

07:50:23.511 --> 07:50:24.951
for multi agent system,

07:50:26.111 --> 07:50:27.810
we can't do multi agent

07:50:28.191 --> 07:50:30.401
RL. On this agent

07:50:30.622 --> 07:50:32.551
system. Right? But, you

07:50:32.691 --> 07:50:34.763
know, this this is very resource consuming.

07:50:35.532 --> 07:50:37.821
And it's hard to optimize. Yeah.

07:50:38.061 --> 07:50:40.331
Hard to achieve the global optimize. Yeah.

07:50:41.202 --> 07:50:43.601
And meanwhile, according

07:50:43.821 --> 07:50:46.011
to our observation, in our

07:50:46.011 --> 07:50:47.691
journey system, or even for most

07:50:48.171 --> 07:50:50.251
system like CloudSearch, Cloud Code, or

07:50:50.251 --> 07:50:51.550
OpenID deep research,

07:50:53.021 --> 07:50:54.671
the, you know, sorry.

07:50:55.941 --> 07:50:58.021
Like, you know, Cloud Code, Open Air Deep

07:50:58.021 --> 07:51:00.051
research is the most critical agent is the

07:51:00.452 --> 07:51:02.671
planner. Since, you know, it's responsible for

07:51:02.671 --> 07:51:04.970
task you know, planning and scheduling for

07:51:04.970 --> 07:51:06.781
all subsequent agents.

07:51:07.261 --> 07:51:09.431
Okay. Therefore, it's the one that truly did

07:51:09.431 --> 07:51:11.621
training and the other agents can steal

07:51:11.621 --> 07:51:13.621
its cues instruction effectively.

07:51:14.181 --> 07:51:16.591
Without additional training. Okay. So

07:51:16.731 --> 07:51:18.962
how can we treat we can

07:51:18.962 --> 07:51:21.202
collect some data and then perform supervised flight

07:51:21.202 --> 07:51:23.261
tuning followed by you know,

07:51:23.261 --> 07:51:24.811
reinforcement learning and directly

07:51:25.292 --> 07:51:27.401
on the planner. This large model. Right?

07:51:27.401 --> 07:51:29.681
And then plug this print agent back into the

07:51:30.241 --> 07:51:32.101
system. This is very

07:51:32.481 --> 07:51:34.933
simple way, but this approach

07:51:34.933 --> 07:51:37.331
sounds promising. In practice, it usually doesn't

07:51:37.331 --> 07:51:39.442
work as if Since during

07:51:39.442 --> 07:51:41.603
our training, it cannot aware other

07:51:41.603 --> 07:51:43.901
agents' context. This will cause

07:51:43.961 --> 07:51:46.081
a significant misalignment

07:51:46.081 --> 07:51:47.761
between training and inference.

07:51:48.561 --> 07:51:50.821
Okay. So what's the best way to optimize

07:51:51.381 --> 07:51:53.292
Our method will directly

07:51:53.511 --> 07:51:55.501
optimize planner agent using the system

07:51:55.821 --> 07:51:58.121
in Online Fusion. To be more specific,

07:51:58.281 --> 07:52:00.441
we draw out the full agent flu

07:52:00.441 --> 07:52:02.921
system. Clicked, and filter the plan nurse.

07:52:02.931 --> 07:52:04.861
Actual trajectory of states.

07:52:05.421 --> 07:52:07.341
Action, and true events. It induces.

07:52:07.661 --> 07:52:09.681
And then use this online trajectory

07:52:09.741 --> 07:52:11.601
to update the planner on policy.

07:52:12.001 --> 07:52:13.991
Yeah. We will call this paradigm as

07:52:14.151 --> 07:52:16.122
in the flu reinforcement learning. Yeah. And,

07:52:16.681 --> 07:52:18.511
here's our formula.

07:52:20.511 --> 07:52:22.531
Yeah. Now we

07:52:22.531 --> 07:52:24.792
have a general idea on in the flow.

07:52:25.011 --> 07:52:27.171
But how to realize it. There are

07:52:27.171 --> 07:52:29.282
two main challenges for First,

07:52:29.282 --> 07:52:31.442
the rule generated by achieving this system are

07:52:31.442 --> 07:52:33.921
multi term. Which can cause model's context

07:52:34.061 --> 07:52:36.300
window to explode. Second, the the real

07:52:36.300 --> 07:52:38.640
world signal for a generic system are extremely

07:52:39.071 --> 07:52:41.481
sparse. To solve this, we propose flow

07:52:41.782 --> 07:52:43.810
GOPO. First, we transform the

07:52:43.810 --> 07:52:45.890
multi terreprocessing learning into a series

07:52:45.890 --> 07:52:48.071
of single terre update. At

07:52:48.071 --> 07:52:50.541
each term, the planner has access to the memory

07:52:50.581 --> 07:52:52.907
contents Then we compute

07:52:52.907 --> 07:52:55.201
a single and variable final outcome

07:52:55.201 --> 07:52:57.221
based reward, to the entire

07:52:57.221 --> 07:52:59.231
trajectory. And then we will put

07:53:00.151 --> 07:53:02.181
to every turn. Using

07:53:02.181 --> 07:53:04.381
this style, you

07:53:04.381 --> 07:53:06.471
know, coupled with group

07:53:07.131 --> 07:53:09.331
normalized advantages to stabilize training.

07:53:09.651 --> 07:53:11.891
Enable robust credits assignments, and

07:53:11.891 --> 07:53:13.891
allows the planner to, you know, if

07:53:14.131 --> 07:53:16.601
effective nonhorizons strategies. From

07:53:16.601 --> 07:53:18.741
Spass feedback. And next part, Haochang

07:53:18.741 --> 07:53:20.681
will introduce our experiment part.

07:53:21.720 --> 07:53:24.220
Yeah. Thanks, Shuo Fu. And

07:53:25.171 --> 07:53:27.601
let me give you a glance of our experiment results.

07:53:27.952 --> 07:53:30.433
And as we train our gensetec system or only

07:53:30.433 --> 07:53:32.571
mass data, mass hard and acoustic search

07:53:32.571 --> 07:53:34.591
search data, We test them on four

07:53:34.591 --> 07:53:36.571
domains, including mass, scientific,

07:53:37.452 --> 07:53:39.890
agentic search, dialects, and search domain.

07:53:40.081 --> 07:53:42.097
And overall, again, from the inference only

07:53:42.097 --> 07:53:45.011
agent flow based on 2,570,000,000

07:53:45.011 --> 07:53:46.231
instructs is reported

07:53:47.131 --> 07:53:49.301
here, and we can see that our flow GOPO tuning

07:53:49.301 --> 07:53:50.611
makes this work.

07:53:52.611 --> 07:53:54.351
And all domain sky is diverse.

07:53:54.831 --> 07:53:57.071
And as a result, compared to the larger,

07:53:57.071 --> 07:53:59.131
proprietary models such as four o,

07:53:59.131 --> 07:54:01.611
g p four, open source, Queen 2.5,

07:54:01.611 --> 07:54:03.621
and Billingstruct, holistic paradigms

07:54:03.621 --> 07:54:05.630
such as search r

07:54:05.730 --> 07:54:07.810
one, t r TIR, and training free agent

07:54:07.810 --> 07:54:09.031
take systems, such as autogenerate.

07:54:09.821 --> 07:54:12.121
Our tuned agent flow surpasses all these baselines.

07:54:13.001 --> 07:54:15.081
Which also prove proved that, oh, with

07:54:15.081 --> 07:54:17.561
the reinforcement learning of the genetic system, system,

07:54:17.591 --> 07:54:19.871
it works. And with further analysis,

07:54:20.671 --> 07:54:22.681
analysis, the distribution shift

07:54:22.681 --> 07:54:24.683
of the tool used on some of

07:54:24.683 --> 07:54:27.069
our testing. Datasets. Here, we found

07:54:27.611 --> 07:54:29.911
some interesting changes. The model can dynamically play

07:54:30.431 --> 07:54:32.691
explore a combo of, like, Wikipedia

07:54:32.751 --> 07:54:34.839
plus web search, that showed on my

07:54:34.839 --> 07:54:36.881
QA, the right side of picture.

07:54:37.101 --> 07:54:39.341
And, oh, they can first search the Wikipedia

07:54:39.341 --> 07:54:41.362
search and got the snippets, and

07:54:41.362 --> 07:54:43.542
then delve into one of the pages of the Wikipedia,

07:54:43.761 --> 07:54:45.821
and further delving to the information we need.

07:54:46.061 --> 07:54:48.061
And on some specific search task,

07:54:48.622 --> 07:54:50.631
such as the two Wiki. A huge leap

07:54:50.631 --> 07:54:53.023
in using Google Search which is

07:54:53.023 --> 07:54:55.291
much stronger than Wikipedia, can be

07:54:56.819 --> 07:54:58.441
automatically optimized by the planner model.

07:55:00.281 --> 07:55:02.319
Okay. And also our further analysis found

07:55:02.319 --> 07:55:04.341
that our paradigm can increase

07:55:04.821 --> 07:55:06.901
performance and mitigate error through calling with a

07:55:06.901 --> 07:55:08.751
larger maximum turn limit.

07:55:08.911 --> 07:55:11.071
And also, it can reach a better

07:55:11.071 --> 07:55:13.060
performance with an extent

07:55:13.341 --> 07:55:15.550
of the larger term limit.

07:55:17.371 --> 07:55:19.761
K. And furthermore, as

07:55:19.821 --> 07:55:22.161
our system design handmade handmade planner,

07:55:22.161 --> 07:55:24.371
to and tuned the output of only

07:55:24.371 --> 07:55:26.441
the planner, so the reward assignment can

07:55:26.441 --> 07:55:28.471
be only assigned into the part of the planning part

07:55:28.631 --> 07:55:30.491
which means that there's no need for gradual

07:55:30.872 --> 07:55:32.781
overland response aligning to

07:55:33.261 --> 07:55:34.881
GRP o one, such as the

07:55:35.442 --> 07:55:37.311
go to gradually longer and longer.

07:55:37.636 --> 07:55:39.900
Response. Which may also have a bridging

07:55:40.121 --> 07:55:42.211
on our training system, and other work all well.

07:55:42.211 --> 07:55:44.341
That in contrast, our planner

07:55:44.341 --> 07:55:45.961
only needs to sync properly.

07:55:46.437 --> 07:55:48.531
Like showing the picture of the left

07:55:48.531 --> 07:55:50.720
with the steps gradually

07:55:50.720 --> 07:55:53.141
longer response were not exceed

07:55:53.211 --> 07:55:55.241
that. Such as maybe into

07:55:56.862 --> 07:55:58.631
the 150

07:55:59.273 --> 07:56:01.251
output tokens, and then it will gradually

07:56:01.491 --> 07:56:03.880
get down. And furthermore, as compared

07:56:03.880 --> 07:56:05.931
to other holistic TRR reasoning models such as

07:56:05.931 --> 07:56:08.001
the two REL, our genetic system

07:56:08.001 --> 07:56:10.381
has efficient training. Process.

07:56:10.741 --> 07:56:12.981
And finally, test on different model sizes,

07:56:12.981 --> 07:56:15.091
such as 2.5, 3,000,000,000, and

07:56:15.091 --> 07:56:17.431
7,000,000,000, instruct. And, also, we have

07:56:17.571 --> 07:56:19.476
conducted also other performance

07:56:19.741 --> 07:56:21.961
of model families, such as lama.

07:56:21.961 --> 07:56:24.171
And also prove that our reinforcement

07:56:24.405 --> 07:56:26.461
learning process, JRPO,

07:56:26.701 --> 07:56:28.541
agentetic system, can work.

07:56:29.900 --> 07:56:32.051
And I can have a final Yeah.

07:56:32.051 --> 07:56:34.452
So finally, let me quickly wrap up a project.

07:56:34.452 --> 07:56:36.862
So we provide a lot of accessible

07:56:37.081 --> 07:56:39.091
resources including our

07:56:39.091 --> 07:56:40.731
code base, live demo,

07:56:41.111 --> 07:56:43.361
YouTube tutorial, and interactive Wiki

07:56:44.001 --> 07:56:46.001
documentation. So if you're feel free to

07:56:46.511 --> 07:56:48.801
resources. To know more details Or

07:56:48.881 --> 07:56:51.122
your future development. And we are very

07:56:51.122 --> 07:56:53.361
happy that our project has been

07:56:53.781 --> 07:56:55.881
drawn attention in both

07:56:55.881 --> 07:56:57.451
at industry and

07:56:58.301 --> 07:57:00.171
So thank you so much for having us.

07:57:00.651 --> 07:57:02.853
So and to know more details feel free to

07:57:02.853 --> 07:57:04.821
check our website. Thank you.

07:57:05.381 --> 07:57:06.011
Thank you.

07:57:11.551 --> 07:57:13.671
Yeah. Thanks so much for the oral paper

07:57:13.671 --> 07:57:15.911
talk. And

07:57:16.151 --> 07:57:17.931
we thank you again, and we welcome

07:57:18.231 --> 07:57:20.491
our next speak invited

07:57:20.550 --> 07:57:22.711
speaker. Quick question. Do we have a So

07:57:22.711 --> 07:57:24.900
so that we don't have time for question. You can

07:57:24.900 --> 07:57:27.061
divert it. You can directly ask them

07:57:27.526 --> 07:57:29.951
Yeah. Yeah. Thanks.

07:57:30.271 --> 07:57:32.691
Yeah. So we are inviting our next speaker,

07:57:33.781 --> 07:57:35.861
professor at CMU, and he will give

07:57:35.861 --> 07:57:38.251
us very interesting

07:57:38.251 --> 07:57:40.531
talk about about skilling.

07:57:40.771 --> 07:57:42.111
Test times. Yeah.

07:57:55.511 --> 07:57:57.671
Hi, everyone. I'm Betty.

07:57:57.671 --> 07:57:59.751
I'm assistant professor and CMU.

07:57:59.751 --> 07:58:01.130
Thanks for staying late.

07:58:02.001 --> 07:58:04.201
My talk. So you

07:58:04.201 --> 07:58:06.470
will be about beyond flops

07:58:06.470 --> 07:58:08.550
and opportunities and challenges of

07:58:08.550 --> 07:58:10.291
the test time scaling on more than

07:58:10.611 --> 07:58:12.771
hardware. So this will be a

07:58:12.771 --> 07:58:14.391
shorter version of our tutorial.

07:58:15.161 --> 07:58:17.241
On Tuesday. So, I will try

07:58:17.241 --> 07:58:19.470
to stay very high level. If you're interested

07:58:19.470 --> 07:58:20.851
in the details and analysis,

07:58:21.651 --> 07:58:24.051
feel free to watch our, two hour

07:58:24.051 --> 07:58:26.201
tutorial. This Tuesday

07:58:26.201 --> 07:58:28.480
too. So, let me get started

07:58:28.781 --> 07:58:30.851
on, reviewing all the

07:58:30.851 --> 07:58:33.081
scaling law the first stage

07:58:33.220 --> 07:58:35.541
of the selling law around 2017

07:58:35.541 --> 07:58:37.550
to 2020 is about the

07:58:37.550 --> 07:58:39.411
model parameters. People are excited

07:58:39.630 --> 07:58:41.731
about increasing the number of parameters,

07:58:41.731 --> 07:58:43.782
so we got more intelligence.

07:58:44.542 --> 07:58:46.622
And the second axis is around

07:58:46.622 --> 07:58:49.042
the data. Because lack of the scaling

07:58:49.151 --> 07:58:51.231
law study, we found that

07:58:51.712 --> 07:58:53.581
not only my parameters should be the x

07:58:54.221 --> 07:58:56.291
that's being scaled, but your data should

07:58:56.291 --> 07:58:58.542
also be scaled

07:58:58.683 --> 07:59:00.692
accordingly as well. And then

07:59:00.692 --> 07:59:02.461
we hit the CERT

07:59:03.499 --> 07:59:05.819
stage of the scaling, which is the context

07:59:05.819 --> 07:59:07.931
lens We see

07:59:07.931 --> 07:59:10.192
from 02/2000, 4,000,

07:59:10.841 --> 07:59:13.161
to like 1,000,002 contact

07:59:13.161 --> 07:59:15.261
lens days. So, we

07:59:15.261 --> 07:59:17.712
kind of see that this is the key

07:59:17.712 --> 07:59:19.581
enabler for all these kind of

07:59:19.961 --> 07:59:21.901
tasks on Asian tech LMS.

07:59:22.542 --> 07:59:24.811
Program automation. And now we have like super,

07:59:26.499 --> 07:59:28.441
long generation as well, which we enter

07:59:28.819 --> 07:59:30.871
the last era, like

07:59:30.871 --> 07:59:33.191
the most current era of test time

07:59:33.191 --> 07:59:33.691
scaling.

07:59:35.311 --> 07:59:37.371
We see this thinking models are released

07:59:37.931 --> 07:59:39.711
from, last year, October

07:59:40.721 --> 07:59:42.921
From, like, o one and deep

07:59:43.401 --> 07:59:45.291
think, cloud, and also

07:59:45.431 --> 07:59:47.351
the great open source models.

07:59:48.231 --> 07:59:49.931
Like r one, Quinn, and Kimmy, and series.

07:59:51.211 --> 07:59:52.872
And they're also widely

07:59:53.551 --> 07:59:55.501
deployed in different applications

07:59:56.141 --> 07:59:58.425
including the coding, math,

07:59:59.681 --> 08:00:01.771
trading, and search. So

08:00:01.771 --> 08:00:03.441
people are very excited about

08:00:03.841 --> 08:00:05.862
the performance of it. And,

08:00:06.023 --> 08:00:08.351
so the opportunity is currently,

08:00:08.411 --> 08:00:10.761
we kind of see that our LMs

08:00:10.761 --> 08:00:12.891
and agents and according to

08:00:12.891 --> 08:00:13.921
all the great talks,

08:00:14.911 --> 08:00:17.321
before me, they're kind of working. Right?

08:00:17.561 --> 08:00:19.811
The exciting part is we're no longer at

08:00:20.051 --> 08:00:22.212
this pre training of evaluating

08:00:22.353 --> 08:00:24.853
my perplexity or the token per second.

08:00:25.042 --> 08:00:27.202
We're now just focusing on the soft

08:00:27.202 --> 08:00:28.921
rate and also the

08:00:30.452 --> 08:00:32.691
task per second. So that is what we care

08:00:32.691 --> 08:00:35.021
about. However, the challenge is,

08:00:35.021 --> 08:00:37.111
we know that, for all these reasoning models,

08:00:37.671 --> 08:00:39.491
especially in cop with this

08:00:40.056 --> 08:00:42.220
Asians, So it has challenges in

08:00:42.220 --> 08:00:43.811
both training and

08:00:44.292 --> 08:00:46.271
inference. Inferencing in the sense of, like, we know that

08:00:47.452 --> 08:00:49.581
users are using all these agents, you

08:00:49.581 --> 08:00:51.591
have to wait for really long. If it's

08:00:51.831 --> 08:00:53.901
calling the tools, if it's ringing for

08:00:53.901 --> 08:00:55.970
really long. For the training part, we

08:00:55.970 --> 08:00:57.961
know that, all the great,

08:00:58.521 --> 08:01:00.221
software engineers and ML engineers

08:01:00.571 --> 08:01:02.461
are set up all this environment

08:01:02.701 --> 08:01:04.611
and, develop this task

08:01:05.251 --> 08:01:07.531
so that the models can, learn

08:01:07.531 --> 08:01:09.593
them and then mid training and

08:01:09.593 --> 08:01:11.605
all the rest are kind

08:01:11.605 --> 08:01:13.692
of, focusing on making the models

08:01:13.751 --> 08:01:15.851
actually usable and solve all the tasks.

08:01:16.731 --> 08:01:18.591
So the key part, we're gonna

08:01:18.841 --> 08:01:20.911
go through today in my talk is about

08:01:21.071 --> 08:01:22.622
how to improve the scalability

08:01:23.421 --> 08:01:25.581
of the test and compute for both of the training

08:01:25.581 --> 08:01:26.720
and, inference.

08:01:29.771 --> 08:01:31.871
So let's do a brief review

08:01:32.091 --> 08:01:34.121
on all the test time compute kind of

08:01:34.121 --> 08:01:36.341
strategies. So we have, like,

08:01:36.341 --> 08:01:38.343
sequential search, which is the low c

08:01:38.343 --> 08:01:40.691
o t everybody is familiar

08:01:40.691 --> 08:01:43.081
with. And we have the parallel

08:01:43.221 --> 08:01:45.271
search, which is known by Basil Finn,

08:01:45.511 --> 08:01:47.981
so you don't necessarily only do

08:01:48.041 --> 08:01:50.031
one trial especially it's

08:01:50.171 --> 08:01:52.131
extremely long. Although it has the benefit of

08:01:53.131 --> 08:01:55.631
it's doing some reflection and is enabling

08:01:55.853 --> 08:01:58.122
some stronger capabilities

08:01:58.181 --> 08:02:00.351
of the models. But it's also

08:02:00.351 --> 08:02:02.726
you can do it at, like, multiple times And

08:02:02.726 --> 08:02:04.282
with this verifier,

08:02:05.021 --> 08:02:07.331
as long as you are doing one of them correct,

08:02:07.971 --> 08:02:10.151
you are able to solve the problem. Right?

08:02:11.591 --> 08:02:13.612
And we also have more advanced

08:02:14.221 --> 08:02:15.601
strategies like tree search.

08:02:16.391 --> 08:02:18.571
So not only you can do independent search,

08:02:18.691 --> 08:02:19.651
you can have,

08:02:21.001 --> 08:02:23.261
building the tree, which is more scalable, and

08:02:23.261 --> 08:02:25.101
each of the branches interacting

08:02:25.319 --> 08:02:26.501
with, each other.

08:02:27.462 --> 08:02:29.781
So the outline for today is we're gonna

08:02:29.781 --> 08:02:31.511
do a introduction on

08:02:32.151 --> 08:02:34.421
the test and scaling laws several of them,

08:02:34.981 --> 08:02:37.401
and the the challenges on the modern hardware.

08:02:37.901 --> 08:02:40.221
And then we're gonna have like several lines

08:02:40.221 --> 08:02:42.720
of work The first line is the parallel reasoning.

08:02:42.781 --> 08:02:44.081
How to improve the scalability.

08:02:44.851 --> 08:02:47.111
And the efficient architecture, how to improve

08:02:48.071 --> 08:02:50.231
scalability. And finally, we're gonna

08:02:50.231 --> 08:02:52.319
talk about the

08:02:52.319 --> 08:02:54.251
applications of, Tessman's compute.

08:02:55.451 --> 08:02:57.701
So So let's first get to the

08:02:57.701 --> 08:02:59.462
first part. So

08:02:59.712 --> 08:03:01.792
we see like the scaling law

08:03:01.792 --> 08:03:03.951
analysis have like two exciting

08:03:03.951 --> 08:03:06.231
factors. The first one

08:03:06.231 --> 08:03:07.911
is from the Brown et al paper.

08:03:08.321 --> 08:03:10.351
That Besselfeld, of course,

08:03:10.691 --> 08:03:12.791
it will have the potential of

08:03:13.351 --> 08:03:15.511
increasing the capabilities of the model,

08:03:15.511 --> 08:03:17.931
but it's not guaranteed that you

08:03:18.230 --> 08:03:20.511
will continue growing the

08:03:20.511 --> 08:03:22.551
small model's ability kind of

08:03:22.551 --> 08:03:24.831
on par with the large model. So we're

08:03:24.831 --> 08:03:26.681
excited to see that this growth

08:03:27.155 --> 08:03:29.191
continues. Although not in a very

08:03:29.191 --> 08:03:30.491
scalable way, but it continues.

08:03:31.640 --> 08:03:33.661
In And then in the

08:03:33.661 --> 08:03:36.001
second scaling law paper

08:03:36.060 --> 08:03:36.321
on

08:03:38.612 --> 08:03:40.952
It is imposing some kind of new paradigm

08:03:41.171 --> 08:03:43.051
on Potentially, we can also

08:03:43.372 --> 08:03:45.711
add the infra time using a smaller model

08:03:46.271 --> 08:03:48.351
by doing, a 100 times

08:03:48.351 --> 08:03:50.781
more test and compute at the inverse

08:03:50.781 --> 08:03:53.013
time, and it's gonna match the large

08:03:53.013 --> 08:03:55.171
model's performance up by being more

08:03:55.171 --> 08:03:57.351
efficient, if we're doing this in

08:03:57.351 --> 08:03:58.691
a compute optimal way.

08:04:01.281 --> 08:04:03.761
But the question is, is

08:04:03.761 --> 08:04:06.023
the compute optimal? Actually

08:04:06.023 --> 08:04:07.847
the right way of And

08:04:09.091 --> 08:04:10.951
By computer, we mean the floating

08:04:11.171 --> 08:04:13.661
points for each of the computation

08:04:13.721 --> 08:04:15.521
of the models, However,

08:04:15.980 --> 08:04:18.021
if you're gonna take a look at the

08:04:18.261 --> 08:04:20.441
plot on the left, the x

08:04:20.441 --> 08:04:22.551
axis is the

08:04:22.551 --> 08:04:24.261
accuracy and, we're

08:04:24.861 --> 08:04:27.181
plotting the parallel frontier of the model

08:04:27.181 --> 08:04:29.121
selection, and, y axis

08:04:29.401 --> 08:04:31.321
is the latency where,

08:04:32.551 --> 08:04:34.650
the seconds. So you kind of see that

08:04:34.650 --> 08:04:36.591
we are contrasting two scaling

08:04:36.890 --> 08:04:38.731
laws, which use the compute optimal

08:04:40.171 --> 08:04:42.411
and the e flops which

08:04:42.411 --> 08:04:44.911
is the compute and memory access optimal.

08:04:45.151 --> 08:04:47.231
And you have large difference on the model

08:04:47.231 --> 08:04:49.191
selection. So,

08:04:49.431 --> 08:04:51.671
if you look at the line with the

08:04:51.671 --> 08:04:53.681
reddish color, it is

08:04:53.681 --> 08:04:55.701
trying this is the previous scaling

08:04:55.701 --> 08:04:57.981
law, which is the compute optimal. To

08:04:58.122 --> 08:05:00.141
reach accuracy of 70%,

08:05:00.641 --> 08:05:02.641
you want to select example,

08:05:03.591 --> 08:05:05.441
around like a four b model,

08:05:05.866 --> 08:05:08.081
However, if you're looking at

08:05:08.241 --> 08:05:10.101
our scaling log, which is the

08:05:10.901 --> 08:05:13.380
greenish color, it's gonna select the 14

08:05:13.380 --> 08:05:15.390
b model. So what do you

08:05:15.390 --> 08:05:17.241
mean by, like, selecting this

08:05:17.721 --> 08:05:19.681
model is, like, under the same cost

08:05:20.871 --> 08:05:22.711
under under the same accuracy of the task.

08:05:23.271 --> 08:05:25.441
What is the optimal cost if you

08:05:25.441 --> 08:05:27.681
select different models. You guys select a small

08:05:27.681 --> 08:05:29.741
model, with a lot of generated tokens

08:05:29.741 --> 08:05:31.871
and test them strategy. You can also

08:05:31.871 --> 08:05:33.331
select your large model

08:05:33.952 --> 08:05:36.032
with a smaller generated, number of

08:05:36.032 --> 08:05:38.091
tokens. They might result in the

08:05:38.091 --> 08:05:40.150
same cost but who is actually

08:05:40.150 --> 08:05:41.781
give you a better

08:05:43.980 --> 08:05:45.991
reduce the cost? Depending on

08:05:45.991 --> 08:05:48.421
your cost model. So if you're doing the

08:05:48.480 --> 08:05:50.571
computer optimal only, you're actually

08:05:50.571 --> 08:05:52.650
do a sub optimal selection. So that

08:05:52.650 --> 08:05:54.941
was the key takeaway

08:05:54.941 --> 08:05:57.261
from this slide. Basically,

08:05:57.640 --> 08:05:59.980
the high level idea is we overlook

08:06:00.487 --> 08:06:02.601
the critical memory access bottleneck

08:06:02.601 --> 08:06:04.641
introduced by the Ambridge time scaling

08:06:05.042 --> 08:06:06.901
strategies and overestimate

08:06:07.202 --> 08:06:08.801
the smaller model impact

08:06:09.763 --> 08:06:12.023
So let's take a closer look

08:06:12.161 --> 08:06:14.271
at what do I mean by EFOS and why this

08:06:14.271 --> 08:06:16.640
is in for the

08:06:16.640 --> 08:06:18.431
test time scaling on modern hardware.

08:06:19.951 --> 08:06:21.991
So eFlows is computed by

08:06:22.212 --> 08:06:24.251
the cost of the compute, plus

08:06:24.691 --> 08:06:26.069
the cost of the memory access,

08:06:27.081 --> 08:06:29.411
times the arithmetic intensity of your

08:06:29.971 --> 08:06:32.031
hardware. So on the left hand side is the

08:06:32.031 --> 08:06:34.491
GPUs, and right hand side is the TPUs.

08:06:34.941 --> 08:06:37.181
The number just shows you that

08:06:37.421 --> 08:06:39.821
when the newer generation of hardware being

08:06:39.821 --> 08:06:41.853
developed, is easier to

08:06:41.853 --> 08:06:44.273
improve your flops than your memory bandwidth.

08:06:44.721 --> 08:06:46.962
That's why your arithmetic intensity is

08:06:46.962 --> 08:06:49.362
actually going up So

08:06:49.631 --> 08:06:52.112
with the newer generation of the hardware, you actually

08:06:52.112 --> 08:06:54.121
want some kind of the workload that is

08:06:54.121 --> 08:06:56.301
more compute bound. Rather than

08:06:56.301 --> 08:06:58.391
the memory bound. And

08:06:58.391 --> 08:07:00.251
this is the reason why in the previous

08:07:01.212 --> 08:07:03.281
scaling law study, if you're using a

08:07:03.281 --> 08:07:05.281
different cost model, you're gonna

08:07:06.001 --> 08:07:08.421
you're gonna have a different optimal selections

08:07:08.480 --> 08:07:10.251
on what to use at the test time.

08:07:10.971 --> 08:07:12.811
And the high level bit of them

08:07:13.292 --> 08:07:15.372
next several slides about, like, the challenges and

08:07:15.372 --> 08:07:17.481
opportunities is the

08:07:17.481 --> 08:07:19.981
key is this memory access is very important

08:07:20.361 --> 08:07:22.691
because when you are generating a lot of

08:07:22.751 --> 08:07:24.981
tokens, in your KaaS admin pued, which

08:07:24.981 --> 08:07:26.991
you would do is increasing

08:07:27.130 --> 08:07:29.121
your memory access a lot.

08:07:29.601 --> 08:07:31.811
So that cost is much

08:07:32.031 --> 08:07:34.361
more than the flop if you do

08:07:34.361 --> 08:07:36.612
the evaluation in this way. So that's

08:07:36.612 --> 08:07:38.851
why, this talk is oriented about

08:07:39.411 --> 08:07:41.191
something beyond flops,

08:07:41.481 --> 08:07:43.681
the cost beyond flops, but the memory access.

08:07:45.361 --> 08:07:47.519
Let's see why for

08:07:47.519 --> 08:07:49.911
the test time strategies, when you have more tokens,

08:07:50.989 --> 08:07:53.211
your memory access increases.

08:07:53.211 --> 08:07:54.829
The reason why is

08:07:55.301 --> 08:07:56.773
if you make the analysis

08:07:57.511 --> 08:07:59.720
about the cost model, the high level

08:07:59.720 --> 08:08:01.631
idea is, your attention,

08:08:01.851 --> 08:08:03.231
kvCash accesses

08:08:04.091 --> 08:08:06.241
are gonna dominate. Your

08:08:06.241 --> 08:08:08.271
test time cost and

08:08:08.831 --> 08:08:11.241
remember that we're gonna have this

08:08:12.122 --> 08:08:14.221
software then task second. So

08:08:14.221 --> 08:08:16.451
basically, we're mainly looking at the cost

08:08:16.451 --> 08:08:18.451
and cannot be amortized

08:08:18.751 --> 08:08:20.862
by more users using it. If

08:08:20.862 --> 08:08:22.861
it's the memory accessory models,

08:08:23.101 --> 08:08:25.261
can see, okay. I'm just hosting one model.

08:08:25.261 --> 08:08:27.381
If a lot of users are using it, the

08:08:27.381 --> 08:08:29.721
cost is amortized. We're good. But

08:08:29.941 --> 08:08:32.301
if you're, but

08:08:32.521 --> 08:08:33.821
the cost of the kvCash

08:08:34.631 --> 08:08:36.581
cannot be amortized because the more users

08:08:37.292 --> 08:08:39.301
or more rightsize you are using it, this

08:08:39.301 --> 08:08:41.231
is an independent and you are introducing

08:08:41.371 --> 08:08:43.481
more memory access. So that's precisely

08:08:43.781 --> 08:08:45.921
the kind of the major bottleneck

08:08:45.921 --> 08:08:47.941
of the cost for, test that

08:08:47.941 --> 08:08:49.761
compute when you generate a lot of tokens.

08:08:52.101 --> 08:08:54.181
And this is some two, detailed

08:08:54.181 --> 08:08:56.417
analysis plots on

08:08:58.211 --> 08:09:00.311
how how serious this is in

08:09:00.311 --> 08:09:02.331
practice. On the right hand

08:09:02.331 --> 08:09:04.692
side, this is benchmark on

08:09:04.741 --> 08:09:06.451
h 200. And you see all these

08:09:06.841 --> 08:09:09.301
green bars is

08:09:09.361 --> 08:09:11.292
at, different models,

08:09:11.751 --> 08:09:13.731
different sequence generation lengths,

08:09:13.821 --> 08:09:15.971
and it's dominating your cost.

08:09:16.372 --> 08:09:18.641
And on the left hand side, this is

08:09:18.641 --> 08:09:20.261
saying that when you're generating tokens,

08:09:20.671 --> 08:09:22.962
are grows longer, your attention

08:09:23.581 --> 08:09:24.071
dominates more.

08:09:26.712 --> 08:09:28.561
So the loss, we just

08:09:29.202 --> 08:09:31.631
saw on the left hand side of this plot, now

08:09:31.991 --> 08:09:34.231
it makes sense. Right?

08:09:34.471 --> 08:09:36.971
So previously, to reach a certain accuracy,

08:09:38.691 --> 08:09:40.981
the compute optimal base

08:09:40.981 --> 08:09:42.841
selection, or select the small model

08:09:43.241 --> 08:09:45.321
with a lot of tokens because we thought the

08:09:45.321 --> 08:09:46.611
generated tokens are free.

08:09:47.650 --> 08:09:49.341
If you were to just compute the flops.

08:09:49.901 --> 08:09:52.111
But if you consider

08:09:52.111 --> 08:09:54.131
the memory accesses, you would rather

08:09:54.191 --> 08:09:56.331
select a larger model with a fewer

08:09:56.331 --> 08:09:58.581
generated token. To reach a

08:09:58.581 --> 08:10:00.821
certain accuracy. So that's why there's a major

08:10:00.821 --> 08:10:02.941
difference on if your cos model

08:10:02.941 --> 08:10:04.811
is different, what your

08:10:04.951 --> 08:10:06.061
scalar law will change.

08:10:07.981 --> 08:10:10.192
So basically, in summary, what we're

08:10:10.192 --> 08:10:12.101
seeing for this scaling

08:10:12.791 --> 08:10:14.811
law, if you if you

08:10:14.811 --> 08:10:16.851
consider the practical efficiency,

08:10:16.851 --> 08:10:18.931
the theory usually use

08:10:18.931 --> 08:10:21.111
flops, but reality is bounded

08:10:21.251 --> 08:10:23.300
by IO, Your memory

08:10:23.300 --> 08:10:25.701
bandwidth, because of the tension quadratic

08:10:25.921 --> 08:10:28.091
in rows, during the long

08:10:28.091 --> 08:10:30.311
generation, and deployment

08:10:30.372 --> 08:10:32.561
reality is like we're out of bandwidth instead of

08:10:32.561 --> 08:10:33.220
the compute.

08:10:35.201 --> 08:10:37.621
So let's see, like, what are the key challenges

08:10:38.036 --> 08:10:39.931
introduced by this,

08:10:40.151 --> 08:10:41.861
practical scaling

08:10:42.341 --> 08:10:43.921
analysis. The first one is

08:10:44.481 --> 08:10:46.211
course, is due to the

08:10:46.531 --> 08:10:48.810
loan COT, You will have, like,

08:10:48.810 --> 08:10:50.111
very low paralysism,

08:10:51.261 --> 08:10:53.341
for this model, so you have to generate one by

08:10:53.341 --> 08:10:55.531
one. And even you increase

08:10:55.531 --> 08:10:57.581
the number of GPUs or GPUs

08:10:57.581 --> 08:10:59.401
is not gonna help with this problem.

08:11:00.601 --> 08:11:02.681
So it's very slow. Second

08:11:02.681 --> 08:11:04.921
one is the memory wall. We're talking about

08:11:04.921 --> 08:11:06.581
this like k v cache.

08:11:07.621 --> 08:11:09.641
Memory excesses. And when

08:11:09.781 --> 08:11:11.452
your hardware generations,

08:11:11.991 --> 08:11:14.421
become latest, or more advanced,

08:11:14.541 --> 08:11:16.781
it's not gonna resolve your problem. Just make

08:11:16.781 --> 08:11:18.801
it even worse So you cannot

08:11:18.801 --> 08:11:20.371
leverage the high fluff.

08:11:20.931 --> 08:11:22.931
Of your newer generation of the hardware

08:11:23.491 --> 08:11:25.331
Instead, we're still bonded by the,

08:11:26.471 --> 08:11:28.931
bandwidth. And, you know, that bandwidth grows

08:11:29.980 --> 08:11:32.093
slower than the flops. And when

08:11:32.093 --> 08:11:33.231
you're generating tokens,

08:11:34.499 --> 08:11:36.761
grow, your attention

08:11:36.821 --> 08:11:38.441
will gonna be more dominating.

08:11:38.872 --> 08:11:40.952
And your asthmatic intensity is lower.

08:11:40.952 --> 08:11:43.221
So this is the reverse way of the

08:11:43.221 --> 08:11:45.321
hardware growth. And that's why this is

08:11:45.321 --> 08:11:47.261
preventing the test time compute

08:11:47.781 --> 08:11:49.031
strategy to be scalable.

08:11:50.800 --> 08:11:52.800
And this is just two slots to,

08:11:53.041 --> 08:11:55.071
two figures to explain

08:11:55.130 --> 08:11:57.381
this problem in details. The

08:11:57.381 --> 08:11:59.452
left one is, in the x axis, is

08:11:59.452 --> 08:12:00.961
our smacking intensity increases across

08:12:02.041 --> 08:12:04.081
hardware generations, and,

08:12:04.321 --> 08:12:06.741
it's just saying that when new generations your

08:12:07.381 --> 08:12:09.491
attention dominates more. And

08:12:09.491 --> 08:12:11.513
on the on the right hand side, it just says

08:12:11.513 --> 08:12:13.591
that that your MOE or that mixture of

08:12:13.591 --> 08:12:15.601
experts does not help, but

08:12:15.601 --> 08:12:17.361
actually exacerbate the problem.

08:12:19.121 --> 08:12:21.201
And there are several challenges. Usually, we're

08:12:21.201 --> 08:12:23.571
gonna deploy this custom compute. In,

08:12:25.311 --> 08:12:27.331
training time. Right? Like, the test time training.

08:12:27.811 --> 08:12:30.151
Which is you're gonna collect all these trajectories

08:12:30.371 --> 08:12:32.301
and rewards and go back

08:12:32.761 --> 08:12:34.951
using reinforcement learning and train a model

08:12:34.951 --> 08:12:37.301
and update your model. And that's imposing

08:12:37.361 --> 08:12:39.566
additional challenges beyond

08:12:39.566 --> 08:12:41.571
the first two we we talked about. Is

08:12:41.571 --> 08:12:43.531
the irregular workflow. Because

08:12:43.829 --> 08:12:46.291
we see this like agent workflow and task.

08:12:46.451 --> 08:12:48.531
They're like not, the

08:12:48.531 --> 08:12:50.841
same time they're gonna end, so their

08:12:50.980 --> 08:12:53.381
irregular workload is copying is

08:12:53.381 --> 08:12:55.811
causing all these bubbles in the scalability

08:12:55.951 --> 08:12:57.421
of the test time training.

08:12:59.581 --> 08:13:01.691
So So next, we're gonna talk

08:13:01.691 --> 08:13:03.501
about, like, three lines of work.

08:13:04.221 --> 08:13:06.421
Kind of, resolving these

08:13:06.421 --> 08:13:08.741
problems. Including the parallel

08:13:08.801 --> 08:13:10.962
reasoning, efficient architecture, and we're

08:13:10.962 --> 08:13:13.291
gonna see that in the application of

08:13:13.291 --> 08:13:15.471
RL, how to scale reinforcement learning

08:13:15.471 --> 08:13:15.971
training.

08:13:18.452 --> 08:13:19.641
So So the parallelization

08:13:20.501 --> 08:13:22.821
can enable the scalability. It's never

08:13:22.821 --> 08:13:25.241
new. And it has been leveraged

08:13:25.241 --> 08:13:27.641
with all this system great work and map reduce

08:13:27.641 --> 08:13:29.692
Spark, and all the rest. And let's

08:13:29.692 --> 08:13:31.471
see that the analysis

08:13:31.851 --> 08:13:34.032
quad on the right is saying that if you

08:13:34.032 --> 08:13:35.921
do your parallelization properly,

08:13:36.081 --> 08:13:38.281
for example, previously we would

08:13:38.281 --> 08:13:40.951
do the analysis of, let's say, 32

08:13:40.951 --> 08:13:42.961
k generation. You'll be the green

08:13:42.961 --> 08:13:45.301
line. Which means that your newer generation

08:13:45.301 --> 08:13:47.381
of hardware appears. You are gonna be more and

08:13:47.381 --> 08:13:49.591
more memory bond and it's not

08:13:49.591 --> 08:13:51.811
scalable. But if you're

08:13:51.811 --> 08:13:54.131
just generating the same number of tokens,

08:13:54.131 --> 08:13:56.542
let's say, 32 k, but with in

08:13:56.542 --> 08:13:58.791
parallel. Like, a group of eight.

08:13:59.542 --> 08:14:01.731
This will resolve the problem make

08:14:01.731 --> 08:14:03.591
the model much better or discolipically

08:14:04.131 --> 08:14:06.191
much better. But the question is

08:14:06.191 --> 08:14:07.541
like, how do we do that? Right?

08:14:08.739 --> 08:14:10.819
So basically, there are like several ways

08:14:10.819 --> 08:14:11.961
of doing the parallel

08:14:14.300 --> 08:14:16.800
generation. So compared to the autograft

08:14:17.021 --> 08:14:19.042
modeling, first of all, the diffusion

08:14:19.181 --> 08:14:20.531
language model, hopefully.

08:14:21.901 --> 08:14:24.111
Can resolve this problem a little bit better.

08:14:24.351 --> 08:14:26.651
And, I understand that because

08:14:26.651 --> 08:14:28.911
of like the discrete deficient problems,

08:14:29.292 --> 08:14:31.631
you need more iterations to actually

08:14:31.692 --> 08:14:33.720
do it. And currently, we

08:14:33.720 --> 08:14:35.800
haven't found the best way of

08:14:35.800 --> 08:14:38.050
dealing with this But with the algorithm

08:14:38.050 --> 08:14:40.202
make, improvement, hope on

08:14:40.202 --> 08:14:42.541
that. And especially with different

08:14:42.541 --> 08:14:44.671
hardwares, for example, TPUs. They

08:14:44.671 --> 08:14:46.371
will actually have higher

08:14:47.442 --> 08:14:49.601
automatic intensity, so even more compute.

08:14:50.081 --> 08:14:51.380
As long as it's parallelizable,

08:14:52.211 --> 08:14:54.431
is very scalable. So that's why

08:14:54.591 --> 08:14:56.371
probably Google is very excited about

08:14:57.331 --> 08:14:59.461
this direction. And, so

08:14:59.461 --> 08:15:01.211
we're gonna talk about today is

08:15:02.730 --> 08:15:05.150
the multiverse model, which was presented in this conference by amazing students.

08:15:07.221 --> 08:15:09.081
So the is

08:15:09.841 --> 08:15:12.261
your language model is secretly decides how

08:15:12.319 --> 08:15:14.511
to merge generation.

08:15:14.661 --> 08:15:16.765
And our task is to design a

08:15:16.765 --> 08:15:18.952
task to train the model in such a way

08:15:18.952 --> 08:15:21.161
that they can internalize this,

08:15:21.640 --> 08:15:23.452
capability. Basically,

08:15:24.671 --> 08:15:26.900
we did analysis on the

08:15:26.900 --> 08:15:28.980
recent traces of the r one

08:15:28.980 --> 08:15:30.712
model. When it came out.

08:15:31.361 --> 08:15:33.391
So, the question is, how

08:15:33.391 --> 08:15:35.601
many times or how many of these

08:15:35.601 --> 08:15:37.931
trajectories are actually doing parallelizable

08:15:38.551 --> 08:15:40.731
computation even they're doing the

08:15:40.792 --> 08:15:42.981
generation autoregressively. And

08:15:42.981 --> 08:15:45.941
surprise Not surprisingly, actually, 99%

08:15:45.941 --> 08:15:47.831
of this trajectory have

08:15:48.551 --> 08:15:50.891
some kind of, parallelizable branches.

08:15:51.001 --> 08:15:53.341
Even is is autograftly generation.

08:15:53.481 --> 08:15:55.441
They're independent. Locally.

08:15:56.321 --> 08:15:58.341
So there are several of them, including

08:15:58.480 --> 08:16:00.622
the one that you executing

08:16:00.921 --> 08:16:03.091
one task. But there are several steps you can

08:16:03.091 --> 08:16:05.542
do it in parallel. Or it

08:16:05.921 --> 08:16:08.069
can be your executing one

08:16:08.351 --> 08:16:10.050
task, but you wanna just try different

08:16:10.721 --> 08:16:13.079
variables or different trials. But anything

08:16:13.220 --> 08:16:14.631
that you can parallel

08:16:15.831 --> 08:16:17.881
analyze it. It exists. In

08:16:17.881 --> 08:16:19.601
this regional traces naturally.

08:16:20.481 --> 08:16:22.141
So the next question is,

08:16:23.023 --> 08:16:25.103
do model know they should parallel or

08:16:25.103 --> 08:16:27.601
merge? Unfortunately, they don't.

08:16:27.851 --> 08:16:29.641
So we can do the probing test,

08:16:30.511 --> 08:16:32.751
at the point that we know that

08:16:32.751 --> 08:16:34.941
it should parallel. Or

08:16:34.956 --> 08:16:36.741
it should kind of merge.

08:16:36.981 --> 08:16:39.031
Generations. It seems like the

08:16:39.031 --> 08:16:41.111
model does is not aware of that due to

08:16:41.111 --> 08:16:42.781
the autoregressive training. Of course.

08:16:43.581 --> 08:16:45.361
So how to do that?

08:16:45.781 --> 08:16:47.861
We are machine learning people. Of course, we'll do

08:16:47.861 --> 08:16:50.041
the design on the task to

08:16:50.101 --> 08:16:52.212
enforce this parallel

08:16:52.212 --> 08:16:54.531
structure into it. So what we do is,

08:16:54.771 --> 08:16:56.751
we are rerate rewriting

08:16:57.077 --> 08:16:58.761
the traces with,

08:16:59.321 --> 08:17:01.491
Gemini model. And maintaining the

08:17:01.550 --> 08:17:03.782
quality and content and also the the

08:17:03.782 --> 08:17:06.181
lens of it, but insert keywords

08:17:06.391 --> 08:17:08.191
at the moment when

08:17:08.431 --> 08:17:10.811
it should do the parallel generation, the

08:17:10.811 --> 08:17:12.862
moment it should merge, And it works

08:17:12.862 --> 08:17:15.311
pretty well. If you just change

08:17:15.311 --> 08:17:17.380
your engine and also

08:17:17.380 --> 08:17:19.601
your attention mask because you wanna

08:17:19.601 --> 08:17:22.091
see that, in a intermediate

08:17:22.470 --> 08:17:24.561
or, like, the local parallel parts,

08:17:24.881 --> 08:17:26.591
the two parallel sentence should not

08:17:26.911 --> 08:17:29.051
be able to each other. So you have to change your

08:17:29.051 --> 08:17:31.351
attention mask and your inference

08:17:31.471 --> 08:17:33.601
engine. According to that. So

08:17:34.220 --> 08:17:36.351
and the results show that on

08:17:36.591 --> 08:17:38.911
different, type of task, and a b 24,

08:17:38.911 --> 08:17:41.031
25, 500, and

08:17:41.511 --> 08:17:43.911
GBQA. Doing very

08:17:43.911 --> 08:17:46.042
well on the

08:17:46.261 --> 08:17:47.721
reasoning task. And here's

08:17:49.212 --> 08:17:51.061
demo how exactly we're doing that.

08:17:51.702 --> 08:17:53.791
So here's a generated summary of the test.

08:17:53.951 --> 08:17:55.569
And it's doing like three parallelizations.

08:17:56.441 --> 08:17:58.941
And it knows that you had to write a conclusion.

08:17:59.581 --> 08:18:01.901
And then it starts to branch out again.

08:18:01.901 --> 08:18:03.601
And there's a branch in the branch,

08:18:04.362 --> 08:18:06.061
and those branch in the merge.

08:18:06.981 --> 08:18:09.441
And conclusion. So this is exactly

08:18:09.441 --> 08:18:10.911
how it is a operating.

08:18:11.901 --> 08:18:14.161
So let's get to the second part.

08:18:14.981 --> 08:18:17.042
Which will will seek how we can

08:18:17.042 --> 08:18:18.901
incorporate the efficient model

08:18:19.122 --> 08:18:21.361
architectures to resolve the scalability

08:18:21.901 --> 08:18:24.181
problems of, the test time compute on the

08:18:24.421 --> 08:18:26.381
hardware as well. So

08:18:27.391 --> 08:18:29.551
So recall that we're seeing this memory

08:18:29.551 --> 08:18:31.591
wall due to the memory access of

08:18:31.591 --> 08:18:33.801
the kvCash, So the other

08:18:33.861 --> 08:18:36.021
way, like, the since the attention cost

08:18:36.021 --> 08:18:38.122
dominating, if we can reduce

08:18:38.122 --> 08:18:40.191
that, that naturally

08:18:40.831 --> 08:18:42.531
help with this bottleneck.

08:18:43.372 --> 08:18:45.532
Right? So in fact, there are a lot of hybrid

08:18:45.532 --> 08:18:47.751
models where far as attention model

08:18:47.751 --> 08:18:49.911
came out, with this open source,

08:18:50.231 --> 08:18:52.371
models, like mini max m one,

08:18:53.011 --> 08:18:54.551
and point next thinking.

08:18:55.192 --> 08:18:57.300
Or deep seek 3.2 using

08:18:57.300 --> 08:18:59.531
the SPARS retention.

08:18:59.731 --> 08:19:01.891
And our lab has been working on Sparse

08:19:01.891 --> 08:19:04.372
Attention, and also efficiency

08:19:04.671 --> 08:19:06.741
and architecture for long contacts. It

08:19:06.741 --> 08:19:08.941
seems like it's extremely useful for

08:19:09.831 --> 08:19:12.071
long generation as well. It's it's actually even

08:19:12.071 --> 08:19:13.641
more. Important.

08:19:14.282 --> 08:19:16.702
So Smart Meetings is actually not very new.

08:19:17.042 --> 08:19:19.141
If we're considering this, scalability.

08:19:20.372 --> 08:19:21.911
It has been used as regularization

08:19:22.452 --> 08:19:24.521
before 2012. And

08:19:24.521 --> 08:19:26.941
efficiency for a really long time, including

08:19:27.001 --> 08:19:29.023
make sure of the experts And

08:19:29.023 --> 08:19:30.621
we see that it can even

08:19:31.261 --> 08:19:33.390
the capability of the test on compute

08:19:33.390 --> 08:19:35.431
because nowadays, we're doing

08:19:35.431 --> 08:19:37.771
the reinforcement learning and agent end task.

08:19:38.031 --> 08:19:40.421
And we just same amount of

08:19:40.421 --> 08:19:42.442
time, if you can generate more tokens,

08:19:42.901 --> 08:19:44.981
your ability just increases, or

08:19:44.981 --> 08:19:47.161
with the same amount of time, you

08:19:47.161 --> 08:19:49.031
can train or get more rewards

08:19:49.381 --> 08:19:51.561
your model will be stronger. So

08:19:51.561 --> 08:19:53.641
scalability or, like, sparsity is

08:19:53.641 --> 08:19:55.801
no longer efficiency. It

08:19:55.862 --> 08:19:57.491
is the capability increase.

08:19:59.301 --> 08:20:01.462
So, the TLDR for this

08:20:01.462 --> 08:20:03.941
is we did same analysis with the kinetics

08:20:04.141 --> 08:20:06.231
scaling law. Just change

08:20:06.451 --> 08:20:08.931
the attention to test time's first attention.

08:20:09.071 --> 08:20:10.761
The very simple version of

08:20:11.241 --> 08:20:13.431
blocked top k. It seems like

08:20:13.650 --> 08:20:15.661
it's doing like, extremely

08:20:15.720 --> 08:20:17.661
well. And a interesting observation

08:20:18.201 --> 08:20:20.561
is with the more compute,

08:20:20.561 --> 08:20:22.411
when you double when you when we study the

08:20:22.651 --> 08:20:24.921
scaling law, we usually study the you double

08:20:24.921 --> 08:20:27.101
a compute, where do you increase. Right?

08:20:27.401 --> 08:20:29.081
So the interesting thing is,

08:20:29.751 --> 08:20:31.487
when we're doing the

08:20:32.141 --> 08:20:34.261
double the compute, we're not aiming

08:20:34.261 --> 08:20:36.541
to actually increase the sparsity budget.

08:20:36.941 --> 08:20:38.621
Or pushing sparse attention to dense.

08:20:39.021 --> 08:20:41.361
Rather, we want to spend more

08:20:42.071 --> 08:20:43.781
computer and memory access or resources

08:20:44.551 --> 08:20:46.181
to the number of generated tokens.

08:20:46.981 --> 08:20:49.281
Meaning that if you are doing like a massive

08:20:50.241 --> 08:20:52.021
generation and aiming for really

08:20:52.452 --> 08:20:54.331
high accuracy region, on

08:20:54.667 --> 08:20:56.941
sparse is even more

08:20:56.941 --> 08:20:58.151
important. There.

08:20:59.051 --> 08:21:01.551
And there are some results on the, additional

08:21:01.771 --> 08:21:03.862
benchmarks. Iron is completely

08:21:03.862 --> 08:21:05.942
different scaling beyond dance for test time

08:21:05.942 --> 08:21:08.221
to compute. Amy and Lifcode

08:21:08.231 --> 08:21:10.091
Bench, and believe in other

08:21:11.291 --> 08:21:13.523
agent testing will be very similar. And

08:21:13.523 --> 08:21:15.603
this is some evaluation on the

08:21:15.603 --> 08:21:17.401
mixture of experts model as

08:21:18.042 --> 08:21:20.231
well. And like what we mentioned earlier, mixture

08:21:20.231 --> 08:21:22.171
of expert model is even worse

08:21:22.622 --> 08:21:24.221
is even more memory bonded.

08:21:24.942 --> 08:21:27.239
So that's making this

08:21:27.239 --> 08:21:29.611
attention better will also, like, resolve

08:21:29.611 --> 08:21:30.761
the problem for that.

08:21:32.361 --> 08:21:34.681
And there's several obligations on,

08:21:34.841 --> 08:21:37.151
test time. The inference time,

08:21:37.471 --> 08:21:39.581
attention. We compare like

08:21:39.581 --> 08:21:41.741
the top k dance and blocks bars

08:21:41.741 --> 08:21:43.931
and local. It seems like

08:21:43.931 --> 08:21:46.271
the the block stars is doing reasonably well.

08:21:46.661 --> 08:21:47.962
Pretty close to the true

08:21:48.671 --> 08:21:50.371
fine grained top k attention.

08:21:50.751 --> 08:21:53.001
Well, unfortunately, local fell, but

08:21:53.691 --> 08:21:56.011
we know that those hybrid, attention

08:21:56.011 --> 08:21:57.861
will be like required to be

08:21:58.101 --> 08:22:00.221
training from scratch, but this is like test time.

08:22:00.622 --> 08:22:02.081
Another exciting thing is,

08:22:03.141 --> 08:22:05.001
the on the right hand side of the

08:22:06.601 --> 08:22:08.380
evaluation on the latency.

08:22:09.091 --> 08:22:11.331
Of different KB budget and different

08:22:11.331 --> 08:22:13.231
speed up we can get for the test time

08:22:13.631 --> 08:22:15.872
compute. This is showing

08:22:15.872 --> 08:22:18.131
that not only this is theoretical analysis,

08:22:18.661 --> 08:22:21.081
but a simple implementation on

08:22:21.221 --> 08:22:22.681
on top of, like, page retention,

08:22:23.462 --> 08:22:25.622
shows that this can actually be realized

08:22:25.622 --> 08:22:26.921
faster on GPU.

08:22:28.800 --> 08:22:31.071
And also, like, we just

08:22:31.371 --> 08:22:33.481
wrote a interface which

08:22:33.481 --> 08:22:35.511
you can write like 200, 20

08:22:35.511 --> 08:22:37.431
lines of code to

08:22:37.671 --> 08:22:39.931
call the word packs. You can write 20

08:22:40.069 --> 08:22:41.941
lines of code over any kind of

08:22:42.181 --> 08:22:44.441
sparse attention variance because we do the abstraction,

08:22:44.501 --> 08:22:46.651
do the heavy lifting, for you

08:22:46.651 --> 08:22:48.941
on the current service system engine, which

08:22:49.421 --> 08:22:51.481
basically their the intuition is like you

08:22:51.481 --> 08:22:53.141
can decouple the,

08:22:54.021 --> 08:22:56.281
attestation back end and the caching

08:22:56.281 --> 08:22:58.341
systems. And write the abstraction code on

08:22:58.341 --> 08:23:00.541
top of that, and it's super simple.

08:23:00.861 --> 08:23:02.319
And this is a very important

08:23:02.956 --> 08:23:05.351
for what we're gonna talk about next

08:23:05.651 --> 08:23:08.071
because once you do the test on,

08:23:09.171 --> 08:23:11.251
as far as attention, you wanna use

08:23:11.251 --> 08:23:13.296
it somewhere no matter is in

08:23:13.296 --> 08:23:15.720
the agent workflows or reinforcement

08:23:15.781 --> 08:23:17.872
learning. You actually need to be

08:23:17.872 --> 08:23:19.980
integrated into the service system.

08:23:19.980 --> 08:23:21.891
Otherwise, even algorithmically,

08:23:22.571 --> 08:23:24.791
you are doing like five x faster.

08:23:24.951 --> 08:23:26.579
But the continuous batching and

08:23:27.561 --> 08:23:29.801
scheduling all the rest, will ease

08:23:29.801 --> 08:23:32.151
up all these advances.

08:23:32.451 --> 08:23:34.792
So it is very important to actually integrate

08:23:34.931 --> 08:23:37.331
all these variants or algorithm change

08:23:37.331 --> 08:23:39.421
in the serving system for the sake

08:23:39.720 --> 08:23:41.329
of test time compute,

08:23:41.941 --> 08:23:44.351
test time compute applications or reinforcement learning.

08:23:45.071 --> 08:23:47.550
So let's get into the, last part,

08:23:47.550 --> 08:23:50.001
which is the application of reinforcement learning.

08:23:50.251 --> 08:23:52.409
So say that implemented

08:23:52.409 --> 08:23:53.150
well on

08:23:54.630 --> 08:23:55.891
as the back end,

08:23:56.692 --> 08:23:58.391
we wanna do a reinforcement

08:23:59.034 --> 08:24:01.291
training. For on the top of this like Sparse

08:24:01.291 --> 08:24:02.781
Attention enabled model,

08:24:03.431 --> 08:24:05.501
what do we do? So the

08:24:05.501 --> 08:24:06.251
challenge is

08:24:07.622 --> 08:24:09.661
at same challenge as the sparse model which

08:24:09.661 --> 08:24:10.721
is like MOE,

08:24:11.872 --> 08:24:14.261
We know that because the inconsistency

08:24:14.560 --> 08:24:16.631
between the inference and

08:24:16.631 --> 08:24:19.011
FSDP, there's could be activating

08:24:19.310 --> 08:24:20.531
different experts.

08:24:21.381 --> 08:24:23.631
When you're doing the rollout when you're doing the training.

08:24:23.872 --> 08:24:26.212
So there's, like, a divergence on the distribution.

08:24:26.991 --> 08:24:29.058
Of your rollout and your training

08:24:29.058 --> 08:24:31.300
model. Right? And as far as

08:24:31.300 --> 08:24:33.380
retention suffers from very very similar

08:24:33.380 --> 08:24:35.720
problem. And also quantization

08:24:35.941 --> 08:24:37.962
and all the rest. But luckily,

08:24:38.421 --> 08:24:40.331
we have a lot of great work

08:24:40.891 --> 08:24:43.141
including the aerial flash aural,

08:24:43.591 --> 08:24:45.801
GSPO, ISFOP, and even deterministic

08:24:46.101 --> 08:24:47.901
kernels trying to resolve this problem.

08:24:48.271 --> 08:24:50.691
To mitigate this, inconsistency.

08:24:51.751 --> 08:24:53.933
Or harness of training these

08:24:53.933 --> 08:24:55.942
different architectures. Or lower

08:24:55.942 --> 08:24:57.881
bid architectures in,

08:24:58.321 --> 08:25:00.441
reinforcement learning. You can see that with

08:25:00.441 --> 08:25:02.331
a simple strategy. It

08:25:02.471 --> 08:25:04.221
is matching the dense

08:25:05.661 --> 08:25:08.151
reinforcement learning results. So those are like training free

08:25:08.841 --> 08:25:10.141
sparse attention not only

08:25:10.941 --> 08:25:12.981
can maintaining and with the same kind

08:25:12.981 --> 08:25:14.991
of cost even doing much better.

08:25:14.991 --> 08:25:17.061
In the infra time. It is also

08:25:17.061 --> 08:25:19.211
on par with the but being

08:25:19.211 --> 08:25:21.273
much faster in the during

08:25:21.273 --> 08:25:22.771
the reinforcement learning

08:25:25.451 --> 08:25:27.470
duration. And we asked the question that

08:25:27.470 --> 08:25:29.641
since we can fix the model,

08:25:30.603 --> 08:25:32.782
for this distribution misaligned problem,

08:25:32.901 --> 08:25:35.050
what if? I want to do an

08:25:35.050 --> 08:25:36.351
even more aggressive

08:25:38.591 --> 08:25:41.081
experiment on I want to roll out with

08:25:41.301 --> 08:25:43.671
a smaller model because it's being faster. Right?

08:25:44.231 --> 08:25:46.471
So all this training or post training is

08:25:46.471 --> 08:25:48.291
bottlenecked by the

08:25:48.591 --> 08:25:50.651
exploration of finding the reward at all.

08:25:51.131 --> 08:25:53.452
So if I can use a smaller model or different

08:25:53.452 --> 08:25:55.550
model to the rollout, but

08:25:55.550 --> 08:25:57.661
I can still train with

08:25:57.661 --> 08:25:59.841
the trajectories that is

08:25:59.841 --> 08:26:02.192
explored by this smaller model. And

08:26:02.192 --> 08:26:04.273
it is much faster than you

08:26:04.273 --> 08:26:05.981
have to stay fully on hold.

08:26:06.381 --> 08:26:08.471
Right? So, we

08:26:08.471 --> 08:26:10.531
asked this question, but the existing

08:26:12.001 --> 08:26:12.901
distribution alignment

08:26:14.433 --> 08:26:16.671
strategies including all this clipping or

08:26:16.671 --> 08:26:18.561
sentence wise rejection and all the rest,

08:26:19.681 --> 08:26:21.362
will fail here, because,

08:26:21.683 --> 08:26:23.773
this distribution mismatch is too

08:26:23.773 --> 08:26:25.792
large. The distance

08:26:25.792 --> 08:26:28.011
or the k l divergence of your

08:26:28.612 --> 08:26:30.911
small model and large model will be a

08:26:30.911 --> 08:26:32.981
very large gap, and it's almost impossible

08:26:32.981 --> 08:26:35.301
to do that reinforcement learning on the top

08:26:35.301 --> 08:26:36.251
of that stable way.

08:26:37.451 --> 08:26:39.611
So the idea of our work

08:26:39.611 --> 08:26:41.431
to be released it's called jackpot,

08:26:42.051 --> 08:26:44.001
The insight is instead

08:26:44.061 --> 08:26:46.400
of doing the clipping,

08:26:47.212 --> 08:26:49.311
after the important sampling,

08:26:49.371 --> 08:26:51.451
of, PPO or

08:26:51.451 --> 08:26:53.462
GRPO. What you can do is

08:26:53.462 --> 08:26:55.541
you can directly align your

08:26:55.811 --> 08:26:58.001
inference and your

08:26:58.701 --> 08:26:59.621
training models.

08:27:01.140 --> 08:27:03.561
That distribution. By

08:27:03.621 --> 08:27:06.031
rejection sampling. But, of course, we know that rejection

08:27:06.331 --> 08:27:08.712
sampling is not very efficient. There's

08:27:08.712 --> 08:27:10.651
a trade off between the sampling

08:27:11.251 --> 08:27:13.291
complexity and all this distribution

08:27:13.291 --> 08:27:15.371
matching. Right? You can exactly have the same

08:27:15.371 --> 08:27:17.560
distribution, but you won't have any tokens

08:27:17.560 --> 08:27:19.212
left. They're all being rejected.

08:27:20.061 --> 08:27:21.431
So to solve this problem,

08:27:23.051 --> 08:27:25.720
there was a amazing 2024

08:27:25.720 --> 08:27:28.050
Icemail paper on optimal

08:27:28.050 --> 08:27:30.230
budget rejection saying

08:27:30.230 --> 08:27:32.251
that, you don't have to be exactly

08:27:32.550 --> 08:27:34.861
the same distribution of

08:27:34.951 --> 08:27:36.980
your roll out model and

08:27:36.980 --> 08:27:38.981
your training model. But you can

08:27:38.981 --> 08:27:41.231
try to set up like, there's a face

08:27:41.231 --> 08:27:43.380
to gap. Or for any kind of

08:27:43.380 --> 08:27:45.621
acceptance rate that you enable, you

08:27:45.621 --> 08:27:48.050
can do a different sampling

08:27:49.241 --> 08:27:51.480
with some kind of hyper parameter tuning on

08:27:51.480 --> 08:27:53.241
this rejection sampling

08:27:54.571 --> 08:27:56.591
algorithm. And to do that,

08:27:56.891 --> 08:27:58.751
you can control how much

08:27:59.011 --> 08:28:01.273
how far away you want to control

08:28:01.331 --> 08:28:03.571
these two distribution online so that you can

08:28:03.571 --> 08:28:05.819
balance your sample complexity

08:28:07.171 --> 08:28:09.081
and your distribution distance.

08:28:10.761 --> 08:28:12.911
So that, in the end, what

08:28:12.911 --> 08:28:14.931
we can do is, if you do that properly,

08:28:15.391 --> 08:28:17.591
you can actually the

08:28:17.591 --> 08:28:19.631
line of algorithm is the

08:28:19.631 --> 08:28:21.751
yellow one. Is not

08:28:22.231 --> 08:28:24.311
like exactly matching the on policy, but

08:28:24.311 --> 08:28:26.712
it's already amazing that you will

08:28:26.712 --> 08:28:28.872
continue to train not collapse. If

08:28:28.872 --> 08:28:31.201
you are just doing the rollout of a

08:28:32.081 --> 08:28:34.400
one b model and train a seven

08:28:34.400 --> 08:28:36.561
b model on top of that. And you see

08:28:36.561 --> 08:28:38.101
that this, like, k l diverges

08:28:39.122 --> 08:28:41.451
was kind of maintained. This this

08:28:41.511 --> 08:28:43.962
yellow line. Across the training.

08:28:45.400 --> 08:28:45.900
So

08:28:48.261 --> 08:28:50.571
the last part of the, reinforce

08:28:52.491 --> 08:28:54.431
challenges of

08:28:55.131 --> 08:28:57.292
the irregular workload we talked about. So you

08:28:57.292 --> 08:28:58.991
could be like, when you are doing exploration,

08:28:59.751 --> 08:29:01.851
you could call a lot of tools

08:29:02.231 --> 08:29:04.481
or agent tasks, and you are

08:29:04.721 --> 08:29:07.122
some kind of, like, the waiting time for some of the examples

08:29:07.122 --> 08:29:09.131
are very long, you will have

08:29:09.131 --> 08:29:11.329
all this dullness. So this is

08:29:11.329 --> 08:29:13.251
a classic asynchronous

08:29:14.291 --> 08:29:16.161
examples. So how to allow

08:29:16.541 --> 08:29:18.721
more asynchronousations since now we're

08:29:18.721 --> 08:29:21.081
doing the agent check RL training

08:29:21.140 --> 08:29:23.239
without hurting the training accuracy.

08:29:24.381 --> 08:29:26.081
So our observation is

08:29:26.741 --> 08:29:29.093
in fact, you just need to

08:29:29.151 --> 08:29:30.881
prevent some of the

08:29:32.251 --> 08:29:33.891
bad tokens or distributions

08:29:34.591 --> 08:29:36.651
misaligned that is very severe

08:29:37.292 --> 08:29:39.321
So in the usual case or usual

08:29:39.321 --> 08:29:41.401
steps of the training, if you don't do any

08:29:41.401 --> 08:29:43.481
kind of the clipping, which is

08:29:43.481 --> 08:29:45.191
what is trying to resolve this

08:29:45.491 --> 08:29:47.761
asynchronous problem, you can train

08:29:47.761 --> 08:29:49.801
well, and your performance won't be bounded.

08:29:49.962 --> 08:29:52.381
But the trade off is you can just crash.

08:29:52.561 --> 08:29:55.011
If you don't do the clipping properly.

08:29:55.131 --> 08:29:57.292
But there's a kind of a

08:29:57.292 --> 08:29:59.391
tension between how much

08:29:59.391 --> 08:30:01.640
you and what we are clipping.

08:30:02.091 --> 08:30:04.212
Because, we know that they

08:30:04.212 --> 08:30:06.391
are some kind of the high entropy tokens

08:30:06.431 --> 08:30:08.831
by the earlier work. That is

08:30:08.831 --> 08:30:10.931
very important for your RL training.

08:30:12.032 --> 08:30:13.651
However, if you clear

08:30:14.141 --> 08:30:16.231
it's very strict. For your

08:30:16.231 --> 08:30:18.462
trans region. Your actually

08:30:18.681 --> 08:30:20.971
very likely to clip all this important

08:30:21.085 --> 08:30:23.319
tokens. So the idea

08:30:23.461 --> 08:30:25.881
is we need some kind

08:30:25.941 --> 08:30:27.980
of algorithm which you can do

08:30:27.980 --> 08:30:30.451
the clipping the ones that are extremely

08:30:30.591 --> 08:30:32.452
bad so that your training

08:30:32.711 --> 08:30:34.730
won't collapse. But at the same time, you have

08:30:34.730 --> 08:30:36.862
to maintain and not

08:30:36.862 --> 08:30:38.952
being too constrained on the high

08:30:38.952 --> 08:30:41.111
entropy tokens which could be very, very

08:30:41.111 --> 08:30:43.171
useful for your training. We

08:30:43.171 --> 08:30:44.901
proposed this second momentum

08:30:46.101 --> 08:30:48.581
trust policy optimization, which is depending

08:30:48.581 --> 08:30:50.640
on the k l

08:30:50.640 --> 08:30:52.761
square. Are

08:30:52.761 --> 08:30:55.001
details in the paper. I won't have the time to go over

08:30:55.001 --> 08:30:56.933
the talk. But here's the result.

08:30:57.853 --> 08:30:59.970
Amazingly we can enable the stable

08:30:59.970 --> 08:31:02.061
training for 256

08:31:02.651 --> 08:31:04.381
steps. And asynchronously.

08:31:06.601 --> 08:31:08.921
Yeah. So I think to conclude, today

08:31:08.921 --> 08:31:11.211
we go over the scaling

08:31:11.211 --> 08:31:13.241
laws of test and compute on

08:31:13.241 --> 08:31:15.251
modern hardware, and the practical

08:31:15.630 --> 08:31:17.933
efficiency is very due

08:31:17.933 --> 08:31:20.061
to the attention dominating for the

08:31:21.181 --> 08:31:23.261
AV memory accesses. And it's not very

08:31:23.261 --> 08:31:25.041
scalable with the hardware development

08:31:25.741 --> 08:31:27.141
because the flops goes faster.

08:31:27.831 --> 08:31:29.911
Than the memory bandwidth. And then we

08:31:29.911 --> 08:31:32.041
talk about three challenges

08:31:32.041 --> 08:31:34.091
on they are problem

08:31:34.300 --> 08:31:36.591
Oh, they are problem for parallelism, there

08:31:36.591 --> 08:31:37.801
are problems for

08:31:38.841 --> 08:31:41.001
the quadratic memory access.

08:31:41.001 --> 08:31:42.701
There are problems for the irregular

08:31:43.671 --> 08:31:45.831
workload. And when you talk about three

08:31:45.831 --> 08:31:48.001
lines of work, which can do the parallel

08:31:48.060 --> 08:31:50.341
reasoning, can be sparse attention or

08:31:50.480 --> 08:31:51.701
efficient architectures,

08:31:52.671 --> 08:31:55.101
and, also, like the stable reinforce

08:31:56.201 --> 08:31:57.681
stable scalable learning

08:31:58.471 --> 08:32:00.491
as the application in the

08:32:00.491 --> 08:32:02.550
end. Thank you so much for

08:32:03.071 --> 08:32:05.221
your attention. I'm happy to take questions if

08:32:05.221 --> 08:32:05.871
there are any.

08:32:22.891 --> 08:32:22.971
Please

08:32:26.900 --> 08:32:29.060
Hi, professor. Thanks for the great

08:32:29.060 --> 08:32:31.041
talk. I have a question regarding the

08:32:31.601 --> 08:32:33.140
small model rollouts idea.

08:32:33.782 --> 08:32:35.681
So people say that the

08:32:36.571 --> 08:32:37.792
upper boundary of reinforcement

08:32:38.651 --> 08:32:40.981
learning is somehow concerned by the

08:32:41.366 --> 08:32:41.751
explore

08:32:44.811 --> 08:32:46.821
So wouldn't we be

08:32:46.821 --> 08:32:49.150
constrained by the capabilities

08:32:49.150 --> 08:32:51.501
of 1.7 b base models

08:32:52.081 --> 08:32:53.640
in this setting here.

08:32:54.581 --> 08:32:57.001
Yeah. So, this is exactly the problem

08:32:57.201 --> 08:32:59.441
that we will worry initially. And

08:32:59.441 --> 08:33:01.521
in the end, what amazing We're still

08:33:01.521 --> 08:33:03.661
like doing the experiments, for

08:33:03.661 --> 08:33:05.441
longer training and larger scale.

08:33:05.891 --> 08:33:07.911
But what what we reserve is

08:33:08.121 --> 08:33:09.951
you will continue to grow

08:33:10.511 --> 08:33:12.571
even like if you're using the

08:33:12.571 --> 08:33:14.671
1.7 b model to do the roll out, and

08:33:15.231 --> 08:33:17.341
four b or seven b to do the training. As

08:33:17.341 --> 08:33:19.773
long as it's not collapsing, your community

08:33:19.901 --> 08:33:22.126
will grow slightly slower,

08:33:22.281 --> 08:33:24.542
if you see like the yellow

08:33:24.542 --> 08:33:26.781
line and, the purple line, So it's

08:33:26.781 --> 08:33:28.861
it will be like slightly slower and

08:33:28.861 --> 08:33:30.962
lower, but if it continue to grow, I

08:33:30.962 --> 08:33:33.398
kind of see that it is still catching up and

08:33:33.398 --> 08:33:35.741
matching. And, we're also investigating

08:33:36.441 --> 08:33:38.501
why this is the case, and it seems like

08:33:39.261 --> 08:33:41.550
we're also doing the reverse k

08:33:41.550 --> 08:33:43.671
l. On the top of this. To

08:33:43.731 --> 08:33:45.751
make the smaller model doing

08:33:46.151 --> 08:33:48.511
much better than its own RL

08:33:48.511 --> 08:33:50.671
training as well. Mhmm. So I think there are like

08:33:50.671 --> 08:33:52.942
mechanisms that we don't fully understand

08:33:52.942 --> 08:33:55.371
here, but the takeaway is even enabling

08:33:56.051 --> 08:33:57.981
this kind of a like, really

08:33:58.201 --> 08:34:00.281
large diversions of, the

08:34:00.281 --> 08:34:02.471
two model training. Is very,

08:34:02.471 --> 08:34:04.581
very interesting. And, we're looking forward

08:34:04.581 --> 08:34:06.731
to see in the end, it cannot

08:34:06.731 --> 08:34:08.773
fully match the large model, maybe

08:34:08.773 --> 08:34:11.171
there are other follow-up works that we can do to

08:34:11.411 --> 08:34:13.421
fix it. Thanks for your question.

08:34:13.581 --> 08:34:14.731
Thanks for the explanation.

08:34:16.531 --> 08:34:18.310
For example, you can use trees.

08:34:18.862 --> 08:34:20.942
Or even more scalable Python compute

08:34:20.942 --> 08:34:23.071
for the model to do. The rollout.

08:34:23.471 --> 08:34:24.691
That could be one idea.

08:34:27.481 --> 08:34:29.282
Yeah. Thanks for the question,

08:34:29.581 --> 08:34:31.601
Anthony. And do everyone

08:34:31.601 --> 08:34:33.731
have more question? Feel free to

08:34:33.731 --> 08:34:35.771
ask Oh, yeah, next one

08:34:35.771 --> 08:34:38.031
more. Yeah. Maybe

08:34:38.091 --> 08:34:40.171
one last. Okay. Thank

08:34:40.171 --> 08:34:41.791
you, Betty, for the great talk.

08:34:42.271 --> 08:34:44.591
I just have a question. So what's your intuition

08:34:44.591 --> 08:34:46.191
on, like, how much,

08:34:48.212 --> 08:34:50.532
hardware utilization we can

08:34:50.532 --> 08:34:52.421
do when you this kind of scaling.

08:34:52.640 --> 08:34:54.841
We also all the method proposed

08:34:54.841 --> 08:34:56.741
here cannot fully utilize the

08:34:57.221 --> 08:34:59.051
hardware. For example, if you do parallel

08:34:59.226 --> 08:35:01.501
generation, it's always constrained by

08:35:01.501 --> 08:35:03.521
the longest, path. Right? By

08:35:03.521 --> 08:35:05.551
the critical path. So there's still a lot

08:35:05.551 --> 08:35:07.781
of resource that are idle. So based

08:35:08.001 --> 08:35:09.811
on your experience, like,

08:35:10.051 --> 08:35:11.821
how many how much performance

08:35:12.282 --> 08:35:14.310
have we extracted from

08:35:14.310 --> 08:35:16.792
the underlying hardware? Yeah.

08:35:16.931 --> 08:35:18.741
That's one of the most

08:35:18.961 --> 08:35:21.427
pain point our lab is encountered. So

08:35:21.712 --> 08:35:24.031
So that's why we're also building the

08:35:24.641 --> 08:35:27.051
interface or abstractions on the top of that. So

08:35:27.112 --> 08:35:29.431
that including the multiverse, including

08:35:29.431 --> 08:35:31.491
the Kinetics, burst of attention, could

08:35:31.491 --> 08:35:33.542
it be actually be scalable and leveraging

08:35:33.761 --> 08:35:36.041
the development of the services system.

08:35:36.201 --> 08:35:38.380
You're right on the naive implementation.

08:35:38.921 --> 08:35:41.091
Is not gonna leverage on

08:35:41.331 --> 08:35:43.511
not even the hardware. But the server

08:35:43.511 --> 08:35:45.591
and system advantages. It's very hard to do

08:35:45.591 --> 08:35:47.702
that, and we're working on it. And for the smart

08:35:47.702 --> 08:35:49.962
assistant, it's very similar. So you have

08:35:49.962 --> 08:35:52.141
to, they are special kernel, specialized

08:35:52.521 --> 08:35:54.731
kernels that you have to integrate and there's

08:35:54.731 --> 08:35:56.956
like a lot of lines that you have to write. To

08:35:56.956 --> 08:35:59.081
even get a properly or correct.

08:35:59.081 --> 08:36:00.942
And of course, when the hardware shifts,

08:36:01.212 --> 08:36:03.111
it will be another, like, length of work.

08:36:03.271 --> 08:36:05.491
So we're trying to actually after

08:36:05.491 --> 08:36:07.541
doing all this amazing algorithms and,

08:36:07.781 --> 08:36:10.171
scalability, exploration, the

08:36:10.171 --> 08:36:12.390
next mission is to build an

08:36:12.390 --> 08:36:14.391
interface or, or, like, the

08:36:14.391 --> 08:36:16.431
front end so that all this method can

08:36:16.431 --> 08:36:17.731
be actually properly

08:36:18.442 --> 08:36:20.711
compiled. And implemented properly

08:36:20.711 --> 08:36:21.291
in this

08:36:23.521 --> 08:36:25.560
different hardware or service system

08:36:25.560 --> 08:36:27.720
agronomists. Yeah. Great. Thank you. For the

08:36:27.720 --> 08:36:29.862
question. Cheer. Thanks

08:36:29.862 --> 08:36:32.122
for the question and asking. And thanks,

08:36:32.263 --> 08:36:34.011
professor Baide again. This.

08:36:34.411 --> 08:36:36.121
Awesome cog. And, yeah,

08:36:47.201 --> 08:36:49.140
Testing Yeah. Thanks everyone

08:36:49.281 --> 08:36:51.341
for coming here. We have one

08:36:51.441 --> 08:36:53.201
last session about

08:36:53.921 --> 08:36:54.991
the best paper of

08:36:56.111 --> 08:36:57.911
the NeurIPS efficient listening

08:36:58.372 --> 08:37:00.161
workshop. Yeah. This year, we have

08:37:00.507 --> 08:37:02.721
normally four of the paper

08:37:03.042 --> 08:37:05.121
as best paper nomination. That

08:37:05.235 --> 08:37:06.911
include when listening,

08:37:07.371 --> 08:37:08.991
it's slow, and

08:37:09.971 --> 08:37:11.291
other paper journey

08:37:12.011 --> 08:37:13.851
parallel scaling with independent generation.

08:37:15.051 --> 08:37:16.751
Yes. Also, in other paper, Inflow

08:37:17.131 --> 08:37:18.431
agent system optimization.

08:37:20.051 --> 08:37:21.831
For efficient planning and tool use.

08:37:22.843 --> 08:37:24.962
And also M1 towards

08:37:25.261 --> 08:37:27.683
scale test time compute with MAM based learning

08:37:27.741 --> 08:37:29.730
model. And we finally

08:37:30.191 --> 08:37:32.112
have one of one best

08:37:32.411 --> 08:37:34.362
paper that is M1,

08:37:34.581 --> 08:37:36.282
yeah, towards skill, time,

08:37:36.601 --> 08:37:38.701
test time computer with mobile

08:37:39.001 --> 08:37:41.181
and we are We're honored

08:37:41.181 --> 08:37:42.361
to invite

08:37:43.241 --> 08:37:45.401
the paper also here to take

08:37:45.401 --> 08:37:47.491
our best paper award.

08:37:48.051 --> 08:37:48.501
Yeah.

08:37:54.542 --> 08:37:56.251
Yeah. Thanks,

08:37:57.231 --> 08:37:59.441
Junshiong Wang, from Together

08:37:59.441 --> 08:38:01.541
AI, who got the best tech early award

08:38:01.541 --> 08:38:03.721
this year. Yeah. You can give

08:38:03.721 --> 08:38:05.782
a sell award. Yeah. You. Thank you

08:38:05.782 --> 08:38:07.702
for the organizers. Yeah.

08:38:08.391 --> 08:38:10.491
And thanks for everyone. Yeah. That's

08:38:10.551 --> 08:38:12.429
all of today's efficient work

08:38:12.561 --> 08:38:14.631
research work about agenda. Yeah.

08:38:14.872 --> 08:38:16.751
Thank everyone for coming here

08:38:17.391 --> 08:38:19.612
to join our Infusion

08:38:19.671 --> 08:38:21.781
listening, and we have also

08:38:21.781 --> 08:38:23.561
have a lot of poster here

08:38:24.292 --> 08:38:26.391
after the whole session. You're welcome

08:38:26.745 --> 08:38:28.831
to like see all the poster, and

08:38:28.831 --> 08:38:31.071
thanks everyone again for joining us here.

08:38:31.071 --> 08:38:33.220
We can now make such a

08:38:33.220 --> 08:38:35.361
big workshop without you. Thank you

08:38:35.421 --> 08:38:35.781
so much.