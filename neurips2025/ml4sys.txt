WEBVTT

00:21:18.824 --> 00:21:19.324
Morning.

00:21:22.890 --> 00:21:24.969
Welcome to our workshop. I'm about

00:21:24.969 --> 00:21:26.040
to get started, so

00:21:27.330 --> 00:21:28.040
let's see.

00:21:36.969 --> 00:21:38.779
Welcome to New Year's twenty twenty

00:21:39.020 --> 00:21:40.960
five workshop on machine learning for systems.

00:21:42.340 --> 00:21:44.660
I'm one of the organizers. Mimi and the other

00:21:44.660 --> 00:21:46.770
organizers are here. We're gonna meet each

00:21:46.770 --> 00:21:47.350
one of us

00:21:49.110 --> 00:21:51.320
As an overview of the workshop, we

00:21:51.320 --> 00:21:53.689
are interdisciplinary workshop that brings

00:21:53.690 --> 00:21:55.740
together researchers and practitioners in

00:21:55.740 --> 00:21:57.760
computer systems and machine learning.

00:21:58.170 --> 00:22:00.519
Focusing on novel applications of ML techniques,

00:22:01.080 --> 00:22:02.310
towards computer system problems.

00:22:03.090 --> 00:22:05.250
This is the ninth iteration of the same

00:22:05.250 --> 00:22:07.290
workshop. So when we we just

00:22:07.290 --> 00:22:09.370
got started, if people remember, they were

00:22:09.370 --> 00:22:11.390
the learned index papers, It

00:22:11.390 --> 00:22:13.810
really gave people this wild range of imagination.

00:22:14.410 --> 00:22:16.100
On what machine learning could do

00:22:16.660 --> 00:22:18.860
to traditional systems. This year,

00:22:18.860 --> 00:22:21.020
we'll additionally focus on compared to last

00:22:21.020 --> 00:22:23.240
year's LMM focus, using LMs

00:22:23.240 --> 00:22:25.560
and agent agentic workflows for computer

00:22:25.560 --> 00:22:27.650
system challenges And we're also

00:22:27.650 --> 00:22:29.760
applying machine learning to address

00:22:29.760 --> 00:22:31.620
emerging systems challenges

00:22:32.280 --> 00:22:34.400
introduced by Agented Workflows. Large

00:22:34.400 --> 00:22:36.480
scale training, and serving of large models,

00:22:36.480 --> 00:22:38.580
including LMs and multimodal models.

00:22:39.210 --> 00:22:41.480
That is not the only thing.

00:22:41.720 --> 00:22:43.740
We can be thinking about in the present

00:22:43.800 --> 00:22:45.710
time. Can also think about

00:22:45.950 --> 00:22:47.780
what this particular approach and technique

00:22:48.260 --> 00:22:50.130
can bring to the larger environment,

00:22:50.490 --> 00:22:52.650
not just computer systems. And this may

00:22:52.650 --> 00:22:54.980
include applying machine learning towards computer systems.

00:22:55.540 --> 00:22:57.590
Sustainability, including power, energy,

00:22:58.140 --> 00:22:59.350
and carbon optimization.

00:23:03.010 --> 00:23:04.710
A few stats about the workshop.

00:23:05.020 --> 00:23:07.099
As I mentioned, this is the ninth iteration

00:23:07.100 --> 00:23:09.250
of the workshop, We have really grown

00:23:09.250 --> 00:23:11.309
into a large field.

00:23:12.160 --> 00:23:14.319
For this time, we accepted 41 papers.

00:23:14.319 --> 00:23:15.540
Out of 60 submissions,

00:23:17.470 --> 00:23:19.780
We'll be presenting today four

00:23:19.780 --> 00:23:21.400
oral spotlight papers.

00:23:22.550 --> 00:23:24.490
We'll have six invited speakers,

00:23:25.130 --> 00:23:27.389
from academia, frontier

00:23:27.390 --> 00:23:28.370
labs, and startups.

00:23:29.600 --> 00:23:31.670
We Additionally, we'll be including

00:23:31.729 --> 00:23:33.739
including a debate

00:23:34.380 --> 00:23:36.079
that's aquifer aquifer sal.

00:23:36.490 --> 00:23:37.310
With four panelists.

00:23:38.960 --> 00:23:40.760
The topic will be, well,

00:23:41.719 --> 00:23:43.770
agents replace systems developers?

00:23:46.740 --> 00:23:48.920
And I'm delighted to announce that

00:23:48.920 --> 00:23:50.660
we're going to be hosting our first

00:23:51.300 --> 00:23:53.150
ever ML for Systems happy hour.

00:23:53.870 --> 00:23:56.070
So finally, we're able to offer free food,

00:23:58.219 --> 00:23:59.030
stay tuned.

00:24:00.660 --> 00:24:02.819
As a short introduction for our organizers, my

00:24:02.820 --> 00:24:04.699
name is Mimi. And can

00:24:05.020 --> 00:24:06.720
we all stand up a bit?

00:24:08.630 --> 00:24:09.450
We have

00:24:14.470 --> 00:24:16.339
Yeah. I think

00:24:16.530 --> 00:24:18.800
Stevia here. Oh, okay.

00:24:19.020 --> 00:24:21.040
Yeah. Come meet us during the breaks.

00:24:25.720 --> 00:24:27.800
So just as a preview of of what's about

00:24:27.800 --> 00:24:29.440
to come, in the morning,

00:24:29.819 --> 00:24:31.920
we'll start with three invited talks

00:24:32.320 --> 00:24:34.700
Take a short break, have another

00:24:34.760 --> 00:24:36.979
talk, and spotlight presentations.

00:24:37.880 --> 00:24:39.900
We'll have a poster session that

00:24:40.060 --> 00:24:41.539
bleeds a little bit into lunch.

00:24:44.969 --> 00:24:47.449
In the afternoon, we'll be opening with another

00:24:47.450 --> 00:24:49.810
keynote before diving into this

00:24:49.810 --> 00:24:50.310
debate.

00:24:51.960 --> 00:24:53.829
I'm really excited about this

00:24:54.390 --> 00:24:56.599
format. We'll see how it goes.

00:24:57.570 --> 00:24:58.880
We'll take a short break,

00:25:00.020 --> 00:25:01.719
have one more speaker,

00:25:02.580 --> 00:25:04.200
and two more spotlight presentations.

00:25:05.350 --> 00:25:07.850
And then we'll have our afternoon poster sessions.

00:25:08.880 --> 00:25:10.500
Any instructions with authors?

00:25:12.190 --> 00:25:13.090
No. Okay.

00:25:15.630 --> 00:25:17.690
Yeah. And this is the information to

00:25:17.830 --> 00:25:19.900
register for happy hour. I'll keep

00:25:19.900 --> 00:25:21.360
the slide down for a minute.

00:25:37.130 --> 00:25:39.450
Yeah. I'm told that to tell you guys who's registered

00:25:39.450 --> 00:25:41.570
right now. This is the spots are

00:25:41.570 --> 00:25:44.069
limited. And we wanna prioritize the attendees.

00:25:45.430 --> 00:25:47.580
And we'll be approving at

00:25:47.740 --> 00:25:50.030
like this minute. If you apply now.

00:26:08.700 --> 00:26:10.200
Then that's how dancing should

00:26:11.630 --> 00:26:12.069
Yeah.

00:27:15.810 --> 00:27:17.890
Yeah. So with that, I'll

00:27:17.890 --> 00:27:20.200
pass the mic to my co organizer Dan

00:27:20.440 --> 00:27:22.680
who will be introducing our first speaker.

00:27:27.820 --> 00:27:29.920
Everyone, I'm, delayed to introduce

00:27:30.220 --> 00:27:32.139
our first our keynote speaker,

00:27:32.380 --> 00:27:33.610
Azalea Mir Hosseini.

00:27:34.410 --> 00:27:36.349
Who will be talking about self improving

00:27:36.490 --> 00:27:38.629
AI and the future of computing systems.

00:27:39.600 --> 00:27:41.850
Azalea is an inter assistant professor of computer

00:27:41.850 --> 00:27:44.070
science and founder of Scaling

00:27:44.070 --> 00:27:46.120
Intelligence Lab at Stanford University,

00:27:46.120 --> 00:27:48.059
She has spent many

00:27:48.280 --> 00:27:50.579
years in industry AI labs including Google

00:27:50.579 --> 00:27:52.359
Brain, Anthropic, and Google DeepMind.

00:27:53.080 --> 00:27:55.500
Her past work includes MOE, neuroarchitecture,

00:27:57.540 --> 00:27:59.920
leading generative AI models. AlphaChip,

00:28:00.219 --> 00:28:02.379
a pioneering work on deep RL

00:28:02.380 --> 00:28:03.599
for layout optimization.

00:28:05.200 --> 00:28:07.700
And research on inference time scaling laws.

00:28:08.800 --> 00:28:11.120
She's also now a cofounder

00:28:11.500 --> 00:28:14.000
and CTO of a new startup called Recursive Intelligence.

00:28:14.770 --> 00:28:17.190
Yeah. Please take it away. Thank you, Dan.

00:28:21.040 --> 00:28:23.200
Thank you all for, coming to

00:28:23.200 --> 00:28:25.460
my talk. It's a pleasure. This

00:28:25.460 --> 00:28:27.479
is was among

00:28:27.480 --> 00:28:29.250
the first organizers

00:28:29.870 --> 00:28:31.930
of the organizers of the first iteration of

00:28:31.930 --> 00:28:34.060
the workshop, and we were there for

00:28:34.440 --> 00:28:36.580
for I was involved for a

00:28:37.140 --> 00:28:38.119
few years.

00:28:39.160 --> 00:28:41.020
A couple years at least, and then

00:28:41.800 --> 00:28:43.880
so so great to see the workshop is living

00:28:43.880 --> 00:28:45.840
on. So so many years,

00:28:46.080 --> 00:28:48.150
and it's becoming more and more

00:28:49.330 --> 00:28:51.440
timely as the time passes.

00:28:53.400 --> 00:28:54.810
I'm going to talk about

00:28:55.450 --> 00:28:57.530
test time scaling and emergence of local

00:28:57.530 --> 00:28:58.030
models.

00:29:01.070 --> 00:29:02.600
Let's start with

00:29:03.240 --> 00:29:04.940
scaling loss for pretraining.

00:29:07.420 --> 00:29:09.500
So probably everyone of you

00:29:09.500 --> 00:29:11.820
in this room have heard of

00:29:11.820 --> 00:29:12.460
scaling laws

00:29:15.069 --> 00:29:17.120
and scaling laws which is

00:29:17.120 --> 00:29:19.140
this very interesting phenomenon

00:29:19.200 --> 00:29:20.440
that suggests that

00:29:21.320 --> 00:29:23.480
as we throw in more compute data

00:29:23.480 --> 00:29:25.780
and parameters, into the model,

00:29:25.780 --> 00:29:27.880
there's this predictable way that

00:29:28.020 --> 00:29:29.420
these additional

00:29:30.235 --> 00:29:32.400
parameters would translate into

00:29:32.400 --> 00:29:34.769
more capability. So For example,

00:29:34.770 --> 00:29:37.020
here, there's an equation

00:29:37.570 --> 00:29:39.750
There's an ex there's

00:29:39.970 --> 00:29:42.189
a parallel equation

00:29:42.329 --> 00:29:44.370
that is correlating

00:29:44.430 --> 00:29:46.239
the test loss of this

00:29:46.479 --> 00:29:48.819
of training this language model with compute

00:29:50.030 --> 00:29:52.190
and this line, this prediction

00:29:52.330 --> 00:29:54.570
is very, very closely following the

00:29:54.570 --> 00:29:57.030
two empirical analysis

00:29:57.089 --> 00:29:59.140
and findings of from

00:29:59.140 --> 00:30:01.159
the data. So what this means

00:30:01.160 --> 00:30:03.319
is that for the first time in the past

00:30:03.320 --> 00:30:05.400
five years, we had

00:30:05.400 --> 00:30:07.790
this recipe that would translate more

00:30:08.090 --> 00:30:09.980
compute to more capability,

00:30:10.680 --> 00:30:12.130
and this has been like

00:30:13.089 --> 00:30:15.500
going on and the guiding

00:30:15.800 --> 00:30:17.900
light for training larger and larger

00:30:19.170 --> 00:30:21.570
models across several frontier labs. And

00:30:21.730 --> 00:30:23.890
then every now and then, we still hear that,

00:30:23.890 --> 00:30:24.870
oh, maybe pretraining's,

00:30:26.209 --> 00:30:28.209
scaling laws have

00:30:28.210 --> 00:30:30.269
hit a ball, but then it turns

00:30:30.270 --> 00:30:31.650
out that it's more like

00:30:32.839 --> 00:30:35.000
scale issue. So something comes up, like,

00:30:35.000 --> 00:30:37.270
some lab comes up with a

00:30:37.589 --> 00:30:39.709
a few new tricks, and then the

00:30:39.709 --> 00:30:41.790
scaling laws continue to hold on. So at

00:30:41.790 --> 00:30:43.329
least up up until now,

00:30:43.840 --> 00:30:45.960
this this phenomena had has

00:30:45.960 --> 00:30:48.120
been continuing to to

00:30:48.120 --> 00:30:50.089
work out. Now,

00:30:51.730 --> 00:30:53.750
in the last year and a half or so,

00:30:53.810 --> 00:30:56.009
a new kind of here

00:30:56.810 --> 00:30:58.969
appeared for scaling and that

00:30:58.969 --> 00:31:00.600
is scaling at test time.

00:31:01.140 --> 00:31:03.010
If you look into the cycle of

00:31:03.670 --> 00:31:05.830
language model training, it starts from

00:31:05.830 --> 00:31:07.870
pretraining follows by

00:31:07.870 --> 00:31:10.190
fine tuning. These are both training stages

00:31:10.190 --> 00:31:12.260
where the parameters of the

00:31:12.400 --> 00:31:13.959
model get updated.

00:31:14.520 --> 00:31:16.679
At test time, it's when the model is ready to

00:31:16.680 --> 00:31:18.660
be used. And we just,

00:31:19.060 --> 00:31:21.320
run sampling and inference on the model.

00:31:21.850 --> 00:31:23.930
And we do not change the parameters of

00:31:23.930 --> 00:31:26.330
the model. So there is no further learning

00:31:26.330 --> 00:31:28.340
on the parameters of the model.

00:31:31.650 --> 00:31:33.970
And last year, last year, we

00:31:33.970 --> 00:31:35.419
published this work called

00:31:35.980 --> 00:31:37.280
Large Language Monkeys.

00:31:39.730 --> 00:31:41.320
The name was inspired from

00:31:41.880 --> 00:31:44.190
the infinite monkey theorem, which suggests

00:31:44.330 --> 00:31:46.269
that if you have a monkey and a type

00:31:46.750 --> 00:31:48.059
typewriter in a room,

00:31:49.590 --> 00:31:51.670
and the monkey is infinitely typing

00:31:51.670 --> 00:31:53.960
on the room, just leave the monkey there.

00:31:56.550 --> 00:31:56.670
Almost

00:31:59.480 --> 00:32:01.190
with a probability of

00:32:01.830 --> 00:32:03.930
almost one, we can

00:32:03.990 --> 00:32:06.150
be sure that the monkey generates the works

00:32:06.150 --> 00:32:08.560
of William's Shakespeare.

00:32:09.510 --> 00:32:11.519
So the name comes from that,

00:32:11.520 --> 00:32:13.599
but the method is a little different. So

00:32:13.600 --> 00:32:15.659
for instead of a monkey, we use

00:32:15.660 --> 00:32:16.980
a large language model.

00:32:17.880 --> 00:32:20.040
The framework works as follows. It's a very

00:32:20.040 --> 00:32:21.980
simple framework for any input

00:32:22.600 --> 00:32:24.630
prop that we

00:32:24.630 --> 00:32:26.950
pass to the model, we sample it many,

00:32:26.950 --> 00:32:29.070
many times instead of

00:32:29.070 --> 00:32:31.120
just once, and then we

00:32:31.120 --> 00:32:33.300
have access to a verifier, which

00:32:33.520 --> 00:32:34.980
I get more into it later,

00:32:36.100 --> 00:32:38.540
but that can tell us which of these samples

00:32:38.600 --> 00:32:39.340
are correct.

00:32:41.270 --> 00:32:43.530
And once the correct sample is selected,

00:32:43.590 --> 00:32:45.610
then it then it's outputted, and

00:32:45.610 --> 00:32:47.680
then we have the full flow. The

00:32:47.680 --> 00:32:49.610
the full framework from the

00:32:50.090 --> 00:32:52.170
single input problem to a single

00:32:52.170 --> 00:32:54.240
output response.

00:32:56.400 --> 00:32:57.960
It turns out that this

00:32:58.440 --> 00:33:00.620
repeated sampling technique is extremely

00:33:01.480 --> 00:33:03.430
efficient. So if we have

00:33:03.510 --> 00:33:05.149
what we are showing here is

00:33:05.630 --> 00:33:07.720
we have three models Llama three eighty

00:33:07.720 --> 00:33:10.209
b, Llama three seventy b, and

00:33:10.430 --> 00:33:12.050
GPT for four o.

00:33:12.540 --> 00:33:14.700
You can see that with only one

00:33:14.700 --> 00:33:16.610
sample, taking asking them

00:33:19.160 --> 00:33:21.380
all of these, for

00:33:21.380 --> 00:33:23.000
pretty much all of these cases,

00:33:23.910 --> 00:33:25.630
GPT four o is a clear winner.

00:33:26.590 --> 00:33:28.480
But, as we increase

00:33:28.940 --> 00:33:30.590
the number of samples for our

00:33:31.070 --> 00:33:33.100
smaller models and use the Wey Fire,

00:33:33.670 --> 00:33:35.740
and we increased that by a

00:33:35.740 --> 00:33:37.820
lot, like we go from one to

00:33:37.820 --> 00:33:39.960
10 to a 100 to all

00:33:39.960 --> 00:33:41.900
the way to 10,000 samples.

00:33:43.140 --> 00:33:45.160
And we see continued continued

00:33:45.710 --> 00:33:47.830
improvement in the performance of

00:33:47.830 --> 00:33:48.970
our smaller models.

00:33:50.280 --> 00:33:52.339
In a sense, it's telling

00:33:52.479 --> 00:33:54.320
us, it's it seems like these

00:33:54.800 --> 00:33:56.580
tiny model, like the HP model,

00:33:57.180 --> 00:33:59.420
already knows the answer to some of

00:33:59.420 --> 00:34:00.479
these really hard

00:34:01.440 --> 00:34:03.460
problems across math and coding here,

00:34:04.390 --> 00:34:06.490
just doesn't output it in the first try

00:34:06.960 --> 00:34:08.800
but we if we ask the model,

00:34:09.600 --> 00:34:11.839
thousand or 10,000 times, it can

00:34:11.840 --> 00:34:13.220
generate a correct answer.

00:34:14.510 --> 00:34:17.010
So we are basically using test on compute

00:34:17.149 --> 00:34:19.129
to list correct

00:34:19.735 --> 00:34:21.499
answers. Now

00:34:23.120 --> 00:34:24.889
We looked into this a bit

00:34:26.250 --> 00:34:28.409
further, and what we observed was

00:34:28.409 --> 00:34:30.350
that interestingly, we

00:34:30.570 --> 00:34:32.740
can write down the correlation

00:34:34.110 --> 00:34:35.810
between the accuracy

00:34:36.730 --> 00:34:38.430
and the number of samples that we have.

00:34:39.950 --> 00:34:41.970
It's not exactly accuracy. By accuracy,

00:34:42.030 --> 00:34:43.990
I mean having access to a

00:34:44.050 --> 00:34:46.130
to an Oracle verifier that can select

00:34:46.130 --> 00:34:48.399
the correct answer. That's why we call

00:34:48.399 --> 00:34:50.840
it coverage. Basically, the fraction

00:34:51.060 --> 00:34:53.140
of problems that are

00:34:53.140 --> 00:34:55.459
solved by at least one of the

00:34:55.460 --> 00:34:57.469
sample. That are taken from the

00:34:57.470 --> 00:34:59.490
model. And k

00:34:59.490 --> 00:35:01.350
in this case is the number of samples.

00:35:02.420 --> 00:35:04.780
So this this was the equation

00:35:05.190 --> 00:35:07.249
that we could model our

00:35:07.750 --> 00:35:09.679
kind of projection of

00:35:09.919 --> 00:35:11.859
the translation of test on compute

00:35:12.079 --> 00:35:13.649
to to, coverage,

00:35:14.829 --> 00:35:16.909
and the blue line is the

00:35:16.909 --> 00:35:18.940
actual coverage. So

00:35:18.940 --> 00:35:21.180
you you can see how the blue and orange line,

00:35:21.180 --> 00:35:22.880
the predicted versus measured

00:35:23.960 --> 00:35:25.150
very very closely

00:35:26.030 --> 00:35:28.190
are following each other. So what's happening here is

00:35:28.190 --> 00:35:29.859
that we are basically

00:35:31.060 --> 00:35:32.920
observing a new form of

00:35:33.140 --> 00:35:35.360
scaling laws this time for test

00:35:35.500 --> 00:35:36.100
time compute.

00:35:40.020 --> 00:35:42.260
Now, in order to explain why this

00:35:42.260 --> 00:35:44.645
is happening, we'll we

00:35:45.520 --> 00:35:47.069
we looked a little bit

00:35:48.349 --> 00:35:50.479
into our data and

00:35:50.480 --> 00:35:52.799
what we saw is that we saw the problem

00:35:52.800 --> 00:35:54.500
backwards. Like, why is

00:35:54.880 --> 00:35:56.980
the power load relationship between k and coverage?

00:35:57.280 --> 00:35:59.459
And it turns out that for this

00:35:59.460 --> 00:36:01.490
property to to hold, it is

00:36:01.490 --> 00:36:03.290
sufficient and necessary

00:36:03.690 --> 00:36:05.860
to have a left tail of heart

00:36:05.860 --> 00:36:07.590
problems in our task.

00:36:07.910 --> 00:36:09.990
So this is the parallel

00:36:09.990 --> 00:36:12.370
relationship that we see across a suite

00:36:12.910 --> 00:36:14.979
of problems and as a number

00:36:14.980 --> 00:36:16.359
of number of samples,

00:36:17.480 --> 00:36:19.820
this is the per problem exponential

00:36:19.960 --> 00:36:22.220
scaling because for a single

00:36:22.520 --> 00:36:24.709
problem, if the probability of being solved is

00:36:25.190 --> 00:36:27.430
PI, then you can write this

00:36:27.430 --> 00:36:29.510
easily. How what's the probability of success

00:36:29.510 --> 00:36:31.830
if he takes k samples. And then

00:36:31.830 --> 00:36:33.610
for this district for this equation

00:36:33.990 --> 00:36:36.169
to hold, we do need to have a

00:36:36.169 --> 00:36:38.370
formation of that looks like this,

00:36:38.370 --> 00:36:40.450
like a long tail for the heart

00:36:40.450 --> 00:36:42.660
problems, in this case, with a very

00:36:42.660 --> 00:36:43.480
low PASAD one,

00:36:45.110 --> 00:36:46.170
in in our task.

00:36:47.810 --> 00:36:49.970
And it turns out that that that was

00:36:49.970 --> 00:36:52.210
the case as we looked into a

00:36:52.210 --> 00:36:54.210
variety of data sets across a

00:36:54.770 --> 00:36:56.930
variety of model sizes. Here, we are

00:36:56.930 --> 00:36:58.710
arranging the model size from

00:36:59.030 --> 00:37:00.570
70,000,000 parameters

00:37:01.450 --> 00:37:03.530
all the way to 12,000,000,000 parameters and

00:37:03.530 --> 00:37:05.430
more. And we have seen that

00:37:07.080 --> 00:37:09.555
the left tail heart problems

00:37:09.930 --> 00:37:11.849
the ability of the model to solve,

00:37:12.169 --> 00:37:14.270
these problems in the in this left

00:37:14.270 --> 00:37:15.690
tail format.

00:37:16.490 --> 00:37:18.649
Persists across all of these. And

00:37:18.649 --> 00:37:20.900
what this means is that that

00:37:20.900 --> 00:37:23.089
we can kind of predict

00:37:23.310 --> 00:37:24.690
the power law exponent

00:37:25.630 --> 00:37:27.640
of how we correlate test

00:37:27.640 --> 00:37:29.690
time compute to capability

00:37:30.040 --> 00:37:32.140
and do so two to four orders of

00:37:32.200 --> 00:37:33.340
magnitude, two to order

00:37:34.460 --> 00:37:36.760
orders of magnitude less inference

00:37:36.900 --> 00:37:39.020
compute. So what what is what

00:37:39.020 --> 00:37:41.360
I'm describing here is a new recipe

00:37:42.500 --> 00:37:44.690
to translate compute to more capability

00:37:45.169 --> 00:37:47.270
but this time at scaling time

00:37:47.330 --> 00:37:49.490
without changing the parameters of the

00:37:49.490 --> 00:37:49.980
model.

00:37:52.850 --> 00:37:55.110
So what this means for the the

00:37:55.860 --> 00:37:58.260
training of LLMs ecosystem

00:37:58.319 --> 00:37:59.379
is the following.

00:38:00.359 --> 00:38:02.030
So, traditionally, a lot of

00:38:02.669 --> 00:38:05.090
capital spends on is spent on pretraining.

00:38:05.750 --> 00:38:06.755
And then

00:38:08.930 --> 00:38:11.430
significantly less capital spent on fine tuning

00:38:11.629 --> 00:38:13.280
and at test time, it has

00:38:13.840 --> 00:38:15.919
it's more like a chatbot scenario. There is

00:38:15.919 --> 00:38:17.470
just a single back and forth

00:38:18.909 --> 00:38:20.490
between the model and the user.

00:38:21.130 --> 00:38:23.209
And that means, like, very, very cheap test

00:38:23.210 --> 00:38:25.380
time. But what we are

00:38:25.380 --> 00:38:27.600
seeing here is that we can spend

00:38:27.600 --> 00:38:29.620
a lot more compute perhaps

00:38:29.620 --> 00:38:31.780
in the millions of dollar or more

00:38:32.020 --> 00:38:34.260
have the model online, go and

00:38:34.260 --> 00:38:36.080
spend explore

00:38:36.430 --> 00:38:38.159
what by that, we mean

00:38:38.440 --> 00:38:40.598
using a lot of test on compute, enabling

00:38:40.599 --> 00:38:42.680
the model to use this tool as

00:38:42.680 --> 00:38:44.930
well as other actual

00:38:44.930 --> 00:38:47.000
software tools. In order to get to

00:38:47.320 --> 00:38:49.784
more and more capability. And

00:38:50.329 --> 00:38:52.510
And the relationship between this

00:38:52.620 --> 00:38:54.809
and the reasoning models or

00:38:54.809 --> 00:38:56.150
thinking models is that

00:38:57.050 --> 00:38:59.320
now we can bring back

00:38:59.320 --> 00:39:01.810
this data and experiences

00:39:02.110 --> 00:39:04.190
generated by the model back to the

00:39:04.190 --> 00:39:05.750
model and fine tune it

00:39:06.310 --> 00:39:08.490
further to make it better and more capable

00:39:08.790 --> 00:39:11.120
and create some sort of a

00:39:11.260 --> 00:39:13.710
data flywheel for the model self

00:39:13.820 --> 00:39:14.320
improvement.

00:39:21.430 --> 00:39:23.720
Now I mentioned earlier

00:39:23.940 --> 00:39:26.160
that for this to work, we do

00:39:26.220 --> 00:39:28.480
need we do need access to,

00:39:31.240 --> 00:39:32.540
verifiers. Basically,

00:39:33.430 --> 00:39:35.880
in the simple repeated sampling setting,

00:39:36.120 --> 00:39:38.220
all we have is a bunch of samples,

00:39:38.280 --> 00:39:40.460
like 10,000 samples. How do we select

00:39:40.740 --> 00:39:42.818
which one is that how do we

00:39:43.060 --> 00:39:45.079
this this this

00:39:45.079 --> 00:39:47.158
select the correct one and still and

00:39:47.159 --> 00:39:49.000
and open question.

00:39:49.400 --> 00:39:51.430
So there to add to

00:39:51.430 --> 00:39:53.450
answer this, there are two kind

00:39:53.590 --> 00:39:53.870
of

00:39:55.800 --> 00:39:57.980
types of tasks that we deal with.

00:39:57.990 --> 00:40:00.409
One is one is that there are domains

00:40:00.470 --> 00:40:02.930
where we have automated verification,

00:40:03.680 --> 00:40:05.935
which I get to in a second, and

00:40:05.935 --> 00:40:08.129
then there are domains that

00:40:08.130 --> 00:40:10.300
we don't have automated verification,

00:40:10.520 --> 00:40:12.700
and there are different ways to go

00:40:12.700 --> 00:40:14.780
about it. For example, best of

00:40:14.780 --> 00:40:17.070
end and majority voting

00:40:17.070 --> 00:40:17.570
or

00:40:19.590 --> 00:40:21.670
majority voting and model based rankers

00:40:21.670 --> 00:40:23.579
is a way to kind of

00:40:25.320 --> 00:40:27.639
approximate best of that, but it turns

00:40:27.639 --> 00:40:29.100
out that there is actually

00:40:29.770 --> 00:40:32.110
a large gap between majority

00:40:32.170 --> 00:40:34.409
voting and reward based modeling

00:40:34.930 --> 00:40:37.169
and the actual true coverage that we can

00:40:37.169 --> 00:40:39.480
get from these models. So

00:40:39.480 --> 00:40:41.820
here in this case, we have the number of samples,

00:40:41.880 --> 00:40:44.130
k, again, varied between

00:40:44.130 --> 00:40:46.430
one to 10,000 then

00:40:46.430 --> 00:40:48.519
on the y axis, we have the success rate

00:40:49.080 --> 00:40:51.560
And while if we had access to an Oracle

00:40:51.560 --> 00:40:53.720
verifier, we could have

00:40:53.720 --> 00:40:55.570
a lot more gains from this

00:40:56.130 --> 00:40:58.290
test time scaling. It turns out that

00:40:58.290 --> 00:41:00.530
with using a method like majority voting

00:41:00.530 --> 00:41:02.659
or reward modeling, we can't

00:41:02.659 --> 00:41:04.770
really follow this and, like,

00:41:04.770 --> 00:41:07.010
distinguish the correct versus incorrect one,

00:41:07.790 --> 00:41:10.273
especially after 10 to 50

00:41:10.273 --> 00:41:11.270
or so

00:41:12.310 --> 00:41:14.390
samples. So the this

00:41:14.390 --> 00:41:16.410
is what's happening This

00:41:16.710 --> 00:41:19.050
is, what's what we call the generation

00:41:19.510 --> 00:41:20.840
verification gap.

00:41:22.130 --> 00:41:24.350
This is this gap that we

00:41:24.410 --> 00:41:26.570
want to kind of close, and we want to be able

00:41:26.570 --> 00:41:28.180
to follow the model and

00:41:28.600 --> 00:41:30.970
select the correct responses.

00:41:32.630 --> 00:41:34.490
Now there's certain applications

00:41:34.710 --> 00:41:37.020
in which where we are lucky

00:41:37.020 --> 00:41:39.180
and we have access or we can

00:41:39.180 --> 00:41:40.879
create some sort of automated

00:41:41.500 --> 00:41:43.760
verifiers, and one of them is

00:41:45.210 --> 00:41:47.290
using AI to to generate

00:41:47.290 --> 00:41:48.190
GPU kernels.

00:41:49.830 --> 00:41:50.330
So

00:41:53.260 --> 00:41:55.510
again, some of you in this room might have

00:41:56.149 --> 00:41:58.389
have to deal with it might it might have dealt

00:41:58.389 --> 00:42:00.690
with writing kudos kernels

00:42:00.690 --> 00:42:02.870
and so on. And

00:42:02.870 --> 00:42:04.970
what and although, if you do a

00:42:04.970 --> 00:42:06.830
good job at kernel design, it's

00:42:07.230 --> 00:42:09.250
amazing. It

00:42:09.310 --> 00:42:11.420
feels really good. But doing

00:42:11.420 --> 00:42:13.080
so, is

00:42:13.460 --> 00:42:15.240
really really hard because

00:42:15.620 --> 00:42:17.899
kernels are generally

00:42:18.200 --> 00:42:20.399
painful to ride, information

00:42:20.400 --> 00:42:22.399
about best practices for current

00:42:22.700 --> 00:42:24.950
design, is very fragmented.

00:42:25.410 --> 00:42:27.430
Best practices are also fragmented.

00:42:27.890 --> 00:42:29.900
It's hard to learn It's labor

00:42:29.900 --> 00:42:32.290
intensive. There are also

00:42:32.290 --> 00:42:34.350
new languages and DSLs that come.

00:42:34.830 --> 00:42:36.290
Come out pretty often.

00:42:37.800 --> 00:42:39.980
And, things change from one hardware

00:42:40.040 --> 00:42:41.890
to to to another.

00:42:42.520 --> 00:42:44.840
The idea here was that can we use

00:42:44.840 --> 00:42:47.050
test time scaling and get

00:42:47.430 --> 00:42:48.730
LLMs to write kernels?

00:42:49.800 --> 00:42:51.960
And the advantage here is that then we will

00:42:51.960 --> 00:42:53.500
have automated verifiers

00:42:55.810 --> 00:42:57.850
because for a given kernel and

00:42:57.850 --> 00:42:59.930
given PyTorch source code in

00:42:59.930 --> 00:43:02.029
this case, all we need to do

00:43:02.030 --> 00:43:03.980
is to to ensure that for

00:43:04.379 --> 00:43:06.639
input pairs going to the PyTorch source code,

00:43:07.360 --> 00:43:09.520
we get the same and and the kernel

00:43:09.520 --> 00:43:11.750
that the l l m design we get

00:43:11.810 --> 00:43:14.129
the same exact outputs. From both

00:43:14.129 --> 00:43:16.330
sources of code. So it's kind of

00:43:16.330 --> 00:43:18.270
a code translation

00:43:18.490 --> 00:43:20.910
from one language, PyTorch, to another language,

00:43:21.480 --> 00:43:23.560
CUDA in this case, and

00:43:23.560 --> 00:43:25.759
we we can just

00:43:25.760 --> 00:43:27.859
match the input output pairs And

00:43:27.859 --> 00:43:29.940
because machine learning models are more

00:43:29.940 --> 00:43:31.940
or less they have these static

00:43:32.140 --> 00:43:34.350
data flows, this becomes

00:43:35.760 --> 00:43:36.910
a much better

00:43:38.040 --> 00:43:40.089
much more suitable for

00:43:40.089 --> 00:43:41.950
our automated verification task.

00:43:42.730 --> 00:43:44.660
So last year or

00:43:45.060 --> 00:43:47.140
earlier this year, seems like a long time

00:43:47.140 --> 00:43:49.130
ago, we introduced

00:43:49.590 --> 00:43:51.820
KernelBench. Which is an evaluation

00:43:51.880 --> 00:43:53.340
framework for Kernel generation.

00:43:54.690 --> 00:43:56.430
Kernel bench is a suite of

00:43:57.069 --> 00:43:59.020
250 problems

00:43:59.740 --> 00:44:01.919
as well as an even and a framework

00:44:01.980 --> 00:44:04.100
that allows you to run these

00:44:04.160 --> 00:44:06.378
models and measure various

00:44:06.379 --> 00:44:08.470
performance correctness and also,

00:44:08.630 --> 00:44:10.090
other performance metrics.

00:44:10.950 --> 00:44:11.940
In terms of

00:44:13.280 --> 00:44:15.560
hardware efficiency. So there are

00:44:15.560 --> 00:44:18.060
three levels to kernel bench. Level one,

00:44:18.710 --> 00:44:21.159
is the simpler kernels

00:44:22.140 --> 00:44:24.640
and those are the single operations

00:44:24.940 --> 00:44:27.130
in PyTorch, such as convolution,

00:44:27.350 --> 00:44:28.100
matrix multiple,

00:44:30.270 --> 00:44:32.349
These are simpler in the sense that the input

00:44:32.350 --> 00:44:33.570
is shorter, smaller,

00:44:34.399 --> 00:44:36.419
but they're actually pretty hard to optimize

00:44:36.480 --> 00:44:38.880
because over baseline because

00:44:39.780 --> 00:44:42.040
baselines such as PyTorch, these are heavily

00:44:42.960 --> 00:44:44.500
ops because these are used a lot.

00:44:45.570 --> 00:44:47.770
Level two, we had effusion of,

00:44:48.470 --> 00:44:50.550
a collection of ops, so these were

00:44:50.550 --> 00:44:52.649
more like layers in a deep learning

00:44:52.790 --> 00:44:53.140
model.

00:44:54.840 --> 00:44:55.250
And

00:44:57.450 --> 00:44:59.550
this would kind of test the LLM's

00:44:59.690 --> 00:45:01.180
ability to,

00:45:01.720 --> 00:45:03.690
write to be

00:45:05.270 --> 00:45:07.429
ability in writing fused ops

00:45:08.069 --> 00:45:10.460
which is a very important skill

00:45:10.460 --> 00:45:12.650
set and for writing good CUDA

00:45:12.710 --> 00:45:14.789
kernels. And level three are end

00:45:14.790 --> 00:45:16.400
to end models,

00:45:17.040 --> 00:45:19.120
such as mobile net, mini, GPT,

00:45:19.120 --> 00:45:21.440
and so on. And in this case, this is

00:45:21.440 --> 00:45:23.740
like a much harder

00:45:23.740 --> 00:45:26.140
task. We are basically giving them the

00:45:26.280 --> 00:45:28.590
entire an entire model and asking it

00:45:28.830 --> 00:45:30.920
write the CUDA kernel for the entire model.

00:45:33.665 --> 00:45:35.840
So I the

00:45:35.840 --> 00:45:37.919
framework works as, follows. So

00:45:37.919 --> 00:45:40.280
we have a problem, and input

00:45:41.450 --> 00:45:43.139
problem in PyTorch. We have the AI

00:45:43.620 --> 00:45:46.040
looks into the problem, generates the kernels,

00:45:46.230 --> 00:45:48.470
and then it gets various

00:45:48.470 --> 00:45:50.409
source of feedback, like it compiles

00:45:52.399 --> 00:45:54.560
it gets feedback about correctness, whether

00:45:54.560 --> 00:45:56.599
the out there's an output mismatch with

00:45:56.600 --> 00:45:58.680
the source code, It also gets

00:45:58.680 --> 00:46:00.620
the execution time from the PyTorch

00:46:00.920 --> 00:46:03.039
Profiler. And all these sort

00:46:03.040 --> 00:46:05.060
of feedback goes back to the model.

00:46:05.419 --> 00:46:07.200
And the model can then generate

00:46:07.579 --> 00:46:09.470
a new version of the

00:46:10.409 --> 00:46:11.990
of the CUDA kernel and so on.

00:46:13.590 --> 00:46:15.590
And when we released the paper, we did

00:46:15.990 --> 00:46:18.050
this set of analysis where

00:46:18.510 --> 00:46:20.330
we looked into how much,

00:46:20.730 --> 00:46:22.810
this iterative loop of test

00:46:22.810 --> 00:46:24.890
time scaling together with using

00:46:26.460 --> 00:46:28.560
feedback from, the execution results

00:46:29.100 --> 00:46:31.240
or for correctness or for

00:46:31.320 --> 00:46:33.180
from the profiler, for the performance,

00:46:34.360 --> 00:46:36.689
helps the model generate better

00:46:36.690 --> 00:46:38.929
and better kernels. And what we see

00:46:38.930 --> 00:46:41.220
here is that it does the model actually

00:46:41.220 --> 00:46:43.270
can leverage this feedback to

00:46:43.270 --> 00:46:45.349
an extent both

00:46:45.350 --> 00:46:47.290
from the accuracy and performance side

00:46:47.510 --> 00:46:49.550
and when we reach to 10

00:46:49.550 --> 00:46:51.750
or so iterations, the models can

00:46:51.750 --> 00:46:54.000
do pretty well I believe

00:46:54.000 --> 00:46:56.460
these are our op

00:46:56.520 --> 00:46:58.599
level one results, and these are, in

00:46:58.600 --> 00:47:00.770
this case, we were seeing whether we can

00:47:00.770 --> 00:47:01.520
match PyTorch

00:47:03.300 --> 00:47:05.570
performance. And what we are showing

00:47:05.570 --> 00:47:08.090
here is in the best case, we get to 70%

00:47:09.050 --> 00:47:10.800
of level one results, matching with

00:47:11.385 --> 00:47:11.885
PyTorch.

00:47:13.900 --> 00:47:16.300
At the same this is great, and things

00:47:16.300 --> 00:47:18.560
are helping, the time scaling is helping,

00:47:18.820 --> 00:47:20.899
We have automated verifiers, but

00:47:20.900 --> 00:47:22.910
we are still a long time a long

00:47:22.910 --> 00:47:25.069
way, to go from

00:47:25.070 --> 00:47:26.850
having AI to generate kernels

00:47:28.500 --> 00:47:30.659
This benchmark generated a lot of interest,

00:47:30.659 --> 00:47:32.850
many companies started reporting

00:47:32.850 --> 00:47:33.849
results on it.

00:47:35.405 --> 00:47:35.905
And

00:47:37.419 --> 00:47:39.660
along the way, we have found many different

00:47:39.660 --> 00:47:41.260
ways that the model can do

00:47:42.360 --> 00:47:44.599
reward hacking or a reward can

00:47:44.600 --> 00:47:46.820
not capture the correct automate the

00:47:46.820 --> 00:47:48.700
correct kind of validation.

00:47:48.860 --> 00:47:50.960
So it has been a long

00:47:50.960 --> 00:47:53.080
process of about a year that we have

00:47:53.720 --> 00:47:54.700
worked on this,

00:47:57.869 --> 00:48:00.360
benchmark we are very happy to see, like,

00:48:02.750 --> 00:48:04.909
many major frontier labs and also

00:48:04.909 --> 00:48:06.850
other companies, application companies,

00:48:07.790 --> 00:48:09.330
building on top of, KernelBench.

00:48:13.600 --> 00:48:15.840
So going back in KernelBench, we

00:48:15.840 --> 00:48:18.135
had auto automated rewards

00:48:20.090 --> 00:48:22.040
but there are other many

00:48:22.180 --> 00:48:23.960
other domains where

00:48:24.520 --> 00:48:26.060
automated verification is harder

00:48:26.710 --> 00:48:28.490
to, to to to

00:48:28.730 --> 00:48:30.669
to have or it's nonexistent.

00:48:32.190 --> 00:48:34.290
So now, in this case,

00:48:34.890 --> 00:48:37.130
I'm going to talk about a project where we

00:48:37.130 --> 00:48:39.369
use test time scaling this

00:48:39.369 --> 00:48:41.609
time to directly kind of close

00:48:41.609 --> 00:48:44.045
the generation verification gap.

00:48:45.250 --> 00:48:45.730
So,

00:48:47.570 --> 00:48:50.050
So the idea that we had was, let's say,

00:48:50.050 --> 00:48:52.370
we have a a large set of samples

00:48:52.830 --> 00:48:55.090
and we want to create to collect the correct

00:48:55.840 --> 00:48:57.860
to, identify the correct sample,

00:48:58.625 --> 00:49:01.020
and let's do that by

00:49:01.080 --> 00:49:03.218
using a technique that

00:49:03.218 --> 00:49:04.329
is more or less

00:49:05.770 --> 00:49:07.790
familiar in the in classic

00:49:07.850 --> 00:49:10.040
machine learning, and that's weak, too, strong,

00:49:10.040 --> 00:49:12.289
super and and

00:49:12.290 --> 00:49:14.069
the way we approached this was that

00:49:14.540 --> 00:49:16.810
we throw in a bunch of

00:49:16.810 --> 00:49:19.070
different verifiers that were all not

00:49:19.710 --> 00:49:21.790
strong as we saw in

00:49:21.790 --> 00:49:23.820
the previous slides, In

00:49:23.820 --> 00:49:25.980
this case, we had an LLM judge,

00:49:25.980 --> 00:49:27.440
we had reward models,

00:49:28.820 --> 00:49:31.020
a suite of them, that each

00:49:31.020 --> 00:49:33.180
independently would score each of

00:49:33.180 --> 00:49:35.030
the generations, and our goal

00:49:35.590 --> 00:49:37.669
was that, can we use these weak

00:49:37.669 --> 00:49:39.850
verifiers and come up with a single

00:49:40.220 --> 00:49:42.640
score for the, quality

00:49:42.640 --> 00:49:43.460
of a sample

00:49:45.280 --> 00:49:47.520
and basically bootstrap from these big

00:49:47.520 --> 00:49:49.629
tech players. So the way it works is that it's

00:49:49.869 --> 00:49:51.950
there are three kind of stages. First

00:49:51.950 --> 00:49:54.180
is the score. In the

00:49:54.180 --> 00:49:56.200
scoring part, again, we go through

00:49:56.200 --> 00:49:58.659
every candidate solution and

00:49:58.720 --> 00:49:59.920
then we have our

00:50:01.120 --> 00:50:03.419
verifier whether it's an LLM

00:50:03.420 --> 00:50:04.800
judge or a model

00:50:05.560 --> 00:50:07.660
add a, like, a sinus score to these samples,

00:50:09.030 --> 00:50:11.059
then we have the waiting, and

00:50:11.060 --> 00:50:13.379
that's the core, like, optimization that

00:50:13.860 --> 00:50:15.940
goes here in this fork that we use week

00:50:15.940 --> 00:50:18.120
to, soup weak

00:50:18.120 --> 00:50:19.140
supervision. To,

00:50:21.930 --> 00:50:22.970
to kind of,

00:50:24.090 --> 00:50:26.169
estimate the quality of each verifier and

00:50:26.169 --> 00:50:27.879
then bring them together.

00:50:28.200 --> 00:50:30.359
Then the selection becomes easy an easier

00:50:30.359 --> 00:50:32.660
problem because that's, like, combining all

00:50:32.660 --> 00:50:34.520
these scores, weighted scores, and,

00:50:35.530 --> 00:50:37.650
choosing the one with the highest that we are

00:50:37.890 --> 00:50:38.869
most confident about.

00:50:40.030 --> 00:50:41.470
Now a little bit of,

00:50:42.150 --> 00:50:44.139
kind of underlying

00:50:44.200 --> 00:50:46.220
math behind this week's supervision.

00:50:46.950 --> 00:50:49.030
So first of all, again, weak supervision is

00:50:49.030 --> 00:50:51.220
a technique in traditional

00:50:51.280 --> 00:50:51.970
machine learning.

00:50:53.660 --> 00:50:55.980
Where we use weak verifiers, noisy

00:50:55.980 --> 00:50:57.830
labeling, sources, and

00:50:58.310 --> 00:51:00.570
use stronger results

00:51:00.950 --> 00:51:03.040
by combining them. The

00:51:03.040 --> 00:51:05.090
setup that we had was that we had

00:51:05.090 --> 00:51:06.735
n problems

00:51:07.640 --> 00:51:09.740
k generated solutions per problem,

00:51:10.820 --> 00:51:13.019
and then we had m verifiers. And

00:51:13.020 --> 00:51:15.180
the goal was that we wanted to predict

00:51:15.180 --> 00:51:17.119
the probability of why

00:51:18.370 --> 00:51:20.860
the correct being correct or the response

00:51:21.000 --> 00:51:23.480
being correct given all these m different

00:51:23.859 --> 00:51:26.220
verifier scores. And the assumption

00:51:26.280 --> 00:51:28.330
that we made was that each of

00:51:28.330 --> 00:51:29.710
these verifiers capture

00:51:30.540 --> 00:51:32.960
independent aspects of the correctness.

00:51:33.619 --> 00:51:35.990
Across these problems. And given

00:51:35.990 --> 00:51:37.520
this assumption, we could write this

00:51:38.940 --> 00:51:41.040
probability equation that relates

00:51:41.300 --> 00:51:43.110
the the the individual

00:51:43.330 --> 00:51:44.390
responses to,

00:51:46.380 --> 00:51:47.840
individual verifier.

00:51:49.060 --> 00:51:50.980
Scores. And given the

00:51:51.220 --> 00:51:53.240
this equation and the other

00:51:53.379 --> 00:51:53.960
probability conditional

00:51:55.810 --> 00:51:57.590
equation that holds for any scenario,

00:51:58.220 --> 00:52:00.350
then could go ahead and back solve

00:52:00.350 --> 00:52:02.520
for for these probabilities

00:52:03.860 --> 00:52:05.880
The the the probability of

00:52:05.880 --> 00:52:07.740
why or the response being correct,

00:52:08.340 --> 00:52:10.459
given all of the, other responses

00:52:10.460 --> 00:52:11.840
that we got from the verifiers.

00:52:13.139 --> 00:52:15.299
And it turns out that this actually does

00:52:15.300 --> 00:52:16.750
work very well.

00:52:17.825 --> 00:52:19.910
So in this case, we are looking

00:52:19.910 --> 00:52:22.170
into harder problems from GPQA

00:52:22.230 --> 00:52:24.720
diamond, math and MLU Pro.

00:52:25.345 --> 00:52:27.280
And for our baseline,

00:52:28.220 --> 00:52:30.459
again, we have repeated sampling in

00:52:30.460 --> 00:52:32.529
our general setup. So

00:52:32.530 --> 00:52:34.470
we are increasing the number of repeated

00:52:34.930 --> 00:52:37.040
samples from one to two to the

00:52:37.100 --> 00:52:37.450
power 10.

00:52:39.210 --> 00:52:41.060
And we are looking into

00:52:41.620 --> 00:52:43.920
majority voting and these other

00:52:43.920 --> 00:52:45.700
methods called called multi agent verification,

00:52:46.680 --> 00:52:48.930
Again, we see similar familiar results

00:52:48.930 --> 00:52:51.280
where the model, these approaches

00:52:51.340 --> 00:52:53.690
failed to follow the true

00:52:54.649 --> 00:52:57.069
coverage that we could get from this setup.

00:52:58.580 --> 00:53:00.440
And then in this case, we are using

00:53:01.020 --> 00:53:03.100
we are showing naive ensemble where

00:53:03.100 --> 00:53:05.250
we just average this scores

00:53:05.250 --> 00:53:06.660
by the models in our

00:53:07.700 --> 00:53:08.440
ensemble of

00:53:10.210 --> 00:53:12.309
vyfers, as a baseline, which

00:53:12.310 --> 00:53:14.409
turns out to be a very strong baseline.

00:53:15.669 --> 00:53:17.989
Then, you're showing the weaver or the speak

00:53:17.990 --> 00:53:19.670
to strong collect

00:53:20.070 --> 00:53:22.310
average rating of the scores where we

00:53:22.310 --> 00:53:24.480
get additional gains over that.

00:53:24.480 --> 00:53:25.860
And we show that these gains

00:53:26.740 --> 00:53:29.000
naturally go up as we have more

00:53:29.830 --> 00:53:31.850
labels for our task

00:53:31.850 --> 00:53:33.730
and the verifier scores for our task.

00:53:34.450 --> 00:53:34.890
So

00:53:37.530 --> 00:53:39.630
what this trend is showing

00:53:39.850 --> 00:53:41.930
that we are again seeing that we

00:53:41.930 --> 00:53:44.280
are using test time scaling this

00:53:44.280 --> 00:53:46.360
time on the not just on

00:53:46.360 --> 00:53:48.230
the generation side, but on the very

00:53:49.590 --> 00:53:51.869
we and that also boosts the

00:53:52.480 --> 00:53:54.100
overall performance of the model.

00:53:55.560 --> 00:53:57.720
And this turns out to be, again, a

00:53:57.720 --> 00:53:59.100
very effective technique

00:54:00.110 --> 00:54:02.350
in bringing up the capability of models

00:54:02.350 --> 00:54:04.050
to kind of a next generation

00:54:04.680 --> 00:54:06.769
or next model size.

00:54:07.010 --> 00:54:09.280
For example, when we apply

00:54:09.280 --> 00:54:11.460
this approach to lama eight b,

00:54:11.869 --> 00:54:14.159
across these problems, we

00:54:14.379 --> 00:54:16.509
are kind of increasing the capability

00:54:16.935 --> 00:54:19.169
the the average correctness of

00:54:19.169 --> 00:54:21.330
the model from 57%

00:54:21.330 --> 00:54:23.399
to 70%. And this

00:54:23.399 --> 00:54:25.719
is very close to the performance of

00:54:25.720 --> 00:54:28.629
LaMDA 70 b, which is 72%

00:54:28.630 --> 00:54:30.850
across these tasks. So we are kind of

00:54:31.410 --> 00:54:33.570
bringing up the 70 b mod seven

00:54:33.570 --> 00:54:35.670
b model all the way to

00:54:35.690 --> 00:54:37.710
the performance of a 70

00:54:37.930 --> 00:54:39.939
b model, using this test time scaling

00:54:40.980 --> 00:54:43.059
on the generation and verification. And the

00:54:43.060 --> 00:54:45.299
same is true for the lama 70

00:54:45.300 --> 00:54:46.419
b and the o three,

00:54:47.585 --> 00:54:49.730
mini. Performance. Again, we

00:54:49.730 --> 00:54:51.889
are going from the

00:54:51.890 --> 00:54:54.230
70 in the 70 range performance

00:54:54.290 --> 00:54:56.390
to 86%. Which

00:54:56.390 --> 00:54:58.290
matches the o t o three mini.

00:54:58.850 --> 00:55:01.010
Performance. And and what is important

00:55:01.010 --> 00:55:03.100
here is that for we

00:55:03.100 --> 00:55:05.359
where we are using a very,

00:55:05.580 --> 00:55:07.760
very small number

00:55:07.760 --> 00:55:10.250
of label label samples across

00:55:13.180 --> 00:55:15.240
across the training set.

00:55:16.430 --> 00:55:18.270
And you're showing results on our

00:55:18.590 --> 00:55:20.835
on a on a variety of dev dev sets all

00:55:20.835 --> 00:55:22.350
at the same time.

00:55:22.910 --> 00:55:25.110
So so that suggests that there is

00:55:25.830 --> 00:55:27.170
more to be kind of

00:55:28.180 --> 00:55:30.260
explored here and elicited from the

00:55:30.260 --> 00:55:32.210
model. By applying these

00:55:32.770 --> 00:55:34.825
scaling methods.

00:55:36.720 --> 00:55:38.940
Now in terms of research,

00:55:39.360 --> 00:55:41.500
before I get to the next part

00:55:41.500 --> 00:55:42.320
of my talk,

00:55:43.900 --> 00:55:45.910
terms of research directions for this

00:55:46.390 --> 00:55:48.470
test time scaling, it feels like

00:55:48.470 --> 00:55:50.250
we are still kind of early in

00:55:50.490 --> 00:55:52.888
our understanding of the core principles

00:55:52.889 --> 00:55:54.590
that drive test time scaling.

00:55:55.650 --> 00:55:58.030
And this property is kind of opening

00:55:58.030 --> 00:56:00.120
a new era for us which

00:56:00.120 --> 00:56:02.220
is the era of synthetic data.

00:56:02.659 --> 00:56:04.690
Because we are kind of creating this

00:56:04.849 --> 00:56:06.840
We have now these agents and

00:56:06.980 --> 00:56:09.220
models that can create these

00:56:09.440 --> 00:56:11.060
new experiences indefinitely

00:56:11.990 --> 00:56:14.009
and those are great sources for us to

00:56:15.050 --> 00:56:17.290
create training data or maybe have better ways like

00:56:17.290 --> 00:56:19.550
continual learning where the

00:56:19.680 --> 00:56:21.930
models keep improving without this

00:56:21.990 --> 00:56:24.280
kind of rather unnatural

00:56:25.980 --> 00:56:28.209
kind of a way that we approach modeling right

00:56:28.210 --> 00:56:30.130
now where the models generate these,

00:56:30.850 --> 00:56:32.710
synthetic data and then offline

00:56:33.410 --> 00:56:35.570
maybe every once in a while, we bring them back to

00:56:35.570 --> 00:56:37.840
the model. It seems like there should

00:56:37.840 --> 00:56:40.159
be a more ways to do this more

00:56:40.159 --> 00:56:42.050
smoothly and continually.

00:56:43.120 --> 00:56:45.199
And the other research in this direction would be

00:56:45.200 --> 00:56:47.250
the infra that we would need for for

00:56:47.250 --> 00:56:49.350
high throughput and low low latency

00:56:49.660 --> 00:56:51.899
test time scaling because these kind

00:56:51.900 --> 00:56:52.230
of

00:56:54.010 --> 00:56:56.020
frameworks look very, very different from

00:56:56.640 --> 00:56:58.980
the mainstream chatbot frameworks.

00:56:59.460 --> 00:57:01.960
Where we have single back and forth with the model.

00:57:02.020 --> 00:57:04.200
So they they require their own custom

00:57:05.159 --> 00:57:07.579
ways that we deal with the hardware

00:57:07.639 --> 00:57:09.800
and the systems that we build around them.

00:57:11.160 --> 00:57:13.399
Now, I want to, for

00:57:13.400 --> 00:57:15.649
the last part of my talk want to talk

00:57:15.649 --> 00:57:17.800
about the recent work that we did

00:57:17.800 --> 00:57:18.955
at Stanford

00:57:20.160 --> 00:57:22.199
that is called Intelligence per

00:57:22.200 --> 00:57:24.260
watt. I get to that. But the

00:57:24.639 --> 00:57:25.139
idea

00:57:27.840 --> 00:57:29.910
that motivated this work was that

00:57:29.910 --> 00:57:32.170
we wanted to see how local models

00:57:33.500 --> 00:57:35.960
small models, how their capability

00:57:36.180 --> 00:57:37.940
is improving over the years,

00:57:38.419 --> 00:57:40.839
and what that means for the future of workload

00:57:40.899 --> 00:57:43.370
distribution or traffic distribution

00:57:43.430 --> 00:57:45.650
for AI. So first of

00:57:45.650 --> 00:57:47.850
all, the demand for compute

00:57:49.040 --> 00:57:50.510
is exploding because of

00:57:51.270 --> 00:57:53.750
the AI specifically because of AI serving

00:57:53.750 --> 00:57:54.250
demand.

00:57:56.530 --> 00:57:58.640
Of So Google Cloud in

00:57:58.640 --> 00:58:00.415
the last twenty months

00:58:01.220 --> 00:58:03.640
had a 1,200 increase in their

00:58:03.660 --> 00:58:05.160
compute needs,

00:58:06.060 --> 00:58:08.140
and NVIDIA had a ten year 10

00:58:08.140 --> 00:58:10.390
x year over year increase in their compute

00:58:10.390 --> 00:58:12.269
like, demands, basically.

00:58:13.070 --> 00:58:15.190
And the interest infra response

00:58:15.190 --> 00:58:17.260
for that, that was that we have

00:58:17.510 --> 00:58:19.920
we need something in order of 250 gigawatt

00:58:20.635 --> 00:58:22.740
power data centers.

00:58:22.980 --> 00:58:25.399
Like, data centers that can serve this much

00:58:25.990 --> 00:58:28.440
power. And the graph on the right

00:58:28.820 --> 00:58:30.909
shows that this really is coming from

00:58:30.909 --> 00:58:33.010
these additional like, this

00:58:33.409 --> 00:58:35.490
explosion in the demand in

00:58:35.490 --> 00:58:37.530
AI serving going from something like

00:58:37.530 --> 00:58:39.510
in order of 160,000,000,000,000 to

00:58:39.669 --> 00:58:41.770
1.3 tokens

00:58:43.730 --> 00:58:45.520
in in in a year and a half. And so

00:58:47.600 --> 00:58:49.650
now we looked into

00:58:49.890 --> 00:58:52.130
this data set of chat GPT, like

00:58:52.130 --> 00:58:53.670
a million chat GPT,

00:58:54.530 --> 00:58:56.710
queries. The dataset was released,

00:58:56.710 --> 00:58:58.420
and we were looking into the

00:58:58.740 --> 00:59:00.449
distribution of this dataset.

00:59:00.849 --> 00:59:03.109
What kind of questions people ask?

00:59:06.440 --> 00:59:08.359
And it turns out that 70 per

00:59:08.520 --> 00:59:10.539
77% of requests

00:59:11.100 --> 00:59:13.360
of, chat GPT are

00:59:14.340 --> 00:59:16.500
practical guidance, information seeking,

00:59:16.500 --> 00:59:18.550
and writing, And what this

00:59:18.550 --> 00:59:20.550
means is that I we

00:59:21.090 --> 00:59:23.410
don't need necessarily the best frontier

00:59:23.410 --> 00:59:24.810
models to answer these

00:59:25.780 --> 00:59:27.940
And instead, local small local

00:59:27.940 --> 00:59:30.090
models, which in this work we define as

00:59:30.090 --> 00:59:31.200
as models with

00:59:32.159 --> 00:59:34.050
20,000,000,000 parameter then,

00:59:34.590 --> 00:59:36.980
active parameters, or

00:59:36.980 --> 00:59:39.350
less. They can they can

00:59:39.830 --> 00:59:42.080
answer these type of questions.

00:59:44.220 --> 00:59:46.380
So that's one side of, one side of

00:59:46.380 --> 00:59:48.450
the story where small

00:59:48.510 --> 00:59:50.580
models are capable of

00:59:51.060 --> 00:59:52.840
addressing a lot of the queries,

00:59:53.880 --> 00:59:55.910
On the other side, we are also seeing

00:59:55.910 --> 00:59:58.050
that more we are we are having

00:59:58.110 --> 01:00:00.540
more and more powerful local

01:00:00.760 --> 01:00:01.220
accelerators

01:00:03.510 --> 01:00:05.290
Since 2012,

01:00:05.550 --> 01:00:07.649
there there was a 126

01:00:07.710 --> 01:00:09.819
x improvement in

01:00:09.820 --> 01:00:11.900
the memory size of the of the local

01:00:11.900 --> 01:00:14.020
accelerators And what that means

01:00:14.020 --> 01:00:15.840
is that we can we can run

01:00:16.640 --> 01:00:18.800
larger and larger models locally, maybe on

01:00:18.800 --> 01:00:20.300
our laptop and so on.

01:00:21.740 --> 01:00:23.530
So the question to that we

01:00:23.930 --> 01:00:25.630
wanted to answer here is that

01:00:26.090 --> 01:00:27.789
what role can

01:00:28.590 --> 01:00:31.050
local inference play in redistributing

01:00:31.510 --> 01:00:33.450
the AI traffic and the inference

01:00:33.670 --> 01:00:35.700
demand? And to

01:00:35.700 --> 01:00:37.800
answer this, we define this new

01:00:37.960 --> 01:00:40.049
metric first, that kind

01:00:40.050 --> 01:00:41.830
of captures both the capability,

01:00:42.210 --> 01:00:44.440
but also the efficiency

01:00:45.260 --> 01:00:47.340
and performance from the hardware and

01:00:47.340 --> 01:00:49.560
system system side of the models, and we

01:00:49.560 --> 01:00:51.660
call this the intelligence per watt.

01:00:52.430 --> 01:00:54.130
So the way this works is that

01:00:55.010 --> 01:00:57.490
on the capability side, we are measuring the accuracy,

01:00:57.490 --> 01:00:59.618
and on the efficiency side, we

01:00:59.619 --> 01:01:01.200
are measuring the mean

01:01:01.815 --> 01:01:03.290
power used to,

01:01:04.340 --> 01:01:06.750
kind of address

01:01:06.890 --> 01:01:09.150
or solve a task by the model. So intelligence

01:01:09.290 --> 01:01:11.200
per watt is basically

01:01:11.580 --> 01:01:12.809
accuracy of the task,

01:01:13.610 --> 01:01:15.820
per watt. Per wattage of the model.

01:01:19.640 --> 01:01:21.900
And we to really

01:01:21.900 --> 01:01:23.840
see how the trends are changing

01:01:23.980 --> 01:01:26.000
and put everything together, we

01:01:26.000 --> 01:01:28.230
looked into a large

01:01:28.600 --> 01:01:30.680
kind of suite of models, 20

01:01:30.680 --> 01:01:32.780
plus state of the art local models

01:01:34.040 --> 01:01:36.230
different types of hardware,

01:01:36.230 --> 01:01:38.090
both local and cloud,

01:01:39.310 --> 01:01:41.810
accelerators, workflows from

01:01:43.339 --> 01:01:45.280
Charge GPT queries, general reasoning,

01:01:46.275 --> 01:01:48.520
and general

01:01:48.580 --> 01:01:49.849
reasoning and

01:01:50.970 --> 01:01:53.050
match problem solving task and on

01:01:53.050 --> 01:01:55.200
the evaluation side, we looked into a

01:01:55.200 --> 01:01:57.620
variety of metrics from accuracy, throughput,

01:01:59.130 --> 01:02:01.430
energy, and so on. And we have released

01:02:01.430 --> 01:02:03.739
all of this data, which

01:02:03.740 --> 01:02:05.818
is like a lot very I think

01:02:05.819 --> 01:02:08.000
we can do a whole lot more with this dataset.

01:02:08.800 --> 01:02:10.959
That had all these kind of, pieces

01:02:10.960 --> 01:02:12.270
in it. But

01:02:13.070 --> 01:02:15.140
our analysis showed the following.

01:02:15.380 --> 01:02:17.620
Which I find way very, very

01:02:17.620 --> 01:02:19.690
interesting. So it turns out

01:02:19.690 --> 01:02:21.285
that local models

01:02:22.140 --> 01:02:24.000
their capabilities improving,

01:02:25.630 --> 01:02:27.690
significantly just in the

01:02:27.690 --> 01:02:29.700
past two years, the

01:02:29.700 --> 01:02:31.790
accuracy on this suite of tasks that

01:02:31.790 --> 01:02:33.660
we we looked into

01:02:34.060 --> 01:02:36.620
the accuracy went up by 3.1

01:02:36.620 --> 01:02:38.770
x. In just two years. So,

01:02:38.770 --> 01:02:41.080
meaning, these like local small

01:02:41.080 --> 01:02:43.370
models are becoming really, really

01:02:43.370 --> 01:02:43.470
good.

01:02:45.740 --> 01:02:47.550
At the same time, right

01:02:47.850 --> 01:02:50.010
now, the way things work is that local

01:02:50.010 --> 01:02:51.630
accelerators in terms of their if

01:02:52.085 --> 01:02:54.080
efficiency, in IPW

01:02:54.300 --> 01:02:56.280
metrics or intelligence per watt,

01:02:56.440 --> 01:02:58.560
they're still lower. Than

01:02:59.600 --> 01:03:01.560
cloud, accelerators like

01:03:02.040 --> 01:03:04.370
an m four Apple m four Max is

01:03:04.610 --> 01:03:06.840
has a lower performance, than

01:03:06.840 --> 01:03:09.000
a b 200 in

01:03:09.000 --> 01:03:10.590
how in in its

01:03:12.340 --> 01:03:14.500
normalized efficiency metric that

01:03:14.500 --> 01:03:16.540
we we define. And that is not

01:03:16.540 --> 01:03:18.039
very surprising because these

01:03:18.760 --> 01:03:20.940
cloud accelerators are heavily optimized

01:03:21.620 --> 01:03:23.780
for running these LLMs versus versus

01:03:23.780 --> 01:03:25.950
the m four might not be as

01:03:25.950 --> 01:03:28.259
optimized or customized to serve these models.

01:03:28.500 --> 01:03:30.440
But that can but that can change.

01:03:31.960 --> 01:03:34.200
And the other piece of information that we

01:03:34.200 --> 01:03:36.540
found was that the

01:03:36.680 --> 01:03:38.549
intelligence efficiency is gaining

01:03:38.790 --> 01:03:40.950
is actually improving significantly as

01:03:40.950 --> 01:03:42.960
well. So in the

01:03:42.961 --> 01:03:45.082
past two years, the improvement here was something in

01:03:45.082 --> 01:03:47.150
the order of 5.3

01:03:47.150 --> 01:03:49.260
x and 3.1

01:03:49.260 --> 01:03:51.500
x of it was from better models, but another

01:03:51.500 --> 01:03:53.710
1.7 x was from

01:03:53.930 --> 01:03:56.090
better hardware. What this trend

01:03:56.470 --> 01:03:58.409
is suggesting that our models,

01:03:58.550 --> 01:04:00.570
our small models are becoming better,

01:04:00.850 --> 01:04:02.930
Our small local hardware is

01:04:02.930 --> 01:04:05.000
also becoming better. And

01:04:05.000 --> 01:04:07.100
that means a lot of the traffic

01:04:07.160 --> 01:04:09.030
of AI inference and AI

01:04:09.590 --> 01:04:11.800
workflows can be actually addressed locally

01:04:13.240 --> 01:04:14.680
without the need for us to

01:04:15.319 --> 01:04:17.020
kind of offload them to

01:04:17.480 --> 01:04:19.639
cloud, to proprietary, or other

01:04:19.639 --> 01:04:21.270
bigger large models.

01:04:25.220 --> 01:04:27.470
And what this means

01:04:27.470 --> 01:04:29.569
is that maybe

01:04:30.030 --> 01:04:31.910
we need newer inference

01:04:32.050 --> 01:04:33.420
serving engines

01:04:34.060 --> 01:04:36.220
that are much more hybrid than what we

01:04:36.220 --> 01:04:38.400
have today. That can

01:04:38.620 --> 01:04:40.930
route traffic between our local

01:04:40.930 --> 01:04:42.790
accelerators and cloud accelerators

01:04:43.700 --> 01:04:46.120
and and that could be an interesting research

01:04:46.180 --> 01:04:47.865
topic but also

01:04:48.870 --> 01:04:49.290
very,

01:04:50.890 --> 01:04:53.150
very, very helpful in terms of how

01:04:53.520 --> 01:04:55.600
we get we can save

01:04:55.600 --> 01:04:57.680
our energy. We can reduce costs and

01:04:57.680 --> 01:04:58.180
so on.

01:05:00.480 --> 01:05:02.560
And, of course, this also needs new models

01:05:02.560 --> 01:05:04.640
and kernels that should go on to this

01:05:04.640 --> 01:05:06.610
local accelerators

01:05:06.830 --> 01:05:08.500
and better ways for us to

01:05:09.005 --> 01:05:10.649
measure intelligence

01:05:11.030 --> 01:05:12.980
per watt and similar metrics.

01:05:14.119 --> 01:05:16.460
So with that, I can conclude the talk,

01:05:17.000 --> 01:05:19.070
and here is the summary we talked

01:05:19.070 --> 01:05:21.240
about test time scaling. We talked about

01:05:21.240 --> 01:05:23.250
different ways that test time scaling make models

01:05:23.250 --> 01:05:25.309
at each size better.

01:05:26.030 --> 01:05:27.915
And that includes smaller models,

01:05:28.100 --> 01:05:30.339
matching the performance of larger models

01:05:30.340 --> 01:05:32.790
in many cases, And we

01:05:32.790 --> 01:05:34.519
also talked about the trend in

01:05:35.080 --> 01:05:36.250
hardware and model

01:05:37.460 --> 01:05:39.700
capability and how a lot of the

01:05:39.700 --> 01:05:41.540
AI inference traffic

01:05:42.320 --> 01:05:44.440
can be, kind of routed

01:05:44.440 --> 01:05:46.310
locally in the future.

01:05:48.310 --> 01:05:49.050
Thank you.

01:06:08.670 --> 01:06:10.790
Okay. We have this

01:06:11.190 --> 01:06:12.970
a few minutes for audience questions.

01:06:13.850 --> 01:06:16.169
It turns out we don't have the mic for the audience,

01:06:16.169 --> 01:06:17.310
but if you

01:06:19.020 --> 01:06:21.100
okay. We do have one, actually. Okay. So we

01:06:21.100 --> 01:06:22.880
have one question over there.

01:06:27.700 --> 01:06:29.919
Hi. Hi. Here. Thanks

01:06:29.919 --> 01:06:32.320
for the talk. I have a question

01:06:32.320 --> 01:06:34.710
from your earlier discussion

01:06:34.710 --> 01:06:36.849
where you talked of the test time scale and

01:06:36.849 --> 01:06:38.950
you did auto generation optimization

01:06:39.250 --> 01:06:41.520
of the kernel, GPU kernels. So

01:06:41.520 --> 01:06:43.720
my question is, are you able to extend

01:06:43.849 --> 01:06:45.534
it to the

01:06:46.550 --> 01:06:48.890
level where we can have just in time

01:06:49.110 --> 01:06:51.460
generation of optimized kernel

01:06:51.600 --> 01:06:53.830
based on the target hardware and the kind

01:06:53.830 --> 01:06:56.030
of workload is coming up. So is

01:06:56.030 --> 01:06:58.270
that the direction it can move to, or how does

01:06:58.270 --> 01:06:59.160
it look like?

01:07:00.580 --> 01:07:02.740
Eventually, yes. I think first, we

01:07:02.740 --> 01:07:04.820
need to kind of hone this down and make

01:07:04.820 --> 01:07:06.690
sure that the model actually

01:07:06.830 --> 01:07:08.930
can generate useful

01:07:08.990 --> 01:07:11.109
kernels. That are faster than

01:07:11.110 --> 01:07:12.870
the baselines that have

01:07:13.250 --> 01:07:14.840
you tried apart from GPU, that

01:07:15.960 --> 01:07:18.300
gen kernel generation for other hardwares,

01:07:19.190 --> 01:07:21.310
other than GPUs? Yes. It can

01:07:21.310 --> 01:07:23.490
do it for anything. I mean, of course,

01:07:24.200 --> 01:07:26.440
GPU, there's a lot more data on

01:07:26.440 --> 01:07:28.560
CUDA and the pre

01:07:28.639 --> 01:07:30.720
in pre training potentially fine tuning

01:07:30.720 --> 01:07:32.860
of the models, so that makes it easier.

01:07:33.390 --> 01:07:35.470
There is a lot of recent work on

01:07:35.470 --> 01:07:37.370
how to apply this to

01:07:37.610 --> 01:07:39.689
even new DSLs that are just

01:07:39.690 --> 01:07:41.790
created in a lab and how we bring

01:07:42.100 --> 01:07:43.700
other techniques such as,

01:07:44.500 --> 01:07:46.360
retrieval augmented methods

01:07:46.900 --> 01:07:48.940
to enable that. Okay. Thanks. I think I'll

01:07:48.940 --> 01:07:51.170
discuss offline more. Thank you. Sounds good.

01:07:54.270 --> 01:07:55.330
Any other questions?

01:08:10.169 --> 01:08:12.219
Yeah. This is Rajesh from

01:08:12.220 --> 01:08:14.380
Akranil Lutz. You had mentioned in your future

01:08:14.380 --> 01:08:15.950
research directions that,

01:08:16.490 --> 01:08:18.380
you are looking for changes

01:08:18.680 --> 01:08:20.850
to the infrastructure, both hardware and software,

01:08:20.850 --> 01:08:21.990
to better enable

01:08:22.870 --> 01:08:25.268
test time compute and relate it. Can you elaborate

01:08:25.269 --> 01:08:27.410
a little more on what changes you foresee

01:08:27.410 --> 01:08:29.560
or what little more on that,

01:08:29.560 --> 01:08:30.060
please?

01:08:32.249 --> 01:08:34.329
So there are system works that

01:08:34.330 --> 01:08:36.370
could that could be done, like my lab did

01:08:36.620 --> 01:08:38.650
if you're interested, take

01:08:38.650 --> 01:08:40.969
a look at the works like hydrogen and

01:08:40.970 --> 01:08:42.980
tochisar is from my lab, but there's

01:08:42.980 --> 01:08:45.009
a lot of other amazing work outside. So

01:08:45.170 --> 01:08:47.510
example, a property that makes it

01:08:48.430 --> 01:08:50.569
kind of makes

01:08:50.570 --> 01:08:52.809
it clear we need a custom way of looking

01:08:52.810 --> 01:08:54.840
into that is for the repeated

01:08:55.220 --> 01:08:57.270
sampling setting, we have the same

01:08:57.270 --> 01:08:59.429
problem, same prefix being used

01:08:59.430 --> 01:09:01.459
over and over again. Right? We are

01:09:01.460 --> 01:09:03.880
doing this attention over the same KB

01:09:04.020 --> 01:09:06.140
cache. And, of course, KB reusing the

01:09:06.140 --> 01:09:08.459
KB cache is one way. To leverage

01:09:08.460 --> 01:09:10.530
this property, but there are other ways

01:09:10.749 --> 01:09:13.068
that, that we have developed to kind

01:09:13.069 --> 01:09:15.140
of enable reuse in this

01:09:15.280 --> 01:09:17.359
specific setup to enable a really,

01:09:17.360 --> 01:09:19.599
really large batch. Like, if when

01:09:19.600 --> 01:09:21.759
we are sampling 10,000 times, that

01:09:21.760 --> 01:09:23.140
creates a lot of complexity.

01:09:23.970 --> 01:09:26.470
On how to efficiently you use our hardware.

01:09:27.709 --> 01:09:29.869
Using this specific property of this

01:09:29.870 --> 01:09:31.840
setup, makes a lot of sense. So

01:09:32.629 --> 01:09:34.560
yeah. Alright. Thank you.

01:09:51.940 --> 01:09:53.880
Does does this light work?

01:09:55.310 --> 01:09:56.050
Oh, okay.

01:10:08.550 --> 01:10:10.629
Okay. Well, while we setting up, I will

01:10:10.630 --> 01:10:12.720
do the the

01:10:12.720 --> 01:10:14.880
meantime. Okay. Alright.

01:10:15.180 --> 01:10:17.420
Awesome. Alright. So I'm excited to

01:10:17.660 --> 01:10:19.920
introduce our next speaker, Hansen

01:10:19.980 --> 01:10:21.760
Wong from OpenAI, my colleague,

01:10:23.300 --> 01:10:25.080
Hanson is a research engineer

01:10:25.620 --> 01:10:27.699
working on codecs, and I

01:10:27.700 --> 01:10:29.779
bet like many of you have already

01:10:29.780 --> 01:10:31.760
using codecs in your day to day life.

01:10:32.960 --> 01:10:35.200
And Hanson worked on training the first

01:10:35.200 --> 01:10:37.080
codecs one model launched

01:10:37.300 --> 01:10:38.840
in May and has been continuously

01:10:40.290 --> 01:10:42.610
iterating on the model since then. Prior

01:10:42.610 --> 01:10:43.590
to joining OpenAI,

01:10:44.690 --> 01:10:46.939
he cofounded a startup building AI

01:10:46.940 --> 01:10:49.280
analysis agents and worked on infrastructure

01:10:49.340 --> 01:10:51.550
at Meta. And without further

01:10:51.550 --> 01:10:53.590
ado, please take it away.

01:10:56.870 --> 01:10:58.900
Thanks so much, Meng Po, and thanks thank

01:10:58.900 --> 01:11:00.930
you all for being here. It's the room is a

01:11:00.930 --> 01:11:02.960
lot larger than it was, the last time I was

01:11:02.960 --> 01:11:05.289
here six years ago, so I'm very grateful

01:11:05.289 --> 01:11:07.330
for that. So, yeah, as Mongkeo

01:11:07.330 --> 01:11:09.570
mentioned, I work on Codecs, which is

01:11:09.570 --> 01:11:11.660
OpenAI's coding agent. And I think

01:11:11.660 --> 01:11:13.560
what just like one of the things to set

01:11:13.959 --> 01:11:15.899
the context that, like, really stands out

01:11:16.039 --> 01:11:18.240
to me is, like, looking at last year's ML

01:11:18.240 --> 01:11:20.190
for Systems conference. I

01:11:20.750 --> 01:11:22.190
colleague, Ahmed, was here talking about

01:11:22.830 --> 01:11:24.819
how OpenAI models were know, like,

01:11:25.060 --> 01:11:27.385
starting to get capable enough to really,

01:11:27.385 --> 01:11:29.530
like, get gold at the IY

01:11:29.589 --> 01:11:31.930
or the Olympiad

01:11:31.990 --> 01:11:33.770
level competitions, but I I think it's

01:11:34.010 --> 01:11:36.019
only really, this year

01:11:36.020 --> 01:11:38.260
in 2025 that we really started

01:11:38.260 --> 01:11:40.379
to reach the

01:11:40.380 --> 01:11:42.700
point where these models could actually start to

01:11:42.700 --> 01:11:44.790
do real world software in work in

01:11:44.790 --> 01:11:46.800
a way that was actually, like, very meaningful

01:11:46.800 --> 01:11:48.839
for users. Which, you know, like

01:11:48.840 --> 01:11:51.180
many of you have probably already experienced. So

01:11:51.310 --> 01:11:53.019
this chart shows

01:11:53.510 --> 01:11:55.669
benchmark called SuiteBench verified. So this is

01:11:55.670 --> 01:11:57.760
from a bunch of

01:11:57.760 --> 01:11:59.840
fine people at Princeton, many of whom

01:11:59.840 --> 01:12:01.520
are at the conference. We

01:12:02.240 --> 01:12:04.359
working on much better and newer benchmarks

01:12:04.359 --> 01:12:06.549
actually. But, but SweeVenture is still

01:12:06.870 --> 01:12:09.109
a great example and so if you're not

01:12:09.110 --> 01:12:11.060
familiar, basically, SweeVenge

01:12:11.300 --> 01:12:13.709
it's a it's a it's a benchmark of 500

01:12:13.709 --> 01:12:15.830
tasks from

01:12:15.830 --> 01:12:18.108
open source issues in GitHub repos,

01:12:18.109 --> 01:12:20.509
and the models need to produce a pull request

01:12:20.510 --> 01:12:22.839
that resolves the issue. And so

01:12:23.080 --> 01:12:23.960
even last year,

01:12:25.560 --> 01:12:27.959
around June when g p GPT four o came

01:12:27.960 --> 01:12:30.089
out, the models were still, like, not

01:12:30.090 --> 01:12:32.190
very good at this task, starting around, I think,

01:12:32.430 --> 01:12:34.059
between 3040%.

01:12:34.700 --> 01:12:36.859
But just in the last year, the progress has

01:12:36.859 --> 01:12:39.190
been pretty incredible and a lot of it's

01:12:39.350 --> 01:12:41.509
been driven by the developments in this,

01:12:41.510 --> 01:12:43.513
like, test time compute as we

01:12:43.513 --> 01:12:44.840
heard. In the previous presentation,

01:12:45.860 --> 01:12:47.880
and so now with these reasoning models,

01:12:48.280 --> 01:12:49.190
the progress has

01:12:50.789 --> 01:12:53.209
really taken off. It's the models are almost, like,

01:12:53.269 --> 01:12:55.310
twice can resolve almost twice as many

01:12:55.310 --> 01:12:56.820
issues as they can before, where

01:12:57.380 --> 01:12:59.460
really, like, nearing the 80% to 90%

01:12:59.460 --> 01:13:01.510
mark, which is probably, like, pretty close

01:13:01.510 --> 01:13:03.530
to the limits as far as this benchmark goes.

01:13:05.300 --> 01:13:07.220
And then also, like, looking at,

01:13:07.540 --> 01:13:09.560
this METR

01:13:09.560 --> 01:13:11.980
time horizon graph, which I think

01:13:11.980 --> 01:13:14.059
is has been shared around quite a a lot

01:13:14.060 --> 01:13:16.169
recently. But basically, like, it's not

01:13:16.170 --> 01:13:18.250
just about completion, but also about, like, the length

01:13:18.250 --> 01:13:20.550
of the tasks and the difficulty of the tasks

01:13:21.010 --> 01:13:22.820
that the models are capable of achieving.

01:13:23.270 --> 01:13:25.480
And so it's almost like this exponential

01:13:26.100 --> 01:13:28.159
curve. Where the

01:13:28.640 --> 01:13:30.900
where the rate of progress in terms of how

01:13:31.640 --> 01:13:33.850
difficult these tasks that the models can

01:13:33.850 --> 01:13:35.909
solve, it's it's actually like increasing

01:13:36.950 --> 01:13:39.129
over time and I think the the codecs models are actually,

01:13:39.129 --> 01:13:41.340
like, pushing the frontier of

01:13:41.340 --> 01:13:42.440
that right now.

01:13:45.399 --> 01:13:47.160
Cool. And yeah. Like, I think going back to the,

01:13:47.479 --> 01:13:49.760
I I think, like, you know, this year,

01:13:49.810 --> 01:13:52.019
the model's achieved some some

01:13:52.240 --> 01:13:54.260
very impressive feats on achieving not

01:13:54.260 --> 01:13:56.600
just like gold at the ILY, but also the IMO.

01:13:57.300 --> 01:13:58.520
And achieving,

01:13:59.510 --> 01:14:01.530
a perfect score at the ACMI ICPC

01:14:01.590 --> 01:14:03.630
finals. So we're and this is

01:14:03.630 --> 01:14:05.669
actually, like, the same underlying reasoning model that

01:14:05.669 --> 01:14:07.739
powers codecs. So when you use codecs, and I

01:14:07.740 --> 01:14:09.660
hope you try it if you haven't already,

01:14:10.410 --> 01:14:12.510
you're getting, like, this level of intelligence

01:14:12.650 --> 01:14:13.550
and reasoning

01:14:14.810 --> 01:14:17.150
at your fingertips whether that's in

01:14:17.150 --> 01:14:19.110
the computer, or in the cloud.

01:14:21.670 --> 01:14:23.920
So, yeah, so so codecs was released

01:14:24.160 --> 01:14:26.400
The CLI was released earlier this year,

01:14:26.560 --> 01:14:28.680
in April. So you can

01:14:28.680 --> 01:14:30.920
just install it right away. It's just a simple

01:14:30.920 --> 01:14:33.069
NPM install. And then

01:14:33.070 --> 01:14:35.149
it's also available in the IDE and

01:14:35.150 --> 01:14:37.190
also, as the cloud product

01:14:37.190 --> 01:14:39.269
in chat JBT, which was released in

01:14:39.269 --> 01:14:41.240
May and

01:14:41.300 --> 01:14:43.320
and what's cool is that the

01:14:43.320 --> 01:14:45.360
same underlying open source agent, which

01:14:45.360 --> 01:14:47.439
we've actually released

01:14:47.439 --> 01:14:49.519
on GitHub as OpenAI slash

01:14:49.519 --> 01:14:51.740
codex, It's open source.

01:14:51.740 --> 01:14:53.740
Like, all that core logic is

01:14:53.800 --> 01:14:55.860
the same kind of, like,

01:14:55.860 --> 01:14:57.160
underlying agent infrastructure,

01:14:58.900 --> 01:15:01.060
that powers all of the different product services

01:15:01.060 --> 01:15:03.070
that we have. So it's really

01:15:03.070 --> 01:15:05.310
cool that, like, we're releasing that to the public.

01:15:05.310 --> 01:15:07.290
Anyone can achieve kinda like the same level

01:15:07.530 --> 01:15:09.630
performance that we do. The models are also available,

01:15:10.250 --> 01:15:12.890
in the API. I think the 5.1

01:15:12.890 --> 01:15:15.070
codecs max model just became available

01:15:15.330 --> 01:15:16.739
the API yesterday.

01:15:18.979 --> 01:15:20.900
And, we're we're really starting to see,

01:15:21.060 --> 01:15:23.200
adoption take off both internally

01:15:23.300 --> 01:15:25.710
and externally. No better evaluation

01:15:26.150 --> 01:15:27.790
than just real world usage.

01:15:28.340 --> 01:15:30.519
So like 95%

01:15:30.520 --> 01:15:32.710
of of, technical stuff at OpenAI

01:15:32.830 --> 01:15:34.910
make use of Codecs. Codecs

01:15:34.910 --> 01:15:37.169
reviews a 100% of pull requests.

01:15:37.630 --> 01:15:39.709
Created an open AI. And

01:15:39.709 --> 01:15:40.209
externally,

01:15:42.240 --> 01:15:44.320
usage as measured in daily messages to

01:15:44.320 --> 01:15:46.140
codecs have increased about

01:15:46.380 --> 01:15:48.459
20 x since, GPD five was

01:15:48.459 --> 01:15:50.620
released in August. Which was kind of like the first

01:15:50.919 --> 01:15:52.179
open AI model that was

01:15:53.220 --> 01:15:55.310
trained to be good at

01:15:55.310 --> 01:15:56.840
these kind of, like, coding agent

01:15:57.320 --> 01:15:57.800
scenarios.

01:16:00.360 --> 01:16:02.519
So cool. Yeah. So like what's under the hood of the

01:16:02.519 --> 01:16:04.569
codex agent? So we've

01:16:04.570 --> 01:16:06.649
intentionally kept the core like

01:16:06.649 --> 01:16:08.810
age infrastructure or, like, the loop

01:16:09.269 --> 01:16:11.470
that powers the agent extremely simple.

01:16:11.470 --> 01:16:13.629
So it's actually like, and it the nice part about

01:16:13.630 --> 01:16:15.600
this is, you know, as I said, it's it's open source.

01:16:15.840 --> 01:16:18.000
There are some links at the bottom. Like, you can actually go to the

01:16:18.000 --> 01:16:20.060
repo and check these out. We've open sourced, like,

01:16:20.060 --> 01:16:22.090
the prompts that tool

01:16:22.390 --> 01:16:24.470
specs, all of that. So, you know,

01:16:24.470 --> 01:16:26.620
like, when you start a task,

01:16:26.620 --> 01:16:28.700
like, you have basically, like, in the in the prompt,

01:16:28.700 --> 01:16:30.980
you have the developer instructions, the

01:16:31.220 --> 01:16:33.299
a set of function calling tools that we provide to the

01:16:33.300 --> 01:16:35.309
agent, the environment

01:16:35.350 --> 01:16:37.510
the user's environment context, and, of course,

01:16:37.510 --> 01:16:39.630
like, the prompt that the user gave us

01:16:39.630 --> 01:16:41.789
to perform the task. And then we basically

01:16:41.789 --> 01:16:43.978
just have this what

01:16:43.979 --> 01:16:46.059
is ultimately quite simple, this

01:16:46.060 --> 01:16:47.800
loop where the model

01:16:48.360 --> 01:16:49.500
receives that prompt,

01:16:50.460 --> 01:16:52.160
it reasons about what to do next.

01:16:52.620 --> 01:16:54.680
It always makes a tool call

01:16:54.680 --> 01:16:56.910
usually, so that

01:16:56.910 --> 01:16:58.770
would be either, like, running a command

01:16:59.210 --> 01:17:01.190
potentially, like, using an MCP if you've

01:17:01.670 --> 01:17:03.850
provided a a model context provider.

01:17:05.830 --> 01:17:08.190
Tool. And then afterwards,

01:17:08.330 --> 01:17:10.569
we provide the agent the results of that tool

01:17:10.570 --> 01:17:12.729
call. And this whole process repeats

01:17:12.729 --> 01:17:14.760
until until completion, essentially.

01:17:14.760 --> 01:17:16.849
And so, like, basically, like, this,

01:17:16.850 --> 01:17:19.010
like, single what is essentially like a single four

01:17:19.010 --> 01:17:21.199
loop, and and then we use

01:17:21.200 --> 01:17:23.600
the responses API, or you can also

01:17:23.600 --> 01:17:25.879
run the GPT OSS, models

01:17:25.880 --> 01:17:28.019
locally as well. But

01:17:28.019 --> 01:17:30.080
basically, like, you just repeat this in

01:17:30.080 --> 01:17:32.219
a loop until the task is complete, and that's kinda like

01:17:32.460 --> 01:17:34.160
ultimately, that's all you need to

01:17:35.090 --> 01:17:36.289
be able to achieve,

01:17:37.519 --> 01:17:39.680
both like these incredible performances

01:17:39.680 --> 01:17:41.650
at these competitions and also like

01:17:41.890 --> 01:17:43.909
solving real world software engineering tasks.

01:17:43.910 --> 01:17:44.410
But

01:17:46.070 --> 01:17:48.139
yeah. So, like, I think underneath that

01:17:48.620 --> 01:17:50.700
kind of, like, simple loop, there's a lot going

01:17:50.760 --> 01:17:52.890
on. And so I wanna kinda, like, provide

01:17:52.890 --> 01:17:55.109
a high level overview of some of the challenges

01:17:55.169 --> 01:17:57.249
that we've solved, which is kind of like

01:17:57.250 --> 01:17:59.070
a really interesting place. It's at the intersection

01:17:59.450 --> 01:18:01.630
of, ML and systems, of course.

01:18:02.660 --> 01:18:04.770
So the the first thing is, like, having

01:18:04.930 --> 01:18:07.100
you know, like, quality environments

01:18:07.260 --> 01:18:09.340
to use, not just for training the

01:18:09.340 --> 01:18:11.470
models, but also to serve them as part

01:18:11.470 --> 01:18:12.870
of the Codex Cloud product.

01:18:15.310 --> 01:18:17.470
There there is a lot of thought that goes into the tools

01:18:17.470 --> 01:18:19.289
that the agent gets to use.

01:18:19.510 --> 01:18:21.539
And then lastly, like, there's a lot of

01:18:21.860 --> 01:18:23.880
constraints around, model behaviors,

01:18:24.100 --> 01:18:26.379
durability, and, like, what does it mean

01:18:26.379 --> 01:18:28.390
to have, like, a you know,

01:18:28.390 --> 01:18:30.549
like, a coaching agent that users actually want to

01:18:30.550 --> 01:18:32.679
use and not just know, these

01:18:32.680 --> 01:18:34.760
things that are that are incredibly capable but, otherwise,

01:18:34.760 --> 01:18:35.899
like, very hard to work with.

01:18:36.940 --> 01:18:36.940
So

01:18:39.150 --> 01:18:40.930
so, environments. So

01:18:41.390 --> 01:18:44.390
if you go to chatgbt.com/codex,

01:18:44.700 --> 01:18:46.730
you can actually, like, create your own

01:18:47.190 --> 01:18:49.350
environment. And I think the really cool thing is,

01:18:49.350 --> 01:18:50.880
like, basically, what we've is we've

01:18:51.440 --> 01:18:53.760
taken the same infrastructure that powers our training

01:18:53.760 --> 01:18:55.850
environments and we've basically shipped it

01:18:56.220 --> 01:18:58.390
as Codex Cloud product.

01:18:58.550 --> 01:19:00.090
And so like a lot of the knobs

01:19:00.570 --> 01:19:02.810
that you see here, you know, when you create

01:19:02.810 --> 01:19:04.969
a codex environment as a user, those

01:19:04.970 --> 01:19:07.089
are, like, the exact same types of

01:19:07.090 --> 01:19:08.550
things that we use during training.

01:19:09.720 --> 01:19:11.880
So, you know, you start with a with a container

01:19:11.880 --> 01:19:13.960
image. You usually

01:19:13.960 --> 01:19:16.040
like a dockerized container. It

01:19:16.040 --> 01:19:18.180
might contain a of pre installed dependencies,

01:19:18.180 --> 01:19:20.340
but, you know, like, users can also provide their own

01:19:22.480 --> 01:19:24.700
then things like, one important

01:19:24.700 --> 01:19:26.839
thing is security. So,

01:19:26.840 --> 01:19:28.860
basically, by default,

01:19:30.019 --> 01:19:32.120
most containers, have Internet

01:19:32.180 --> 01:19:34.530
access disabled, but users can select

01:19:35.170 --> 01:19:37.310
access it's very important for things like,

01:19:38.140 --> 01:19:40.539
you know, like many enterprises have concerns around

01:19:40.539 --> 01:19:42.880
code exfiltration, or,

01:19:43.200 --> 01:19:45.280
you know, like accidentally installing bad

01:19:45.280 --> 01:19:47.380
software. The rest of Internet or,

01:19:47.380 --> 01:19:49.620
like, bad dependencies. So it's important to

01:19:49.620 --> 01:19:51.869
have those knobs, also,

01:19:51.870 --> 01:19:54.059
like, dedicated training to make sure that the

01:19:54.220 --> 01:19:56.229
model is, like, less vulnerable to that type

01:19:56.230 --> 01:19:56.930
of injection?

01:19:59.320 --> 01:20:01.640
So and then yeah. So, like, then then the tools

01:20:01.640 --> 01:20:03.800
in agent loop So we actually have, like,

01:20:03.800 --> 01:20:05.970
a fairly, opinionated

01:20:06.430 --> 01:20:08.620
take on on, like, kind of, like, what tools you

01:20:08.620 --> 01:20:10.759
should give the agent. So in

01:20:10.760 --> 01:20:12.740
general, like, our philosophy is that the,

01:20:12.990 --> 01:20:15.079
the the the terminal is like the

01:20:15.080 --> 01:20:17.019
the bread and butter of what codecs uses

01:20:17.180 --> 01:20:19.339
to accomplish most tasks. And in general, like,

01:20:19.340 --> 01:20:21.460
we do think that for a lot of things, like,

01:20:21.460 --> 01:20:23.510
the terminal is kind of like all you need. So that's

01:20:23.510 --> 01:20:25.690
not true for all tasks, of course. But, like,

01:20:25.750 --> 01:20:27.869
I think do find it to be true for the majority

01:20:27.870 --> 01:20:30.349
of tasks. So, like, you can read files,

01:20:30.350 --> 01:20:32.490
write files, run tests, and

01:20:32.490 --> 01:20:34.650
review and commit changes. And that's kind of

01:20:34.650 --> 01:20:36.690
like the the fundamental, like, software

01:20:37.010 --> 01:20:39.229
the inner loop of the software development cycle.

01:20:40.510 --> 01:20:42.770
And then so yeah. So, like, I think below is

01:20:43.390 --> 01:20:45.790
the roughly, like, the inter interface that we

01:20:45.850 --> 01:20:47.939
use for, that's for

01:20:47.940 --> 01:20:49.880
the exact or the terminal tool.

01:20:50.430 --> 01:20:51.290
Used in the agent

01:20:52.539 --> 01:20:54.619
So a couple of things to call out here. So one is

01:20:54.620 --> 01:20:56.690
like we do expose like knobs

01:20:56.690 --> 01:20:59.019
that let the model control,

01:20:59.879 --> 01:21:01.740
basically, like, how much token usage

01:21:02.370 --> 01:21:04.450
any any tool call takes and and how long

01:21:04.450 --> 01:21:06.550
that tool call is allowed to take. So

01:21:06.550 --> 01:21:08.630
the so the model has pretty fine grain control

01:21:08.630 --> 01:21:10.900
over this, like, cost and latency trade

01:21:11.440 --> 01:21:13.450
off, which which then, you know, like,

01:21:13.450 --> 01:21:14.970
the model is actually able to

01:21:15.680 --> 01:21:17.919
almost like learn by itself how to best optimize that

01:21:17.920 --> 01:21:19.870
through the reinforcement learning process.

01:21:21.800 --> 01:21:23.300
And then yeah. So I one

01:21:23.860 --> 01:21:26.200
philosophy we've taken also is to enable sandboxing

01:21:26.420 --> 01:21:28.720
by default. So on

01:21:28.720 --> 01:21:30.880
macOS and and the Linux, there exists, like, these

01:21:30.880 --> 01:21:33.310
sandboxing primitives where you can execute

01:21:33.530 --> 01:21:35.590
processes in, like, a a

01:21:35.590 --> 01:21:37.950
pseudo sandbox where file

01:21:37.950 --> 01:21:40.120
and network access

01:21:40.120 --> 01:21:42.140
is is tightly restricted by default.

01:21:42.670 --> 01:21:44.830
My my colleague, Fuad, has a excellent talk

01:21:44.830 --> 01:21:47.060
about some of the considerations that when in

01:21:47.379 --> 01:21:49.790
this design. But, basically, like, know, on

01:21:50.730 --> 01:21:52.840
macOS, can define these, like, sandboxing policies

01:21:52.840 --> 01:21:54.590
and similarly on on Linux. And based

01:21:54.910 --> 01:21:57.010
basically, we we have, like, a a fairly,

01:21:57.790 --> 01:21:59.790
well thought out set of,

01:22:00.410 --> 01:22:02.530
sandboxing policies that make

01:22:02.530 --> 01:22:04.770
it make it so that the agent can't just, like,

01:22:04.770 --> 01:22:06.560
you know, delete your home directory

01:22:06.780 --> 01:22:08.480
without explicit you know, unless you

01:22:11.269 --> 01:22:13.430
and then yeah. So, like, basically, just with,

01:22:13.590 --> 01:22:15.130
the terminal tool alone,

01:22:16.060 --> 01:22:18.000
a benchmark called terminal bench.

01:22:19.520 --> 01:22:21.570
Where the agent has to do various

01:22:21.730 --> 01:22:24.070
like, not just coding tasks purely, but

01:22:24.190 --> 01:22:26.430
things like, you know, like, plays

01:22:26.430 --> 01:22:27.730
orc, like, start a server.

01:22:29.120 --> 01:22:31.400
Or, like, you know,

01:22:31.400 --> 01:22:33.230
like download videos from the Internet.

01:22:34.590 --> 01:22:36.390
So without too much

01:22:36.710 --> 01:22:38.950
so basically, yeah, like, with that simple agent

01:22:38.950 --> 01:22:41.390
loop and a you know, like, relatively

01:22:41.390 --> 01:22:43.470
simple interface to execute

01:22:43.470 --> 01:22:45.719
commands, we're already

01:22:45.720 --> 01:22:47.786
able to achieve these kind of like state

01:22:47.787 --> 01:22:49.920
of the art performances on on these

01:22:49.920 --> 01:22:50.479
real world,

01:22:52.180 --> 01:22:53.729
tasks that require interaction with

01:22:54.520 --> 01:22:55.100
a peer.

01:22:56.700 --> 01:22:59.039
So, look, one interesting anecdote is that

01:23:00.120 --> 01:23:01.950
so you would think that models can

01:23:02.350 --> 01:23:04.429
just use Git to apply patches or

01:23:04.430 --> 01:23:06.230
or edit codes. It's actually like a

01:23:06.710 --> 01:23:08.790
pretty interesting design space of

01:23:08.790 --> 01:23:11.060
like how do you provide models tools

01:23:11.060 --> 01:23:13.099
to edit files. And so, like,

01:23:13.100 --> 01:23:14.879
git diffs are you know, well, they're

01:23:15.180 --> 01:23:17.200
one of the big advantages that they're well represented

01:23:17.340 --> 01:23:19.309
in the pretraining data.

01:23:19.870 --> 01:23:21.399
But, one of the downsides is that

01:23:22.730 --> 01:23:25.050
and contain a lot of references to things like line numbers,

01:23:25.050 --> 01:23:26.910
which make it very difficult to

01:23:27.150 --> 01:23:29.300
so that like, that that's basically impossible to

01:23:29.860 --> 01:23:32.040
predict upfront in a in a, like, auto

01:23:32.720 --> 01:23:34.860
aggressive model,

01:23:34.999 --> 01:23:37.100
manner. And so we actually have

01:23:37.260 --> 01:23:39.159
a custom patch format. So we've showed that

01:23:39.680 --> 01:23:41.460
with our developers as well. But

01:23:41.760 --> 01:23:43.930
basically, know,

01:23:43.930 --> 01:23:46.170
like, it's it's it's basically a simplified

01:23:46.170 --> 01:23:47.519
version of the GitDIP format

01:23:48.300 --> 01:23:50.430
No line numbers, but it's still kind

01:23:50.430 --> 01:23:52.470
of like the same approximate format

01:23:52.470 --> 01:23:54.410
where you can you can provide these, like,

01:23:54.999 --> 01:23:57.060
like, headings, and

01:23:57.060 --> 01:23:59.180
then context, and then the

01:23:59.180 --> 01:24:01.580
the patch format itself is at the core, like, still

01:24:01.580 --> 01:24:02.240
the same.

01:24:03.629 --> 01:24:05.789
And then, basically, during serving time,

01:24:05.789 --> 01:24:08.230
we also have these, like,

01:24:08.530 --> 01:24:10.689
basically, the the the grammar of the patch

01:24:10.690 --> 01:24:12.700
syntax is extremely simple. So that

01:24:12.700 --> 01:24:14.480
means, you can use things like constraint

01:24:14.860 --> 01:24:17.060
sampling, to ensure

01:24:17.060 --> 01:24:19.479
that, you know, the model always generates a syntact syntactically

01:24:20.100 --> 01:24:20.910
valid patch.

01:24:22.189 --> 01:24:24.330
So So the last thing I want to touch on

01:24:24.330 --> 01:24:26.429
is, like, concerns around, you

01:24:26.430 --> 01:24:28.539
know, models behavior

01:24:28.539 --> 01:24:30.640
and its durability. So, it's a big

01:24:30.920 --> 01:24:32.999
and important piece of what makes coding

01:24:32.999 --> 01:24:35.030
agents actually useful. In the real

01:24:35.030 --> 01:24:37.390
world So, you know, one thing

01:24:37.530 --> 01:24:39.629
that we've worked with other companies to try

01:24:39.630 --> 01:24:41.040
and standardize is the

01:24:41.680 --> 01:24:42.770
agent's MD.

01:24:43.900 --> 01:24:46.060
Basically, a a readme for agents in the

01:24:46.060 --> 01:24:48.480
same way that we have, like, contributor guides for humans.

01:24:49.879 --> 01:24:51.480
And then the idea here is that

01:24:53.290 --> 01:24:55.310
we we provide the the model

01:24:55.530 --> 01:24:57.800
with guidelines around things like code structure,

01:24:58.770 --> 01:25:00.850
build and test commands, style style

01:25:00.850 --> 01:25:02.910
guidelines, how to make a PR in the

01:25:02.970 --> 01:25:05.240
repo, and then we actually

01:25:05.630 --> 01:25:07.175
do enforce

01:25:08.200 --> 01:25:10.249
that the model adheres tightly

01:25:10.250 --> 01:25:12.330
to these instructions. One

01:25:12.330 --> 01:25:14.420
interesting thing is the

01:25:14.420 --> 01:25:16.499
models are actually very good at creating these

01:25:16.499 --> 01:25:18.839
files example, like, a model like OpenAI

01:25:19.140 --> 01:25:21.220
deep research is actually quite good at doing

01:25:21.220 --> 01:25:22.439
that of the box. So

01:25:23.240 --> 01:25:25.339
one trick that we've done is actually, like, for

01:25:25.399 --> 01:25:27.555
for many, source repositories,

01:25:27.710 --> 01:25:29.950
it's actually, like, possible to get a pretty good

01:25:29.950 --> 01:25:32.170
agent centri out of the box just by

01:25:32.170 --> 01:25:34.200
deploying our own agents to to do that.

01:25:35.959 --> 01:25:38.150
So and then I think I touched

01:25:38.150 --> 01:25:40.479
on the benchmarks earlier, but one limitation

01:25:40.539 --> 01:25:42.619
of a lot of the current set of benchmarks is that if

01:25:42.620 --> 01:25:44.760
they only capture the concept of

01:25:44.760 --> 01:25:46.780
correctness, as as specifically

01:25:47.320 --> 01:25:49.570
measured by ability to pass

01:25:49.570 --> 01:25:51.500
the reference unit tests?

01:25:52.740 --> 01:25:54.749
But in the real world, like, there's

01:25:54.749 --> 01:25:57.080
so much more to to software

01:25:57.539 --> 01:25:58.920
engineering and coding than than

01:25:59.540 --> 01:26:01.410
just correctness. Concerns like

01:26:01.870 --> 01:26:04.320
coding style, engineering best practices,

01:26:04.790 --> 01:26:07.030
you know, like, when to not repeat

01:26:07.030 --> 01:26:09.110
yourself versus, you know, like, maybe it's okay

01:26:09.110 --> 01:26:11.220
to to duplicate some code. It's a

01:26:11.269 --> 01:26:13.320
very nuanced decision. How

01:26:13.320 --> 01:26:15.100
to structure your code to be modular

01:26:16.600 --> 01:26:18.300
even things like usage of comments,

01:26:18.680 --> 01:26:20.380
overly defensive programming,

01:26:21.140 --> 01:26:23.220
patterns. So I think, like, you know, when it

01:26:23.220 --> 01:26:25.640
doesn't go right, I think this this picture

01:26:25.640 --> 01:26:28.090
on the right is actually from a Twitter Twitter comment, but

01:26:28.310 --> 01:26:30.330
basically, sometimes the models

01:26:30.330 --> 01:26:32.500
are prone to doing things

01:26:32.500 --> 01:26:34.620
in in things that

01:26:34.620 --> 01:26:36.499
might look a bit AI. And so

01:26:36.740 --> 01:26:39.210
know, if we don't do our jobs right, then like

01:26:39.350 --> 01:26:41.590
that kind of leak out. Now, we try our best to

01:26:42.310 --> 01:26:44.470
to mitigate these types

01:26:44.470 --> 01:26:45.009
of things.

01:26:46.850 --> 01:26:48.780
And then another interesting thing is that

01:26:49.020 --> 01:26:51.240
codex is actually a great code reviewer

01:26:51.459 --> 01:26:53.500
by itself. So on on GitHub,

01:26:53.500 --> 01:26:55.660
if you are connected to to codex, you can

01:26:55.660 --> 01:26:58.100
just, like, add codex and ask it to review your PRs.

01:26:58.760 --> 01:27:01.240
But like I mentioned earlier, like, all PRs at OpenAI

01:27:01.240 --> 01:27:03.267
are reviewed this way and there's

01:27:03.267 --> 01:27:05.490
been a surprising number of like actually, like, quite

01:27:05.490 --> 01:27:07.319
serious issues that have been flagged

01:27:08.060 --> 01:27:09.039
early as a result.

01:27:10.630 --> 01:27:13.130
And then we've we we published a blog post recently

01:27:13.300 --> 01:27:15.522
on the new, OpenAI alignment blog,

01:27:15.522 --> 01:27:17.109
which I recommend checking out.

01:27:17.910 --> 01:27:20.220
But, basically, like, there's been a lot of work that's gone

01:27:20.539 --> 01:27:22.619
into making the models very

01:27:22.620 --> 01:27:24.710
good at code review. And I think, like, one concern

01:27:24.710 --> 01:27:26.720
is specifically for code review, if you've used,

01:27:26.720 --> 01:27:28.810
like, other code review tools especially is, like,

01:27:29.050 --> 01:27:31.470
there's a very delicate trade off

01:27:31.530 --> 01:27:33.970
here between precision and recall or, like, the signal to noise

01:27:33.980 --> 01:27:36.020
ratio. So you flag things that are, like,

01:27:36.030 --> 01:27:38.190
kind of not true issues, it actually,

01:27:38.190 --> 01:27:40.289
like, often waste more time,

01:27:40.600 --> 01:27:43.018
than actually save the user. It's kinda like

01:27:43.019 --> 01:27:44.520
suggested by this formula here.

01:27:45.560 --> 01:27:47.239
So you you have this, like, very delicate

01:27:47.960 --> 01:27:50.380
trade off you can achieve between, you know, precision

01:27:50.600 --> 01:27:52.770
of the of the, accuracy

01:27:52.770 --> 01:27:54.950
of the the flagged issues versus

01:27:55.500 --> 01:27:57.880
you know, being able to flag

01:27:58.100 --> 01:28:00.280
the the actual issues that occur in practice.

01:28:01.229 --> 01:28:03.228
So this is something we've worked a lot to,

01:28:03.470 --> 01:28:05.479
really tune and then not

01:28:05.480 --> 01:28:07.630
just like find the right spot on the curve, but

01:28:07.630 --> 01:28:09.120
also push the curve

01:28:09.660 --> 01:28:11.800
upward as you can see in the as you can see in this plot

01:28:13.630 --> 01:28:15.696
There's a lot of safety training that goes into our

01:28:15.696 --> 01:28:17.690
models, not just codex, but also, like, all

01:28:18.170 --> 01:28:20.250
OpenAI models specifically. But, you know, like, some of the

01:28:20.250 --> 01:28:22.470
things that are specific to coding agents in

01:28:22.530 --> 01:28:24.710
particular, prompt injections,

01:28:24.850 --> 01:28:27.060
not just through user input,

01:28:27.379 --> 01:28:29.580
but also files in the sys file system

01:28:29.580 --> 01:28:31.950
or even dependencies where developers

01:28:31.950 --> 01:28:34.109
might try to sneak in attacks

01:28:34.109 --> 01:28:35.580
against LLM agents.

01:28:38.050 --> 01:28:40.249
One area that you know, like,

01:28:40.650 --> 01:28:42.729
adversaries in the in the wild are are are

01:28:42.730 --> 01:28:43.810
actually using these

01:28:45.649 --> 01:28:47.320
like, things like

01:28:50.200 --> 01:28:51.580
trying to exploit cybersecurity

01:28:52.550 --> 01:28:54.649
risks. That's something we actively work

01:28:54.650 --> 01:28:56.970
to mitigate. Also things like jail

01:28:56.970 --> 01:28:59.230
breaks and and general model safety training.

01:29:00.400 --> 01:29:02.880
One interesting thing to call out from from our system

01:29:02.880 --> 01:29:05.060
card, which is linked above is that,

01:29:06.570 --> 01:29:08.650
there's there's these, cool techniques we

01:29:08.650 --> 01:29:10.759
can use to make the models more

01:29:10.760 --> 01:29:13.220
collaborative. So one thing is that previously,

01:29:13.220 --> 01:29:15.459
we saw that models would be prone to kind of

01:29:15.460 --> 01:29:17.490
like stepping over user changes

01:29:17.490 --> 01:29:19.570
if they were if you're, like, editing a file at

01:29:19.570 --> 01:29:21.930
the same time as the coding agents editing the

01:29:21.930 --> 01:29:24.050
file sometimes. You end up with these conflicts

01:29:24.050 --> 01:29:26.160
that end up in you kinda,

01:29:26.160 --> 01:29:27.940
like, stepping over each other's work.

01:29:28.240 --> 01:29:30.100
And so we actually, like, simulated

01:29:30.970 --> 01:29:31.870
during training, like, users,

01:29:33.710 --> 01:29:35.950
like another model would be inserted in

01:29:35.950 --> 01:29:37.680
the loop, and it would kind of like make

01:29:38.080 --> 01:29:40.189
changes at the same time as the as the

01:29:40.190 --> 01:29:42.229
original models trying to make the changes. And

01:29:42.229 --> 01:29:44.590
then, basically, by doing so, you can kind of simulate

01:29:44.590 --> 01:29:46.660
the effect of, you know,

01:29:46.660 --> 01:29:48.900
cooperating with the user on on your laptop

01:29:48.900 --> 01:29:50.810
and then making sure that the model

01:29:51.370 --> 01:29:53.610
behaves responsibly when there's when there's

01:29:53.610 --> 01:29:54.510
conflicting changes.

01:29:56.810 --> 01:29:58.890
So kind of like what's next in terms

01:29:58.890 --> 01:29:59.980
of coding agents. Right?

01:30:01.070 --> 01:30:03.479
So one thing that we, we saw in the previous

01:30:04.180 --> 01:30:06.580
presentation that's very relevant for coding agents, the

01:30:06.580 --> 01:30:08.450
idea of, like, using parallel test time

01:30:08.850 --> 01:30:11.120
compute to improve performance So,

01:30:11.580 --> 01:30:13.650
we published this number with the Codex

01:30:13.650 --> 01:30:15.820
one model, so it's a bit old. But

01:30:15.980 --> 01:30:18.069
can still see the trend still holds

01:30:18.070 --> 01:30:20.230
true in practice where, you know, the more attempts you

01:30:20.230 --> 01:30:21.720
give these models, you actually get

01:30:22.840 --> 01:30:25.130
a pretty smooth scaling curve in terms

01:30:25.690 --> 01:30:27.850
of the improvement in accuracy. The

01:30:27.850 --> 01:30:29.960
models can often, like, it might not solve

01:30:29.960 --> 01:30:32.120
the problem the first try but if you give it four tries or

01:30:32.120 --> 01:30:34.340
eight tries, the probability

01:30:34.399 --> 01:30:36.559
that you get a correct solution increases quite a

01:30:36.560 --> 01:30:38.670
lot. This is something we've actually exposed in

01:30:38.670 --> 01:30:40.830
the Codex Cloud product. So basically, you

01:30:40.830 --> 01:30:43.289
can ask when you provided

01:30:43.289 --> 01:30:45.789
a task, you can you can ask for multiple

01:30:46.445 --> 01:30:46.945
responses

01:30:48.600 --> 01:30:50.420
Another interesting, technique is

01:30:50.900 --> 01:30:53.140
to extend the context of these models via this technique

01:30:53.140 --> 01:30:55.380
called compaction so the model summarize its

01:30:55.440 --> 01:30:57.539
work often compressing the

01:30:57.539 --> 01:30:59.938
tokens used in the context by up to a 100

01:30:59.939 --> 01:31:01.399
times. So

01:31:02.289 --> 01:31:04.300
And then I think Yeah. To close off,

01:31:04.300 --> 01:31:06.380
I think the the best part about working on codex is

01:31:06.380 --> 01:31:08.449
that as we get

01:31:08.450 --> 01:31:10.070
a better codecs, it makes

01:31:10.630 --> 01:31:12.330
the future of codex models

01:31:12.710 --> 01:31:15.009
better. It makes it faster to develop these

01:31:15.010 --> 01:31:17.030
models, whether that's through better tooling

01:31:17.089 --> 01:31:18.980
for for the research side,

01:31:19.150 --> 01:31:20.780
faster development on the codecs product,

01:31:21.390 --> 01:31:23.588
Codecs kind of can start to review its

01:31:23.589 --> 01:31:25.799
own code. Agents

01:31:25.800 --> 01:31:27.950
can even help us make more tasks

01:31:27.950 --> 01:31:29.970
in training environments. And ultimately,

01:31:30.980 --> 01:31:32.890
our purpose as a company to

01:31:33.930 --> 01:31:36.010
to towards our goal of accelerating progress

01:31:36.010 --> 01:31:37.910
towards AGI. I

01:31:39.240 --> 01:31:41.330
Cool. So that's all I had have.

01:31:41.810 --> 01:31:42.550
You for listening.

01:31:58.870 --> 01:32:00.490
We have some time for questions.

01:32:15.360 --> 01:32:16.849
Yeah. Hi.

01:32:17.410 --> 01:32:19.569
This is Mart from BrowserUse. Thanks

01:32:19.570 --> 01:32:21.580
for the talk because it was nice to hear.

01:32:21.580 --> 01:32:23.910
All the aspects. Just wanted

01:32:23.910 --> 01:32:25.939
to ask about you said there are many

01:32:25.939 --> 01:32:28.179
components to training, like the safety training

01:32:28.180 --> 01:32:30.350
and then aligning with, like, the real world

01:32:30.350 --> 01:32:32.669
usage and also the reinforcement learning

01:32:32.669 --> 01:32:34.859
on on your

01:32:34.860 --> 01:32:37.020
typical tasks. And I just wanted to ask how

01:32:37.020 --> 01:32:39.330
do you reconcile all of these

01:32:39.330 --> 01:32:41.350
to do to do it at

01:32:41.350 --> 01:32:43.210
scale? Because I guess you can generate

01:32:43.680 --> 01:32:45.680
a lot of these test cases with,

01:32:45.920 --> 01:32:47.949
that you can run and verify, but

01:32:48.269 --> 01:32:50.289
same is not true for, I guess, like, safety.

01:32:50.580 --> 01:32:52.800
Or or the alignment

01:32:52.800 --> 01:32:55.040
alignment for collaborating with

01:32:55.040 --> 01:32:57.149
humans. And I just wanted to know, how do

01:32:57.150 --> 01:32:59.480
you how do you actually reconcile those two

01:32:59.880 --> 01:33:01.740
do do all of that scale? Is it just like

01:33:02.030 --> 01:33:03.740
all combined into one pipeline or

01:33:04.060 --> 01:33:06.080
do you try to do these stages

01:33:06.140 --> 01:33:07.470
separately? And try to make it work?

01:33:08.190 --> 01:33:10.430
Thank you. Yeah. I think, you know, I think

01:33:10.430 --> 01:33:12.530
we're all about things that scale well. So we as

01:33:12.530 --> 01:33:14.200
much as possible, we try to do it like

01:33:15.950 --> 01:33:17.980
try to address, like, all these things at the at the same time, if

01:33:17.980 --> 01:33:20.160
that makes sense. And then, ultimately, like,

01:33:20.190 --> 01:33:22.210
are are North Star is, like, aligning

01:33:22.350 --> 01:33:24.510
these things with how the

01:33:24.510 --> 01:33:26.350
tools are used in the real world? So

01:33:26.830 --> 01:33:28.959
much as possible, like capturing you know, like, how

01:33:28.959 --> 01:33:31.199
do people actually use the product in practice and making

01:33:31.200 --> 01:33:33.570
sure that the training reflects that

01:33:33.959 --> 01:33:34.940
that real usage

01:33:41.780 --> 01:33:43.950
I I have a, also, I'm

01:33:43.950 --> 01:33:46.070
Mylene. I'm affiliated with

01:33:46.630 --> 01:33:48.790
Zurich. I wanted to ask, do you

01:33:48.790 --> 01:33:50.870
think that there's a problem with,

01:33:51.120 --> 01:33:53.212
the compaction method? Do you think that

01:33:53.213 --> 01:33:55.430
we need new solutions for that? Because I'm

01:33:55.670 --> 01:33:57.749
not super familiar how it works, but I would assume that

01:33:57.749 --> 01:33:59.870
it's some kind of summary. The question is, like,

01:33:59.870 --> 01:34:01.570
what do we put into the summary?

01:34:02.130 --> 01:34:03.910
Like, do we miss relevant information?

01:34:04.290 --> 01:34:06.180
Do you think that, so that

01:34:06.420 --> 01:34:08.580
like, there's some interesting work and that you're,

01:34:08.580 --> 01:34:10.910
like, rethinking the strategies for this.

01:34:11.390 --> 01:34:13.890
I think there's definitely room for more sophisticated

01:34:14.269 --> 01:34:16.550
strategies, but find that

01:34:16.550 --> 01:34:18.630
this approach works well. And I think, like, one of the powerful

01:34:18.630 --> 01:34:20.859
things about this approach is that it's basically the

01:34:20.860 --> 01:34:22.880
model that decides what to put in

01:34:22.880 --> 01:34:24.960
the summary. And so you can imagine, like, over the course

01:34:24.960 --> 01:34:27.060
of of training, like, this the model

01:34:27.060 --> 01:34:29.110
like gradually learns how to do this on its own.

01:34:29.750 --> 01:34:31.269
Which is pretty interesting to see.

01:34:33.030 --> 01:34:35.379
So I'm Shamus Patel. I'm affiliated

01:34:35.380 --> 01:34:37.469
with Best Buy. I wanted to understand when

01:34:37.470 --> 01:34:39.810
you are talking about the parallel test compute

01:34:39.870 --> 01:34:42.280
scaling, how how do you plan to do the verifiers

01:34:42.280 --> 01:34:44.560
here, which can also scale, at the same time?

01:34:46.790 --> 01:34:48.340
I'm sorry. Could you clarify what you mean? So

01:34:48.820 --> 01:34:51.019
The panel that you Oh, you mean like as a

01:34:51.020 --> 01:34:53.068
as a user, how can you scale the verification

01:34:53.068 --> 01:34:55.409
process? Yeah. I think that's

01:34:55.570 --> 01:34:57.420
an area that we're actively looking into.

01:35:09.439 --> 01:35:10.800
One last question.

01:35:11.740 --> 01:35:13.729
Hi. Hey.

01:35:14.530 --> 01:35:16.769
I'm a big fan of Kodak's. I use it on a

01:35:16.770 --> 01:35:18.859
daily basis. I'm Pranav. I'm

01:35:18.860 --> 01:35:21.149
from a start up. I wanna understand,

01:35:21.149 --> 01:35:23.229
you mentioned the token consumption is

01:35:23.229 --> 01:35:25.270
dynamic The model learns to

01:35:25.270 --> 01:35:27.580
know how to consume

01:35:27.720 --> 01:35:29.800
more tokens or less tokens. Is that applied

01:35:29.800 --> 01:35:31.990
at a global level or is it like

01:35:32.230 --> 01:35:34.710
personalized for every user basis their coding

01:35:34.710 --> 01:35:36.840
patterns? You

01:35:36.840 --> 01:35:38.470
you mean, like, specifically with the

01:35:38.950 --> 01:35:41.100
tool? The How many how many tokens

01:35:41.100 --> 01:35:43.259
does it consume? Is it personalized according to every

01:35:43.260 --> 01:35:45.049
user? Is it, like, RL?

01:35:45.610 --> 01:35:47.610
Applied on, like, per user level?

01:35:48.090 --> 01:35:50.320
Or I think the one I showed specifically, that's at the

01:35:50.320 --> 01:35:52.640
tool usage level. So, basically, like, when a model runs a command,

01:35:52.640 --> 01:35:54.900
it can kinda control, like, how many tokens

01:35:56.140 --> 01:35:58.020
like, the maximum number of tokens that it

01:35:58.420 --> 01:36:00.810
should see from the tool call. And that's personalized

01:36:00.950 --> 01:36:03.249
for every particular session?

01:36:03.249 --> 01:36:05.409
Or is it like I guess it depends on, like, the

01:36:05.410 --> 01:36:07.560
command. So, like, you know, if you're running,

01:36:07.560 --> 01:36:09.880
like, a test command and you might see a lot of output, then

01:36:10.040 --> 01:36:12.090
it might make sense for the model to, like, be a little more

01:36:12.090 --> 01:36:13.710
restrictive about how many tokens

01:36:14.320 --> 01:36:16.149
should be seen from the result. Got it.

01:36:17.430 --> 01:36:18.250
Cool. Thanks.

01:36:27.910 --> 01:36:29.990
Alright. So for our next speaker,

01:36:30.150 --> 01:36:32.510
it's my my pleasure to introduce Veena

01:36:32.510 --> 01:36:34.530
Grover from NVIDIA. I had

01:36:34.530 --> 01:36:36.870
a pleasure to actually work with V Note

01:36:36.930 --> 01:36:39.249
like a while back. I was interning at NVIDIA,

01:36:39.310 --> 01:36:41.249
and that was a really good experience.

01:36:42.210 --> 01:36:44.610
V Not is a senior distinguished engineer

01:36:44.610 --> 01:36:46.540
at NVIDIA, where he was

01:36:47.180 --> 01:36:49.570
has worked there since 2007,

01:36:50.609 --> 01:36:52.689
he led the team that created CUDA C

01:36:52.689 --> 01:36:54.629
plus plus language and compiled

01:36:55.730 --> 01:36:57.670
helped, make GPU computing faster

01:36:58.010 --> 01:36:59.730
and easier across many fields.

01:37:00.195 --> 01:37:02.419
Since 2017, he has applied

01:37:02.419 --> 01:37:04.819
language and compiler ideas to accelerate deep learning

01:37:04.820 --> 01:37:06.990
models, leading a smaller group focused

01:37:06.990 --> 01:37:08.610
on performance and developer

01:37:09.470 --> 01:37:11.890
productivity. And today, he would talk about essence

01:37:11.950 --> 01:37:13.879
of CUDA, C plus plus, and

01:37:14.120 --> 01:37:14.940
for GPUs.

01:37:20.560 --> 01:37:22.740
Thank thank you, Magpog. Thank you.

01:37:23.200 --> 01:37:25.350
And you for inviting me here

01:37:25.350 --> 01:37:27.630
and glad to be here. So I'm gonna,

01:37:27.950 --> 01:37:29.970
try to give you a feel for

01:37:29.970 --> 01:37:32.470
what CUDA C plus plus is or was,

01:37:32.570 --> 01:37:34.040
and where it's going.

01:37:38.120 --> 01:37:39.660
So this is a slide

01:37:40.810 --> 01:37:43.070
so it's basically by one of our ninjas.

01:37:44.440 --> 01:37:46.720
Created this thing about me and him.

01:37:47.200 --> 01:37:49.280
So this is, what I do. I have a

01:37:49.280 --> 01:37:50.870
lot of people in my team,

01:37:51.510 --> 01:37:52.980
very enthusiastic, and I hope

01:37:54.350 --> 01:37:56.790
helped them basically you know, develop

01:37:56.930 --> 01:37:57.510
new ideas.

01:37:59.105 --> 01:38:01.060
And fun job.

01:38:01.120 --> 01:38:02.810
Okay. So here's my agenda.

01:38:03.189 --> 01:38:05.049
I'll go through this as quickly as I can.

01:38:05.690 --> 01:38:07.470
And, leave enough time for questions.

01:38:07.850 --> 01:38:09.800
And I'll be here at the end.

01:38:11.010 --> 01:38:13.169
So the basic idea is about what is

01:38:13.170 --> 01:38:14.230
a GPU computation?

01:38:15.460 --> 01:38:16.880
So let's go into that. So

01:38:17.590 --> 01:38:19.209
early on in early two thousands,

01:38:19.910 --> 01:38:22.390
NVIDIA and other

01:38:22.690 --> 01:38:24.840
people, ATI at the time were talking

01:38:25.400 --> 01:38:27.740
about GPU. GPU was general purpose

01:38:27.880 --> 01:38:29.910
GPU it is a nice,

01:38:29.910 --> 01:38:31.879
catchy term, and what

01:38:32.100 --> 01:38:34.330
they did was they used the

01:38:34.330 --> 01:38:35.230
shaded libraries

01:38:36.420 --> 01:38:38.710
pixel shaders and all those things to

01:38:38.770 --> 01:38:41.030
do so something like DirectX, OpenGL,

01:38:41.610 --> 01:38:44.030
and they were trying to do matrix multiply

01:38:44.250 --> 01:38:45.070
for simulation.

01:38:47.750 --> 01:38:49.850
And it was kind of a nice

01:38:50.229 --> 01:38:51.770
idea and then

01:38:52.709 --> 01:38:54.570
around 2025,

01:38:56.240 --> 01:38:56.939
Mark Harris

01:38:58.689 --> 01:39:00.729
he was at NVIDIA. He and he

01:39:00.730 --> 01:39:01.640
came with the term

01:39:03.010 --> 01:39:04.950
GPU and created some organization.

01:39:05.910 --> 01:39:07.990
And Nvidia published, a nice

01:39:07.990 --> 01:39:10.070
book. Called

01:39:10.070 --> 01:39:12.305
GPU Gems. And

01:39:12.690 --> 01:39:14.810
it's not very convenient way

01:39:14.810 --> 01:39:17.070
to program GPUs if you're not

01:39:17.289 --> 01:39:18.250
doing graphics. Right?

01:39:20.470 --> 01:39:22.549
And people used to struggle, they tried tens

01:39:22.550 --> 01:39:24.080
of thousands of lines of code,

01:39:24.720 --> 01:39:25.859
to do something simple

01:39:27.720 --> 01:39:30.300
And then around 2006, 2007,

01:39:31.289 --> 01:39:32.720
there was this one guy who came

01:39:33.820 --> 01:39:36.000
to NVIDIA. His name is John Nichols.

01:39:36.650 --> 01:39:38.620
And he created CUDA

01:39:39.020 --> 01:39:40.900
the vision for CUDA. And

01:39:41.360 --> 01:39:43.520
then that call it SIMT, a single instruction

01:39:43.520 --> 01:39:44.489
multiple threads.

01:39:45.899 --> 01:39:48.350
And so what they did was basically

01:39:48.350 --> 01:39:49.870
you have this compute hierarchy

01:39:50.420 --> 01:39:52.540
in our GPUs, threads,

01:39:52.540 --> 01:39:54.680
warps, and so on. There's a memory

01:39:54.680 --> 01:39:57.010
hierarchy, register

01:39:57.010 --> 01:39:59.030
files, which is very huge.

01:39:59.170 --> 01:40:01.510
And they published NVIDIA

01:40:01.510 --> 01:40:02.849
CUDA programming guide. And

01:40:03.790 --> 01:40:05.790
it's not officially released at that time,

01:40:06.479 --> 01:40:08.270
and I came along at that time

01:40:09.780 --> 01:40:11.869
in the middle of it and I

01:40:11.870 --> 01:40:14.030
didn't know what the what the GPU was,

01:40:14.030 --> 01:40:15.830
so everybody used to laugh at me.

01:40:17.410 --> 01:40:19.520
And I learned And

01:40:19.680 --> 01:40:21.740
gonna try to tell you what it is. So

01:40:21.740 --> 01:40:23.820
there's basically this idea of a

01:40:23.820 --> 01:40:25.129
thread really a lane

01:40:25.850 --> 01:40:27.555
so you have a Buddha thread,

01:40:28.070 --> 01:40:30.019
the right hand side is something called CUDA core.

01:40:30.560 --> 01:40:32.720
Then these are organized in thread blocks,

01:40:33.400 --> 01:40:35.559
and then there's a streaming, processor,

01:40:35.560 --> 01:40:37.709
on the right, which is what

01:40:37.710 --> 01:40:39.950
the hardware looks like. And then

01:40:40.010 --> 01:40:41.520
then we have a grid of

01:40:42.080 --> 01:40:42.849
this thread box

01:40:44.240 --> 01:40:46.319
and you have the cuda cable or GPU, the

01:40:46.320 --> 01:40:48.510
entire thing. So essentially the GPU is

01:40:48.690 --> 01:40:51.032
consist of a large number of SMs, Each

01:40:51.032 --> 01:40:53.160
SM is a thread block

01:40:53.900 --> 01:40:55.700
and each thread block contains

01:40:56.260 --> 01:40:58.050
thousands of threads or you can configure them.

01:40:58.450 --> 01:41:00.490
And they're very

01:41:00.675 --> 01:41:02.080
fast. Or at least

01:41:04.890 --> 01:41:06.590
So memory hierarchy is essentially,

01:41:07.590 --> 01:41:09.590
cartoon picture for

01:41:09.635 --> 01:41:11.800
ampere, so you have the different SMs

01:41:12.440 --> 01:41:14.530
and then each SM has access to

01:41:14.530 --> 01:41:16.569
a very fast register file. And

01:41:16.570 --> 01:41:18.830
that keeps changing, how many register files we have.

01:41:19.149 --> 01:41:21.199
Right? And then there's

01:41:21.200 --> 01:41:23.359
l one cache and shared memory which

01:41:23.359 --> 01:41:25.459
is pretty close to register

01:41:25.920 --> 01:41:28.070
speeds. And then there's l two.

01:41:28.230 --> 01:41:29.730
And then global memory.

01:41:31.440 --> 01:41:33.850
So let's get back to the software side. So each thread

01:41:33.850 --> 01:41:36.249
in the GPU in CUDA

01:41:36.249 --> 01:41:37.709
has a logical coordinate.

01:41:39.050 --> 01:41:40.929
So basically, so it's a two pole

01:41:42.209 --> 01:41:44.319
internally So they're called thread ID,

01:41:44.640 --> 01:41:46.650
Thread ID is a structure with three d

01:41:47.780 --> 01:41:49.940
and then there's a block ID which is its position. So

01:41:49.940 --> 01:41:52.137
essentially these two to define coordinate

01:41:52.137 --> 01:41:54.550
of a thread. And each

01:41:54.550 --> 01:41:56.150
thread that at the hardware level,

01:41:57.110 --> 01:41:58.810
threads are organized in

01:41:59.350 --> 01:42:01.050
groups of 32, they run-in parallel

01:42:01.850 --> 01:42:02.920
and they usually

01:42:04.240 --> 01:42:06.800
have a single program counter. All 32

01:42:06.800 --> 01:42:09.033
threads in a warp have a single

01:42:09.033 --> 01:42:10.930
program counter. So when you advance it,

01:42:11.090 --> 01:42:13.249
all threads move to the next instruction. So that's where you

01:42:13.249 --> 01:42:15.269
get the massive speed up

01:42:15.530 --> 01:42:17.539
at the hardware level, but you lose

01:42:17.539 --> 01:42:18.120
some flexibility,

01:42:20.050 --> 01:42:22.200
as a result of that. Then

01:42:22.200 --> 01:42:24.140
at the higher level, we have GPU kernels.

01:42:25.090 --> 01:42:26.390
It's like a function,

01:42:27.170 --> 01:42:29.119
and I've seen there's a picture below

01:42:29.999 --> 01:42:30.740
In this picture,

01:42:32.450 --> 01:42:34.610
basic idea is that a kernel is a C plus

01:42:34.610 --> 01:42:36.970
plus like function is a kind of qualifier

01:42:37.510 --> 01:42:39.630
called global and then inside of that, you

01:42:39.630 --> 01:42:41.570
can basically write the code for a single

01:42:41.985 --> 01:42:44.030
thread. When you launch it,

01:42:44.190 --> 01:42:46.340
it'll run on however

01:42:46.340 --> 01:42:47.920
many threads you want it to run on.

01:42:50.410 --> 01:42:52.489
So give you a quick cartoon picture of

01:42:52.490 --> 01:42:54.789
what a control flow looks like in a thread block.

01:42:55.270 --> 01:42:57.350
So here's a picture of a a

01:42:57.910 --> 01:42:59.930
function called kernels. It has two arguments.

01:43:00.170 --> 01:43:02.270
An integer x and a pointer y.

01:43:03.550 --> 01:43:05.369
So when you start this kernel,

01:43:06.010 --> 01:43:08.170
on a GPU, x and

01:43:08.170 --> 01:43:10.590
y have the same value for all the threads.

01:43:11.050 --> 01:43:12.360
And at the curly basis,

01:43:13.200 --> 01:43:15.210
conceptually, you can think of all the threads

01:43:15.850 --> 01:43:17.050
start together. Right?

01:43:18.410 --> 01:43:20.429
And then they do something, s one,

01:43:21.380 --> 01:43:23.460
and then there's synchronization statement called

01:43:23.460 --> 01:43:25.660
sync threads. So all

01:43:25.660 --> 01:43:27.739
the threads must arrive at that

01:43:27.740 --> 01:43:29.849
sync threads before they can go

01:43:29.850 --> 01:43:31.860
move to s two. Right?

01:43:32.100 --> 01:43:33.990
Before that, they can go in any order

01:43:34.230 --> 01:43:36.100
don't guarantee any order. But

01:43:36.240 --> 01:43:38.360
at the performance there is some order, but you

01:43:38.360 --> 01:43:39.939
can't assume it. To greatness.

01:43:40.420 --> 01:43:42.500
And then, at the close at

01:43:42.500 --> 01:43:44.740
the exit of a kernel, all threats

01:43:44.740 --> 01:43:46.790
must arrive. Some threats can't

01:43:46.790 --> 01:43:48.680
leave and some threats are still executing that

01:43:48.920 --> 01:43:51.120
cannot happen. So that's the basic

01:43:51.120 --> 01:43:53.170
model. Now I'm going to

01:43:53.170 --> 01:43:55.210
move to heterogeneous competition. So that was

01:43:55.450 --> 01:43:57.000
just a device computation so far?

01:43:58.040 --> 01:44:00.300
So So what is heterogeneous computation? So

01:44:00.890 --> 01:44:03.209
typically with a GPU, you have a CPU attached

01:44:03.209 --> 01:44:05.430
to it. Or maybe more than one CPU.

01:44:06.789 --> 01:44:09.139
So you have CPU and GPU working together.

01:44:09.830 --> 01:44:11.860
So a kernel is launched from CPU.

01:44:12.640 --> 01:44:13.939
So CPU thread

01:44:14.939 --> 01:44:16.769
x 86 or ARM or

01:44:17.890 --> 01:44:19.510
So what happens is the CPU

01:44:20.650 --> 01:44:22.929
does some work of its own,

01:44:23.090 --> 01:44:24.750
It can also do some work

01:44:25.230 --> 01:44:27.310
for the GPU. It can say, hey, allocate some

01:44:27.310 --> 01:44:28.800
memory asynchronously

01:44:30.000 --> 01:44:31.550
put some of this data in the memory,

01:44:32.980 --> 01:44:35.060
this data from the GPU, It

01:44:35.060 --> 01:44:36.920
can do so, essentially, it can do computations.

01:44:37.999 --> 01:44:40.160
Coordinating and communicating with the GPU.

01:44:41.039 --> 01:44:42.910
Synchronizing sending data

01:44:43.420 --> 01:44:45.660
back and forth, and and they can work

01:44:45.660 --> 01:44:47.960
together, right? So you can basically say,

01:44:47.960 --> 01:44:50.030
start a kernel, While the kernel is running, you

01:44:50.030 --> 01:44:52.269
can start preparing the data for the next kernel you want

01:44:52.269 --> 01:44:54.370
to launch. You can pipeline and

01:44:54.370 --> 01:44:56.560
a parallelized CPU and GPU computation.

01:44:56.920 --> 01:44:58.989
And that's the key concept. Of

01:44:58.990 --> 01:45:01.120
a heterogeneous competition. And that is

01:45:01.120 --> 01:45:03.250
very important especially these days.

01:45:04.200 --> 01:45:05.430
This kind of evolved

01:45:07.030 --> 01:45:09.320
kind of organically.

01:45:09.480 --> 01:45:11.620
But people were trying to do just

01:45:11.620 --> 01:45:13.100
get the last cycle of performance

01:45:13.740 --> 01:45:15.689
and they kept adding stuff. So this is what it is today.

01:45:16.250 --> 01:45:18.410
Okay. So what is programming could I

01:45:18.410 --> 01:45:20.420
mean here? So there's a language called

01:45:20.420 --> 01:45:22.424
CUDA C plus plus a lot of

01:45:22.425 --> 01:45:24.310
people have tried to implement it.

01:45:24.790 --> 01:45:26.530
And there's been some confusion

01:45:27.320 --> 01:45:29.560
what CUDA C plus plus is. So effectively,

01:45:29.620 --> 01:45:31.912
it's an extended C plus

01:45:32.360 --> 01:45:32.860
plus language.

01:45:34.530 --> 01:45:36.230
Where you can write host computations

01:45:36.920 --> 01:45:39.320
So host computations just look like normal C plus

01:45:39.320 --> 01:45:41.400
plus. You write c plus

01:45:41.400 --> 01:45:43.700
plus, inside of that, you write some device code

01:45:43.740 --> 01:45:45.990
with some qualified like

01:45:45.990 --> 01:45:48.430
host device thread ID, special

01:45:48.650 --> 01:45:51.080
things. You have a unified CUDA

01:45:51.700 --> 01:45:54.120
CUDA program, and then the

01:45:54.234 --> 01:45:56.430
compiler will essentially

01:45:56.430 --> 01:45:58.830
do the following. So this is really the true essence

01:45:58.830 --> 01:46:00.930
of CUDA. So

01:46:00.930 --> 01:46:02.960
what happens is that at the top,

01:46:02.960 --> 01:46:04.199
you have a CUDA file,

01:46:05.080 --> 01:46:07.220
There's CUDA front end. What the

01:46:07.220 --> 01:46:09.300
CUDA front end does is it splits

01:46:09.300 --> 01:46:11.700
your program into two parts. So

01:46:11.760 --> 01:46:13.470
all the the host code

01:46:14.269 --> 01:46:16.430
all the C plus plus code, it can contain Windows

01:46:16.430 --> 01:46:18.730
specific things, It can contain anything

01:46:18.730 --> 01:46:20.930
you want. That's Visual C plus

01:46:21.230 --> 01:46:23.180
plus or GCC, or Clang

01:46:23.240 --> 01:46:25.545
or whatever C plus plus compiler you

01:46:25.545 --> 01:46:27.670
want. It supports that dialect. So we

01:46:27.670 --> 01:46:29.979
don't even look at that. We

01:46:29.979 --> 01:46:32.269
just put it out. Then

01:46:32.270 --> 01:46:34.419
we have the device code, which

01:46:34.420 --> 01:46:36.840
goes to our compiler, through LLVM, we call it NVVM

01:46:36.980 --> 01:46:39.029
IR, We use LVM optimizer.

01:46:39.030 --> 01:46:41.080
We we then generate

01:46:41.080 --> 01:46:43.160
PTEX at the end. PTEX is our assembly, virtual

01:46:43.160 --> 01:46:45.229
assembly. And that produces

01:46:45.289 --> 01:46:46.629
a kind of a binary.

01:46:47.360 --> 01:46:49.140
What we do is we serialize that binary

01:46:49.700 --> 01:46:51.430
and then we put that into

01:46:51.830 --> 01:46:53.860
a variable inside the C plus plus code, and then we

01:46:53.860 --> 01:46:56.060
give it to the C plus plus compiler

01:46:56.060 --> 01:46:57.050
and we get this

01:46:58.190 --> 01:47:00.650
a dot out that you can run and then it will

01:47:00.727 --> 01:47:03.000
basically start managing the computation. So essentially it's

01:47:03.000 --> 01:47:05.150
it's a way to program

01:47:05.710 --> 01:47:07.919
GPUs in your favorite c

01:47:07.920 --> 01:47:08.969
plus plus environment.

01:47:09.930 --> 01:47:12.269
So it's not a special language.

01:47:12.269 --> 01:47:13.410
It's just an extended

01:47:14.830 --> 01:47:17.130
host language that's that's been how

01:47:17.130 --> 01:47:19.459
it is so far. And it creates

01:47:19.460 --> 01:47:21.910
some challenges but but that's what basically

01:47:22.370 --> 01:47:24.780
gives people a way to

01:47:24.780 --> 01:47:27.069
program very quickly. So if you know C plus

01:47:27.070 --> 01:47:29.320
plus, learning good

01:47:29.400 --> 01:47:31.530
is you know, not a big deal.

01:47:31.530 --> 01:47:33.470
I mean, it still requires an effort,

01:47:35.090 --> 01:47:36.620
Okay. So okay. So

01:47:37.419 --> 01:47:39.760
changing gears, So so

01:47:39.820 --> 01:47:42.000
now what's the future, of the present?

01:47:43.130 --> 01:47:44.830
So NVIDIA announced

01:47:45.680 --> 01:47:47.199
Ku Tile and Tile IR recently.

01:47:48.080 --> 01:47:50.370
At GTC. And yesterday, a few

01:47:50.370 --> 01:47:52.510
days ago, they released it. So

01:47:52.510 --> 01:47:53.970
tile programming is the future.

01:47:55.370 --> 01:47:57.520
So the the motivation behind this

01:47:57.520 --> 01:47:59.470
was that most

01:47:59.870 --> 01:48:01.890
CUDA C plus plus kernels for

01:48:01.890 --> 01:48:03.930
DL they really work on

01:48:03.930 --> 01:48:05.880
a tile level. So this just use

01:48:06.280 --> 01:48:08.600
very low level construct to do it. So, for example, if you want

01:48:08.600 --> 01:48:10.370
to try to jam or attention or

01:48:10.689 --> 01:48:12.820
any such thing, you basically organize

01:48:12.820 --> 01:48:14.280
your tensors into tiles,

01:48:15.440 --> 01:48:17.220
and then you do some crazy things.

01:48:19.150 --> 01:48:21.630
And so it gives you a lot of power, a lot of

01:48:21.630 --> 01:48:21.930
flexibility

01:48:23.870 --> 01:48:25.912
at the lowest level So what

01:48:25.912 --> 01:48:28.240
we did was we basically created a

01:48:28.530 --> 01:48:29.330
little higher level

01:48:30.880 --> 01:48:32.260
DSL or language called Kotair.

01:48:33.400 --> 01:48:35.620
It basically has two

01:48:36.300 --> 01:48:38.530
two front ends. The first one that

01:48:38.530 --> 01:48:40.159
is released Python

01:48:40.800 --> 01:48:42.589
another one will be c plus plus.

01:48:43.070 --> 01:48:45.130
So you have a concept of a tile

01:48:45.370 --> 01:48:47.530
and then there's no memory spaces. There's

01:48:47.530 --> 01:48:49.550
no threads. You just have tile

01:48:49.550 --> 01:48:51.700
blocks, so they're not to So here's

01:48:51.700 --> 01:48:52.580
an example

01:48:53.780 --> 01:48:55.910
on the left, you have a a

01:48:56.370 --> 01:48:57.669
Python program.

01:48:58.900 --> 01:49:01.009
It contains a function

01:49:01.010 --> 01:49:03.070
called matmul, and then it

01:49:03.070 --> 01:49:05.050
has a decorator called c t dot kernel

01:49:05.530 --> 01:49:08.030
and then inside of that, it looks pretty straightforward

01:49:08.170 --> 01:49:09.959
and it's performing a simple matrix multiplications.

01:49:10.960 --> 01:49:13.129
But under the covers, it

01:49:13.130 --> 01:49:15.210
gets translated to the GPU.

01:49:15.210 --> 01:49:17.450
And then the right hand side, is similar

01:49:17.450 --> 01:49:19.530
program by C plus So we added some

01:49:19.530 --> 01:49:20.590
c plus plus extensions,

01:49:23.380 --> 01:49:25.429
and so the only difference you can see that you

01:49:25.670 --> 01:49:27.730
see namespaces on the right hand side,

01:49:28.290 --> 01:49:29.920
then there's a tile global function,

01:49:30.620 --> 01:49:32.689
and you can see some cases, templates,

01:49:32.689 --> 01:49:34.710
which says zeros. Of, template

01:49:34.710 --> 01:49:36.520
intraniation of of float.

01:49:38.120 --> 01:49:40.350
And so it makes it much simpler

01:49:41.530 --> 01:49:43.780
to write high performance GPU

01:49:43.780 --> 01:49:46.150
programs. And in in the end,

01:49:46.950 --> 01:49:49.029
c plus plus, I think will give you more

01:49:49.030 --> 01:49:50.650
power, so a lot of people use Cutlass

01:49:51.130 --> 01:49:53.059
the learning curve hopefully will be less.

01:49:53.700 --> 01:49:55.720
Okay. So that's pretty much I think

01:49:55.960 --> 01:49:58.220
what I want to talk about for the tile programming

01:49:58.280 --> 01:50:00.470
model. The next thing

01:50:00.470 --> 01:50:02.510
is essentially how are

01:50:02.510 --> 01:50:04.610
you going to use AI for GPU programming?

01:50:05.680 --> 01:50:07.870
So these are some results of internal

01:50:07.870 --> 01:50:10.169
research and speculations from our group.

01:50:10.810 --> 01:50:13.030
And other people at NVIDIA. So

01:50:13.030 --> 01:50:14.819
this is not yet public,

01:50:15.220 --> 01:50:17.340
a sense that we have not committed to

01:50:17.420 --> 01:50:19.100
if they will ever be released or

01:50:19.795 --> 01:50:20.295
when,

01:50:22.720 --> 01:50:24.879
So so first thing I wanna talk about

01:50:24.880 --> 01:50:26.590
is AI for GPUs is

01:50:27.700 --> 01:50:29.680
do you really wanna do? So so

01:50:29.805 --> 01:50:32.289
the one of the challenges that

01:50:33.240 --> 01:50:34.790
programmers and developers have

01:50:35.479 --> 01:50:36.910
is writing

01:50:37.490 --> 01:50:37.990
correct

01:50:39.959 --> 01:50:42.010
deal and GP programs. So if

01:50:42.010 --> 01:50:43.070
you do that today,

01:50:44.379 --> 01:50:46.410
and you are new, there is a

01:50:46.410 --> 01:50:47.480
large learning curve

01:50:48.520 --> 01:50:50.840
And especially with the new languages like the tile

01:50:50.840 --> 01:50:52.890
programming it's all new. There's not

01:50:52.890 --> 01:50:55.010
much experience there. That's

01:50:55.010 --> 01:50:57.180
the first challenge. The second challenge

01:50:57.180 --> 01:50:59.220
is how do you construct high performance

01:50:59.440 --> 01:51:01.519
GP programs? And that's

01:51:01.520 --> 01:51:03.600
also kind of a profession, so to

01:51:03.600 --> 01:51:05.734
speak. Today. As you might see,

01:51:05.734 --> 01:51:08.130
people write custom kernels and you see GPU

01:51:08.130 --> 01:51:10.265
mode and a lot of of groups

01:51:10.440 --> 01:51:12.300
very excited about it because it's important.

01:51:13.470 --> 01:51:15.709
So so one thing we learned from writing

01:51:15.709 --> 01:51:17.980
high performance GP programs is that

01:51:19.860 --> 01:51:22.120
if you want to write, let's say,

01:51:22.120 --> 01:51:24.200
a thousand line CUDA program just to do

01:51:24.200 --> 01:51:26.269
a simple gem, right? It contains

01:51:26.269 --> 01:51:28.299
a lot of stuff. So

01:51:28.860 --> 01:51:31.010
and even humans have troubles know, coming

01:51:31.010 --> 01:51:33.360
up with that. So you need I think for

01:51:33.360 --> 01:51:35.070
agents, you need high level abstractions.

01:51:35.920 --> 01:51:37.999
To reason and think about it. You don't just generate thousand

01:51:38.000 --> 01:51:40.380
lines of you know, code from

01:51:40.380 --> 01:51:42.550
some auto aggressive model. I mean you could,

01:51:42.550 --> 01:51:44.480
but that will be

01:51:45.120 --> 01:51:47.215
not easy. Right? So so

01:51:47.430 --> 01:51:49.639
so you need high level of abstractions. Then

01:51:49.640 --> 01:51:51.800
for performance, you also need to have some kind

01:51:51.800 --> 01:51:53.950
of an idea about the

01:51:53.965 --> 01:51:55.700
model the analytical

01:51:56.320 --> 01:51:58.279
and empirical performance model, of what you're doing.

01:51:59.080 --> 01:52:00.460
People use auto tuning

01:52:01.170 --> 01:52:03.160
sometimes that's very ad hoc,

01:52:04.810 --> 01:52:06.230
And then restructuring

01:52:07.459 --> 01:52:09.499
pieces of code for high performance. So you can

01:52:09.499 --> 01:52:11.970
write a live, let's say, matrix

01:52:12.155 --> 01:52:14.200
multiply But to

01:52:14.200 --> 01:52:16.380
really take advantage of, let's say, volatile GPU

01:52:16.440 --> 01:52:18.410
versus Hopper versus black hole,

01:52:18.729 --> 01:52:20.600
we may need to restructure it differently.

01:52:20.999 --> 01:52:23.220
Right? So so an agent or

01:52:23.440 --> 01:52:25.539
or AI has to be able to do

01:52:25.539 --> 01:52:27.680
that and a

01:52:27.680 --> 01:52:29.709
lot of this knowledge is not

01:52:29.710 --> 01:52:30.890
explicitly, you know,

01:52:31.769 --> 01:52:34.229
written down by ninjas.

01:52:34.300 --> 01:52:35.860
So so

01:52:36.420 --> 01:52:38.660
here is the first example of what we are

01:52:38.660 --> 01:52:40.410
doing. So

01:52:41.390 --> 01:52:43.629
so we have a developer, and, this is

01:52:43.630 --> 01:52:45.720
actually the what we do is we basically

01:52:45.720 --> 01:52:47.100
ask the developer asks

01:52:47.910 --> 01:52:49.800
question, write a CuTile program

01:52:50.330 --> 01:52:52.700
for something and then

01:52:52.700 --> 01:52:53.760
it goes to an LLM

01:52:55.100 --> 01:52:57.390
and then we get some program

01:52:57.960 --> 01:53:00.200
Then we have a verifier. So the verifier, the way

01:53:00.200 --> 01:53:02.380
it works is it takes the program,

01:53:02.960 --> 01:53:05.350
and we have a service on the site where

01:53:05.350 --> 01:53:07.420
you try to run the program. If you

01:53:07.420 --> 01:53:09.330
get errors, you try again.

01:53:09.950 --> 01:53:11.810
With the error message attached to the context.

01:53:12.470 --> 01:53:14.550
If it works, then

01:53:14.550 --> 01:53:16.369
we try to make sure that it works on

01:53:16.610 --> 01:53:18.650
all the interesting test cases to give the

01:53:18.650 --> 01:53:21.150
correct answer. And then we

01:53:21.210 --> 01:53:23.150
omit that. So below is

01:53:24.030 --> 01:53:26.140
there's actual prompt that you

01:53:26.360 --> 01:53:28.540
can use it just says in English,

01:53:29.370 --> 01:53:31.450
or at least as semi formal English,

01:53:31.450 --> 01:53:33.000
say, please generate a

01:53:33.620 --> 01:53:35.790
kernel for ReLU of mat model

01:53:36.030 --> 01:53:37.729
of tanh of a comma b.

01:53:38.450 --> 01:53:40.609
And then you say, where is this, b

01:53:40.610 --> 01:53:42.668
is that. And then you can say

01:53:42.669 --> 01:53:44.850
Matt Mullis should be in the floor 32 precision,

01:53:45.590 --> 01:53:47.905
and the final result is cost to

01:53:48.330 --> 01:53:50.470
floor 16. Right? We can say

01:53:50.470 --> 01:53:52.519
that and you get back

01:53:52.940 --> 01:53:54.960
something like this. After a

01:53:54.960 --> 01:53:55.700
few iterations.

01:53:57.810 --> 01:53:59.690
And and we verified this is correct.

01:54:02.220 --> 01:54:04.240
So so this is this is just functionality

01:54:04.380 --> 01:54:06.820
and this is just very early stages, very speculative.

01:54:09.360 --> 01:54:11.460
But this is one one area where I think

01:54:11.680 --> 01:54:13.790
agents and AI can

01:54:13.790 --> 01:54:15.960
help people or write

01:54:16.629 --> 01:54:18.870
programs where there's limited general

01:54:18.870 --> 01:54:19.739
knowledge. Right?

01:54:22.610 --> 01:54:24.830
Among the practitioners or or

01:54:25.070 --> 01:54:27.460
programmers Right? So

01:54:28.200 --> 01:54:29.389
that, it's pretty useful.

01:54:33.000 --> 01:54:35.159
So let's see. Now, the VisionTek

01:54:35.160 --> 01:54:37.440
GP programming, you know, we can basically

01:54:37.440 --> 01:54:39.610
like to get more things

01:54:39.610 --> 01:54:41.390
done. So we have a new profiler

01:54:42.680 --> 01:54:45.169
that NVIDIA released, inside Python.

01:54:45.410 --> 01:54:47.680
It can do a lot of experimental stuff. But

01:54:47.920 --> 01:54:49.620
profiling things and doing experiments.

01:54:50.640 --> 01:54:52.499
And we can also automate that

01:54:55.150 --> 01:54:57.310
So so my last, part is

01:54:57.310 --> 01:54:59.439
essentially some abstractions. So we have

01:54:59.439 --> 01:55:01.300
three or four abstractions that we created.

01:55:01.640 --> 01:55:03.190
For writing work specialized code.

01:55:03.750 --> 01:55:05.599
And, so something is

01:55:05.919 --> 01:55:08.180
called a future asynchronous computations.

01:55:09.580 --> 01:55:11.920
We have a paper at CGO early

01:55:11.980 --> 01:55:13.990
next year So the basic idea is

01:55:13.990 --> 01:55:15.790
that you have a full empty container

01:55:16.330 --> 01:55:18.389
for a tile, it's full

01:55:18.390 --> 01:55:20.325
or empty. And then

01:55:21.220 --> 01:55:23.459
basically it's a wrapper around a synchronization barrier

01:55:23.459 --> 01:55:25.549
and a buffer. We arrange

01:55:25.550 --> 01:55:27.720
a lot of them in a pipeline state in a circular

01:55:28.104 --> 01:55:30.390
manner. You can put and get from it.

01:55:32.249 --> 01:55:34.670
And the last thing I wanna talk about here

01:55:34.729 --> 01:55:34.919
is

01:55:37.080 --> 01:55:39.140
how do you model the GP program? So this

01:55:39.300 --> 01:55:41.380
is kind of one vision we have, and we did

01:55:41.380 --> 01:55:43.459
this by hand. So there's an archive paper

01:55:43.459 --> 01:55:45.549
we published, So the we essentially took

01:55:45.550 --> 01:55:46.930
a gem, which is more specialized,

01:55:47.640 --> 01:55:49.679
and tried to tune it. So first we come

01:55:49.680 --> 01:55:51.630
up with a proper model, analytical model,

01:55:52.010 --> 01:55:54.340
instrument a program, and validate

01:55:54.340 --> 01:55:56.420
it, took a lot of time

01:55:56.990 --> 01:55:59.110
do all those experiments, and then we

01:55:59.110 --> 01:56:01.370
have different parameters, the load latency,

01:56:02.010 --> 01:56:04.209
versus, you know, compute latency and this

01:56:04.210 --> 01:56:06.450
and that. And then we keep checking

01:56:07.190 --> 01:56:09.410
until it fits then we can use that to predict.

01:56:09.410 --> 01:56:10.870
So this is really a workflow

01:56:12.080 --> 01:56:14.160
that could be automated by an agent or

01:56:14.160 --> 01:56:16.180
AI. So so I think this is

01:56:16.180 --> 01:56:18.190
very promising idea, because if you have

01:56:18.190 --> 01:56:20.310
to do all this, by hand, even

01:56:20.310 --> 01:56:22.700
for something simple, it's very time consuming

01:56:22.700 --> 01:56:24.709
and error prone. Is where I

01:56:24.710 --> 01:56:26.850
think AI could really help. With

01:56:26.850 --> 01:56:28.950
that, I can take some

01:56:29.170 --> 01:56:29.360
questions.

01:56:51.010 --> 01:56:52.910
Hello? Can you hear me?

01:56:53.150 --> 01:56:55.229
Thank you. Thank you. That's a very

01:56:55.229 --> 01:56:57.410
great talk. Actually, it is

01:56:57.670 --> 01:57:00.150
very inspiring for me because I have been

01:57:00.150 --> 01:57:01.229
working on the

01:57:02.669 --> 01:57:05.090
programming using general AI for a while.

01:57:05.680 --> 01:57:07.250
Of the challenge for me was, like,

01:57:07.810 --> 01:57:09.970
writing tests. Right? So, writing

01:57:09.970 --> 01:57:12.030
tests using AI is

01:57:12.030 --> 01:57:14.109
notoriously hard because AI need to read

01:57:14.110 --> 01:57:15.970
the source script understand

01:57:16.190 --> 01:57:18.450
it, and understand the environment it

01:57:18.590 --> 01:57:20.720
it runs. And then write a code

01:57:20.720 --> 01:57:21.940
that can actually

01:57:22.940 --> 01:57:25.280
result error on its own. Right? So

01:57:25.420 --> 01:57:26.960
you have, discussing

01:57:27.600 --> 01:57:29.540
how to in your team to develop

01:57:30.050 --> 01:57:31.400
solutions for

01:57:32.470 --> 01:57:34.570
GPU programming. I imagine that

01:57:34.570 --> 01:57:36.650
would be very hard to actually write a

01:57:36.650 --> 01:57:38.400
test under a

01:57:38.700 --> 01:57:40.490
parallel environment. Right? So

01:57:40.980 --> 01:57:43.140
I have been writing tests for parallel

01:57:43.140 --> 01:57:45.320
programming, and it's very hard because

01:57:45.380 --> 01:57:47.810
the is a lot of unexpected

01:57:48.030 --> 01:57:50.090
behavior that happening during

01:57:50.090 --> 01:57:52.500
the concurrent situation, not happening

01:57:52.500 --> 01:57:54.420
in the single thread situation?

01:57:55.380 --> 01:57:57.480
So what is, from your perspective,

01:57:58.410 --> 01:58:00.590
a better direction for

01:58:01.340 --> 01:58:03.129
Nvidia or in general GPU

01:58:03.890 --> 01:58:06.150
programming to develop abstraction

01:58:06.930 --> 01:58:09.168
to write tests that is more robust under

01:58:09.169 --> 01:58:09.830
the parallel.

01:58:11.270 --> 01:58:13.290
That's a great question. I think so.

01:58:13.470 --> 01:58:15.390
I'd try to give a short answer.

01:58:15.870 --> 01:58:17.898
So so I think chat panel programming

01:58:17.899 --> 01:58:20.210
even for experts, people

01:58:20.210 --> 01:58:22.360
who have know, ten years of experience,

01:58:22.660 --> 01:58:24.940
is very challenging. So

01:58:24.940 --> 01:58:27.100
so what what I think we can do with AI in

01:58:27.100 --> 01:58:29.129
the in the short medium term would

01:58:29.129 --> 01:58:31.300
be to all the chores

01:58:31.600 --> 01:58:33.859
and mechanical things are error prone. We can eliminate

01:58:33.919 --> 01:58:36.290
those and let the programmer focus on the

01:58:36.350 --> 01:58:38.810
essential task. Then eventually, I think

01:58:38.920 --> 01:58:40.910
once we have better understood

01:58:41.050 --> 01:58:43.260
abstractions, then I think agents

01:58:43.260 --> 01:58:45.670
and AI can do more and more of that. Coding

01:58:46.070 --> 01:58:48.240
but you I think there'll be like

01:58:48.240 --> 01:58:50.479
an assistant rather than autonomous

01:58:51.070 --> 01:58:53.219
completely autonomous. That's my

01:58:53.220 --> 01:58:54.120
issue. Concern.

01:58:57.850 --> 01:59:00.110
Hi. This is Apala from Microsoft.

01:59:01.585 --> 01:59:03.840
Mhmm. I have a question

01:59:03.840 --> 01:59:05.959
that when you are writing

01:59:05.959 --> 01:59:08.200
performant kernels, it becomes even more

01:59:08.200 --> 01:59:10.570
hard to verify that

01:59:10.709 --> 01:59:12.950
the logical equivalent you got

01:59:12.950 --> 01:59:15.280
logically equivalent program to what you

01:59:15.340 --> 01:59:17.120
actually wanted. And,

01:59:17.980 --> 01:59:20.440
and even if, automated

01:59:20.580 --> 01:59:22.920
agent did say, oh, this is logically equivalent,

01:59:23.300 --> 01:59:25.099
it is very hard to

01:59:26.060 --> 01:59:28.139
convince the human that, yes,

01:59:28.140 --> 01:59:30.350
it is the agent is

01:59:30.350 --> 01:59:32.510
correct. And, so for

01:59:32.510 --> 01:59:34.640
example, if all this was happening

01:59:34.640 --> 01:59:36.799
before flash attention was invented and

01:59:36.800 --> 01:59:38.810
the agent just went and wrote

01:59:38.810 --> 01:59:40.240
a flash attention. Right.

01:59:40.880 --> 01:59:42.960
Then, it'd be very hard to convince the

01:59:42.960 --> 01:59:44.900
human that, yeah, you got

01:59:45.440 --> 01:59:47.480
something there. So how how do you deal with

01:59:48.440 --> 01:59:49.270
this problem of

01:59:50.640 --> 01:59:52.520
verifying highly

01:59:52.740 --> 01:59:54.840
performant kernels and convincing the programmer

01:59:54.900 --> 01:59:57.020
that yes, this is

01:59:57.020 --> 01:59:58.469
performant as well as

01:59:59.030 --> 02:00:01.149
functionally correct? Right. So so I think

02:00:01.149 --> 02:00:03.229
there are two challenges here. I think there are two

02:00:03.229 --> 02:00:05.290
challenges. So so

02:00:06.280 --> 02:00:08.639
the first part is verification of the output.

02:00:08.880 --> 02:00:11.219
I think that itself is is

02:00:11.700 --> 02:00:14.019
very tricky. For example, you could you could basically

02:00:14.019 --> 02:00:15.859
take a handful of test cases and make

02:00:16.100 --> 02:00:18.230
sure that your program is correct, then they may have

02:00:18.230 --> 02:00:19.700
a lot of corner cases.

02:00:20.240 --> 02:00:21.850
And I think that's really

02:00:23.350 --> 02:00:25.010
don't think it's a problem

02:00:25.570 --> 02:00:27.730
specific to AI, it's a problem

02:00:27.730 --> 02:00:29.810
specific to writing parallel

02:00:29.810 --> 02:00:32.270
programs. Parallel programming is very hard.

02:00:33.700 --> 02:00:36.160
And then I think AI can

02:00:36.220 --> 02:00:37.530
make make certain things easier

02:00:38.610 --> 02:00:40.639
and so one of the things, for example, if

02:00:40.640 --> 02:00:42.180
you if you write custom kernels

02:00:43.570 --> 02:00:45.870
even the experienced people at

02:00:45.880 --> 02:00:48.104
the right more and more layers by

02:00:48.104 --> 02:00:50.259
hand that's where things go

02:00:50.260 --> 02:00:52.499
wrong. As the complexity of your

02:00:52.499 --> 02:00:54.769
program let's say number of layers

02:00:54.769 --> 02:00:56.890
or or some other

02:00:56.890 --> 02:00:57.390
dimension,

02:00:59.360 --> 02:01:00.140
So I think there's

02:01:02.110 --> 02:01:03.960
that that's something that needs to be solved

02:01:04.700 --> 02:01:06.730
and AI, I don't think will solve it by itself.

02:01:11.910 --> 02:01:14.069
Alright. I think we run out of time. If you have

02:01:14.070 --> 02:01:16.090
more questions, you can talk to Veena

02:01:16.790 --> 02:01:18.820
after this. Let's thank you, the speaker

02:01:18.820 --> 02:01:19.320
again.

02:01:24.100 --> 02:01:26.259
So next we're gonna take a short break

02:01:26.260 --> 02:01:28.350
for ten minutes and then we gonna

02:01:28.350 --> 02:01:30.850
come back at 10:25 sharp

02:01:30.990 --> 02:01:33.210
to start the next invited

02:01:33.510 --> 02:01:35.530
speaker, and then spotlight.

02:01:37.080 --> 02:01:39.240
And for people and then there will

02:01:39.240 --> 02:01:41.260
be poster session after that, so if you have

02:01:41.260 --> 02:01:43.420
poster, please come to the front row

02:01:43.420 --> 02:01:45.479
to pick up the tape. So you can start

02:01:45.480 --> 02:01:47.580
putting up your poster on the wall.

02:01:48.120 --> 02:01:50.140
And, if you are

02:01:50.140 --> 02:01:52.300
the spotlight speaker, you can also come find

02:01:52.300 --> 02:01:54.640
us to test your computer and

02:01:55.675 --> 02:01:56.175
AV.

02:14:39.920 --> 02:14:42.160
Thank you, everyone, for joining. We

02:14:42.160 --> 02:14:44.340
can resume the next

02:14:44.340 --> 02:14:45.560
part of our workshop.

02:14:48.930 --> 02:14:50.079
You I'll give you a minute or two

02:14:51.480 --> 02:14:53.530
Yeah. Anyway, and that's not on. Does it something

02:14:53.530 --> 02:14:55.640
have to happen there? Do they need

02:14:55.640 --> 02:14:56.470
to switch on?

02:14:57.890 --> 02:14:59.110
Did it last time.

02:15:02.490 --> 02:15:04.370
Do we have to turn this on? Somehow?

02:15:06.540 --> 02:15:08.440
Okay. Okay.

02:15:14.710 --> 02:15:15.870
We just stay so that we

02:15:17.860 --> 02:15:18.560
So that, like,

02:15:23.670 --> 02:15:25.772
So just to recap, a little bit

02:15:25.773 --> 02:15:27.880
of the schedule here, thank you for putting up

02:15:27.880 --> 02:15:29.940
the posters. The poster

02:15:29.940 --> 02:15:31.460
session formally will begin,

02:15:33.040 --> 02:15:35.280
after the, ninja and spotlight

02:15:35.280 --> 02:15:35.780
talks.

02:15:37.340 --> 02:15:39.500
In about forty five minutes. If you

02:15:39.500 --> 02:15:41.720
have not put your post up, that's okay. We'll

02:15:41.720 --> 02:15:43.089
get an opportunity again.

02:15:44.530 --> 02:15:46.950
So just to get started on the workshop,

02:15:48.010 --> 02:15:50.020
again, it's

02:15:50.020 --> 02:15:52.360
my great pleasure to introduce Nirja

02:15:52.870 --> 02:15:54.950
She's an assistant professor in the department of

02:15:54.950 --> 02:15:57.280
ECE at UT Austin. Her

02:15:57.280 --> 02:15:59.280
works travel the boundaries of system and l

02:15:59.680 --> 02:16:01.909
ML specifically advances in

02:16:01.910 --> 02:16:04.410
systems, machine learning, and hardware architectures.

02:16:05.080 --> 02:16:07.480
To launch a new era of entire

02:16:07.480 --> 02:16:08.860
cloud as a computer.

02:16:09.860 --> 02:16:12.040
Bridging these complementary fields, our research

02:16:12.420 --> 02:16:14.509
focusing is on using and developing ML

02:16:14.510 --> 02:16:16.590
techniques for systems, building systems

02:16:16.590 --> 02:16:18.210
for ML. Welcome,

02:16:18.670 --> 02:16:19.170
Nirja.

02:16:24.130 --> 02:16:26.430
Alright. Thank you. That was a long introduction.

02:16:27.280 --> 02:16:29.430
So I would I would do a short one again,

02:16:29.430 --> 02:16:31.670
just reminding my name. Hey, everyone. My name is

02:16:31.670 --> 02:16:33.880
Nirajay Adwad. I'm an assistant professor

02:16:33.880 --> 02:16:35.679
at UT Austin in the department of

02:16:36.080 --> 02:16:38.319
ECE. I lead the UT system research

02:16:38.320 --> 02:16:40.470
group, and I am very, very

02:16:40.471 --> 02:16:42.410
thankful, to all the organizers

02:16:42.710 --> 02:16:44.390
particularly Mimi, who has been, you know,

02:16:45.029 --> 02:16:47.160
inviting me for three years and, you know,

02:16:47.320 --> 02:16:49.569
dealing with that patiently. I really appreciate

02:16:49.570 --> 02:16:51.649
that. Very happy to be here. Today,

02:16:52.480 --> 02:16:54.649
I will talk to you about some of the recent

02:16:54.650 --> 02:16:56.510
work that my group has been doing

02:16:56.930 --> 02:16:59.090
in the context of identifying and then

02:16:59.090 --> 02:17:00.840
addressing systemic implications.

02:17:01.480 --> 02:17:03.980
That are raised by these AI or ML for systems.

02:17:05.260 --> 02:17:07.100
I would like to begin with a little bit of,

02:17:07.420 --> 02:17:09.537
personal journey here. You

02:17:09.537 --> 02:17:11.670
know, that sort of can be described as from ML

02:17:11.670 --> 02:17:13.610
for systems, to AI everywhere.

02:17:13.930 --> 02:17:16.010
Right? In the early days, I

02:17:16.010 --> 02:17:18.050
don't know how many of you know this, I might be dating

02:17:18.050 --> 02:17:20.130
myself by saying that, but ML four systems

02:17:20.130 --> 02:17:22.219
actually was unconventional. Right?

02:17:22.220 --> 02:17:24.300
Because systems relied. They were deeply rooted

02:17:24.300 --> 02:17:26.150
in heuristics that were

02:17:26.780 --> 02:17:29.170
and by, you know, having experience

02:17:29.170 --> 02:17:31.250
and expertise that were sort of built in. They

02:17:31.250 --> 02:17:33.310
were very simple, so when I

02:17:33.310 --> 02:17:35.469
said as a as an early, you know, master's

02:17:35.470 --> 02:17:37.540
student, at India or

02:17:37.540 --> 02:17:39.720
early grad student at Berkeley, when I said machine learning

02:17:39.720 --> 02:17:41.980
actually can help systems, I got puzzled looks.

02:17:42.310 --> 02:17:44.470
Right? Raise eyebrows. But obviously, that

02:17:44.470 --> 02:17:46.630
has changed now. I don't to convince this crowd.

02:17:46.630 --> 02:17:48.540
Obviously, the talks and everything we have been

02:17:48.861 --> 02:17:50.481
we are hearing, this has changed

02:17:51.021 --> 02:17:53.110
drastically. Over the last decade. Right? And now

02:17:53.110 --> 02:17:55.520
LLMs different kinds of models, machine

02:17:55.579 --> 02:17:57.900
learning, AI, small, large, everything, actually

02:17:57.940 --> 02:17:59.709
has been driving system design.

02:18:00.670 --> 02:18:02.730
Right? Let me dive a little bit deeper

02:18:02.730 --> 02:18:04.890
with examples on what I mean by that. Right? We

02:18:04.890 --> 02:18:07.059
we are starting to see machine

02:18:07.060 --> 02:18:09.380
learning models, LLMs, small models, agents

02:18:09.380 --> 02:18:11.698
everywhere across the stack. The

02:18:11.699 --> 02:18:14.040
systems. Right? So, you know, one good example

02:18:14.180 --> 02:18:15.640
is to begin with, the core

02:18:16.310 --> 02:18:18.050
of OS, the kernel layer. Where,

02:18:18.510 --> 02:18:20.900
you know, you have sched p, for instance. That's

02:18:21.140 --> 02:18:23.300
sort of is very good at engaging in

02:18:23.300 --> 02:18:25.340
policy search, sort of generating

02:18:25.340 --> 02:18:27.819
new strategies, and safeguarding the

02:18:27.820 --> 02:18:30.140
outcomes that come. Right? That this was something that was

02:18:30.140 --> 02:18:32.160
core basically done by human

02:18:32.641 --> 02:18:34.719
human experts. Right? That is now done by,

02:18:34.800 --> 02:18:36.948
an agent. Going

02:18:36.949 --> 02:18:39.029
a little bit abstract, a little bit about

02:18:39.029 --> 02:18:41.060
data center which is, you know, data center

02:18:41.060 --> 02:18:43.109
as one computer. The

02:18:43.270 --> 02:18:45.770
SREs particularly, the site reliability engineers,

02:18:46.151 --> 02:18:48.197
they had to deal with keeping the systems

02:18:48.198 --> 02:18:50.640
actually up and running. So they were the glue.

02:18:50.800 --> 02:18:53.140
They would find out issues and so on and so forth. LLM

02:18:53.199 --> 02:18:55.404
for SRE, this agent actually now

02:18:55.404 --> 02:18:57.470
is at least this paper shows

02:18:57.470 --> 02:18:59.700
that it is capable of, parsing

02:18:59.700 --> 02:19:01.939
the logs automatically, identifying problems, doing

02:19:01.940 --> 02:19:04.179
root cause analysis, and and even

02:19:04.180 --> 02:19:06.420
going forward and giving fixes to that.

02:19:06.660 --> 02:19:08.820
Right? I don't think I need to talk a lot about this.

02:19:08.820 --> 02:19:10.830
There is the provocative paper that I'm sure

02:19:10.830 --> 02:19:12.868
Eon would be talking about. If not, we will ask

02:19:12.869 --> 02:19:15.269
him questions about it. Which basically

02:19:15.270 --> 02:19:17.289
threatens to append all systems research.

02:19:17.510 --> 02:19:19.750
Right? By by coming up various,

02:19:20.050 --> 02:19:21.430
various not just,

02:19:22.401 --> 02:19:24.349
you know, getting one solution to run, but

02:19:25.260 --> 02:19:27.550
identifying what solution can actually work.

02:19:28.601 --> 02:19:30.680
Again, this is something that's a moot point at this point,

02:19:30.681 --> 02:19:32.520
because there has been a full fledged talks about

02:19:32.740 --> 02:19:35.000
how to use these LLM agents

02:19:35.000 --> 02:19:37.080
for, writing code, for optimizing

02:19:37.080 --> 02:19:39.330
the return code, and so on and so forth. Lastly,

02:19:39.650 --> 02:19:41.720
you know how systems researchers assistance researchers,

02:19:41.720 --> 02:19:43.919
we think optimize is

02:19:43.919 --> 02:19:46.120
kind of the cornerstone? Anything you want to optimize,

02:19:46.120 --> 02:19:48.247
we write that as an objective function, and we you

02:19:48.247 --> 02:19:50.310
know, sort of optimize it, find

02:19:50.311 --> 02:19:52.651
out what optimizes that. Right? So,

02:19:53.191 --> 02:19:55.300
you know, it n p hard, and they

02:19:55.300 --> 02:19:57.620
complete, then we add in, constraints. We find,

02:19:57.620 --> 02:19:59.990
you know, heuristics. Relax

02:20:00.050 --> 02:20:02.210
those and find heuristics of blah, so on and so

02:20:02.210 --> 02:20:04.570
forth. Right? That actually,

02:20:04.570 --> 02:20:07.050
formally solving an optimization problem is

02:20:07.050 --> 02:20:08.900
all now becoming under the purview of

02:20:09.300 --> 02:20:11.480
LMM agents. Right? That this is a starting step.

02:20:11.800 --> 02:20:13.880
So my point is that LLM agents

02:20:13.880 --> 02:20:15.029
actually aren't just

02:20:17.391 --> 02:20:19.490
Right? They are actually participating

02:20:19.790 --> 02:20:21.669
inside the core systems

02:20:21.810 --> 02:20:23.981
layers driving the design. But my point here

02:20:23.981 --> 02:20:26.420
is, once the agents or the

02:20:26.640 --> 02:20:28.720
LLMs are permitting every layer, they don't

02:20:28.720 --> 02:20:31.199
actually just solve the problems. Right?

02:20:31.199 --> 02:20:33.561
They are now creating new ones. Okay?

02:20:34.055 --> 02:20:36.140
And in my group, we have been focusing

02:20:36.140 --> 02:20:38.000
about focusing on such problems. And

02:20:38.210 --> 02:20:40.270
today's talk, I thought I would focus on

02:20:40.270 --> 02:20:42.440
four such very basic

02:20:42.440 --> 02:20:44.539
systemic, problems that are interlocked,

02:20:45.079 --> 02:20:47.180
with each other. Okay? So,

02:20:47.180 --> 02:20:48.990
you know, first and foremost,

02:20:49.471 --> 02:20:51.630
now that we have LLMs everywhere in

02:20:51.630 --> 02:20:53.860
different layers, hopefully, that I can convince

02:20:53.860 --> 02:20:56.090
you of, we are not talking

02:20:56.090 --> 02:20:58.280
about deploying and configuring one LMM agent.

02:20:58.480 --> 02:21:00.601
Right? We're talking about deploying so

02:21:00.601 --> 02:21:03.000
many of them. And turns out that every

02:21:03.000 --> 02:21:05.390
LLM actually can be deployed in hundreds

02:21:05.390 --> 02:21:07.420
of different configurations. Change

02:21:07.420 --> 02:21:09.500
compression technique you use, change con, the

02:21:10.060 --> 02:21:12.380
strategies you use, and so on and so forth. Right?

02:21:12.380 --> 02:21:14.509
So your choice

02:21:14.510 --> 02:21:16.909
actually, has to be from all that hundreds of different

02:21:16.909 --> 02:21:18.971
configurations for all those LLMs, and your

02:21:18.971 --> 02:21:20.900
choice matters. It can decide

02:21:21.040 --> 02:21:23.200
whether your service is going to run eight x low

02:21:23.200 --> 02:21:25.230
or whether it is going to you 25 x

02:21:25.230 --> 02:21:27.440
more. So you care about it. LLM deployment

02:21:27.440 --> 02:21:28.980
today is bleeding money quietly.

02:21:30.500 --> 02:21:32.280
We know that all these accelerators,

02:21:33.221 --> 02:21:35.231
and GPUs and such, are

02:21:35.231 --> 02:21:37.300
very expensive. Right? So we are forced to

02:21:37.300 --> 02:21:39.340
share them. Wherever we can. Right?

02:21:39.340 --> 02:21:41.770
So we have been packing have been running multiple

02:21:41.910 --> 02:21:44.330
LLMs on the same GPU, which basically

02:21:44.790 --> 02:21:46.870
gets us to a multi tenant scenario. And

02:21:46.870 --> 02:21:49.289
as you would imagine, that gives rise to resource contention.

02:21:49.510 --> 02:21:51.610
Memory pressure, also compute concurrency,

02:21:52.500 --> 02:21:54.119
and and, and interference.

02:21:54.700 --> 02:21:56.880
Performance interference. Lastly,

02:21:57.420 --> 02:21:59.600
while we have focused so much on

02:21:59.820 --> 02:22:02.139
optimizing utilization, actually, turns out that

02:22:02.140 --> 02:22:04.221
we are taking you know, we're taking one step forward

02:22:04.221 --> 02:22:06.580
with utilization and two steps back. It

02:22:06.580 --> 02:22:08.790
comes to sustainability. Right, of the planet

02:22:08.790 --> 02:22:11.030
that we live on. So in this

02:22:11.030 --> 02:22:13.220
talk, I decided that I'll actually focus

02:22:13.280 --> 02:22:15.601
mainly, given the time, I'll focus mainly on

02:22:15.601 --> 02:22:17.680
the first two, that two in differing,

02:22:17.839 --> 02:22:20.239
lens. And I'll leave you with some

02:22:20.239 --> 02:22:22.270
links and you can feel free to

02:22:22.270 --> 02:22:24.410
check out our group's website. That's

02:22:24.410 --> 02:22:26.650
more updated. Mine is not.

02:22:27.030 --> 02:22:29.151
So, look at that because the know,

02:22:29.151 --> 02:22:31.549
the groups when it's maintained by students. So, obviously,

02:22:31.550 --> 02:22:32.420
that's very well maintained.

02:22:34.340 --> 02:22:36.580
Okay. So let's start with the first one, the

02:22:36.580 --> 02:22:38.449
deployment complexity. Like I said, LLM

02:22:38.750 --> 02:22:41.040
deployment today, the configuration

02:22:41.040 --> 02:22:43.060
choice and everything has been quietly played

02:22:44.100 --> 02:22:46.420
money. I told you that there can be hundreds of different

02:22:46.420 --> 02:22:48.760
configurations for a single LLM. Right?

02:22:48.760 --> 02:22:50.860
Changing, changing various things like

02:22:51.340 --> 02:22:53.119
compression or, parallelism techniques.

02:22:53.659 --> 02:22:55.749
The problem is the

02:22:55.749 --> 02:22:57.760
implication of a choice, of a configuration

02:22:57.760 --> 02:22:59.500
on level

02:22:59.976 --> 02:23:02.050
metrics. You know, the performance in terms of

02:23:02.050 --> 02:23:04.400
latency, throughput, or cost, memory used,

02:23:04.400 --> 02:23:06.640
and so on and so forth, is not trivial to figure

02:23:06.640 --> 02:23:08.430
out. That is hard, and that's

02:23:08.650 --> 02:23:10.691
why today's systems be it industry

02:23:10.691 --> 02:23:13.010
driven systems or, anything research

02:23:13.010 --> 02:23:15.070
wise also, I actually has relied on

02:23:15.070 --> 02:23:17.140
going with default choices. They either come up

02:23:17.140 --> 02:23:19.299
with and stick to the default choices,

02:23:19.300 --> 02:23:21.699
or they, let users

02:23:21.699 --> 02:23:23.850
figure out what they want? Both

02:23:23.850 --> 02:23:26.010
actually actually end up being suboptimal, either in

02:23:26.010 --> 02:23:27.310
time or cost.

02:23:28.249 --> 02:23:30.329
Perspectives. Right? So our goal

02:23:30.329 --> 02:23:32.660
here was then we we want to meet in

02:23:32.901 --> 02:23:34.981
of these different applications. You know, when LLMs

02:23:34.981 --> 02:23:37.010
are in different layers, they actually end up exerting

02:23:37.010 --> 02:23:39.250
different requirements from them. So our aim

02:23:39.250 --> 02:23:41.400
was to meet them, but do that automatically

02:23:41.400 --> 02:23:43.720
and also at a fraction of a cost of today's

02:23:43.720 --> 02:23:45.919
operators. While doing so. So

02:23:45.919 --> 02:23:48.079
that was our goal. Right? And we believe that

02:23:48.079 --> 02:23:50.460
there are two main things that need to come together

02:23:50.460 --> 02:23:52.490
to actually meet that goal. First,

02:23:52.570 --> 02:23:54.681
we need to be cost efficient.

02:23:54.990 --> 02:23:57.390
When we are trying to figure out the relationship between

02:23:57.390 --> 02:23:59.360
deployment configuration and the performance

02:23:59.874 --> 02:24:01.909
metrics. That we care about. Right?

02:24:01.909 --> 02:24:03.660
And that has to be accurate also.

02:24:04.220 --> 02:24:06.080
Going forward, once we choose a deployment

02:24:06.540 --> 02:24:08.730
configuration that is accurately, you know, profiled

02:24:08.730 --> 02:24:10.500
and everything, that meets the intent

02:24:10.820 --> 02:24:12.981
you have to actually automate all the deployment.

02:24:12.981 --> 02:24:14.989
Because if you don't, you don't

02:24:14.989 --> 02:24:17.150
actually end up getting and seeing the improvements that you imagined.

02:24:17.150 --> 02:24:19.220
And I'll show you how. So let's begin

02:24:19.220 --> 02:24:21.540
with the first one. How do we actually achieve cost

02:24:21.540 --> 02:24:23.590
efficiency while being accurate in

02:24:23.650 --> 02:24:25.250
profiling so that I can predict the,

02:24:26.370 --> 02:24:28.651
performance of a deployment configuration. What

02:24:28.651 --> 02:24:31.029
makes this expensive? We could, you know, for accuracy,

02:24:31.029 --> 02:24:33.260
we could just brute force. Right? We could

02:24:33.260 --> 02:24:35.120
just deploy an LLM

02:24:35.481 --> 02:24:37.641
across all different configurations, which is ridiculous. Right?

02:24:37.641 --> 02:24:39.910
It would be trivial you know,

02:24:39.970 --> 02:24:42.023
accurate, most accurate. It is expensive. Nobody's going

02:24:42.023 --> 02:24:44.050
to do that. Right? What

02:24:44.050 --> 02:24:46.090
makes this expensive, this whole process

02:24:46.250 --> 02:24:48.330
the two factors. Right? And those we attack

02:24:48.330 --> 02:24:50.590
individually to bring the cost down significantly.

02:24:51.280 --> 02:24:53.361
The first thing is that LLMs by themselves

02:24:53.361 --> 02:24:55.050
are really, really giant heavy.

02:24:55.610 --> 02:24:57.230
Right? So if you deployed that,

02:24:57.810 --> 02:24:59.970
it's just gonna cost efficiency, cost is

02:24:59.970 --> 02:25:02.210
out of the window. So we refuse. Deploy

02:25:02.210 --> 02:25:04.400
the whole LLM. What can we do? And

02:25:04.460 --> 02:25:06.560
secondly, we have to understand the implication

02:25:06.779 --> 02:25:08.641
of every possible deployment configuration.

02:25:08.780 --> 02:25:10.861
So it's not just the one giant LLM that is

02:25:10.861 --> 02:25:12.990
a problem all these configurations. Right? So

02:25:12.990 --> 02:25:15.029
let's take one at a time. One problem at

02:25:15.029 --> 02:25:17.249
a time. I refuse to use the entire

02:25:17.570 --> 02:25:19.620
LLM. Right? So we were like, okay. What do we do?

02:25:19.620 --> 02:25:21.861
If we wanted to avoid that? We were studying

02:25:21.861 --> 02:25:23.761
the LLM architecture then. Right?

02:25:24.120 --> 02:25:26.441
All all our work is kind of trying to see how

02:25:26.441 --> 02:25:28.460
both these systems and and machine learning kind

02:25:28.460 --> 02:25:30.600
of come together very well. So we were studying this.

02:25:31.010 --> 02:25:33.010
And you see how the inputs, you know, get

02:25:33.170 --> 02:25:35.240
you get the encodings, then sort of you do a

02:25:35.481 --> 02:25:37.721
lot of there are these hidden layers that compute your

02:25:37.721 --> 02:25:39.640
attention and so on and so forth. Right?

02:25:39.960 --> 02:25:42.199
We noticed that every model, actually, that we were studying,

02:25:42.199 --> 02:25:44.619
particularly the transform base models,

02:25:44.760 --> 02:25:46.539
had these hidden layers which were

02:25:47.289 --> 02:25:49.789
multiple of such hidden layers. And we we basically

02:25:49.930 --> 02:25:52.050
thought that if we actually reduce that redundancy,

02:25:52.730 --> 02:25:54.860
replacing that whole

02:25:54.860 --> 02:25:57.080
block of hidden layers with just one hidden layer,

02:25:57.080 --> 02:25:59.111
Right? Then created a version,

02:25:59.111 --> 02:26:01.060
a compact proxy of that model,

02:26:01.220 --> 02:26:03.079
that was just 2% of the entire

02:26:03.300 --> 02:26:05.510
model. Almost 2%. Then the cost of

02:26:05.510 --> 02:26:07.369
actually having to deploy this model

02:26:07.580 --> 02:26:09.520
for profiling reduces significantly.

02:26:10.260 --> 02:26:12.511
Right? And that's what our first insight comes

02:26:12.511 --> 02:26:14.330
from. We also verified that

02:26:14.731 --> 02:26:16.890
to be able to actually use this insight, we verified

02:26:16.890 --> 02:26:18.350
that there is a linear relationship.

02:26:19.079 --> 02:26:21.239
Between one hidden hidden layer and multiple hidden

02:26:21.239 --> 02:26:22.950
layers. Right? And this was kind of

02:26:23.271 --> 02:26:25.510
made it easy for us to come up with what we call LLM

02:26:25.510 --> 02:26:26.811
fingerprint. And I hope,

02:26:27.790 --> 02:26:29.950
there there are a lot many more details in the paper that

02:26:29.950 --> 02:26:32.030
was just presented last month

02:26:32.030 --> 02:26:34.010
at SC this year. But

02:26:34.290 --> 02:26:36.450
that was our keen side. This compact proxy

02:26:36.450 --> 02:26:38.630
that we call LMM Finger actually

02:26:39.090 --> 02:26:41.180
gets us closer to the, to the aim of cost

02:26:41.180 --> 02:26:43.180
efficient profile while being accurate.

02:26:43.720 --> 02:26:45.960
Now, the the second thing there was, even if

02:26:45.960 --> 02:26:47.989
we had this LLM fingerprint, you

02:26:47.989 --> 02:26:50.010
still required to understand different configurations.

02:26:50.240 --> 02:26:52.210
Using those. Still too many,

02:26:52.449 --> 02:26:54.529
configuration options and still too much cost

02:26:54.529 --> 02:26:56.560
there. Again, we refuse

02:26:56.560 --> 02:26:58.699
to sort of have to do that. Then one thing

02:26:58.699 --> 02:27:00.860
that comes to mind is, wouldn't it be nice to

02:27:00.860 --> 02:27:02.940
have an understanding that sort

02:27:02.940 --> 02:27:05.180
of takes us from few measurements to

02:27:05.180 --> 02:27:07.340
entire. Space of complete, the

02:27:07.900 --> 02:27:10.059
deployment configurations. So then, again, we

02:27:10.060 --> 02:27:12.119
sat down and tried to understand where where are

02:27:12.119 --> 02:27:14.360
these metrics, spending, you

02:27:14.360 --> 02:27:16.300
know, their time and effort in.

02:27:16.540 --> 02:27:18.780
What is latency, built off? Right? And then

02:27:18.780 --> 02:27:21.020
we understood that from the model's

02:27:21.020 --> 02:27:23.129
architecture. And translated that

02:27:23.129 --> 02:27:25.200
to an analytical model. Of our

02:27:25.200 --> 02:27:27.490
understanding. Okay? With that, our

02:27:27.490 --> 02:27:29.810
hope was that we would just measure using

02:27:30.369 --> 02:27:32.630
a couple of our three, four kind of measurements

02:27:32.690 --> 02:27:35.000
from using the LLM fingerprints

02:27:35.000 --> 02:27:37.090
and predict using the analytical models.

02:27:37.170 --> 02:27:39.240
So we would have eschewed the cost

02:27:39.240 --> 02:27:41.610
behind all of this that existing systems have.

02:27:41.850 --> 02:27:44.010
So here's what we did. I don't have the time, so I'm

02:27:44.010 --> 02:27:46.190
just gonna give you, the idea

02:27:46.300 --> 02:27:48.540
how we built these models, but, and hopefully,

02:27:48.540 --> 02:27:50.630
this is good enough plug

02:27:50.630 --> 02:27:52.710
for you to go see our paper. So let's

02:27:52.710 --> 02:27:54.949
say this is an LLM. A bit

02:27:54.949 --> 02:27:57.029
of notation not too much. DTP and

02:27:57.029 --> 02:27:59.250
DPP respectively define or

02:27:59.250 --> 02:28:01.489
show the degree of parallelism, different types of

02:28:01.489 --> 02:28:03.550
parallelism, tensor and parallel. So if you

02:28:03.550 --> 02:28:05.950
are fortunate enough to have a model and big enough

02:28:05.950 --> 02:28:08.279
GPU, they can actually work together

02:28:08.279 --> 02:28:10.360
without parallelism, that's what you do. You take the

02:28:10.360 --> 02:28:12.460
model, you completely deploy it on

02:28:12.460 --> 02:28:14.710
that one GPU that you had fortunately, and you

02:28:14.710 --> 02:28:16.230
measure the, latency.

02:28:16.740 --> 02:28:18.500
Of those. Right? Let's say that latency was x.

02:28:19.300 --> 02:28:21.250
Now let's say I I I don't have that

02:28:21.310 --> 02:28:23.580
fortunate situation, and I have to distribute that model.

02:28:23.901 --> 02:28:26.240
Let's say I do a tensor parallelism. I distribute

02:28:26.540 --> 02:28:28.160
the model across these GPUs.

02:28:28.710 --> 02:28:30.830
By the very fact that now every GPU has to do

02:28:31.231 --> 02:28:33.540
much lesser amount of work, the latency

02:28:33.540 --> 02:28:35.151
gets reduced. Proportionately.

02:28:35.630 --> 02:28:37.790
The degree of tensor parallelism. But then

02:28:37.790 --> 02:28:39.490
we have to do this all reduce operation.

02:28:39.811 --> 02:28:41.970
Introduces some overhead. That we capture

02:28:41.970 --> 02:28:44.320
using you know, I'm grossly simplifying the formulation

02:28:44.320 --> 02:28:46.640
we had, but, we captured that

02:28:46.640 --> 02:28:48.960
into, this other term, overhead

02:28:48.960 --> 02:28:51.120
of tipi, and lot of constants

02:28:51.120 --> 02:28:53.249
are kind of in b. Let's

02:28:53.249 --> 02:28:55.340
say we wanted to we needed to divide the

02:28:55.340 --> 02:28:57.500
model further. By using a combination

02:28:57.561 --> 02:28:59.570
of, you know, tensor and parallel and

02:28:59.570 --> 02:29:01.650
pipeline parallelism. That would basically

02:29:01.650 --> 02:29:03.941
reduce the work of every GPU once again,

02:29:03.941 --> 02:29:05.560
and so you will see the difference

02:29:06.100 --> 02:29:08.320
in, the latency. Once again, we are dividing

02:29:08.320 --> 02:29:10.670
by proportionately dividing Proportional

02:29:11.050 --> 02:29:13.550
value of that latency comes from that division

02:29:13.690 --> 02:29:15.870
of the original latency with the

02:29:16.380 --> 02:29:18.489
PP. Then there is an overhead added because

02:29:18.489 --> 02:29:20.633
these GPUs have to communicate to achieve that

02:29:20.633 --> 02:29:22.960
task. That ends up being an overhead

02:29:23.361 --> 02:29:25.280
term yet again there. And lastly, you

02:29:25.441 --> 02:29:27.920
because this is a combination of 10 and pipeline

02:29:27.980 --> 02:29:30.230
parallelism, there is going to

02:29:30.230 --> 02:29:32.390
be an all reduce, which, you know,

02:29:32.390 --> 02:29:34.740
again, for simplification is

02:29:34.800 --> 02:29:36.419
kind of represented in that one,

02:29:37.220 --> 02:29:39.490
constant there. So let's focus on this. Right?

02:29:39.651 --> 02:29:41.891
If I asked you to focus on this,

02:29:41.891 --> 02:29:43.990
and you would note that there are three unknowns.

02:29:44.310 --> 02:29:46.329
The latency x, the overhead

02:29:46.390 --> 02:29:48.659
of parallelism, overhead of pipeline parallelism.

02:29:49.280 --> 02:29:51.500
So we have a linear equation in these three unknowns.

02:29:51.500 --> 02:29:53.709
Let's invoke our high school math. Very

02:29:53.709 --> 02:29:55.789
simple. Right? Three unknowns, we need three

02:29:55.789 --> 02:29:58.000
equations. So basically, we need measurements

02:29:58.620 --> 02:30:00.400
using fingerprints just three measurements.

02:30:00.640 --> 02:30:02.830
Plug it in, solve, and you have

02:30:02.830 --> 02:30:04.910
these values of these unknowns. Now, obviously,

02:30:04.910 --> 02:30:06.709
in reality, there are many more,

02:30:07.029 --> 02:30:09.350
Greek symbols in our formulation that I have completely

02:30:09.350 --> 02:30:11.510
glossed over. But please take a look at that.

02:30:11.750 --> 02:30:13.610
This actually significantly improved,

02:30:14.070 --> 02:30:16.230
you know, we we had to use fingerprint, which

02:30:16.231 --> 02:30:18.090
was two, 3% of the whole model.

02:30:18.460 --> 02:30:20.720
Just a couple two to three actually, three, literally.

02:30:21.180 --> 02:30:23.260
Runs with those, and we knew everything we

02:30:23.260 --> 02:30:25.600
had to know. We wanted to know about all configurations

02:30:25.790 --> 02:30:27.820
and their implications. On user intent or

02:30:27.820 --> 02:30:29.910
metrics that applications would care

02:30:29.910 --> 02:30:31.990
about. Right? Similarly, there is, you know, how do

02:30:31.990 --> 02:30:34.010
we predict memory? And similar analysis.

02:30:34.151 --> 02:30:36.489
Where does the memory footprint come from

02:30:36.629 --> 02:30:38.650
for models? Have the time to go in, but,

02:30:38.651 --> 02:30:40.890
you know, I would really, point you to our

02:30:40.890 --> 02:30:42.830
people for that. So with that, hopefully,

02:30:43.680 --> 02:30:46.000
I have shown you how to achieve accurate and cost

02:30:46.000 --> 02:30:48.000
efficient profiling for

02:30:48.060 --> 02:30:49.860
LMS. To choose to go from all these

02:30:50.180 --> 02:30:52.230
deployment configurations to

02:30:53.083 --> 02:30:55.269
you know, choice of that configuration that so that you

02:30:55.270 --> 02:30:57.330
can meet user intent or

02:30:57.330 --> 02:30:59.830
application level requirements in terms of latency, memory,

02:31:00.470 --> 02:31:02.790
footprint, number of GPUs that need overall cost in other

02:31:02.790 --> 02:31:04.580
terms. But we are not done.

02:31:04.960 --> 02:31:06.420
Right? And I'll tell you why.

02:31:07.330 --> 02:31:09.570
Even if you had the configuration of choice

02:31:09.570 --> 02:31:11.830
that meets, unless you did close

02:31:11.830 --> 02:31:14.170
the loop, by looking at what available resources

02:31:14.170 --> 02:31:16.570
you have in your GPU cluster, you don't see the benefits.

02:31:16.730 --> 02:31:18.810
You do not see the cost going down. You

02:31:18.810 --> 02:31:20.949
see a lot of GPUs being added. So the

02:31:20.949 --> 02:31:23.029
first thing is you need to avoid idling, and there is

02:31:23.029 --> 02:31:25.159
a lot of work, so we borrowed load of waste

02:31:25.476 --> 02:31:27.490
strategies. Like you know, if you are from

02:31:27.490 --> 02:31:29.380
systems world, you know that you should you must pack

02:31:29.620 --> 02:31:31.250
at low load and distribute your work.

02:31:31.731 --> 02:31:33.779
At high load. We did that. Right? But

02:31:33.779 --> 02:31:35.860
even then, the number of GPUs we

02:31:35.860 --> 02:31:38.199
expected it to be to

02:31:38.199 --> 02:31:40.040
be required too much. Right?

02:31:40.760 --> 02:31:42.700
Definitely above what we had expected.

02:31:42.980 --> 02:31:45.060
And we realized that here is what was going on.

02:31:45.060 --> 02:31:47.220
Again, very small, very simple

02:31:47.220 --> 02:31:48.440
insight in here. When

02:31:49.540 --> 02:31:51.861
we chose a configuration that, let's say, for example,

02:31:51.861 --> 02:31:54.270
said pipeline parallelism degree of two, We

02:31:54.409 --> 02:31:56.430
divided the LLM into equal chunks.

02:31:56.670 --> 02:31:58.750
Right? And what happens then is

02:31:58.750 --> 02:32:00.800
if you have a class cluster that does not have

02:32:00.800 --> 02:32:02.641
two such GPUs that can accommodate,

02:32:03.160 --> 02:32:05.239
those equal chunks of the model, you had

02:32:05.239 --> 02:32:07.446
to add a new GPU. So we decided that,

02:32:07.446 --> 02:32:09.170
actually, this conventional wisdom, we

02:32:09.570 --> 02:32:11.729
let go of that. We are gonna break free from that. And we

02:32:11.730 --> 02:32:14.160
are gonna divide the LLM

02:32:14.726 --> 02:32:16.600
and an unbalanced way.

02:32:16.840 --> 02:32:19.005
Okay? And if I do that, knowing

02:32:19.005 --> 02:32:21.330
what availability of resources I have,

02:32:21.330 --> 02:32:22.580
now I'm able to find

02:32:23.620 --> 02:32:25.600
fragmented resources in the same cluster I

02:32:25.840 --> 02:32:27.779
without having to add a new GPU. So,

02:32:29.050 --> 02:32:31.210
this actually, the simple thing, just letting go

02:32:31.210 --> 02:32:33.539
of having to have a even

02:32:33.539 --> 02:32:35.850
chunks of model, actually goes a long way.

02:32:36.010 --> 02:32:38.090
Then you're gonna say, you know what, Neeraj? We knew

02:32:38.090 --> 02:32:40.540
that, but there is going to be. A latency

02:32:40.920 --> 02:32:43.047
impact. That's why we included that's why

02:32:43.047 --> 02:32:44.846
we said that you have to be,

02:32:45.103 --> 02:32:47.300
balanced Right? From, what about the pipeline

02:32:47.520 --> 02:32:49.361
bubbles that you introduced? What about those? Right?

02:32:49.720 --> 02:32:52.140
So let me tell you that actually and again,

02:32:52.670 --> 02:32:54.990
don't have enough time, but, see, in training,

02:32:54.990 --> 02:32:57.170
you have forward as well as backward pass. And backward

02:32:57.230 --> 02:32:59.500
pass, is much more work, not just equal,

02:32:59.500 --> 02:33:01.670
but much more work than the forward pass. In inference,

02:33:01.670 --> 02:33:03.779
a you just have forward pass. And

02:33:03.779 --> 02:33:05.600
so even though there are those bubbles get

02:33:06.080 --> 02:33:08.400
introduced, the end to end latency impact is actually

02:33:08.400 --> 02:33:10.540
negligible. Enough that you

02:33:10.540 --> 02:33:12.520
can actually utilize this very simple

02:33:12.720 --> 02:33:14.550
idea of you know, intentionally

02:33:14.930 --> 02:33:16.709
splitting the model in an

02:33:17.379 --> 02:33:19.529
imbalanced pipeline manner. Okay? So

02:33:19.529 --> 02:33:21.690
we built a system, you know, that sort

02:33:21.690 --> 02:33:23.850
of has these three personas. The we get the

02:33:23.850 --> 02:33:25.851
model from, model you

02:33:25.851 --> 02:33:27.930
know, providers. We we have

02:33:27.930 --> 02:33:30.010
this fingerprint controller as part of our system

02:33:30.010 --> 02:33:32.390
called Maverick. That generates

02:33:32.390 --> 02:33:34.790
the fingerprint profiles and estimates and

02:33:34.850 --> 02:33:37.010
creates this map of, you know, for that LLM,

02:33:37.010 --> 02:33:39.096
for a given configuration, what would be

02:33:39.096 --> 02:33:41.420
the impact on different metrics that we care about.

02:33:41.560 --> 02:33:42.390
The deployment control

02:33:44.420 --> 02:33:45.960
application developer, their

02:33:47.080 --> 02:33:49.029
intent, and makes deployment

02:33:49.090 --> 02:33:50.811
decisions looking at you know,

02:33:51.111 --> 02:33:53.271
the available cluster going ahead with our

02:33:53.271 --> 02:33:55.520
load aware and fragmentation aware

02:33:55.520 --> 02:33:57.680
deployment strategies. And that is then served in

02:33:57.680 --> 02:33:59.846
our scheduler. This was first and first served, but,

02:33:59.846 --> 02:34:01.910
you know, this opens up, like, our code is very

02:34:01.910 --> 02:34:03.890
modular. You can have more

02:34:03.949 --> 02:34:06.159
sophisticated scheduling strategies. You can plug

02:34:06.159 --> 02:34:08.210
in. I

02:34:08.210 --> 02:34:10.470
I don't want to go over this, but you

02:34:10.470 --> 02:34:12.409
know, please take a look at this paper.

02:34:12.869 --> 02:34:15.000
We do deliver on promise. We do meet various

02:34:15.000 --> 02:34:17.200
intents, and we do it at fraction of

02:34:17.200 --> 02:34:19.110
a cost that existing systems had.

02:34:19.270 --> 02:34:21.349
Like, promised. Right? I want to really

02:34:21.350 --> 02:34:22.970
go over, you know,

02:34:23.390 --> 02:34:25.550
the the second problem here. Right? I

02:34:25.550 --> 02:34:27.880
I told you that when we are forcing ourselves,

02:34:27.880 --> 02:34:30.040
these all these different LLMs, they're going getting

02:34:30.040 --> 02:34:32.289
deployed, and we have them sharing the underlying

02:34:32.590 --> 02:34:35.030
resources. Ends up creating resource contention

02:34:35.030 --> 02:34:37.530
in terms of memory, and the third point here is compute.

02:34:37.941 --> 02:34:39.720
Right? I won't actually

02:34:40.151 --> 02:34:42.340
be going into that It's

02:34:42.580 --> 02:34:43.830
eighteen minutes right now.

02:34:48.220 --> 02:34:48.720
Okay.

02:34:50.930 --> 02:34:51.430
Okay.

02:34:53.300 --> 02:34:54.440
Right? Okay. Thank you.

02:34:56.130 --> 02:34:58.191
Terms of resource contention over

02:34:58.191 --> 02:35:00.290
memory. Right? So where does

02:35:00.290 --> 02:35:02.370
the memory footprint actually of an LLM comes

02:35:02.370 --> 02:35:04.430
from? Right? It not just

02:35:04.430 --> 02:35:06.770
parameters. It's the working memory, working size,

02:35:06.900 --> 02:35:09.230
that we call context or kvCash.

02:35:09.230 --> 02:35:11.390
Actually, it's very, very important Every time

02:35:11.390 --> 02:35:13.810
you turn it a new token, you need to know what what has happened,

02:35:14.029 --> 02:35:16.210
and you have to have that. And that up being a sizable

02:35:16.210 --> 02:35:18.470
chunk of memory. Right? And

02:35:18.470 --> 02:35:20.630
so you see that impact, actually. If you see

02:35:20.630 --> 02:35:22.518
as request size or the in

02:35:22.694 --> 02:35:24.869
the workload keeps growing, your

02:35:24.869 --> 02:35:27.080
your kvCatch available kvCash sort

02:35:27.080 --> 02:35:29.170
of is not sufficient, and that shows

02:35:29.170 --> 02:35:31.600
up in increased latency. Right? Significant

02:35:31.901 --> 02:35:33.760
impact on the latency because of kvCash,

02:35:33.980 --> 02:35:35.440
and there is so much work

02:35:36.239 --> 02:35:38.340
from systems community on optimizing kvCash.

02:35:39.039 --> 02:35:41.360
One such thing that we have done. So basically,

02:35:41.360 --> 02:35:43.440
what we are saying is kvCash matters. Give me more

02:35:43.440 --> 02:35:45.650
of it. Right? You don't have it. That's

02:35:45.650 --> 02:35:47.890
decided by the of the world, whatever GPU,

02:35:47.890 --> 02:35:50.030
HBM that you have. For instance. Right?

02:35:50.271 --> 02:35:52.130
So what we can do is we can play around

02:35:52.530 --> 02:35:54.570
and move stuff around. And we know

02:35:54.570 --> 02:35:57.070
how to do that as systems, developers or architects.

02:35:57.210 --> 02:35:59.530
So I'm gonna, like, you know, what what literature

02:35:59.530 --> 02:36:01.460
has suggested that, let's take some part of

02:36:01.861 --> 02:36:03.780
kvCash. Let's use the CPU memory

02:36:04.101 --> 02:36:06.410
DRAM, as an extension of that. Effectively

02:36:06.630 --> 02:36:08.750
expanding the k v cache. Right? And you

02:36:08.750 --> 02:36:11.170
would say there is going to be overhead of transferring

02:36:11.311 --> 02:36:13.130
this kvCash back and forth between

02:36:13.351 --> 02:36:15.510
CPU and GPU memory. People do tricks

02:36:15.510 --> 02:36:17.517
like you overlap the trans the transfer

02:36:17.517 --> 02:36:19.790
time with computation time. Right? And that actually

02:36:19.790 --> 02:36:21.250
helps in improving latency.

02:36:21.811 --> 02:36:23.890
You know, huge improvement. But then when we were looking

02:36:23.890 --> 02:36:25.984
at this, we realized another thing, that this was

02:36:25.984 --> 02:36:28.060
actually good, but it was doing a lot of

02:36:28.380 --> 02:36:30.150
extra work that really was not needed.

02:36:32.540 --> 02:36:34.210
So why don't you use peer to peer memory

02:36:35.410 --> 02:36:37.529
can I take the question, you know, little bit? Let

02:36:37.529 --> 02:36:38.500
me finish this.

02:36:39.941 --> 02:36:42.210
Right. So we realized that there is an

02:36:42.770 --> 02:36:45.030
extra work overhead that was added. Why? Because

02:36:45.170 --> 02:36:47.279
you know that the you know,

02:36:47.279 --> 02:36:49.759
the cache that is part of it that was on CPU

02:36:49.760 --> 02:36:51.820
would basically, for every token, you would have

02:36:51.820 --> 02:36:53.919
synchronization. Right? So this second

02:36:53.919 --> 02:36:56.240
arrow that I just added there was extra.

02:36:56.481 --> 02:36:58.960
Was not required. That simple insight, that simple

02:36:58.960 --> 02:37:01.050
observation, basically, then pushed us

02:37:01.050 --> 02:37:03.170
to think about what else we can do. And we realized

02:37:03.170 --> 02:37:05.420
that if you see the GPU memory

02:37:05.901 --> 02:37:08.220
is taken by the parameters and kvCash. Parameters

02:37:08.220 --> 02:37:10.230
are static. They are not changing. So

02:37:10.230 --> 02:37:12.470
if we actually made our system work with

02:37:12.470 --> 02:37:14.720
just that, you know, basically

02:37:14.720 --> 02:37:16.739
take a few a few parameters, repurpose

02:37:16.800 --> 02:37:19.050
that memory as kvCash, You

02:37:19.050 --> 02:37:21.210
don't need to copy that because you can maintain a copy on

02:37:21.210 --> 02:37:23.330
CPU memory. Right? And you got the

02:37:23.330 --> 02:37:25.490
exact same result. Effective kvCash was increased

02:37:25.490 --> 02:37:26.870
without that burden of synchronizing.

02:37:27.530 --> 02:37:29.690
Right? And just with that,

02:37:30.090 --> 02:37:32.441
obviously, this is not as easy as I'm making

02:37:32.441 --> 02:37:34.760
it sound. We developed ONEEROS, which basically

02:37:34.760 --> 02:37:36.930
stands for a dream. Which sort of

02:37:37.170 --> 02:37:38.790
you know, expands kvCASH,

02:37:39.765 --> 02:37:41.980
depending on, you know, models

02:37:41.980 --> 02:37:44.060
we have available and so on and so forth. So

02:37:44.060 --> 02:37:46.220
our crux of the system,

02:37:46.220 --> 02:37:48.380
basically, is this remapping engine that has

02:37:48.380 --> 02:37:50.691
to answer these questions When to trigger this remapping,

02:37:50.691 --> 02:37:52.930
what models, how many layers, and which of

02:37:52.930 --> 02:37:55.029
those layers? It matters. And we utilize

02:37:55.090 --> 02:37:57.030
that these LLMs have different layers

02:37:57.191 --> 02:37:59.511
and they go back for every token. Right? So we utilize

02:37:59.511 --> 02:38:01.340
that pattern to decide which layers show

02:38:01.580 --> 02:38:03.660
be thrown out without actually seeing the impact on

02:38:03.660 --> 02:38:05.900
latency. We do really

02:38:05.900 --> 02:38:08.220
well compared to, state of the art baselines

02:38:08.220 --> 02:38:10.340
like VLLM. We improve

02:38:10.340 --> 02:38:12.660
not just, latency, but also throughput.

02:38:12.901 --> 02:38:14.981
Because we are just by not doing the extra

02:38:14.981 --> 02:38:17.199
amount of work. Whole insight about this work was

02:38:17.199 --> 02:38:19.470
identifying the key inefficiency that we were

02:38:19.690 --> 02:38:21.981
unnecessarily doing some work. Alright.

02:38:22.120 --> 02:38:23.820
So because we are

02:38:24.570 --> 02:38:26.651
running out of time, and I promise you that I will only

02:38:26.651 --> 02:38:27.850
talk about these two,

02:38:28.880 --> 02:38:31.010
I will leave you with you know, these two

02:38:31.010 --> 02:38:33.170
links that I'm not gonna talk about right now. But,

02:38:33.990 --> 02:38:36.010
in compute concurrency sense, if

02:38:36.010 --> 02:38:38.090
you think about what is actually placed

02:38:38.090 --> 02:38:40.580
together on one GPU,

02:38:41.040 --> 02:38:43.420
the kind of compute sharing

02:38:43.420 --> 02:38:45.680
that you do depends on the concurrency mechanism.

02:38:45.690 --> 02:38:48.010
Right? You can time multiplex, you can special multiplex.

02:38:48.010 --> 02:38:50.290
In special multiplexing, you have reservation

02:38:50.430 --> 02:38:52.340
based or packing based, and everything ends up

02:38:52.580 --> 02:38:54.820
depending on what models are sharing it. What

02:38:54.820 --> 02:38:57.239
load you have. And so that requires automated

02:38:57.779 --> 02:38:59.971
mechanisms to improve utilization. And and so

02:38:59.972 --> 02:39:02.060
on and so forth. Lastly, like I said,

02:39:02.619 --> 02:39:04.779
this whole focus on improving utilization is

02:39:04.779 --> 02:39:06.910
taking us two steps back in terms

02:39:06.910 --> 02:39:09.080
of sustainability metrics. We realize that

02:39:09.080 --> 02:39:11.401
won't make the same decisions if you care about power,

02:39:11.401 --> 02:39:13.900
if you care about carbon or care about energy.

02:39:14.040 --> 02:39:16.060
So power or such

02:39:16.060 --> 02:39:18.380
sustainability metrics need to be, promoted

02:39:18.380 --> 02:39:20.440
to the first class citizen And we need to solve

02:39:20.440 --> 02:39:22.840
those optimization problems, in harmony

02:39:22.840 --> 02:39:24.999
with performance compatibility. Of just

02:39:24.999 --> 02:39:26.459
focusing on performance compatibility.

02:39:27.000 --> 02:39:29.180
Okay. I'll stop here. For that.

02:39:41.780 --> 02:39:43.840
I think we have time for one

02:39:43.840 --> 02:39:46.200
question. From the audience.

02:39:53.521 --> 02:39:55.220
So you mentioned, you're using

02:39:55.725 --> 02:39:58.220
kvC cache and you're using system DRAM.

02:39:59.790 --> 02:40:02.170
You're using system DRAM There's a

02:40:02.170 --> 02:40:03.640
limitation on system DRAM. It's

02:40:04.760 --> 02:40:06.770
So Why you're not using

02:40:07.379 --> 02:40:09.420
peer to peer Drake DMA transfer

02:40:09.480 --> 02:40:10.459
from GPU

02:40:11.670 --> 02:40:13.611
to PCI devices, potentially CXL

02:40:13.750 --> 02:40:15.959
devices. Which which will

02:40:15.959 --> 02:40:17.970
give you a way

02:40:17.970 --> 02:40:18.710
more scalability

02:40:21.010 --> 02:40:22.550
and way more IOPS and performance.

02:40:23.940 --> 02:40:26.260
The the basic point I was trying to make was

02:40:26.260 --> 02:40:28.280
not that. Yes. We can we can go ahead and

02:40:28.280 --> 02:40:30.420
actually literally do GPU direct or these and we

02:40:30.420 --> 02:40:32.500
link between GPUs. We can share,

02:40:33.440 --> 02:40:34.920
we can we can do all of that. And

02:40:35.480 --> 02:40:37.640
going to CPU for kvCash expansion was

02:40:37.640 --> 02:40:40.120
not something we did, actually. That is what literature

02:40:40.120 --> 02:40:42.140
does. Are actually not doing that at all.

02:40:42.380 --> 02:40:44.440
We are basically letting go of

02:40:44.440 --> 02:40:46.510
parameters. We're deleting them, repurposing

02:40:46.510 --> 02:40:47.990
that part of HBM,

02:40:48.550 --> 02:40:51.000
as kvCash memory. That's all actually we did. Right?

02:40:51.380 --> 02:40:53.540
But but I agree with you. There are so many

02:40:53.540 --> 02:40:55.670
different ways another configuration navigation problem,

02:40:55.670 --> 02:40:57.820
like, what should be used for and

02:40:57.820 --> 02:40:59.949
and there are cost implications. I agree with

02:40:59.949 --> 02:41:00.449
you.

02:41:04.710 --> 02:41:05.210
Okay.

02:41:11.730 --> 02:41:13.810
Thank you, Nirja. So now moving to

02:41:13.810 --> 02:41:15.430
the next part of the work

02:41:16.150 --> 02:41:18.250
workshop, we have two spotlight talks right now.

02:41:18.870 --> 02:41:20.910
The first one is from Yousheng

02:41:20.970 --> 02:41:23.130
Zhang, PhD student from University

02:41:23.130 --> 02:41:25.160
of California, Santa Cruz, who'll talk

02:41:25.160 --> 02:41:27.130
about towards AGENTEC OS

02:41:27.611 --> 02:41:30.030
and LLM agent framework for Linux schedulers.

02:42:00.400 --> 02:42:02.789
Hello, everyone. My name is Yoo

02:42:02.789 --> 02:42:05.029
Seong Jeong, and I'm mainly, like, working

02:42:05.029 --> 02:42:07.230
on EVPS and operating some

02:42:07.230 --> 02:42:09.550
domain and also maintaining some open

02:42:09.870 --> 02:42:11.740
open source organization like,

02:42:13.040 --> 02:42:15.170
and show in this talk,

02:42:15.170 --> 02:42:17.230
I will talk about, like, we

02:42:17.230 --> 02:42:19.534
want to let LRM agents

02:42:21.350 --> 02:42:23.480
to fully automatically optimize operating

02:42:23.480 --> 02:42:24.300
system schedulers

02:42:25.990 --> 02:42:28.150
Well, actually, we want to make it

02:42:28.150 --> 02:42:30.219
fully optimize the entire

02:42:30.220 --> 02:42:32.150
OS. But currently,

02:42:33.010 --> 02:42:35.090
the system only has,

02:42:35.090 --> 02:42:37.549
like, extensible interface,

02:42:37.550 --> 02:42:39.710
which is the scariest can

02:42:39.710 --> 02:42:42.050
have significant performance. Impact.

02:42:42.425 --> 02:42:44.700
On most of the applications.

02:42:44.840 --> 02:42:46.779
So we are starting from the schederers.

02:42:48.220 --> 02:42:50.391
Why we want to do that? For example,

02:42:51.691 --> 02:42:53.710
there's two major problems. One is

02:42:53.710 --> 02:42:54.620
the semantic gaps.

02:42:56.250 --> 02:42:58.430
OS schedulers, the default English

02:42:58.489 --> 02:43:00.190
version now is e EVG.

02:43:00.749 --> 02:43:03.010
Is failing to understand application needs.

02:43:03.810 --> 02:43:05.830
So it's a long history problems.

02:43:06.521 --> 02:43:08.760
Application has different requirements. Like,

02:43:08.760 --> 02:43:10.460
some application require latency,

02:43:11.450 --> 02:43:13.230
Some application requires throughput.

02:43:13.611 --> 02:43:15.820
Some like, some application is

02:43:16.080 --> 02:43:18.160
batch processing, while some application is,

02:43:18.160 --> 02:43:19.060
like, interactive.

02:43:20.760 --> 02:43:22.940
With, like, a QI. So they have

02:43:23.691 --> 02:43:25.620
only have different SLOs.

02:43:26.340 --> 02:43:28.580
Also, we have a lot of knobs,

02:43:28.580 --> 02:43:30.590
and such is possible interface,

02:43:30.590 --> 02:43:32.760
which allow us to ingest some,

02:43:32.760 --> 02:43:34.840
like, algorithm and policies and

02:43:34.840 --> 02:43:37.080
code into code into OS

02:43:37.080 --> 02:43:39.500
kernel. But, typically, it

02:43:39.561 --> 02:43:41.880
requires like,

02:43:41.880 --> 02:43:44.120
deep kernel as best and workload as best.

02:43:44.120 --> 02:43:46.191
However, the workload

02:43:46.191 --> 02:43:48.431
developer who is developing the applications does

02:43:48.431 --> 02:43:49.780
not understand kernel in turn.

02:43:50.691 --> 02:43:53.191
Like, system admin with deploying

02:43:53.250 --> 02:43:55.191
the application to the kernel

02:43:55.441 --> 02:43:57.760
may and it lacks workload insights

02:43:57.760 --> 02:43:59.840
limit not a lot of the kernel and

02:44:00.364 --> 02:44:02.029
environments. Like, typically,

02:44:02.650 --> 02:44:04.740
like, left left or something

02:44:04.740 --> 02:44:06.310
like smartphone users.

02:44:07.030 --> 02:44:09.530
They they don't they don't know how to write code. Like,

02:44:09.670 --> 02:44:11.779
both kernel and application expertise.

02:44:13.590 --> 02:44:16.090
We have multiple solutions to software's

02:44:16.150 --> 02:44:18.580
problems now. For example,

02:44:18.641 --> 02:44:20.900
we have traditional IRR base or

02:44:20.900 --> 02:44:23.140
like, machine learning based models. Late

02:44:23.140 --> 02:44:24.440
heavy query require, like,

02:44:26.440 --> 02:44:28.700
human descending, like, say, action reward

02:44:29.350 --> 02:44:31.590
specific. So it's, like, typically,

02:44:31.590 --> 02:44:33.130
turning some parameters and

02:44:34.770 --> 02:44:36.410
only predefined workspace.

02:44:37.181 --> 02:44:39.420
Problem space. And it may require

02:44:39.420 --> 02:44:41.450
some, like, per workload return

02:44:41.450 --> 02:44:43.636
and per environment returns. And

02:44:43.636 --> 02:44:45.659
may may not be. May not

02:44:45.659 --> 02:44:47.760
be good to transfer to new problems.

02:44:48.060 --> 02:44:50.060
And, also, like, if you

02:44:50.110 --> 02:44:50.610
want to

02:44:52.640 --> 02:44:54.690
inference or involve ML model

02:44:55.329 --> 02:44:57.190
in the OS scheduler, it may cause

02:44:57.670 --> 02:44:59.829
inference overhang. Your whole scheduling pass because you

02:44:59.829 --> 02:45:01.449
need to make decisions in

02:45:02.850 --> 02:45:05.160
microseconds. And you also

02:45:05.160 --> 02:45:07.340
like semantic understanding to our application.

02:45:08.600 --> 02:45:10.840
Naive or Asian

02:45:11.400 --> 02:45:13.640
Solutions has been proposed many times, for

02:45:13.640 --> 02:45:15.310
example, people use it to

02:45:15.790 --> 02:45:17.989
write a GPU kernels. It can also

02:45:17.989 --> 02:45:19.730
be used to improve OS kernel

02:45:20.280 --> 02:45:21.900
But, technically, it's like

02:45:23.100 --> 02:45:25.510
we also have previous working list domain,

02:45:25.830 --> 02:45:28.070
People can specify some goals in

02:45:28.070 --> 02:45:29.770
English, which, like,

02:45:30.215 --> 02:45:32.640
the LLLM will transfer

02:45:32.861 --> 02:45:35.250
the config English into some configuration

02:45:35.710 --> 02:45:38.010
or algorithms, but it this means

02:45:38.010 --> 02:45:40.330
you still need human defined hellhole

02:45:40.330 --> 02:45:42.699
goals and specific context. Human

02:45:42.699 --> 02:45:45.101
still need to understand the workloads

02:45:45.640 --> 02:45:48.020
and OS. And, also, if you just apply

02:45:48.020 --> 02:45:50.310
agents to the system

02:45:50.310 --> 02:45:52.010
is unsafe, and my performance

02:45:53.159 --> 02:45:55.659
instead of improve it. We've test some

02:45:58.306 --> 02:46:00.340
basic like, cloud code and just

02:46:00.340 --> 02:46:02.420
ask you to write a basic scheduler for

02:46:02.420 --> 02:46:04.510
eBPF. It costs, like,

02:46:04.510 --> 02:46:05.790
thirty minutes,

02:46:07.230 --> 02:46:09.269
$6, hundreds of

02:46:09.270 --> 02:46:11.350
cost to do that, and spell a lot

02:46:11.350 --> 02:46:13.510
of language, narcissist. Successful.

02:46:15.420 --> 02:46:17.441
Our our team says, like,

02:46:18.930 --> 02:46:21.250
we want error and agents to serve

02:46:21.250 --> 02:46:23.260
as a OS level

02:46:23.260 --> 02:46:25.440
components, which so we can bridge

02:46:25.499 --> 02:46:27.699
the semantic gaps and not gap

02:46:27.699 --> 02:46:29.940
problems we described before. But also

02:46:29.940 --> 02:46:31.640
Kibla OS app

02:46:32.489 --> 02:46:34.739
abstractions, which means application developer does

02:46:34.739 --> 02:46:36.890
not need to understand kernel and

02:46:37.450 --> 02:46:38.840
short because they have standard interface.

02:46:39.760 --> 02:46:41.910
They, kernel kernel developer

02:46:41.910 --> 02:46:43.770
doesn't need to have too much knowledge about

02:46:44.010 --> 02:46:46.440
what a specific workload is doing. And

02:46:46.940 --> 02:46:48.980
so we have

02:46:48.980 --> 02:46:51.300
two parts here, and the first

02:46:51.300 --> 02:46:53.679
part is the AI age AI

02:46:53.680 --> 02:46:55.820
engine design, which we treat

02:46:55.820 --> 02:46:57.840
the problem space as two stages.

02:46:57.840 --> 02:46:59.920
The first thing is, like, we want to

02:46:59.920 --> 02:47:02.289
let agents use tools to

02:47:02.350 --> 02:47:04.449
analyze its workload intents and structures.

02:47:05.100 --> 02:47:07.040
Enhance its system environment,

02:47:08.870 --> 02:47:11.370
AI agent can generate some, like, workflow profile

02:47:11.430 --> 02:47:13.880
or contain the information

02:47:13.880 --> 02:47:14.860
about our workloads.

02:47:16.010 --> 02:47:17.390
So and, like,

02:47:18.490 --> 02:47:20.770
SO organization goal for our class,

02:47:21.401 --> 02:47:23.869
Then there are RMM agent can

02:47:23.869 --> 02:47:26.190
start to do the actual actually policy

02:47:26.190 --> 02:47:28.100
synthesis. Which is like

02:47:28.560 --> 02:47:30.420
configuration or generate new schedulers

02:47:32.170 --> 02:47:34.490
based on the lenses. So it's, like, fully

02:47:34.490 --> 02:47:34.990
automatically.

02:47:36.820 --> 02:47:38.890
Processed. So

02:47:39.370 --> 02:47:41.529
we also want to separate the

02:47:41.529 --> 02:47:43.770
system part, which is allow us to do

02:47:43.770 --> 02:47:45.800
a safe and efficient pre

02:47:45.840 --> 02:47:48.215
railing code with

02:47:49.680 --> 02:47:52.000
with the AI agents. So system can still remain

02:47:52.000 --> 02:47:53.910
safe and useful as AI agents gets

02:47:54.391 --> 02:47:56.400
better. So we need to separate AI's

02:47:56.860 --> 02:47:59.279
rule of reasoning from system rule of execution.

02:48:00.640 --> 02:48:02.260
You want to manage

02:48:02.940 --> 02:48:04.960
Yeah. So

02:48:04.960 --> 02:48:07.210
our goal is, like, we're to make

02:48:07.691 --> 02:48:09.850
AI agents manage operations from, like, human

02:48:09.850 --> 02:48:12.030
society to deploy a application

02:48:12.090 --> 02:48:13.230
and reduce overhead.

02:48:14.691 --> 02:48:16.870
The system architecture include

02:48:17.430 --> 02:48:19.611
MCP server and

02:48:20.540 --> 02:48:22.720
system demo. So it's, like, part of, like,

02:48:23.040 --> 02:48:24.340
agent and powerful interface.

02:48:25.209 --> 02:48:27.210
Or preliminary

02:48:27.351 --> 02:48:29.260
evaluations include two paths to

02:48:29.500 --> 02:48:31.580
One pi is less configuration, one pi is

02:48:31.580 --> 02:48:33.740
the synthesis. You can

02:48:33.740 --> 02:48:36.080
make, like, two times better performance

02:48:36.300 --> 02:48:38.530
and, like, reduce a lot of, like,

02:48:39.651 --> 02:48:41.870
cost and latency. And, well,

02:48:41.870 --> 02:48:44.010
we can let like RM agent

02:48:44.010 --> 02:48:46.190
can do actually generate new schedulers

02:48:46.330 --> 02:48:47.470
based on the answers.

02:48:48.710 --> 02:48:51.159
Well, let's kind of tool is mainly

02:48:51.300 --> 02:48:53.320
like a proof concept. It still needs standardized

02:48:53.380 --> 02:48:55.720
agent take bench marks with

02:48:55.721 --> 02:48:57.970
clear defined test and, like,

02:48:58.450 --> 02:48:59.830
go inference or SOS.

02:49:00.629 --> 02:49:02.940
And sometimes, MCP server is

02:49:03.420 --> 02:49:05.540
not a best interface. Maybe best is

02:49:05.780 --> 02:49:07.950
better, or we need better tools for

02:49:08.670 --> 02:49:09.170
providing

02:49:11.499 --> 02:49:13.760
Community also has proposing many other interface

02:49:13.820 --> 02:49:15.850
beyond schedulers. Like cache

02:49:15.850 --> 02:49:18.210
and CPU frequency. We are also working on,

02:49:18.210 --> 02:49:20.420
like, GPU memory management and

02:49:20.720 --> 02:49:22.600
scheduling. As our next step.

02:49:23.630 --> 02:49:25.650
To summarize, we propose

02:49:26.130 --> 02:49:28.289
SCLC P, a control plan framework

02:49:28.289 --> 02:49:30.500
can OS levels interface to enable

02:49:30.500 --> 02:49:32.825
fully automatic optimized OS.

02:49:33.930 --> 02:49:36.420
And we separate Go inference from

02:49:36.560 --> 02:49:38.650
parallelization synthesis into like, as a two

02:49:38.870 --> 02:49:41.270
stage problem, so we can achieve a lot of performance

02:49:41.270 --> 02:49:41.770
improvement.

02:49:50.580 --> 02:49:52.659
In the interest of time, we'll

02:49:52.659 --> 02:49:54.800
do the other spotlight talk and then do

02:49:54.800 --> 02:49:56.800
questions together. If

02:49:56.860 --> 02:49:59.100
you could have wait here for the other spotlight talk and

02:49:59.100 --> 02:50:01.490
take the questions later. Okay. Yeah. Thank

02:50:01.490 --> 02:50:03.600
you. So the next

02:50:03.600 --> 02:50:05.540
spotlight talk is by Janani Ramamurti,

02:50:05.680 --> 02:50:07.890
who's a PhD student at UT

02:50:07.890 --> 02:50:10.130
Austin, and will talk about automated

02:50:10.130 --> 02:50:12.151
multi agent workflows for RDL.

02:50:13.169 --> 02:50:13.669
Design.

02:50:52.570 --> 02:50:53.550
Hi everyone. I'm

02:50:54.660 --> 02:50:56.820
Janani Romo Muthi and I just wanted to clarify

02:50:56.820 --> 02:50:59.140
really quickly. I'm actually an undergraduate

02:50:59.280 --> 02:51:01.320
student at UT Austin studying elective

02:51:01.560 --> 02:51:03.579
electrical and computer engineering. I'm I'm a junior.

02:51:04.160 --> 02:51:06.200
And today, I wanted to

02:51:06.200 --> 02:51:08.320
present Veramass, which is a

02:51:08.320 --> 02:51:09.760
multi agent

02:51:10.760 --> 02:51:12.941
architecture search being done to generate

02:51:13.080 --> 02:51:14.141
their log code.

02:51:18.699 --> 02:51:19.040
Okay.

02:51:21.529 --> 02:51:23.610
So, nowadays, large language models

02:51:23.610 --> 02:51:25.640
are starting to be widely used

02:51:25.640 --> 02:51:28.140
for RTO design tasks, including Verilog

02:51:28.280 --> 02:51:30.440
generation. And for those of you who are unfamiliar

02:51:30.659 --> 02:51:33.060
with Verilog, that's basically a hardware description

02:51:33.060 --> 02:51:35.180
language. But

02:51:35.180 --> 02:51:37.279
fine tuning these LLMs is very expensive

02:51:37.339 --> 02:51:39.670
and requires large GPU memory,

02:51:39.670 --> 02:51:42.070
big token budgets, and often a lot of manual

02:51:42.070 --> 02:51:44.230
iteration. And frontier models

02:51:44.230 --> 02:51:46.550
can outperform fine tune models using prompt

02:51:46.550 --> 02:51:48.575
based reasoning a alone,

02:51:48.575 --> 02:51:50.880
but they come with high compute requirements as well

02:51:50.880 --> 02:51:53.330
as very long inference times and they essentially

02:51:53.330 --> 02:51:54.710
overthink every problem.

02:51:55.920 --> 02:51:57.930
So, existing methods such as Verit

02:51:58.250 --> 02:52:00.750
Thoughts already embed formal checks into prompting

02:52:01.191 --> 02:52:03.431
but they still assume one fixed structure, such

02:52:03.431 --> 02:52:05.910
as only using, like, chain of thought reasoning.

02:52:06.380 --> 02:52:08.820
But this is very inefficient. Because

02:52:09.040 --> 02:52:11.300
some dialogue tasks, such as implementing a multiplexer,

02:52:12.050 --> 02:52:13.909
don't need debate or multistep

02:52:14.210 --> 02:52:16.000
reasoning, while tougher modules like

02:52:16.320 --> 02:52:18.401
complex finite state machines might require

02:52:18.401 --> 02:52:20.710
deeper refinement. So we want

02:52:20.851 --> 02:52:23.090
an adaptive reasoning strategy where the model uses

02:52:23.090 --> 02:52:25.111
simple strategies when possible and

02:52:25.111 --> 02:52:27.270
escalates only when failures indicate

02:52:27.270 --> 02:52:27.770
complexity.

02:52:29.601 --> 02:52:31.720
So this is where mass or multi agent

02:52:32.200 --> 02:52:34.619
architecture search comes in. So in this context,

02:52:34.840 --> 02:52:37.079
an agentic operator is just a reasoning

02:52:37.079 --> 02:52:39.240
primitive or a prompting strategy

02:52:39.240 --> 02:52:41.350
such as chain of thought, react, or debates.

02:52:41.670 --> 02:52:43.930
And instead of picking one strategy manually,

02:52:44.611 --> 02:52:46.630
mass lets a controller compose operators

02:52:46.691 --> 02:52:48.740
into workflows like performing chain

02:52:48.740 --> 02:52:50.800
of thought and then moving on to something called

02:52:51.040 --> 02:52:53.440
self refine or doing a one shot prompting

02:52:53.440 --> 02:52:55.220
method depending on the task.

02:52:56.840 --> 02:52:59.330
So, here's the pipeline of how our method

02:52:59.811 --> 02:53:02.130
works. We start by taking an RTO hardware

02:53:02.130 --> 02:53:04.240
design problem statement and then

02:53:04.240 --> 02:53:06.400
we sample a set of candidate reasoning

02:53:06.400 --> 02:53:08.340
operators. And then each operator

02:53:08.930 --> 02:53:09.310
a

02:53:12.430 --> 02:53:14.530
against through EOSYS, which is an open source

02:53:14.530 --> 02:53:17.030
tool that performs synthesis and formal

02:53:17.590 --> 02:53:19.830
verification checks. And then we also run it through

02:53:19.830 --> 02:53:22.000
Open SDA for timing and power

02:53:22.560 --> 02:53:24.720
analysis. And the critical step here

02:53:24.720 --> 02:53:26.851
is that Veramas uses the

02:53:26.851 --> 02:53:28.930
error logs generated by all

02:53:28.930 --> 02:53:30.940
of these synthesis checks to

02:53:30.940 --> 02:53:33.040
decide whether to continue processing

02:53:33.100 --> 02:53:34.720
with more complex operators

02:53:35.240 --> 02:53:37.401
or stop early because we're satisfied with

02:53:37.401 --> 02:53:39.180
the design that it already generated.

02:53:41.260 --> 02:53:43.580
So, the set of operators that we use spans

02:53:43.820 --> 02:53:45.981
spectrum from simple to complex. We

02:53:45.981 --> 02:53:48.219
start with something called zero shot IO prompting,

02:53:48.220 --> 02:53:50.070
which has minimal reasoning

02:53:50.230 --> 02:53:52.310
You're simply putting in an input and getting

02:53:52.310 --> 02:53:54.500
an an output from the LLM.

02:53:54.820 --> 02:53:57.160
And then we have chain of thought, which has structured

02:53:57.220 --> 02:53:58.441
intermediate reasoning.

02:53:59.420 --> 02:54:01.740
And then we have React which alternates reasoning

02:54:01.740 --> 02:54:04.129
steps with external tool

02:54:04.129 --> 02:54:06.570
interaction. Lastly, we have self refine,

02:54:06.630 --> 02:54:08.490
where the model critiques and improves its

02:54:08.731 --> 02:54:10.590
own output, iteratively.

02:54:10.971 --> 02:54:13.139
And these are what we call agent operators

02:54:13.140 --> 02:54:15.320
or the modular reasoning tools that

02:54:15.620 --> 02:54:17.510
the controller assembles into workflows.

02:54:18.910 --> 02:54:21.220
So our controller is the brain of

02:54:21.460 --> 02:54:23.620
our algorithm, where and it runs

02:54:23.620 --> 02:54:25.890
in a cascade. So, we start with using

02:54:25.890 --> 02:54:28.050
a very simple operator, and then we move

02:54:28.050 --> 02:54:29.830
on to more complex operators.

02:54:30.270 --> 02:54:32.510
And at each stage, it evaluates the batch

02:54:32.510 --> 02:54:34.650
of generated designs using a metric

02:54:34.650 --> 02:54:37.070
that we compute from the EOSIS and OpenSTA

02:54:37.530 --> 02:54:39.699
results. We're checking for things like

02:54:39.699 --> 02:54:41.749
how many didn't have the correct syntax or

02:54:41.749 --> 02:54:44.010
how many failed formal verification checks.

02:54:44.380 --> 02:54:46.540
And this produces a confidence score. So

02:54:46.540 --> 02:54:48.660
if most designs succeed, then

02:54:48.660 --> 02:54:50.740
we stop, but, if many of the

02:54:50.740 --> 02:54:52.779
designs are failing, then we're gonna

02:54:52.779 --> 02:54:54.800
escalate to the next reasoning operator.

02:54:56.660 --> 02:54:58.740
So formally, we're optimizing for two

02:54:58.740 --> 02:55:00.750
things. The first being utility,

02:55:00.750 --> 02:55:02.830
and that's measured with PASET k, which is

02:55:02.830 --> 02:55:04.920
a standard cogener metric,

02:55:04.920 --> 02:55:07.320
and it's essentially the probability that at least

02:55:07.320 --> 02:55:08.710
one of k generated samples

02:55:09.410 --> 02:55:11.570
is correct. And the cost is measured

02:55:11.570 --> 02:55:13.690
as the token usage per query.

02:55:13.930 --> 02:55:15.789
And we use a small trade off coefficient,

02:55:16.010 --> 02:55:18.060
lambda, to balance these out, and this is

02:55:18.060 --> 02:55:20.320
just a metric from the original paper

02:55:20.460 --> 02:55:22.611
detailing multi agent search.

02:55:24.360 --> 02:55:26.680
Training our controller is extremely lightweight.

02:55:26.680 --> 02:55:28.840
All we do is sample 500 data

02:55:28.840 --> 02:55:30.940
points from the VerithaTuts training set, and for

02:55:31.239 --> 02:55:33.160
each data point, we generate

02:55:33.300 --> 02:55:35.380
about 20 candidates and run them through

02:55:35.380 --> 02:55:37.410
YOSIS and OpenSTA and count the

02:55:37.410 --> 02:55:37.910
failures.

02:55:39.499 --> 02:55:41.579
So if we look at some results, we see

02:55:41.579 --> 02:55:43.880
that on OpenAI models like o four Mini

02:55:43.880 --> 02:55:45.600
and GPT four o Mini, we get

02:55:46.159 --> 02:55:48.239
improvements in pass at one and pass at 10. But

02:55:48.239 --> 02:55:50.330
because these models are already strong, gains are

02:55:50.330 --> 02:55:51.950
smaller but still very consistent.

02:55:53.409 --> 02:55:55.489
And then on open source instruct models,

02:55:55.489 --> 02:55:57.614
we see larger improvements because these models

02:55:57.614 --> 02:55:59.740
are good on their own, and we see

02:55:59.740 --> 02:56:01.820
up to seven to 12% gains in PASET

02:56:01.820 --> 02:56:03.870
one when without any sort of fine

02:56:03.870 --> 02:56:06.320
tuning, just multi agent architecture search.

02:56:07.119 --> 02:56:09.199
And then, lastly, one thing I wanna cover

02:56:09.199 --> 02:56:11.239
real quick is that controller is

02:56:11.239 --> 02:56:13.279
very flexible. We can optimize for different

02:56:13.279 --> 02:56:15.180
goals without retraining the underlying

02:56:15.401 --> 02:56:17.480
LLM. So we can optimize for things such

02:56:17.481 --> 02:56:19.370
as power performance, and area.

02:56:19.530 --> 02:56:21.960
We evaluate this on a subset

02:56:22.555 --> 02:56:24.850
of data

02:56:24.850 --> 02:56:26.230
called PPA Tiny,

02:56:27.350 --> 02:56:29.530
So, if we look at that here, we see that

02:56:29.530 --> 02:56:31.851
we get improvements in area and delays sometimes

02:56:31.851 --> 02:56:34.150
by over 20%. We

02:56:34.150 --> 02:56:36.050
see some increases in power, but

02:56:36.770 --> 02:56:39.190
overall, we get improvements in area and delay.

02:56:40.930 --> 02:56:43.010
And this is just a general summary of our work.

02:56:43.010 --> 02:56:45.390
It's a multi agent prompting framework that integrates

02:56:45.390 --> 02:56:47.630
formal verification feedback into the operator

02:56:47.630 --> 02:56:49.940
selection loop. And in the interest of

02:56:49.940 --> 02:56:52.120
time, I'll stop here. But, feel

02:56:52.120 --> 02:56:53.021
free to check out.

02:57:00.361 --> 02:57:02.440
Have we have maybe time for one

02:57:02.440 --> 02:57:04.780
question each for both the Spotlight

02:57:05.080 --> 02:57:06.151
talks? Anybody?

02:57:17.869 --> 02:57:19.450
Could you please stay at the beam and if

02:57:20.600 --> 02:57:22.840
Hello. Thank you for the great talks. This is Utku

02:57:22.840 --> 02:57:25.000
from Harvard. So my question

02:57:25.000 --> 02:57:27.122
is for first talk Do you think

02:57:27.122 --> 02:57:29.247
there is any security implications of

02:57:29.247 --> 02:57:31.230
using LLMs within the OS?

02:57:32.361 --> 02:57:32.630
Yes.

02:57:36.200 --> 02:57:37.800
Laredo. There's significantly

02:57:38.340 --> 02:57:40.601
security problems based on the eBPF

02:57:40.820 --> 02:57:42.879
security mode because eBPF can,

02:57:42.879 --> 02:57:45.199
like, break a lot of your users based application

02:57:45.199 --> 02:57:46.900
and like, cause a lot of

02:57:47.459 --> 02:57:49.250
unfair in scheduler. So we need to have

02:57:49.601 --> 02:57:52.030
designer verifier or some, like, steady

02:57:52.030 --> 02:57:54.220
check or dynamic roll back to BigLab.

02:57:59.120 --> 02:58:00.500
Any other questions? Okay.

02:58:02.580 --> 02:58:03.861
Are you planning to present this,

02:58:04.960 --> 02:58:06.680
your work at LBC this year?

02:58:07.710 --> 02:58:09.000
I will not present this

02:58:17.360 --> 02:58:18.750
the other person also asked?

02:58:21.760 --> 02:58:22.760
That's fine. That's fine.

02:58:25.070 --> 02:58:26.660
Oh, maybe we can try later. Right?

02:58:27.530 --> 02:58:29.610
Because Awesome. Thank

02:58:29.610 --> 02:58:32.029
you, everyone. We have a wonderful

02:58:33.400 --> 02:58:35.659
set of posters, and

02:58:35.800 --> 02:58:38.010
please go and talk to the authors.

02:58:38.010 --> 02:58:40.351
They might have some great insights to share.

02:58:41.200 --> 02:58:43.380
After that, there'll be lunch,

02:58:43.600 --> 02:58:45.590
and then we'll have the post lunch session

02:58:45.670 --> 02:58:47.690
which will start with a keynote

02:58:47.990 --> 02:58:50.150
from, professor Ian Steiker

02:58:50.150 --> 02:58:52.100
from UC Berkeley. Thank you,

02:58:52.159 --> 02:58:52.480
everyone.

04:46:30.090 --> 04:46:30.090
So

05:15:53.840 --> 05:15:56.160
Hello? Okay. Now it works. Okay.

05:15:56.160 --> 05:15:58.080
Well, we'll come back to our afternoon

05:15:58.220 --> 05:16:00.230
session. And just a

05:16:00.230 --> 05:16:02.730
quick recap, we are going to have,

05:16:03.520 --> 05:16:05.610
an afternoon keynote from Yon Soiga,

05:16:05.610 --> 05:16:07.620
fall followed by

05:16:07.620 --> 05:16:09.660
a debate panel. So please

05:16:09.660 --> 05:16:11.820
stick around. They are very relevant. They are

05:16:11.820 --> 05:16:14.140
similar topics, and I I really hope that

05:16:14.140 --> 05:16:16.380
you will enjoy it as much as we are very

05:16:16.380 --> 05:16:18.560
excited about both the talk and the debate.

05:16:19.240 --> 05:16:21.500
And then we can have a short break. We're gonna

05:16:21.560 --> 05:16:23.570
have more invited talks, spotlight

05:16:24.130 --> 05:16:26.150
and, post their session.

05:16:26.290 --> 05:16:28.609
So but first thing first,

05:16:29.230 --> 05:16:31.330
for the next talk, I am very honored

05:16:31.390 --> 05:16:33.570
to introduce Ion Staiger,

05:16:34.910 --> 05:16:36.370
professor from UC Berkeley.

05:16:37.509 --> 05:16:39.800
He is the director of the Sky COMP lab

05:16:40.600 --> 05:16:42.540
and the executive chairman of Databricks

05:16:42.680 --> 05:16:45.070
and Annie Scale His current research

05:16:45.070 --> 05:16:47.310
focus on AI system and cloud computing,

05:16:47.310 --> 05:16:49.460
and his work includes numerous

05:16:49.460 --> 05:16:51.640
open source projects, such as VLLM,

05:16:52.100 --> 05:16:54.490
SGLAND, Shadbot Arena, SkyPilot,

05:16:54.970 --> 05:16:57.370
and many more. He's a member of National

05:16:57.370 --> 05:16:59.750
Cutting Up Engineering, honorary member of

05:17:01.109 --> 05:17:03.288
Romanian Academy, ASACM Fellow,

05:17:03.770 --> 05:17:06.010
and he's cofounded many companies that

05:17:06.010 --> 05:17:07.530
you probably heard of. And,

05:17:08.230 --> 05:17:10.390
I would like to welcome Eon for his

05:17:10.390 --> 05:17:11.870
very excited talk today.

05:17:17.750 --> 05:17:19.910
Thank you so much for having me. And,

05:17:20.390 --> 05:17:21.509
it's again this is,

05:17:22.490 --> 05:17:24.890
like they say, hot on the press. It's like just

05:17:24.890 --> 05:17:27.001
getting the results. Few

05:17:27.001 --> 05:17:29.321
minutes ago, the latest one. So, so

05:17:29.321 --> 05:17:30.921
we'll we'll excuse a little bit,

05:17:31.850 --> 05:17:33.812
maybe not being the most Polish talk,

05:17:33.930 --> 05:17:36.050
I ever given. First of all, I

05:17:36.050 --> 05:17:38.368
just want to recognize the fact that this is actually

05:17:38.368 --> 05:17:40.501
a very, you know, a group

05:17:40.560 --> 05:17:42.902
effort. I am only talking about it.

05:17:43.171 --> 05:17:45.350
Many of the students are here in the audience.

05:17:45.892 --> 05:17:48.009
So please feel free

05:17:48.009 --> 05:17:50.179
to contact them and ask the hard

05:17:50.179 --> 05:17:52.180
questions. So this

05:17:52.180 --> 05:17:54.421
is about, you know, using

05:17:54.421 --> 05:17:56.380
AI to improve, to

05:17:56.679 --> 05:17:59.179
accelerate, system research, in particular

05:17:59.600 --> 05:18:01.520
the performance, system

05:18:01.680 --> 05:18:03.300
the system performance research.

05:18:04.650 --> 05:18:06.270
Now using machine learning,

05:18:07.130 --> 05:18:09.160
and AI to improve the

05:18:09.160 --> 05:18:11.240
system, it's obviously not a new

05:18:11.240 --> 05:18:13.490
topic. It's actually

05:18:13.730 --> 05:18:15.830
one of the first talk here, a very influential

05:18:15.890 --> 05:18:17.429
talks given by Jeff Dean

05:18:18.091 --> 05:18:20.411
was in 2018 at

05:18:20.411 --> 05:18:22.470
New York, actually, that time was called

05:18:22.689 --> 05:18:24.849
NIPS. And since then, there are a lot of

05:18:24.849 --> 05:18:26.230
papers which are written

05:18:27.284 --> 05:18:29.730
published using AI in

05:18:29.790 --> 05:18:31.950
different domains, like databases,

05:18:32.250 --> 05:18:32.750
networking,

05:18:34.490 --> 05:18:36.570
and systems operating systems

05:18:36.570 --> 05:18:38.740
and more. So

05:18:38.740 --> 05:18:41.140
what is kind of different now, and I think

05:18:41.140 --> 05:18:43.448
why at least personally, I'm quite excited.

05:18:44.070 --> 05:18:46.009
I think we are seeing a phase

05:18:46.150 --> 05:18:48.380
change. So before when we use

05:18:48.380 --> 05:18:50.620
AI for improving systems, we are looking

05:18:50.620 --> 05:18:52.689
at the you know, AI

05:18:52.689 --> 05:18:54.860
a little bit like, a

05:18:55.100 --> 05:18:57.341
black box. Right? It's like, basically, you have a system,

05:18:57.341 --> 05:18:59.698
we have an workload. And then

05:18:59.698 --> 05:19:02.120
we either optimize a bunch of knobs, parameter,

05:19:02.980 --> 05:19:05.060
or you try to replace some of the components

05:19:05.060 --> 05:19:06.740
of the system like query optimize

05:19:07.510 --> 05:19:08.010
with

05:19:09.640 --> 05:19:11.800
with neural networks, which hopefully is our

05:19:11.800 --> 05:19:13.921
packet classification in working with a

05:19:13.921 --> 05:19:16.140
neural network, which are doing things better.

05:19:17.050 --> 05:19:19.130
However, what we see right now, at

05:19:19.130 --> 05:19:20.990
least what I'm going to talk today,

05:19:21.150 --> 05:19:23.390
is that the the the system

05:19:23.390 --> 05:19:25.523
and so forth is also more like a

05:19:25.523 --> 05:19:27.790
white box. We expose the system to the

05:19:27.790 --> 05:19:29.330
to the to the neural network,

05:19:29.970 --> 05:19:32.050
and the neural network is going to operate

05:19:32.050 --> 05:19:34.340
on the system code and improve that code.

05:19:34.580 --> 05:19:37.080
That's why how you are going to improve the systems.

05:19:37.610 --> 05:19:38.110
Okay?

05:19:39.769 --> 05:19:41.849
And a lot what

05:19:41.849 --> 05:19:43.929
I'm going to talk today is mostly focused

05:19:43.929 --> 05:19:46.130
on performance improvements It's a

05:19:46.130 --> 05:19:48.370
easier problem in general, but it's still very

05:19:48.370 --> 05:19:50.150
widely prevalent in

05:19:50.390 --> 05:19:52.570
early system conferences. When I'm talking systems

05:19:52.630 --> 05:19:54.800
and a system here, I'm referring

05:19:54.940 --> 05:19:57.360
to sync systems broadly. It's databases,

05:19:58.060 --> 05:19:58.560
networking,

05:20:00.150 --> 05:20:02.470
operating systems, even programming

05:20:02.470 --> 05:20:04.280
languages and things like that.

05:20:04.680 --> 05:20:06.841
So what are the disruptors here? So one,

05:20:06.841 --> 05:20:08.600
I think, is one of the,

05:20:08.850 --> 05:20:11.349
influential what first at least, you

05:20:11.410 --> 05:20:13.429
know, was, this fund surge.

05:20:14.370 --> 05:20:15.349
Which was using,

05:20:16.530 --> 05:20:18.530
from coming from the mind using,

05:20:20.550 --> 05:20:22.251
live language models for discovering

05:20:22.630 --> 05:20:24.400
new solution for

05:20:24.960 --> 05:20:27.420
mass problems and algorithms problems.

05:20:28.060 --> 05:20:30.080
Then it was late earlier this year was Alpha

05:20:30.140 --> 05:20:32.280
Evolve, which was published

05:20:32.900 --> 05:20:35.010
showing that you can extend that the same idea

05:20:35.581 --> 05:20:37.751
to other problems is pretty general, and

05:20:37.751 --> 05:20:40.200
the are giving some impressive results

05:20:40.260 --> 05:20:42.400
in that paper. Actually, it was

05:20:42.400 --> 05:20:44.560
June 2025, so it was even

05:20:44.720 --> 05:20:46.840
yeah. So very

05:20:46.960 --> 05:20:47.460
recent.

05:20:49.421 --> 05:20:51.501
And then there was open evolve.

05:20:51.501 --> 05:20:53.690
This is open source version of the

05:20:54.150 --> 05:20:56.251
Alpha Evolve Alpha Evolve again. You know,

05:20:56.251 --> 05:20:58.411
given the name you you know that it came

05:20:58.411 --> 05:21:00.590
from DMind. And this

05:21:00.590 --> 05:21:02.570
is what has happened. So we all

05:21:02.810 --> 05:21:05.150
at this paper early on then, and then

05:21:05.530 --> 05:21:07.580
over the summer, and,

05:21:07.800 --> 05:21:09.958
you know, JEPI is, you know, our

05:21:09.958 --> 05:21:11.580
own paper in this space,

05:21:12.280 --> 05:21:14.440
and many others. And, of course, you have this code

05:21:14.440 --> 05:21:16.759
assistance. So what we decided

05:21:16.820 --> 05:21:18.840
then in the summer, this summer,

05:21:19.300 --> 05:21:21.320
is like, well, you know, let's see how

05:21:21.990 --> 05:21:23.689
well these techniques are working.

05:21:25.340 --> 05:21:27.500
And, we are asking the

05:21:27.500 --> 05:21:29.800
students here, why don't

05:21:29.800 --> 05:21:31.810
apply your technique these techniques on

05:21:31.810 --> 05:21:33.860
your problems? Not benchmarks,

05:21:33.860 --> 05:21:35.940
on your own problems, your own research

05:21:35.940 --> 05:21:37.940
problems. And

05:21:38.560 --> 05:21:40.720
then you report back, and you have presented

05:21:42.960 --> 05:21:45.120
you know, try to understand where you

05:21:45.120 --> 05:21:47.460
are. And I think the results

05:21:47.460 --> 05:21:49.480
were were better than expected,

05:21:49.860 --> 05:21:51.970
and that that's why I am here.

05:21:52.991 --> 05:21:55.071
So you look about, I think, around

05:21:55.071 --> 05:21:56.878
ten eleven problems,

05:21:57.099 --> 05:21:59.340
and these are the problems. You see this from

05:21:59.340 --> 05:22:01.370
different domains.

05:22:02.150 --> 05:22:03.900
And, there are different

05:22:04.540 --> 05:22:06.560
level of maturity in this project.

05:22:06.620 --> 05:22:08.921
Some of them were already published.

05:22:08.980 --> 05:22:11.060
I think here, eight of

05:22:11.060 --> 05:22:12.400
them, were published in

05:22:13.439 --> 05:22:15.470
conferences you know, top conferences more or

05:22:15.470 --> 05:22:17.010
less in the in their fields.

05:22:18.190 --> 05:22:20.470
And, then

05:22:20.710 --> 05:22:22.670
what we got is that in

05:22:23.070 --> 05:22:25.490
large majority of the cases you see here,

05:22:27.251 --> 05:22:29.570
you know, eight or

05:22:29.570 --> 05:22:31.609
nine cases, people got

05:22:31.609 --> 05:22:33.960
better results than the

05:22:34.019 --> 05:22:36.060
SOTA, either the published in the

05:22:36.060 --> 05:22:38.460
published papers or the current

05:22:38.460 --> 05:22:40.690
best results you're getting in their

05:22:41.091 --> 05:22:42.890
particular research. Okay?

05:22:44.980 --> 05:22:47.060
Not only that, but it was quite

05:22:47.060 --> 05:22:49.470
cheap. They spend at least for this original

05:22:50.110 --> 05:22:52.270
getting early results, a few tens of bucks for

05:22:52.270 --> 05:22:53.310
each of this problem.

05:22:54.730 --> 05:22:56.810
So that's kind of, you know, what's made us

05:22:56.810 --> 05:22:58.970
exciting. Excited. Right? It's like

05:22:59.290 --> 05:23:01.370
you apply these tools to the your

05:23:01.370 --> 05:23:02.990
problems, research problems,

05:23:03.550 --> 05:23:05.790
you get pretty good results pretty

05:23:05.790 --> 05:23:07.687
fast. And pretty

05:23:08.730 --> 05:23:10.180
cheap. Okay? What not to like.

05:23:11.540 --> 05:23:13.570
So, so

05:23:13.570 --> 05:23:15.970
now let me take a step back and think about

05:23:15.970 --> 05:23:18.120
what is kind of the impact? This is what

05:23:18.280 --> 05:23:20.290
you know, you always ask.

05:23:21.171 --> 05:23:23.251
And if if you think about how do we

05:23:23.251 --> 05:23:25.320
do resell, at least system research, so

05:23:25.320 --> 05:23:26.210
how do we do it?

05:23:27.410 --> 05:23:29.430
We we are going to come up with a

05:23:29.570 --> 05:23:31.620
problem to add to solve. Right?

05:23:31.620 --> 05:23:33.482
Improving, you know,

05:23:35.870 --> 05:23:38.030
you know, transaction like throughput or

05:23:38.030 --> 05:23:40.050
improving latency for networking.

05:23:40.550 --> 05:23:42.710
Traffic, things like that. Right?

05:23:45.161 --> 05:23:47.241
And you have the program formulation. You

05:23:47.241 --> 05:23:49.420
know? And then what you do,

05:23:49.420 --> 05:23:51.670
you you try to figure out what is the

05:23:51.990 --> 05:23:54.070
evaluation framework, what is this is it going to be

05:23:54.070 --> 05:23:56.170
the system? And in many cases, you have a

05:23:56.230 --> 05:23:58.410
simulator. You know, which you are going to develop

05:23:58.410 --> 05:24:00.790
your algorithms. And then

05:24:00.790 --> 05:24:03.070
you are going to start developing the algorithm

05:24:03.310 --> 05:24:05.090
algorithms using this kind of evaluation

05:24:05.550 --> 05:24:07.652
framework. Right? It's not

05:24:07.652 --> 05:24:09.730
only about the simulator and so forth. You also

05:24:09.730 --> 05:24:11.890
have to decide which are the benchmarks,

05:24:11.890 --> 05:24:13.751
like your databases, TPCDS,

05:24:14.050 --> 05:24:16.509
or whatever, DPC,

05:24:16.650 --> 05:24:18.910
and things like that. You are going to decide

05:24:19.330 --> 05:24:21.830
know, what the benchmarks you are going to want to

05:24:21.995 --> 05:24:24.090
you know, to target. And then

05:24:24.090 --> 05:24:26.510
you start to design solutions and implement

05:24:26.570 --> 05:24:28.790
them and you evaluate them,

05:24:28.790 --> 05:24:30.890
and you are going to go this

05:24:31.370 --> 05:24:33.530
repeatedly until you get, in

05:24:33.530 --> 05:24:35.170
general, good enough

05:24:35.730 --> 05:24:37.890
results. And when you get good enough results, you

05:24:37.890 --> 05:24:39.980
write a paper. Right? That's

05:24:39.980 --> 05:24:42.170
what we do. Right?

05:24:43.560 --> 05:24:45.609
And, and,

05:24:45.609 --> 05:24:47.310
basically, here, what

05:24:47.680 --> 05:24:50.020
we are trying to focus on,

05:24:50.241 --> 05:24:52.501
on this kind of middle in this pipeline,

05:24:53.570 --> 05:24:55.890
basically, the solution generating the solution

05:24:55.890 --> 05:24:58.150
and evaluating it. Okay?

05:25:00.241 --> 05:25:02.279
And, this is

05:25:02.279 --> 05:25:04.519
what we try to make it

05:25:04.519 --> 05:25:06.770
automatic. And if you think about

05:25:06.770 --> 05:25:09.240
this, what all these kind of systems do,

05:25:09.880 --> 05:25:11.980
basically, they have a prompt generator,

05:25:12.501 --> 05:25:14.680
now the solution generator is a language

05:25:14.741 --> 05:25:16.821
model, which takes a prompt and generate a

05:25:16.821 --> 05:25:18.929
solution. Then you have the evaluator

05:25:19.310 --> 05:25:21.402
using you're taking that solution and

05:25:21.402 --> 05:25:23.642
evaluating it. And then you

05:25:23.642 --> 05:25:25.421
store the results in a

05:25:26.110 --> 05:25:28.270
database, in a storage system. And then you have

05:25:28.270 --> 05:25:30.380
a way to pick previous

05:25:30.440 --> 05:25:32.520
solutions and feedback to the next to the

05:25:32.520 --> 05:25:34.460
prompt so then you iterate

05:25:34.770 --> 05:25:37.110
Right? Because every new iteration

05:25:37.331 --> 05:25:38.470
is you are going to

05:25:39.562 --> 05:25:41.720
be based on some of the results you get

05:25:41.720 --> 05:25:43.757
in the previous iterations. And

05:25:43.757 --> 05:25:45.970
you do that until, you get

05:25:45.970 --> 05:25:48.020
hopefully good results or until your

05:25:48.020 --> 05:25:50.180
budget you you, you know,

05:25:50.180 --> 05:25:51.751
you are done, you know,

05:25:52.370 --> 05:25:54.710
with the budget, compute budget.

05:25:56.179 --> 05:25:58.190
Okay? Now you can have also an

05:25:58.510 --> 05:26:00.911
outer loop in which you basically you can go

05:26:00.911 --> 05:26:03.390
take us from time to time and look as there is results,

05:26:03.450 --> 05:26:05.610
the logs, and so forth. And may you may want

05:26:05.610 --> 05:26:07.850
to change the prompt or things like that or

05:26:07.850 --> 05:26:10.180
even the algorithm. But that's kind of

05:26:10.180 --> 05:26:11.400
the main idea. Right?

05:26:12.550 --> 05:26:14.892
So here is an example, and it's an example.

05:26:14.950 --> 05:26:17.320
It's, it's expert parallelism

05:26:17.320 --> 05:26:19.400
load balancing. Everyone kind of

05:26:19.400 --> 05:26:21.402
knows that. And, you

05:26:21.402 --> 05:26:23.560
know, this may model of experts,

05:26:23.560 --> 05:26:24.940
you have a lot of experts.

05:26:25.940 --> 05:26:28.140
And now when the model is larger, you have

05:26:28.140 --> 05:26:30.400
a lot of traffic, now the question is about

05:26:30.400 --> 05:26:32.720
you want to assign these experts

05:26:32.859 --> 05:26:35.019
on GPUs, and you want to ex to

05:26:35.019 --> 05:26:37.130
assign the experts on the GPU such

05:26:37.670 --> 05:26:39.911
that to have very good

05:26:39.911 --> 05:26:41.770
load balancing because that will maximize

05:26:41.991 --> 05:26:44.100
the throughput and will reduce the cost.

05:26:44.100 --> 05:26:46.180
Right? And it's not only you the

05:26:46.180 --> 05:26:48.290
problem is not only to assign this kind of

05:26:48.530 --> 05:26:50.770
of experts because some experts will be very popular

05:26:50.770 --> 05:26:52.810
and you also want to replicate the experts,

05:26:53.370 --> 05:26:55.830
and then, you know, assign the replicas. Right?

05:26:55.890 --> 05:26:57.970
So that's basically what what is

05:26:57.970 --> 05:27:00.259
the problem. And there

05:27:00.259 --> 05:27:02.599
are two like, I'll I already alluded.

05:27:02.660 --> 05:27:04.890
There are two metrics you care about,

05:27:05.110 --> 05:27:07.530
balancing factor about how well

05:27:07.800 --> 05:27:09.880
is this x the GPUs

05:27:09.880 --> 05:27:12.029
are load balanced. Balanced,

05:27:12.029 --> 05:27:14.130
the load. The experts are load balanced on GPUs.

05:27:14.269 --> 05:27:16.440
And it's you can think about average

05:27:16.440 --> 05:27:18.540
GPU load over the maximum GPU

05:27:18.679 --> 05:27:20.700
load. And the

05:27:21.001 --> 05:27:23.470
running time the algorithm because it's a very dynamic

05:27:24.769 --> 05:27:26.630
system. As you know, a token

05:27:27.220 --> 05:27:29.380
the experts are token it's using, you

05:27:29.380 --> 05:27:30.920
forward that expert to.

05:27:31.450 --> 05:27:33.530
Can, you know, depend from token to token,

05:27:33.530 --> 05:27:35.940
and also from layer to layer can do it.

05:27:36.581 --> 05:27:38.600
So you need to be pretty quick

05:27:38.661 --> 05:27:40.470
if you need to reassign

05:27:40.849 --> 05:27:42.870
these experts to the GPUs.

05:27:43.921 --> 05:27:46.341
To improve the load balancing. Right?

05:27:46.402 --> 05:27:48.890
So how fast you can compute a new

05:27:49.029 --> 05:27:50.650
assignment is also very important.

05:27:51.190 --> 05:27:51.690
Okay?

05:27:53.560 --> 05:27:55.020
And this is, again,

05:27:55.930 --> 05:27:58.250
the loop, the open evolve loop. And let's see what happens.

05:27:58.250 --> 05:28:00.360
So the prompt, you know, this

05:28:00.360 --> 05:28:01.820
simple example, you just

05:28:02.501 --> 05:28:04.460
describe the problem. Right?

05:28:04.620 --> 05:28:06.700
You say what is the problem, and then you

05:28:06.700 --> 05:28:08.980
say what are the goals. As you see, you improve

05:28:08.980 --> 05:28:11.400
the algorithms to achieve better load balancing

05:28:11.618 --> 05:28:13.470
and improve the algorithms to be

05:28:13.790 --> 05:28:15.780
more efficient. Right, assignment.

05:28:16.310 --> 05:28:18.390
Okay? This is what you described. Right? It's very

05:28:18.390 --> 05:28:20.620
natural. Then you give the code.

05:28:20.780 --> 05:28:22.940
In this case, you have a simulator, so you give the

05:28:22.940 --> 05:28:25.160
entire simulator. And but you

05:28:25.160 --> 05:28:27.220
are going to define a block block

05:28:27.220 --> 05:28:29.350
in this in this simulator This

05:28:29.350 --> 05:28:31.510
is where you want to I want you to

05:28:31.510 --> 05:28:33.780
change because this is where the

05:28:33.841 --> 05:28:35.400
load balancer code

05:28:35.939 --> 05:28:37.080
is, right, in this

05:28:38.690 --> 05:28:40.950
evolve block start and evolve block end.

05:28:41.730 --> 05:28:42.970
Right? That's what you do. Right?

05:28:44.009 --> 05:28:44.990
And then,

05:28:46.230 --> 05:28:48.410
you ask you you know,

05:28:48.790 --> 05:28:50.880
z solution generator is

05:28:50.880 --> 05:28:53.380
going to take this prompt and is going

05:28:53.921 --> 05:28:56.000
to hopefully provide,

05:28:56.620 --> 05:28:58.880
better algorithms in this between

05:28:58.960 --> 05:29:01.250
in this block which you asked you

05:29:01.630 --> 05:29:02.660
know, to be a vault.

05:29:04.368 --> 05:29:06.390
Okay? Then

05:29:07.251 --> 05:29:09.310
are going to run now the simulator, which

05:29:09.310 --> 05:29:11.470
has a new algorithm which was proposed by

05:29:11.470 --> 05:29:13.599
the LLM. And you are going to get

05:29:13.599 --> 05:29:15.840
the results. And you are going to store

05:29:15.840 --> 05:29:18.210
this in the storage.

05:29:18.210 --> 05:29:20.290
You are going to run on multiple benchmarks. You

05:29:20.290 --> 05:29:22.368
are going to take the average, and, again, you

05:29:22.368 --> 05:29:24.530
are going to the new solutions and

05:29:24.610 --> 05:29:26.950
the results in this storage. And now you are going

05:29:27.010 --> 05:29:27.991
to iteratively

05:29:29.210 --> 05:29:31.290
build the new prompt. And the new prompt, you

05:29:31.290 --> 05:29:33.310
are going to use some of the

05:29:33.310 --> 05:29:35.091
solutions previous solutions.

05:29:36.060 --> 05:29:38.300
One way to do it is like again, there are many

05:29:38.300 --> 05:29:40.320
ways to do it, but think about one easy

05:29:40.320 --> 05:29:42.411
way. You can choose a two best

05:29:42.411 --> 05:29:44.490
solutions. Right? So you say, okay. Do better

05:29:44.490 --> 05:29:46.470
than this one. And then, also,

05:29:46.610 --> 05:29:49.070
you can you can also

05:29:49.070 --> 05:29:50.850
select a bunch of random

05:29:51.390 --> 05:29:53.439
solutions. And the reason for that is

05:29:53.439 --> 05:29:54.340
provide diversity.

05:29:55.970 --> 05:29:58.310
Right? Maybe there are good ideas in those solutions,

05:29:58.610 --> 05:30:01.000
which allow you to develop

05:30:01.140 --> 05:30:03.171
even a better algorithm. Okay?

05:30:03.930 --> 05:30:05.470
And you add this to the prompt.

05:30:06.680 --> 05:30:08.840
And, then you go to the next

05:30:08.840 --> 05:30:10.380
iteration. There are a few

05:30:10.970 --> 05:30:13.370
you know, is a little more sophisticated actually.

05:30:13.370 --> 05:30:15.430
This this, a frame have a

05:30:15.430 --> 05:30:17.210
lot of configuration parameters.

05:30:17.591 --> 05:30:19.710
But one thing is that they have this kind of

05:30:19.710 --> 05:30:21.751
term of island. You have different which is think about

05:30:21.751 --> 05:30:23.800
different group of solutions. Again, for

05:30:23.800 --> 05:30:25.880
diversity, you you are going to

05:30:25.880 --> 05:30:28.120
evolve in parallel, and from time to time, you are

05:30:28.280 --> 05:30:29.990
moving the best solutions from one

05:30:31.040 --> 05:30:33.300
this island to another. But,

05:30:33.300 --> 05:30:35.410
basically, this is what it is. Okay?

05:30:35.788 --> 05:30:38.109
So next, I'm going to tell you about

05:30:38.109 --> 05:30:39.530
a bunch of results and a

05:30:40.411 --> 05:30:42.491
about, you know, some lesson we learned and, you

05:30:42.491 --> 05:30:44.110
know, things. So these are the results.

05:30:46.320 --> 05:30:48.400
So we try to do this for deep seek

05:30:48.400 --> 05:30:50.820
r one for this is this is our problem.

05:30:51.210 --> 05:30:53.010
It's expert parallelism,

05:30:53.230 --> 05:30:55.530
APLB. And the

05:30:55.550 --> 05:30:57.630
baseline was to we have to is an

05:30:57.630 --> 05:30:59.790
OSS implement implementation,

05:31:00.070 --> 05:31:02.090
which was provided by DeepSig

05:31:02.230 --> 05:31:04.470
by DeepSig, the company. And

05:31:04.470 --> 05:31:06.530
we also have an internal

05:31:06.530 --> 05:31:08.650
access to their internal implementation.

05:31:09.050 --> 05:31:11.280
Okay? And what are the results

05:31:11.280 --> 05:31:13.180
are pretty good? The for

05:31:13.490 --> 05:31:15.970
wanna use this OpenInvolve, the balance

05:31:15.970 --> 05:31:18.320
factor of the same is

05:31:18.320 --> 05:31:20.099
point 66 in this case.

05:31:20.982 --> 05:31:23.241
And but the rebalancing algorithm

05:31:23.460 --> 05:31:25.640
run time, significant improvements.

05:31:26.060 --> 05:31:27.759
It was order of magnitude

05:31:28.140 --> 05:31:30.020
faster than the open

05:31:30.160 --> 05:31:32.599
source implementation. And even

05:31:32.660 --> 05:31:34.740
30 times faster than the

05:31:34.740 --> 05:31:35.960
internal implementation,

05:31:37.230 --> 05:31:38.640
of this company.

05:31:39.280 --> 05:31:41.020
Okay? So

05:31:41.940 --> 05:31:42.440
implementation.

05:31:45.240 --> 05:31:47.530
So and it was a final round kind

05:31:47.530 --> 05:31:49.790
of is, like, five hours, 300

05:31:50.165 --> 05:31:52.780
iteration. And it was less than $220,

05:31:52.780 --> 05:31:55.040
and it was kind of a combination. Gemini,

05:31:55.150 --> 05:31:57.820
2.5 Flash and Gemini 2.5

05:31:58.970 --> 05:32:01.171
light. So not the

05:32:01.171 --> 05:32:03.251
most powerful models. And this is kind

05:32:03.251 --> 05:32:05.350
of the evolution you

05:32:05.350 --> 05:32:07.210
know, so to speak, what how happened.

05:32:07.671 --> 05:32:09.380
You have the iteration here. It's,

05:32:09.700 --> 05:32:12.206
you have only whatever. 100

05:32:13.250 --> 05:32:15.259
probably 70 iteration.

05:32:15.720 --> 05:32:17.260
And you can see

05:32:17.821 --> 05:32:19.902
each of this improvement is a

05:32:19.902 --> 05:32:22.090
combined score. So you have

05:32:22.090 --> 05:32:24.520
here two You you care about

05:32:24.660 --> 05:32:26.671
balance factor and balancing algorithm. Runtime.

05:32:26.671 --> 05:32:28.751
You combine them because open evolve

05:32:28.751 --> 05:32:30.770
ask you to provide only one combined

05:32:31.071 --> 05:32:33.370
score. And here, I think is the weight is half

05:32:33.370 --> 05:32:35.560
half between the two. Okay?

05:32:35.698 --> 05:32:38.170
And you can see so the higher the better.

05:32:38.330 --> 05:32:40.570
And you can see here different points where you

05:32:40.570 --> 05:32:42.750
kind of make significant progress when

05:32:42.750 --> 05:32:44.290
you discover different techniques.

05:32:45.110 --> 05:32:46.570
And you are going to,

05:32:47.190 --> 05:32:49.370
use those in in in the load balancing

05:32:49.600 --> 05:32:51.280
algorithms. Okay? So,

05:32:51.900 --> 05:32:53.440
vectorized PyTorch operations,

05:32:54.800 --> 05:32:57.040
do more vectorization, and then zigzag

05:32:57.040 --> 05:32:58.850
pattern, which is by

05:32:59.171 --> 05:33:00.870
one technique to assign

05:33:01.331 --> 05:33:03.350
this expert replica to the GPS.

05:33:04.010 --> 05:33:06.050
Okay? We also try

05:33:06.050 --> 05:33:08.130
from then since then, we tried

05:33:08.130 --> 05:33:10.269
other two other revolutionary

05:33:10.269 --> 05:33:12.230
systems. One is JEPA, which is

05:33:12.349 --> 05:33:14.759
we developed at Berkeley, and this

05:33:14.759 --> 05:33:17.100
is, one difference is that

05:33:17.600 --> 05:33:20.100
it handles multiple results independently.

05:33:20.320 --> 05:33:22.220
So in general, if you run

05:33:22.460 --> 05:33:24.880
your, solution on multiple

05:33:24.940 --> 05:33:27.040
benchmarks, in the case of open evolve,

05:33:27.600 --> 05:33:29.940
average. Here, you report the results independently

05:33:30.160 --> 05:33:32.070
on each benchmark. And also, you have

05:33:32.190 --> 05:33:34.270
you select the solution on a parrot

05:33:34.270 --> 05:33:36.599
or frontier, And, also, the way is

05:33:36.599 --> 05:33:38.679
is not a fundamental limitation, but the

05:33:38.679 --> 05:33:40.690
way the JAPA is working today you rewrite the

05:33:40.690 --> 05:33:42.930
entire program because originally it was designed

05:33:42.930 --> 05:33:44.940
to generate multiple instances of the same

05:33:44.940 --> 05:33:46.740
program. Okay?

05:33:47.591 --> 05:33:49.830
And then, Shinkau, which had derived from

05:33:49.830 --> 05:33:52.010
OpenInvolve, and it

05:33:52.010 --> 05:33:54.410
adds richer feedback. What that means

05:33:54.410 --> 05:33:56.650
is, like, more or less, like, every

05:33:56.650 --> 05:33:58.730
10, iteration, 10

05:33:58.730 --> 05:34:01.000
solution, it summarizes

05:34:01.000 --> 05:34:03.400
its solutions, and then it

05:34:03.660 --> 05:34:05.720
on then it provides insights

05:34:05.780 --> 05:34:07.870
of all these 10 solution from the

05:34:07.870 --> 05:34:09.890
summaries and also provide some recommendations.

05:34:11.128 --> 05:34:13.208
This is supposed also to help

05:34:13.208 --> 05:34:15.349
the human you go back

05:34:15.349 --> 05:34:17.689
and to understand what kind of happened.

05:34:18.370 --> 05:34:20.450
I'm not sure how much we've done that, but

05:34:20.450 --> 05:34:22.870
just wanted to because Itauka is

05:34:23.360 --> 05:34:25.841
people from Shinkai, and I think they pointed

05:34:25.841 --> 05:34:28.081
to us that that's one of the when

05:34:28.081 --> 05:34:30.161
they tried, this is what, you know, to make it

05:34:30.161 --> 05:34:32.180
much easier for humans to interact.

05:34:32.690 --> 05:34:34.950
With this evolution algorithms by providing

05:34:35.010 --> 05:34:37.190
more explanations about what the

05:34:37.591 --> 05:34:39.010
evolution algorithms done has done.

05:34:40.130 --> 05:34:42.450
Now this one, it has more you need more

05:34:42.450 --> 05:34:44.720
inference steps because you need to do

05:34:44.940 --> 05:34:47.340
the summarizations, and you need to do the they regard

05:34:47.340 --> 05:34:49.480
the insights and provide recommendations.

05:34:50.120 --> 05:34:52.360
And here are the results. Okay. These

05:34:52.360 --> 05:34:53.340
are 10 cases.

05:34:54.460 --> 05:34:56.710
From the original problems. In all cases,

05:34:56.790 --> 05:34:58.570
we try to do 100 iterations,

05:34:59.671 --> 05:35:01.620
with but with,

05:35:01.679 --> 05:35:03.902
you have a little bit more LLM In

05:35:03.902 --> 05:35:06.180
general, one iteration is on LLM call.

05:35:06.741 --> 05:35:09.161
And, you know, there is no overwhelming

05:35:09.780 --> 05:35:11.806
winner. I am you don't need to look all these numbers.

05:35:11.806 --> 05:35:13.400
I'll tell you a little bit more.

05:35:14.171 --> 05:35:15.880
Open a World is a winner in

05:35:16.591 --> 05:35:18.751
six cases, Japan

05:35:18.751 --> 05:35:20.850
in three and the Shinkai in three.

05:35:22.170 --> 05:35:24.590
GPT five, it's winner in six KGs,

05:35:24.680 --> 05:35:26.880
and, Geminis three and four.

05:35:28.760 --> 05:35:30.860
And then you start to look and, obviously,

05:35:31.320 --> 05:35:33.460
the the main question It will be,

05:35:33.460 --> 05:35:35.520
well, you know, you have all these algorithms. You have

05:35:35.841 --> 05:35:37.921
all of these things, you know, if I want to use it, when

05:35:37.921 --> 05:35:40.038
should I use it On

05:35:40.038 --> 05:35:42.359
which I have this problem, which algorithms I should use

05:35:42.359 --> 05:35:44.368
it. Unfortunately,

05:35:44.590 --> 05:35:46.609
we started to analyze it. I, unfortunately,

05:35:46.670 --> 05:35:48.810
I will not be able to give you a definitive answer

05:35:48.810 --> 05:35:51.210
to that. I'm just giving you some very preliminary

05:35:51.210 --> 05:35:53.220
observations. So

05:35:53.220 --> 05:35:55.460
the program structure in terms of size. So

05:35:55.460 --> 05:35:57.480
it it turns out that open Evolve

05:35:57.480 --> 05:35:59.980
in general is going to

05:36:00.320 --> 05:36:01.779
generate shorter programs.

05:36:02.410 --> 05:36:04.490
Shorter than Java and shorter than

05:36:04.490 --> 05:36:06.790
Shinka. By a factor

05:36:06.790 --> 05:36:08.700
of two. Or more.

05:36:09.251 --> 05:36:10.490
Okay? Shorter.

05:36:11.300 --> 05:36:13.480
And also, Gemini, it's also

05:36:14.690 --> 05:36:15.429
in general,

05:36:16.830 --> 05:36:19.001
generating much more compact

05:36:19.540 --> 05:36:21.759
programs. Than GPD five

05:36:21.759 --> 05:36:23.290
by a factor of 1.8.

05:36:24.010 --> 05:36:26.110
At the same time, when you look at the structure,

05:36:26.730 --> 05:36:28.269
while Gemini three

05:36:29.180 --> 05:36:30.081
code is shorter,

05:36:31.190 --> 05:36:32.810
GPT five is more modular.

05:36:33.270 --> 05:36:35.671
Like, you look in these examples for

05:36:35.671 --> 05:36:37.650
Germany g p I four

05:36:37.730 --> 05:36:39.970
g p I three, you have two functions. While

05:36:39.970 --> 05:36:42.090
g p five, you have nine functions.

05:36:42.630 --> 05:36:44.950
Right? So it's more modular. That's also

05:36:44.950 --> 05:36:47.150
one reason it's more code.

05:36:47.790 --> 05:36:48.270
Okay?

05:36:50.591 --> 05:36:52.671
So it turns out that the shorter

05:36:52.671 --> 05:36:54.130
programs in general

05:36:55.040 --> 05:36:56.980
tend to perform better. So OpenInvolve,

05:36:57.360 --> 05:36:59.470
it's wins in six cases

05:36:59.470 --> 05:37:01.690
versus Japan and Sri And Sri Lanka and Sri.

05:37:02.491 --> 05:37:04.270
The modularity seems to help.

05:37:04.751 --> 05:37:06.770
Especially in the case of JEPA,

05:37:06.911 --> 05:37:09.030
where the majority of the winds

05:37:09.030 --> 05:37:11.370
are coming from GPT five instead

05:37:11.590 --> 05:37:13.620
of Gemini. And

05:37:13.620 --> 05:37:15.800
one our hypothesis is that modular

05:37:15.860 --> 05:37:17.230
pro in in jet, you

05:37:19.070 --> 05:37:21.310
you override the entire program. Right? You

05:37:21.310 --> 05:37:23.429
regenerate the entire program. In the case

05:37:23.429 --> 05:37:25.280
of Open EVOLVE and Shinkai, it's on

05:37:25.520 --> 05:37:27.690
the block you indicated that has to be

05:37:27.690 --> 05:37:30.179
changed. So the more you are going

05:37:30.240 --> 05:37:32.630
to generate it's like it

05:37:32.630 --> 05:37:33.751
seems to us is that,

05:37:34.790 --> 05:37:37.130
if his code is modular, it's easier to reuse.

05:37:37.661 --> 05:37:40.062
Right? So if you have to rewrite the entire

05:37:40.062 --> 05:37:41.990
code, it's much more prone to

05:37:42.550 --> 05:37:44.630
failures and so forth. That's kind of

05:37:44.790 --> 05:37:46.150
again. And,

05:37:47.171 --> 05:37:49.411
also GPT five seems to handle better

05:37:49.411 --> 05:37:51.850
reach feedback. So the

05:37:51.990 --> 05:37:54.480
the Open EVOLVE is the simplest

05:37:54.779 --> 05:37:56.800
feedback. It gives you if you have the solution

05:37:57.019 --> 05:37:59.120
and the results. For that solution.

05:38:00.380 --> 05:38:02.550
For JEPA, you have like

05:38:02.550 --> 05:38:04.730
I mentioned, if you have if you

05:38:05.470 --> 05:38:07.650
run your solution on multiple benchmarks,

05:38:08.930 --> 05:38:10.982
then you report instead of

05:38:10.982 --> 05:38:13.062
the average, the result for each

05:38:13.062 --> 05:38:15.171
benchmark. Okay? And

05:38:15.310 --> 05:38:17.630
Shinka is the most provides you

05:38:17.630 --> 05:38:19.960
the most feedback because provides you

05:38:19.960 --> 05:38:22.440
also, you know, every 10 solution. It provides

05:38:22.440 --> 05:38:24.730
you the insights over all these

05:38:24.730 --> 05:38:26.930
10 solution or whatever. And also

05:38:26.930 --> 05:38:28.310
provides you some recommendations.

05:38:29.150 --> 05:38:29.540
Okay?

05:38:31.540 --> 05:38:33.581
So in Shinkai, JPT five wins,

05:38:33.581 --> 05:38:35.440
you know, the majority of cases,

05:38:35.600 --> 05:38:36.850
compared with the other ones.

05:38:38.130 --> 05:38:40.350
Okay. So now I am going to say a few

05:38:40.350 --> 05:38:42.591
lesson learned from this experience, which

05:38:42.591 --> 05:38:44.840
are communicated back to me by the student.

05:38:46.185 --> 05:38:48.220
So the first one I call

05:38:48.220 --> 05:38:50.460
this kind of it's a theme. Less is more and more

05:38:50.460 --> 05:38:52.890
is less. So you

05:38:52.950 --> 05:38:54.820
may expect that

05:38:55.460 --> 05:38:57.640
if you can start from the strongest

05:38:57.700 --> 05:38:59.810
baselines, you are

05:38:59.810 --> 05:39:02.120
going to get the best

05:39:03.160 --> 05:39:05.269
results. It turns out that's

05:39:05.269 --> 05:39:07.710
not the case. And sometimes,

05:39:08.490 --> 05:39:10.370
if you ask us starting from

05:39:10.690 --> 05:39:12.930
from a weaker baseline, you get a better result than

05:39:12.930 --> 05:39:14.570
you start from a strong baseline.

05:39:14.970 --> 05:39:17.210
We hypothesize this is because

05:39:17.210 --> 05:39:19.269
if you are in a strong you have a

05:39:19.269 --> 05:39:21.349
strong baseline, you are in kind of

05:39:21.349 --> 05:39:23.360
local minimum, and it's

05:39:23.360 --> 05:39:25.520
sometimes harder to get out from this local

05:39:25.520 --> 05:39:27.120
minimum. Okay?

05:39:28.448 --> 05:39:30.609
So here is one the strategy which is

05:39:30.609 --> 05:39:33.091
quite effective it's also,

05:39:33.790 --> 05:39:36.290
you know, is recommended by Open and World people.

05:39:36.591 --> 05:39:38.540
Is to use different baselines.

05:39:38.950 --> 05:39:40.900
It's a good strategy. Okay?

05:39:42.259 --> 05:39:44.280
The second one is about the hints.

05:39:44.591 --> 05:39:46.590
When you solve a problem, you

05:39:46.890 --> 05:39:49.128
know quite a bit about this problem. You have

05:39:49.128 --> 05:39:50.910
a hunch about what solution you

05:39:51.550 --> 05:39:53.170
should have and so forth. Right?

05:39:54.130 --> 05:39:56.200
Now it turns so and you

05:39:56.200 --> 05:39:58.280
know, the hints are good because

05:39:58.280 --> 05:40:00.610
you reduce a self space, because it's

05:40:00.671 --> 05:40:02.769
prevent directions. Right? Like,

05:40:02.769 --> 05:40:05.190
you advise a student and say, okay. This is a problem.

05:40:05.250 --> 05:40:06.470
Try z sync. Right?

05:40:08.070 --> 05:40:10.390
Now the fewer fees, hints, on

05:40:10.390 --> 05:40:12.180
the other hand, provides more freedom.

05:40:12.741 --> 05:40:14.821
Okay? And you can

05:40:14.821 --> 05:40:17.081
find better solutions. So

05:40:17.280 --> 05:40:19.460
you have to use the hints with care.

05:40:20.360 --> 05:40:22.440
Right? And for instance, one thing is don't

05:40:22.440 --> 05:40:24.630
use all the hints you know. And once,

05:40:25.730 --> 05:40:27.670
because you reduce the self space,

05:40:28.130 --> 05:40:30.300
and you the diversity. One

05:40:30.300 --> 05:40:32.540
thing you can do is you can have try different

05:40:32.540 --> 05:40:34.860
runs in each run, you have a different

05:40:34.920 --> 05:40:36.880
hint. Or different subset of hints.

05:40:37.360 --> 05:40:39.610
Or you can try this kind of multistage.

05:40:40.150 --> 05:40:42.010
You start with a few hints, if any,

05:40:42.260 --> 05:40:44.421
You get with some solutions, and maybe you're gonna

05:40:44.421 --> 05:40:45.450
get kind of stuck.

05:40:46.570 --> 05:40:48.870
You are going to now I'm

05:40:48.870 --> 05:40:51.029
going to give news other hints. It's like,

05:40:51.029 --> 05:40:53.251
again, it's a go and do

05:40:53.390 --> 05:40:55.640
this problem. And it's kinda, okay. I cannot make progress

05:40:55.640 --> 05:40:57.800
and so forth. Now you you start giving some hints.

05:40:57.800 --> 05:40:59.830
Right? So that's kind of

05:40:59.830 --> 05:41:01.720
the analogy. And the

05:41:01.859 --> 05:41:04.099
final one is restricting access to high level,

05:41:04.099 --> 05:41:06.240
say, library API. So it's a level

05:41:06.240 --> 05:41:08.480
of abstraction here can lead to better

05:41:08.480 --> 05:41:08.980
solutions.

05:41:10.820 --> 05:41:13.130
And, because

05:41:13.130 --> 05:41:15.210
a high level API, you can use

05:41:15.210 --> 05:41:17.220
this API and to generate the

05:41:17.220 --> 05:41:19.460
code faster, but you don't have this API,

05:41:19.460 --> 05:41:21.482
you can discover better optimization. Like,

05:41:21.482 --> 05:41:23.690
we we use libraries. Right? In many

05:41:23.690 --> 05:41:25.790
cases, we use libraries for our program.

05:41:25.929 --> 05:41:28.020
But in some cases, you know,

05:41:28.020 --> 05:41:30.180
this if we implement this

05:41:30.180 --> 05:41:32.440
kind of functionality from the libraries

05:41:32.440 --> 05:41:34.690
is not optimized for our use case. So if you don't

05:41:34.690 --> 05:41:36.730
use that and if you write yourself your

05:41:36.730 --> 05:41:38.790
functionality, you you are implementing

05:41:39.090 --> 05:41:41.090
yourself the functionality can be faster. So he see

05:41:41.330 --> 05:41:43.600
or he seems that with the PHP PAAP LB,

05:41:43.660 --> 05:41:45.841
you know, the providing PyTorch API,

05:41:45.900 --> 05:41:47.920
which If you provide the PyTorch API,

05:41:48.220 --> 05:41:50.720
it leads to reduce a custom operator with PyTorch.

05:41:51.650 --> 05:41:53.890
However, if you don't provide a PyTorch API,

05:41:53.890 --> 05:41:55.870
it leads to open evolve

05:41:55.929 --> 05:41:57.750
to direct directly optimize the

05:41:57.990 --> 05:41:59.830
custom operators and do a better job.

05:42:01.269 --> 05:42:03.509
Then is Cloudcast. This is another one. It's

05:42:03.509 --> 05:42:05.920
like, you know, it's

05:42:06.060 --> 05:42:08.520
like this is about streaming

05:42:08.661 --> 05:42:11.109
and moving the data between different

05:42:11.109 --> 05:42:13.189
clouds efficiently and, not

05:42:13.189 --> 05:42:15.470
spending too much money. Is like,

05:42:16.009 --> 05:42:17.790
providing the entire simulator

05:42:18.170 --> 05:42:20.241
to LLM at least to

05:42:20.241 --> 05:42:22.270
waste the LLM

05:42:22.410 --> 05:42:24.270
wasting times. You're looking at irrelevant

05:42:24.490 --> 05:42:26.710
code, like, for instance, saving generated pass.

05:42:27.190 --> 05:42:29.510
Has nothing to do with optimizing about

05:42:29.510 --> 05:42:31.790
how you are going to move the data between

05:42:31.790 --> 05:42:33.410
clouds or between cloud regions.

05:42:33.890 --> 05:42:35.930
Okay? So it's we do very careful

05:42:35.930 --> 05:42:37.770
about the level of abstraction you provide.

05:42:40.161 --> 05:42:42.661
The other thing is about the solution

05:42:43.690 --> 05:42:45.950
It's only as good as a evaluator.

05:42:46.760 --> 05:42:48.880
Okay? No question about that.

05:42:48.880 --> 05:42:51.040
Right? A float evaluator is a primary

05:42:51.040 --> 05:42:52.670
cause of flow solutions.

05:42:53.708 --> 05:42:54.030
Okay?

05:42:56.440 --> 05:42:58.060
And you need to prevent overfitting.

05:42:58.980 --> 05:43:01.160
Like, you know that. It's like, you know,

05:43:01.460 --> 05:43:03.660
so you need to divide use diverse

05:43:03.660 --> 05:43:05.800
workloads. And, you know, it's it's

05:43:05.860 --> 05:43:07.929
actually a lot of papers, system

05:43:07.929 --> 05:43:10.124
papers, are overfitted.

05:43:10.690 --> 05:43:12.930
Okay? Because what you do, you

05:43:12.930 --> 05:43:15.260
basically have a benchmark, and

05:43:15.501 --> 05:43:17.200
are going to beat that benchmark.

05:43:17.660 --> 05:43:19.640
That's it. Right? TPCC,

05:43:20.019 --> 05:43:22.099
you beat that benchmark. TPCS, you beat

05:43:22.099 --> 05:43:23.630
that benchmark. Okay?

05:43:24.270 --> 05:43:26.620
But if you think about that's yeah. That

05:43:26.940 --> 05:43:29.020
ultimate definition of overfitting. And

05:43:29.020 --> 05:43:31.520
in many system conferences, it's perfectly acceptable.

05:43:32.220 --> 05:43:34.650
But of course, in order to generalize,

05:43:34.790 --> 05:43:37.091
you try to have some holdout work workloads

05:43:37.091 --> 05:43:38.220
to do the testing.

05:43:39.340 --> 05:43:41.440
Prevent your hacking. Okay?

05:43:41.780 --> 05:43:43.400
Need to be very careful here.

05:43:43.860 --> 05:43:46.340
So here is like, for instance,

05:43:46.400 --> 05:43:48.520
in ePLB, is

05:43:48.581 --> 05:43:50.759
like, if you have

05:43:51.060 --> 05:43:53.310
an an expert, you know, is like you're

05:43:53.310 --> 05:43:55.430
safe to assign a fraction of an expert

05:43:55.430 --> 05:43:57.850
kind of to a GPU, which is less than

05:43:57.991 --> 05:44:00.171
one. It just is is like,

05:44:00.171 --> 05:44:02.290
do it is clump clump to zero,

05:44:02.349 --> 05:44:04.400
and then get rid of this experts, and then

05:44:04.400 --> 05:44:06.620
you have a you you allocate only a

05:44:06.860 --> 05:44:09.180
few experts and for which the load balancing

05:44:09.180 --> 05:44:10.070
looks very good.

05:44:11.429 --> 05:44:13.450
For the Cloudcast, if you are not careful,

05:44:13.750 --> 05:44:15.960
it reduces the data volume is transferring

05:44:15.960 --> 05:44:17.581
to get better results. Okay?

05:44:19.345 --> 05:44:21.820
You alleviate reward hacking?

05:44:21.820 --> 05:44:22.560
No secret.

05:44:23.831 --> 05:44:26.010
No no silver bullet. Comprehensive test

05:44:26.470 --> 05:44:28.360
and, it's also you

05:44:28.519 --> 05:44:30.840
another one, it's it it turns out it's pretty

05:44:30.840 --> 05:44:33.000
good, it's your constraints, the

05:44:33.000 --> 05:44:35.241
number of line changes. Right?

05:44:35.241 --> 05:44:37.321
It's similar with the k l divergence

05:44:37.321 --> 05:44:39.750
on penalty for RL. Right? You don't

05:44:39.750 --> 05:44:41.530
want to go too further

05:44:41.830 --> 05:44:43.650
from a solution which is kind of

05:44:44.030 --> 05:44:44.960
working. Right?

05:44:47.208 --> 05:44:49.529
Now another question is about you

05:44:49.529 --> 05:44:51.770
you run Actually, in this case, we assume

05:44:51.770 --> 05:44:53.930
that we have the solution, it runs, and you

05:44:53.930 --> 05:44:55.940
get the results. But

05:44:55.940 --> 05:44:58.100
in many cases, you can modify the code. You are

05:44:58.100 --> 05:45:00.198
going to have an error. Right? So

05:45:00.198 --> 05:45:02.160
what will happen in that case? One

05:45:02.220 --> 05:45:04.380
solution one one thinking about

05:45:04.540 --> 05:45:06.780
I I I this was my initial thing

05:45:06.780 --> 05:45:08.900
is that, hey. You just remove that

05:45:08.900 --> 05:45:10.921
solution. Right? Right? To

05:45:10.921 --> 05:45:12.740
ignore it. Right? It's like it's wrong.

05:45:14.179 --> 05:45:15.880
But it turns out

05:45:17.001 --> 05:45:19.260
that it's not always that clear

05:45:19.570 --> 05:45:21.810
because the wrong solution, which, you know, has an

05:45:21.810 --> 05:45:23.880
error, syntax error or something like that,

05:45:24.120 --> 05:45:25.660
may still have a good idea.

05:45:26.300 --> 05:45:28.460
Right? So maybe you want to have

05:45:28.460 --> 05:45:30.160
it part of the combined metric.

05:45:31.429 --> 05:45:33.480
Okay? With a very low small

05:45:33.540 --> 05:45:34.720
weight. Okay?

05:45:37.040 --> 05:45:39.280
So the next question is that, you

05:45:39.280 --> 05:45:41.290
know, you try to think then, okay, add

05:45:41.370 --> 05:45:42.751
this address, we call this

05:45:43.890 --> 05:45:45.410
address. It's like it's like,

05:45:46.780 --> 05:45:47.280
it's,

05:45:50.029 --> 05:45:52.241
and, you know,

05:45:52.241 --> 05:45:54.320
it works still quite well. So why do

05:45:54.320 --> 05:45:56.439
they work? You try to think start to think about that.

05:45:56.439 --> 05:45:58.859
And I think there are a few things.

05:45:59.000 --> 05:46:01.029
First of all, we are using

05:46:01.029 --> 05:46:02.570
it for performance problems.

05:46:03.520 --> 05:46:05.680
As a performance problems are kind of easier

05:46:05.680 --> 05:46:07.750
because when you write a simulator or when you

05:46:07.750 --> 05:46:09.830
are looking and you do the changes to improve

05:46:09.830 --> 05:46:11.890
the performance, in

05:46:11.890 --> 05:46:13.910
theory, you shouldn't impact

05:46:14.520 --> 05:46:16.600
the correctness of the system, the semantics. You

05:46:16.600 --> 05:46:18.860
shouldn't change the semantics of the system.

05:46:19.900 --> 05:46:22.060
Yes. In some cases, you do that, but

05:46:22.060 --> 05:46:24.540
in general, you shouldn't do it. Right? It's like

05:46:25.001 --> 05:46:26.760
and so it's a little bit easier.

05:46:27.081 --> 05:46:29.320
Right? The second one

05:46:30.660 --> 05:46:32.660
this is a funny one. It's like

05:46:32.900 --> 05:46:34.700
it's a a as a

05:46:35.000 --> 05:46:37.259
researcher, when you when you work on a problem,

05:46:38.710 --> 05:46:40.950
you are going to iterate until you get

05:46:40.950 --> 05:46:43.071
good enough solutions. The solutions

05:46:43.071 --> 05:46:45.152
are good enough. You are going to write

05:46:45.152 --> 05:46:47.240
a paper. But

05:46:47.460 --> 05:46:49.850
these things, you know, they don't

05:46:49.850 --> 05:46:51.630
know about it. They just keep going.

05:46:52.050 --> 05:46:53.490
Until they exhaust the budget.

05:46:54.609 --> 05:46:56.849
Right? So that's another one. The last,

05:46:56.968 --> 05:46:58.130
the the last one

05:46:59.010 --> 05:47:01.350
is that maybe it's, again, it's like,

05:47:02.290 --> 05:47:03.190
in some sense,

05:47:04.930 --> 05:47:07.091
these models, they do have an

05:47:07.091 --> 05:47:09.390
advantage. Okay. Maybe they have more advantage

05:47:09.870 --> 05:47:11.950
than than us. But one advantage

05:47:11.950 --> 05:47:13.960
is that they are trained on

05:47:13.960 --> 05:47:16.190
the entire literature from all the

05:47:16.190 --> 05:47:18.349
fields. And in many

05:47:18.349 --> 05:47:20.450
cases, some of the solutions reduces

05:47:20.590 --> 05:47:22.680
to apply some technique from

05:47:22.680 --> 05:47:24.410
one domain to your domain.

05:47:24.730 --> 05:47:26.750
But, of course, you need to know that technique.

05:47:27.400 --> 05:47:29.580
Okay? So LLMs

05:47:29.640 --> 05:47:31.960
probably are more better

05:47:32.340 --> 05:47:34.370
than most, to identify a technique from a

05:47:34.370 --> 05:47:36.150
different domain to apply to your domain.

05:47:36.550 --> 05:47:38.430
Okay? That's kind of what it is.

05:47:39.630 --> 05:47:41.710
While people, we specialize in one

05:47:41.710 --> 05:47:43.900
field. Right? And,

05:47:44.140 --> 05:47:46.240
here are some things we found, and

05:47:47.120 --> 05:47:49.439
you know, kind of evidence. Again, all the

05:47:49.439 --> 05:47:51.671
findings are very early. And you

05:47:51.671 --> 05:47:53.911
can see some of the techniques, which

05:47:53.911 --> 05:47:56.331
is the original domain it's in different,

05:47:56.550 --> 05:47:58.710
you know, political sciences, social

05:47:58.710 --> 05:48:00.510
social you

05:48:00.972 --> 05:48:03.060
know, social choice

05:48:03.118 --> 05:48:05.519
theory and electrical engineering. Okay. Electrical engineering

05:48:05.519 --> 05:48:06.640
is closer, but still.

05:48:08.640 --> 05:48:10.840
Okay. So

05:48:10.840 --> 05:48:13.080
next, I'm going to

05:48:14.960 --> 05:48:16.240
talk a little bit about,

05:48:17.280 --> 05:48:19.360
where we are and and try to I'm trying

05:48:19.360 --> 05:48:21.540
to be objective here. Okay? Maybe

05:48:21.540 --> 05:48:23.900
with our success. But, I think

05:48:24.060 --> 05:48:26.220
what we see is promising, but it's a lot

05:48:26.220 --> 05:48:28.340
of more work to be done. So

05:48:28.750 --> 05:48:30.830
this fall after

05:48:30.830 --> 05:48:32.330
the seminar in the summer,

05:48:32.870 --> 05:48:34.490
we we had this class

05:48:36.029 --> 05:48:38.060
in, at at Berkeley,

05:48:39.590 --> 05:48:41.530
and it was 33, mostly PhD

05:48:41.750 --> 05:48:43.990
students. And at the end of the class, actually,

05:48:43.990 --> 05:48:46.480
this Monday, we had a survey.

05:48:46.618 --> 05:48:48.080
And here are a few results.

05:48:51.110 --> 05:48:53.341
The first one, you know, did you

05:48:53.341 --> 05:48:55.420
get a gain here, right, for your problem?

05:48:55.420 --> 05:48:57.560
Everyone applying for their own

05:48:58.019 --> 05:48:58.660
problems. And,

05:49:00.350 --> 05:49:01.661
you can see,

05:49:02.690 --> 05:49:04.720
first is that significant gain

05:49:04.720 --> 05:49:06.421
over 5% improvement

05:49:06.800 --> 05:49:08.180
is 29%.

05:49:09.280 --> 05:49:11.380
Yes. Game, but more the marginal,

05:49:11.679 --> 05:49:14.110
it was 32%. So

05:49:14.251 --> 05:49:15.910
totally 61% pretty good.

05:49:17.050 --> 05:49:19.368
Now it might have performance. So in 61%,

05:49:19.368 --> 05:49:20.990
you can guide of better results.

05:49:22.500 --> 05:49:24.330
You know, as some definition of better.

05:49:26.280 --> 05:49:28.080
Are the primary bottlenecks?

05:49:28.640 --> 05:49:31.060
Well, here are the bottlenecks, and a lot of them,

05:49:31.198 --> 05:49:32.179
you can see

05:49:33.410 --> 05:49:35.490
context window limitations. Right? Because

05:49:35.490 --> 05:49:37.260
the context is growing if you add

05:49:37.501 --> 05:49:39.581
and more. But it's not only as that.

05:49:39.581 --> 05:49:41.900
The more is a the bigger the context, as

05:49:41.900 --> 05:49:44.300
you know, the less precise the LMM

05:49:44.300 --> 05:49:46.520
is. Right? It's not about 1,000,000

05:49:46.520 --> 05:49:48.600
or 1,000,000,000 context window. You

05:49:48.600 --> 05:49:50.280
don't exhaust that. Right?

05:49:52.440 --> 05:49:54.600
And then, you have the round time

05:49:54.600 --> 05:49:57.100
failures. The incorrect evaluations,

05:49:57.400 --> 05:49:59.650
and evaluation latency. So the

05:49:59.650 --> 05:50:01.570
simulations becomes a bottleneck.

05:50:01.970 --> 05:50:02.470
Okay?

05:50:05.581 --> 05:50:07.280
And then inefficient search.

05:50:08.001 --> 05:50:09.740
Right? It takes the search, it takes too long.

05:50:10.939 --> 05:50:11.010
So

05:50:13.190 --> 05:50:15.270
So another question is that

05:50:15.270 --> 05:50:17.210
how much heavy lifting did you do?

05:50:17.850 --> 05:50:19.950
Ideally, you give the prompt, give

05:50:19.950 --> 05:50:22.030
the problem, go away, come back when

05:50:22.030 --> 05:50:24.490
you have a better result, good.

05:50:25.534 --> 05:50:27.470
So the fully auto

05:50:27.630 --> 05:50:29.710
autonomous was only in twelve percent of the

05:50:29.710 --> 05:50:31.630
cases. Occasional

05:50:31.849 --> 05:50:34.280
nudges, forty percent. And

05:50:34.500 --> 05:50:36.520
heavy supervision is 40%.

05:50:36.870 --> 05:50:39.030
And I think the students probably will tell

05:50:39.030 --> 05:50:41.040
you probably it's the heavy supervision. It's

05:50:41.040 --> 05:50:42.440
more than 40%. Right?

05:50:43.720 --> 05:50:45.550
But, again, this is and the

05:50:46.030 --> 05:50:48.350
the the the last one, which I think is a ultimate,

05:50:48.350 --> 05:50:50.490
you know, from my perspective, this is a real

05:50:51.370 --> 05:50:53.529
question. Okay. Are you going to use this in your

05:50:53.529 --> 05:50:55.830
work? Right? That's the ultimate question.

05:50:55.830 --> 05:50:56.290
Right?

05:50:58.210 --> 05:51:00.034
So 59

05:51:01.220 --> 05:51:01.880
said yes.

05:51:03.911 --> 05:51:05.930
You know, and the rest

05:51:05.930 --> 05:51:08.250
maybe or no. Right? They were nice. The people

05:51:08.570 --> 05:51:09.948
you know, like, oh, maybe.

05:51:11.288 --> 05:51:13.519
You know, I'll come back in a few months if they're

05:51:13.519 --> 05:51:15.690
improved. Right? If they are improved, you

05:51:15.690 --> 05:51:16.890
know, maybe I'm going to use them.

05:51:17.770 --> 05:51:19.929
So, you know, these are the results. Of course, they are biased

05:51:19.929 --> 05:51:22.050
because these people are

05:51:22.050 --> 05:51:24.090
taking the class. So it's a little bit of some

05:51:24.090 --> 05:51:26.380
cost. You know? They know, they spend

05:51:26.380 --> 05:51:28.880
all this time in the class, you know, of all the pain.

05:51:29.340 --> 05:51:31.460
So has to work be worth something, so I'm

05:51:31.460 --> 05:51:32.841
going to use it. Right?

05:51:34.280 --> 05:51:36.520
Okay. So about research challenges,

05:51:36.520 --> 05:51:38.790
about the good evaluation,

05:51:38.929 --> 05:51:41.010
you evaluators are the key. They

05:51:41.010 --> 05:51:43.091
need to be fast, faithful,

05:51:43.091 --> 05:51:45.109
and correct. I think it's

05:51:45.109 --> 05:51:47.290
a lot of interesting research here.

05:51:48.120 --> 05:51:50.220
You may even build problem specific

05:51:50.280 --> 05:51:52.380
simulators. If especially if you

05:51:52.380 --> 05:51:54.402
have a big system. Right, like

05:51:54.402 --> 05:51:56.661
an operating system. What does it mean that? Right?

05:51:56.680 --> 05:51:58.698
Again, if you look only at the scheduling, at the

05:51:58.698 --> 05:52:00.790
disc and so forth, you can build simulators

05:52:00.790 --> 05:52:03.029
only for that, and you abstract away the rest

05:52:03.029 --> 05:52:03.849
of the system.

05:52:06.840 --> 05:52:08.970
Then, cascading

05:52:09.030 --> 05:52:11.450
evaluators. You can have different kind of evaluators.

05:52:11.509 --> 05:52:13.780
You can you can start from parametric models,

05:52:13.780 --> 05:52:15.860
which are very easy to check, but they are not

05:52:15.860 --> 05:52:17.730
very accurate. Although, to emulators

05:52:17.870 --> 05:52:18.849
and real systems.

05:52:20.980 --> 05:52:23.060
You can also leverage form a certification.

05:52:23.460 --> 05:52:25.620
Of course, one of the interesting direction is to go

05:52:25.620 --> 05:52:26.840
beyond system performance.

05:52:28.340 --> 05:52:30.010
Good evaluators will be the key.

05:52:30.652 --> 05:52:32.640
And also efficient search.

05:52:33.040 --> 05:52:35.090
Right? So

05:52:35.890 --> 05:52:37.189
I think it's

05:52:38.400 --> 05:52:39.760
so now we have,

05:52:40.720 --> 05:52:42.860
also announced it. Like, we have also

05:52:42.860 --> 05:52:45.180
a leaderboard for our problems here,

05:52:45.180 --> 05:52:47.020
which we published published today.

05:52:47.402 --> 05:52:49.820
And, we have a bunch of

05:52:49.960 --> 05:52:52.130
solution of of all these kind

05:52:52.130 --> 05:52:53.950
of frameworks I mentioned to you.

05:52:54.270 --> 05:52:56.170
We also developed

05:52:56.410 --> 05:52:58.490
as part of this class and

05:52:58.490 --> 05:52:58.990
effort,

05:53:00.860 --> 05:53:03.020
AutoEvolve, another framework, which is based

05:53:03.020 --> 05:53:05.060
on OpenEvolve, And, ideally,

05:53:05.060 --> 05:53:07.458
what it tries to do not ideally, is,

05:53:07.458 --> 05:53:09.540
like, at the high level, what it tries to do

05:53:09.540 --> 05:53:11.599
is to dynamically adjust always to have

05:53:11.599 --> 05:53:13.300
exploration versus exploitation.

05:53:13.620 --> 05:53:15.850
Right? So when you appear

05:53:15.850 --> 05:53:18.030
to get stuck, you want to increase

05:53:18.091 --> 05:53:20.140
exploration. You are going

05:53:20.298 --> 05:53:22.378
when you grow quick, when your performance

05:53:22.378 --> 05:53:24.429
grow quickly, you are good. You know,

05:53:24.429 --> 05:53:26.900
you want to exploit, to do more exploitation.

05:53:29.341 --> 05:53:31.501
One thing we do it, which seems to be very

05:53:31.501 --> 05:53:33.600
pretty good, is to have different prompts

05:53:33.600 --> 05:53:35.620
when you do exploration versus exploitation.

05:53:36.460 --> 05:53:38.939
And the other one is when the things

05:53:39.179 --> 05:53:41.400
when you generate code has an error,

05:53:41.671 --> 05:53:43.450
just ask to retry it. That's kind of

05:53:43.770 --> 05:53:45.390
pretty obvious thing. Right?

05:53:46.400 --> 05:53:48.429
Okay. And, it's working

05:53:48.429 --> 05:53:50.210
quite well, except one case.

05:53:50.540 --> 05:53:51.920
Is providing the best results.

05:53:53.349 --> 05:53:55.590
For all these. Nine out of 10 provides

05:53:55.590 --> 05:53:56.650
the best results.

05:53:57.810 --> 05:53:59.750
And, yeah, this is I got another one. Okay.

05:54:00.390 --> 05:54:02.260
So two other thing before finishing.

05:54:03.062 --> 05:54:04.730
So where where does it leave us?

05:54:05.210 --> 05:54:07.450
Right? Because it's a lot of question. Because the paper we

05:54:07.450 --> 05:54:09.910
had is very controversial. Title.

05:54:10.620 --> 05:54:11.440
On purpose.

05:54:13.171 --> 05:54:15.030
So, obviously, these are tools.

05:54:15.390 --> 05:54:17.009
Right? But you the way to think about

05:54:17.595 --> 05:54:19.609
them it's, again, so

05:54:19.609 --> 05:54:21.628
useful is an AI research assistant.

05:54:21.960 --> 05:54:22.970
It's exactly what it is.

05:54:24.130 --> 05:54:26.150
Which will elevate, hopefully, you

05:54:26.501 --> 05:54:28.760
So instead of spending time on developing

05:54:28.821 --> 05:54:30.890
solutions, spend time on selecting

05:54:30.890 --> 05:54:32.930
which problem to solve, formulating the

05:54:32.930 --> 05:54:35.110
problem carefully, and evaluation strategies.

05:54:35.866 --> 05:54:38.180
Right? And if this

05:54:38.180 --> 05:54:39.890
going is going to be

05:54:40.210 --> 05:54:42.630
very successful, the limitation will be our

05:54:43.090 --> 05:54:45.316
time to manage this

05:54:46.300 --> 05:54:48.580
AI assistance. Right? So it's a multiplier.

05:54:49.788 --> 05:54:51.849
Right? I think that's kind

05:54:51.849 --> 05:54:54.210
of exciting. So we can really accelerate

05:54:54.210 --> 05:54:56.260
the research. That's basically what it is.

05:54:57.900 --> 05:55:00.060
So one last thought, because this is what, at

05:55:00.060 --> 05:55:02.160
least personally, why I'm so excited

05:55:02.380 --> 05:55:04.660
about direction. Is because

05:55:05.316 --> 05:55:05.680
I

05:55:08.251 --> 05:55:08.751
today,

05:55:10.800 --> 05:55:13.060
building this software stack

05:55:13.060 --> 05:55:15.440
for AI it's very

05:55:15.440 --> 05:55:17.080
complex. More complex than ever.

05:55:17.800 --> 05:55:19.850
And why is that? For some very good

05:55:19.850 --> 05:55:22.100
reasons. First of all, is

05:55:22.640 --> 05:55:25.140
that, you know, we we build that very clearly distributed

05:55:25.360 --> 05:55:27.370
system since know, fifteen years

05:55:27.370 --> 05:55:29.790
ago. And then it was

05:55:30.270 --> 05:55:32.330
yes. Zero distributed system, but the

05:55:32.330 --> 05:55:34.509
world was much simpler. It was a homogeneous

05:55:34.650 --> 05:55:36.960
distributed system, have one server. The server has

05:55:36.960 --> 05:55:38.902
a CPU, hard disk drives, they're connected

05:55:39.040 --> 05:55:41.310
by Ethernet. That's it. Okay?

05:55:41.370 --> 05:55:43.460
And even the low workloads like

05:55:44.100 --> 05:55:46.440
we started with Spark and so forth, they are

05:55:46.910 --> 05:55:49.310
you know, bulk synchronous processing

05:55:49.310 --> 05:55:51.490
models, regular parallelism, you know,

05:55:51.650 --> 05:55:53.570
things like that. Today,

05:55:53.970 --> 05:55:56.160
you look at the infrastructure, it's far more complex.

05:55:56.320 --> 05:55:58.480
We have a myriad of accelerators, of

05:55:58.480 --> 05:56:00.350
course, Yeah. Oh, okay.

05:56:00.991 --> 05:56:03.210
Sorry. Yeah. Yeah. I'll I'll be I'll done. I'll say

05:56:03.210 --> 05:56:05.368
that. In the VLM, we follow how

05:56:05.368 --> 05:56:07.323
many accelerators type we have to

05:56:07.382 --> 05:56:08.429
we have. We see

05:56:09.390 --> 05:56:11.590
400. Okay? And the

05:56:11.590 --> 05:56:13.609
networking, it's also very complicated

05:56:13.670 --> 05:56:16.070
now. And the application is very

05:56:16.130 --> 05:56:18.390
complicated now. And I don't

05:56:18.390 --> 05:56:20.470
know, like, and things are evolving much

05:56:20.470 --> 05:56:22.890
faster. You have a GPU every year.

05:56:23.581 --> 05:56:25.581
From NVIDIA. By the time we can actually

05:56:25.741 --> 05:56:27.780
our system to make it well, kind of

05:56:27.780 --> 05:56:30.120
well for the current generation, the new generation

05:56:30.500 --> 05:56:32.560
arrive. And you want to the

05:56:32.560 --> 05:56:34.640
performance is a is king, which means

05:56:34.640 --> 05:56:36.460
that you this modularity and

05:56:36.679 --> 05:56:38.990
so forth you know, that's kind of in theory.

05:56:38.990 --> 05:56:41.090
You kind of you do cross layer optimization.

05:56:41.610 --> 05:56:44.110
So for all these reasons, this stack

05:56:44.171 --> 05:56:46.220
is very complicated. You cannot

05:56:46.220 --> 05:56:48.491
keep up. Right?

05:56:48.630 --> 05:56:50.850
So now think about if you

05:56:50.850 --> 05:56:53.200
can't do it 10 times faster,

05:56:53.200 --> 05:56:55.430
100 times faster? Now that will

05:56:55.430 --> 05:56:57.765
be a material change. Right?

05:56:58.341 --> 05:57:00.200
To advance, everyone.

05:57:01.420 --> 05:57:02.400
Okay? So,

05:57:04.230 --> 05:57:05.689
so in summary, again,

05:57:06.790 --> 05:57:08.868
system we believe that system

05:57:08.868 --> 05:57:10.650
is also changing the system research.

05:57:11.890 --> 05:57:14.010
And you cannot ignore it. There are some sort

05:57:14.010 --> 05:57:16.120
of solution. You can say that oh, for

05:57:16.120 --> 05:57:18.060
the majority of problems, it doesn't

05:57:18.200 --> 05:57:20.310
work. But for some problem, it does work. And

05:57:20.310 --> 05:57:22.550
you can expect that the set of problems is going

05:57:22.550 --> 05:57:23.900
to work. It's just going to grow.

05:57:25.740 --> 05:57:27.820
And there are many questions to remain.

05:57:28.060 --> 05:57:30.300
Remain to be addressed, so it's a lot of

05:57:30.300 --> 05:57:32.310
exciting exciting there are a

05:57:32.310 --> 05:57:33.200
of exciting

05:57:34.800 --> 05:57:36.878
research, problems.

05:57:37.820 --> 05:57:40.050
And I think at the end of the day,

05:57:40.210 --> 05:57:42.480
what you need to that might take away,

05:57:42.480 --> 05:57:44.480
it always, you know, it's like it's about

05:57:44.560 --> 05:57:45.620
you need to refocus.

05:57:46.780 --> 05:57:48.700
From really only solving the problem

05:57:48.940 --> 05:57:50.480
and iterating on the solutions

05:57:51.640 --> 05:57:53.980
to focusing which problems you are going to solve,

05:57:53.980 --> 05:57:56.220
and how you are going to evaluate it.

05:57:56.620 --> 05:57:58.700
You are going to demonstrate that is the

05:57:58.860 --> 05:58:00.640
you you solve the problem successfully.

05:58:01.760 --> 05:58:03.950
Okay? And fundamental, it's again, it's like

05:58:04.110 --> 05:58:06.270
the world is changing so fast. So

05:58:06.270 --> 05:58:08.751
we have to look at this as a way

05:58:08.751 --> 05:58:10.860
to help us to keep up with

05:58:10.860 --> 05:58:13.099
these changes because otherwise, there is no way we can keep

05:58:13.099 --> 05:58:15.100
up. Thank you.

05:58:23.450 --> 05:58:25.860
Alright. I think we have time for a few questions.

05:58:26.120 --> 05:58:26.620
And,

05:58:29.720 --> 05:58:31.780
kinda debaters come to

05:58:31.780 --> 05:58:33.000
us first too?

05:58:34.458 --> 05:58:36.580
Okay. Hi. Jan. Good to see you

05:58:36.580 --> 05:58:38.820
again. I'm Thomas Chirsky, one

05:58:38.820 --> 05:58:39.560
of the coauthors

05:58:41.948 --> 05:58:44.029
My question is, much of the systems

05:58:44.029 --> 05:58:46.130
research assumes sort of

05:58:46.189 --> 05:58:48.300
a fixed goal point. And then we

05:58:48.300 --> 05:58:50.341
optimize systems around maybe

05:58:50.341 --> 05:58:51.860
a benchmark or, you know, you mentioned,

05:58:52.740 --> 05:58:54.765
one particular benchmark we sort of overfit to

05:58:54.765 --> 05:58:57.080
that in the real world, we

05:58:57.080 --> 05:58:59.320
have more dynamic systems where that goal

05:58:59.320 --> 05:59:01.350
point is constantly evolving.

05:59:01.831 --> 05:59:04.071
How do you think, you know, methodologies like

05:59:04.071 --> 05:59:06.170
evolutionary methods will

05:59:06.170 --> 05:59:08.220
need to be improved or fixed

05:59:08.460 --> 05:59:10.700
in order to move with that goal point?

05:59:10.700 --> 05:59:12.929
For example, perhaps there's

05:59:12.929 --> 05:59:15.009
lessons from your your experience at Databricks or

05:59:15.009 --> 05:59:17.220
any scale where you have constantly evolving

05:59:17.359 --> 05:59:19.140
systems that need to be constantly tweaked.

05:59:20.580 --> 05:59:22.430
Yeah. I I I think that's exactly the problem

05:59:22.831 --> 05:59:24.930
The the problem is the the systems

05:59:24.991 --> 05:59:26.740
everywhere infrastructure

05:59:27.118 --> 05:59:29.198
as the application becoming more complex, they

05:59:29.198 --> 05:59:31.200
are changing continuously. So what you

05:59:31.200 --> 05:59:33.360
are going to do about the systems and the

05:59:33.360 --> 05:59:35.520
stack in between Because like

05:59:35.520 --> 05:59:37.780
you said, this was the assumption. The assumption

05:59:38.070 --> 05:59:40.210
is that know, it's pretty fixed. The hardware

05:59:40.210 --> 05:59:42.689
is pretty fixed. You know, it's x 86

05:59:42.830 --> 05:59:45.070
servers and so forth. The application is pretty

05:59:45.790 --> 05:59:48.029
fixed. Web app web application, and you

05:59:48.029 --> 05:59:50.160
only have a LAMP stack. And that's pretty much

05:59:50.160 --> 05:59:52.460
it for the next fifteen years. This is gone.

05:59:53.341 --> 05:59:55.421
Right? So the the question is that, yeah, it's

05:59:55.421 --> 05:59:57.921
exactly how fast you can but fundamentally,

05:59:58.220 --> 06:00:00.720
you still have to assume when you develop something,

06:00:00.800 --> 06:00:02.900
you need to assume something about the boundaries.

06:00:04.050 --> 06:00:06.160
Right? There is fundamentally, you have to

06:00:06.160 --> 06:00:08.187
do it. Because otherwise, how you

06:00:08.187 --> 06:00:10.250
test it? How you make sure that you are going to

06:00:10.250 --> 06:00:12.270
work with that the other thing the other side.

06:00:12.730 --> 06:00:14.970
Right? So but if you are going so

06:00:14.970 --> 06:00:17.259
so say, you know, if you can if

06:00:17.480 --> 06:00:19.482
you can build

06:00:19.482 --> 06:00:21.570
a system, right, in one

06:00:21.570 --> 06:00:23.679
day, Right? As complex

06:00:23.679 --> 06:00:25.970
as, you know, original Mammoth or Spanish

06:00:25.970 --> 06:00:28.170
or something like that. Things are going to be very

06:00:28.170 --> 06:00:30.240
different. If your workload changes tomorrow,

06:00:30.240 --> 06:00:32.310
it's okay. I'm going to build another system, or

06:00:32.310 --> 06:00:34.382
I'm going to evolve the system. So that's kind of

06:00:34.382 --> 06:00:36.560
the thing. It's about the speed of the of the

06:00:36.560 --> 06:00:38.770
speed. Can you develop faster than the

06:00:38.770 --> 06:00:40.880
speed of changes? And now you cannot. It's

06:00:40.880 --> 06:00:41.300
very hard.

06:00:44.340 --> 06:00:46.460
Yeah. Hi. I'm Reika

06:00:46.520 --> 06:00:48.841
Singhal from Tata Consultancy Services,

06:00:48.841 --> 06:00:50.859
industry side side. So my question is,

06:00:51.099 --> 06:00:53.321
we look at system as a source code

06:00:53.321 --> 06:00:55.341
primarily here. What how about the deployment architecture,

06:00:55.640 --> 06:00:58.020
especially when we look at edge cloud deployment

06:00:58.160 --> 06:01:00.240
and then generating the source code for the

06:01:00.240 --> 06:01:02.300
whole system, and it becomes, you know, chicken and

06:01:02.300 --> 06:01:04.340
egg problem, but because first you have a

06:01:04.820 --> 06:01:07.060
code for the, architecture which

06:01:07.060 --> 06:01:09.000
you have in mind from a solution architect

06:01:09.421 --> 06:01:11.501
or you develop the code and then see

06:01:11.501 --> 06:01:13.660
how do you want to deploy it efficiently, like

06:01:13.660 --> 06:01:15.980
algorithm. So how do you solve it? Is it a joint

06:01:15.980 --> 06:01:18.009
optimization problem or so? Yeah. It has

06:01:18.009 --> 06:01:20.410
to be joint optimization problem. But what I want to say

06:01:20.410 --> 06:01:22.260
is that fundamentally,

06:01:22.960 --> 06:01:24.900
because you have to test this model,

06:01:25.060 --> 06:01:27.140
Actually, the testing will be even more and

06:01:27.140 --> 06:01:29.160
more important. Right?

06:01:29.160 --> 06:01:31.500
You need to have some specification

06:01:33.232 --> 06:01:35.020
specification testing for these problems.

06:01:35.421 --> 06:01:37.501
Right? So the testing is going to be harder

06:01:37.501 --> 06:01:39.609
and harder, and I don't think it's I

06:01:39.609 --> 06:01:41.760
I don't think this is I'm not

06:01:41.760 --> 06:01:43.860
advocating here for a more ad hoc

06:01:43.920 --> 06:01:45.940
world. What I'm advocating here, a

06:01:45.940 --> 06:01:47.780
more hopefully principled world.

06:01:48.421 --> 06:01:50.501
Right, which should enable to define what you

06:01:50.501 --> 06:01:52.630
want to do more precisely and there we need to

06:01:52.630 --> 06:01:54.950
spend a lot of time. And doing

06:01:55.091 --> 06:01:57.330
it, that should be fast. Right?

06:01:57.628 --> 06:01:59.788
But if you think about that, it has to

06:01:59.788 --> 06:02:01.910
be just testing and so forth, the

06:02:01.910 --> 06:02:04.010
faster you move, the more automated

06:02:04.070 --> 06:02:05.840
it has to be, the better it has to be.

06:02:07.760 --> 06:02:09.390
Okay. This is the last question.

06:02:14.868 --> 06:02:15.769
One more question.

06:02:20.279 --> 06:02:22.470
Hi, professor. Thank you very much

06:02:22.470 --> 06:02:24.790
for the talk. Do you think these tools actually

06:02:24.790 --> 06:02:27.230
can expand who can become

06:02:27.230 --> 06:02:29.380
a researcher? Because maybe

06:02:29.440 --> 06:02:31.600
you have people that are because of their domain knowledge or

06:02:31.600 --> 06:02:33.812
other skills, are good at

06:02:33.812 --> 06:02:35.750
formuling problems or evaluation. But

06:02:35.910 --> 06:02:38.270
may not be necessary good problem solvers.

06:02:38.410 --> 06:02:38.820
Right?

06:02:40.820 --> 06:02:42.920
I think that TELUS will expand incrementally.

06:02:43.160 --> 06:02:44.520
To get more and more

06:02:45.560 --> 06:02:47.640
but for researchers, I mean, it's like who

06:02:47.640 --> 06:02:49.670
is you still need probably at

06:02:49.670 --> 06:02:51.800
least a way I'm thinking about this

06:02:51.800 --> 06:02:54.050
today, still need to, at

06:02:54.050 --> 06:02:56.439
some level, you need to

06:02:56.439 --> 06:02:58.510
specify pretty clear what needs to be done.

06:02:58.911 --> 06:03:01.232
Right? I think that what will happen

06:03:01.232 --> 06:03:02.980
is probably you're going to go

06:03:04.860 --> 06:03:06.940
to to to do that at a higher level of obstruction

06:03:06.940 --> 06:03:09.070
and these things to do more together. Right?

06:03:09.070 --> 06:03:11.330
And, you know, you can do whatever. But eventually,

06:03:11.470 --> 06:03:13.251
you have to trust Right?

06:03:13.550 --> 06:03:15.180
This is this is in some sense

06:03:15.741 --> 06:03:17.982
is kind of boggles my mind. It's like because this is the biggest

06:03:17.982 --> 06:03:20.150
problem. The biggest problem is reliability.

06:03:21.152 --> 06:03:23.250
Right? It's like with AI. That's where you should

06:03:23.250 --> 06:03:25.241
spend the time. Right? Because

06:03:25.380 --> 06:03:27.440
if you think about and I'm going

06:03:27.440 --> 06:03:29.600
to end up with this with this rant, so to

06:03:29.600 --> 06:03:31.810
speak. You think about software

06:03:31.810 --> 06:03:33.500
engineering, how many of you here

06:03:34.380 --> 06:03:35.439
are software engineers?

06:03:36.800 --> 06:03:38.960
Okay. Where do you spend that time as a

06:03:38.960 --> 06:03:39.860
software engineer?

06:03:41.930 --> 06:03:43.880
Right? Do you spend time for building

06:03:44.200 --> 06:03:46.520
prototypes and feature, you know, demonstrating the feature?

06:03:46.520 --> 06:03:48.991
No. But you spend most of the time is

06:03:48.991 --> 06:03:51.460
taking the features and productizing them.

06:03:52.260 --> 06:03:54.250
Right? This is what you do.

06:03:55.120 --> 06:03:57.220
Right? Of course you might do observe, but

06:03:57.500 --> 06:03:59.790
this is what you do. Most of your time.

06:04:00.091 --> 06:04:01.400
At least based on what I know.

06:04:02.790 --> 06:04:05.110
So but that what it is, is making these features

06:04:05.110 --> 06:04:07.250
working reliable in all these

06:04:07.250 --> 06:04:09.490
use cases. So that's capital with the

06:04:09.490 --> 06:04:11.341
software engineering. Right?

06:04:12.290 --> 06:04:14.370
The same has to happen here, right, is

06:04:14.370 --> 06:04:16.675
the reliability, predictability. Are

06:04:16.675 --> 06:04:18.670
key. To trust these tools.

06:04:19.550 --> 06:04:21.630
To trust your application. That's where you spend

06:04:21.630 --> 06:04:23.900
the time. Yeah.

06:04:24.700 --> 06:04:25.720
Thank you.

06:04:46.660 --> 06:04:47.960
Is Dylan here?

06:04:48.730 --> 06:04:50.841
Oh, okay. Oh,

06:04:50.841 --> 06:04:53.030
wait. Maybe you switched sides.

06:04:53.190 --> 06:04:55.410
So you are together. Okay.

06:04:55.410 --> 06:04:56.690
Because it's like, yes and no.

06:04:57.660 --> 06:04:59.921
There. Who is no? You're

06:04:59.921 --> 06:05:02.161
slight. Yeah. Think you know.

06:05:02.161 --> 06:05:03.430
Right? Yeah. Are you not

06:05:04.380 --> 06:05:06.690
Yeah. He's He's no.

06:05:14.421 --> 06:05:15.880
And, know?

06:05:53.179 --> 06:05:53.540
Yeah.

06:06:22.560 --> 06:06:24.270
Right. We're going to be

06:06:24.911 --> 06:06:27.000
hello? We're going to

06:06:27.000 --> 06:06:28.260
be starting our debate

06:06:29.921 --> 06:06:32.001
It's a very exciting topic. Be

06:06:32.001 --> 06:06:34.380
asking this the panelist

06:06:34.380 --> 06:06:35.730
here the following question.

06:06:36.690 --> 06:06:38.759
Will AI agents replace systems

06:06:38.759 --> 06:06:41.198
developers? Now we'll have two teams.

06:06:41.250 --> 06:06:43.620
Whose positions are balanced. First,

06:06:43.620 --> 06:06:45.779
we'll have the proposition, meaning that they think

06:06:45.779 --> 06:06:48.060
that systems developers will be replaced

06:06:48.198 --> 06:06:50.310
by AI agents. So that's a

06:06:50.310 --> 06:06:52.429
replacement camp. First, I'll introduce Jason

06:06:53.618 --> 06:06:55.950
Jason's a research scientist at meta.ai.

06:06:56.510 --> 06:06:58.210
And a technical lead PyTorch Compilers.

06:06:58.751 --> 06:07:01.029
He started the Torch Dynamo

06:07:01.029 --> 06:07:02.960
and Torch in doctor

06:07:03.040 --> 06:07:05.380
projects. Which brings flexible

06:07:05.380 --> 06:07:07.420
graph capture and a high performance compiler

06:07:07.900 --> 06:07:08.630
to PyTorch two.

06:07:10.150 --> 06:07:12.360
He received his PhD from

06:07:12.421 --> 06:07:14.581
MIT and has over fifteen years of

06:07:14.581 --> 06:07:16.760
experience in machine learning, compilers,

06:07:17.540 --> 06:07:18.350
programming languages.

06:07:19.790 --> 06:07:21.020
Next, we have

06:07:22.220 --> 06:07:23.921
next, we have Dylan Patel.

06:07:25.050 --> 06:07:27.001
The founder, CEO, and chief analyst

06:07:27.366 --> 06:07:28.591
of SimeAnalysis.

06:07:29.421 --> 06:07:31.581
The preeminent authority on all

06:07:31.581 --> 06:07:32.720
things AI and semiconductors.

06:07:33.770 --> 06:07:35.230
So do ask questions.

06:07:37.990 --> 06:07:39.939
Do you want me to continue the introductions?

06:07:43.280 --> 06:07:45.440
You can check out his Wikipedia page.

06:07:46.720 --> 06:07:48.341
Next, we have the opposition.

06:07:50.610 --> 06:07:52.850
Meaning that they do not think that AI agents

06:07:52.850 --> 06:07:54.310
will replace system developers.

06:07:55.280 --> 06:07:56.820
First, we have our speaker, Niraja.

06:07:58.320 --> 06:08:00.500
She's an assistant professor in the

06:08:00.560 --> 06:08:02.660
department of ECE, IUT Austin.

06:08:03.320 --> 06:08:05.400
She's a cloud computing system researcher with

06:08:05.400 --> 06:08:07.190
a strong background in machine learning.

06:08:07.911 --> 06:08:10.152
Her work straddle the boundaries of systems and

06:08:10.152 --> 06:08:12.290
ML. Specifically advances in

06:08:12.290 --> 06:08:14.730
systems, machine learning, and hardware architecture

06:08:14.730 --> 06:08:16.780
that are about to launch a new era.

06:08:16.780 --> 06:08:18.860
In which we can use the entire cloud

06:08:18.860 --> 06:08:19.680
as a computer.

06:08:20.982 --> 06:08:23.081
On the other hand, new ML architectures

06:08:23.140 --> 06:08:25.070
are being developed for solving complex

06:08:25.630 --> 06:08:27.800
management problems in systems. Similarly,

06:08:27.800 --> 06:08:29.958
system research is being,

06:08:30.359 --> 06:08:32.679
is getting influenced by properties of an emerging

06:08:32.679 --> 06:08:34.769
ML. So we have seen from

06:08:34.769 --> 06:08:36.849
her talk that she's thinking a lot

06:08:36.849 --> 06:08:38.880
about how machine

06:08:38.940 --> 06:08:40.710
learning for systems research

06:08:40.950 --> 06:08:42.810
can really shape the system's ecosystem.

06:08:45.300 --> 06:08:47.620
Next, we have our other speaker, Hanssen

06:08:47.620 --> 06:08:49.630
Wang. Who's a research engineer at

06:08:49.630 --> 06:08:51.810
OpenAI. He focuses on the codex

06:08:51.810 --> 06:08:54.190
models. Integrated into Chatter BT.

06:08:54.591 --> 06:08:56.880
With codex, users can delegate

06:08:57.099 --> 06:08:58.640
coding tasks to parallel agents

06:08:59.270 --> 06:09:01.350
working autonomously in the cloud to analyze

06:09:01.350 --> 06:09:03.540
the codebase and generate poll

06:09:03.540 --> 06:09:05.700
requests Hanson worked on training the

06:09:05.700 --> 06:09:07.770
first Kotex one model launched

06:09:07.770 --> 06:09:09.320
in May and has been continuously

06:09:09.930 --> 06:09:12.010
continuously iterating on the model

06:09:12.010 --> 06:09:14.030
since then. Prior

06:09:14.030 --> 06:09:16.110
to joining OpenAI, he cofounded a start

06:09:16.110 --> 06:09:18.250
up building AI analyst agents and

06:09:18.250 --> 06:09:20.040
worked on ML infrastructure and meta.

06:09:21.160 --> 06:09:22.800
Where he actually was a

06:09:23.360 --> 06:09:25.230
workshop attendee, apparently, six years ago.

06:09:25.950 --> 06:09:27.070
So welcome back.

06:09:30.590 --> 06:09:33.080
So, before we dig begin the debate, we

06:09:33.140 --> 06:09:35.180
want to define the terms a little bit better.

06:09:35.341 --> 06:09:37.501
By systems developers, we mean that the people who

06:09:37.501 --> 06:09:40.001
build and operate the foundational layers of computing.

06:09:40.430 --> 06:09:42.520
That may include not just the

06:09:42.520 --> 06:09:44.590
general business logic programming,

06:09:44.590 --> 06:09:46.400
but also compilers,

06:09:46.700 --> 06:09:48.610
kernels, infrastructure,

06:09:49.140 --> 06:09:51.380
hardware, software, codesign, performance engineering,

06:09:51.380 --> 06:09:53.402
large scale ML system. Basically,

06:09:53.402 --> 06:09:55.472
all the things that you know, all of us

06:09:55.472 --> 06:09:56.160
do today.

06:09:57.921 --> 06:10:00.241
By replace, we also do not mean assist,

06:10:00.241 --> 06:10:02.251
accelerate, or collaborate. We mean that

06:10:02.251 --> 06:10:04.290
the primary design implementation,

06:10:04.820 --> 06:10:06.960
and debugging work will be done autonomously

06:10:06.960 --> 06:10:09.070
by agents with humans

06:10:09.070 --> 06:10:10.820
no longer in the critical path.

06:10:11.620 --> 06:10:13.720
So if that changes your position, it's

06:10:13.859 --> 06:10:14.679
time to

06:10:17.850 --> 06:10:20.130
Yeah. So each side will present

06:10:20.130 --> 06:10:22.170
two speakers, followed by some cross

06:10:22.170 --> 06:10:24.040
examination and audience q and a.

06:10:25.160 --> 06:10:27.260
You are if you already have questions,

06:10:28.312 --> 06:10:30.380
you could ask the questions here.

06:10:30.540 --> 06:10:32.700
By scanning the QR code, and this will be a lot

06:10:32.700 --> 06:10:33.610
easier for us.

06:10:34.730 --> 06:10:36.850
A quick poll here Who here

06:10:36.850 --> 06:10:39.290
already uses agents? Weekly?

06:10:43.130 --> 06:10:44.550
We need a photo for this.

06:10:45.910 --> 06:10:48.150
Okay. And who believes

06:10:49.411 --> 06:10:50.991
replacement is inevitable?

06:10:53.411 --> 06:10:54.950
That's a smaller number.

06:11:00.910 --> 06:11:03.171
So keep these prompts in mind, and we'll be soliciting

06:11:03.230 --> 06:11:04.450
questions from you shortly.

06:11:07.070 --> 06:11:09.009
First, I'll begin with opening statements.

06:11:09.960 --> 06:11:12.060
For each of you to explain your position.

06:11:12.831 --> 06:11:14.900
In generality. So about, like,

06:11:14.900 --> 06:11:16.390
two minutes each.

06:11:26.460 --> 06:11:28.010
Do you want this you want

06:11:30.470 --> 06:11:32.730
Test test. Oh, there there we go.

06:11:33.421 --> 06:11:35.160
Alright. Start with the opening statement.

06:11:36.241 --> 06:11:38.482
So agentic AI is a tidal wave that is crashing

06:11:38.482 --> 06:11:40.001
over the entire,

06:11:40.620 --> 06:11:42.779
our entire industry. Anyone who doesn't

06:11:42.779 --> 06:11:44.859
see this is in denial, obviously, or hasn't

06:11:44.859 --> 06:11:46.740
attended this conference so far,

06:11:46.900 --> 06:11:48.759
as we're already starting to see widespread

06:11:48.980 --> 06:11:51.010
adoption of AI in nearly every area.

06:11:51.331 --> 06:11:53.280
If you were to broaden this debate topic

06:11:53.520 --> 06:11:55.700
to will a gigantic AI systems replace developers,

06:11:55.841 --> 06:11:58.001
not just systems developers, I think you could

06:11:58.001 --> 06:12:00.208
make a strong argument in the affirmative.

06:12:00.208 --> 06:12:02.470
But this debate is about systems development.

06:12:03.030 --> 06:12:05.270
So the key question is, in this tidal wave that's

06:12:05.270 --> 06:12:07.331
coming, is

06:12:07.390 --> 06:12:09.430
systems development high ground or low ground?

06:12:09.751 --> 06:12:11.831
They say, are we gonna be affected more or less than

06:12:11.831 --> 06:12:13.950
than other areas in the in the system?

06:12:14.270 --> 06:12:16.350
Unfortunately, I think we're in low ground as system

06:12:16.350 --> 06:12:18.458
developers. Systems development is

06:12:18.458 --> 06:12:20.501
high complexity, don't think this is

06:12:20.501 --> 06:12:22.769
gonna save us. AI has proven very

06:12:22.769 --> 06:12:24.550
good at handling high complexity problems,

06:12:25.300 --> 06:12:27.640
because it has a longer context window than humans.

06:12:28.450 --> 06:12:30.550
But the real killer here is training data.

06:12:31.040 --> 06:12:33.440
Training data is the fuel that that helps AI improve

06:12:33.440 --> 06:12:35.620
and is currently the main bottleneck getting

06:12:35.620 --> 06:12:37.939
better models. And for training

06:12:37.939 --> 06:12:40.320
data, systems has more training

06:12:40.320 --> 06:12:41.939
data than most other areas.

06:12:42.550 --> 06:12:44.790
System software is widely open sourced. It's open

06:12:44.790 --> 06:12:46.580
sourced a lot more than product code.

06:12:47.220 --> 06:12:49.380
And, there's a lot of problems in

06:12:49.380 --> 06:12:51.501
systems such as performance, you

06:12:51.501 --> 06:12:53.490
can easily generate infinite training data

06:12:53.730 --> 06:12:55.810
to generate these recursive self improvement

06:12:55.810 --> 06:12:57.860
loops which can

06:12:57.860 --> 06:13:00.070
very easily cause AI to rapidly improve

06:13:00.470 --> 06:13:02.009
for specific sub problems.

06:13:02.990 --> 06:13:05.150
Moreover, I think there's evidence that AI

06:13:05.150 --> 06:13:07.330
is already replacing system developers.

06:13:08.040 --> 06:13:10.280
I'll remind you that this debate prompt doesn't say all

06:13:10.280 --> 06:13:12.300
developers. It just requires some developers

06:13:12.880 --> 06:13:14.980
to be replaced. And and as

06:13:17.300 --> 06:13:19.080
as a system developer

06:13:19.380 --> 06:13:21.222
myself, I've already been replaced.

06:13:23.321 --> 06:13:25.250
AI agents have completely changed

06:13:25.570 --> 06:13:27.902
my workflow in the past six months. About

06:13:27.902 --> 06:13:30.222
how I write code. I no longer write code directly. Often,

06:13:30.222 --> 06:13:32.599
I don't even open my IDE. I just

06:13:32.599 --> 06:13:34.620
have a codecs

06:13:34.920 --> 06:13:37.330
and claud prompt

06:13:37.330 --> 06:13:39.720
open, and I'm just prompting the models,

06:13:40.040 --> 06:13:42.120
to to write code on my behalf. And I'm reviewing

06:13:42.120 --> 06:13:43.140
and then giving feed feedback.

06:13:44.380 --> 06:13:46.779
So basically, I'm not a system developer anymore.

06:13:46.779 --> 06:13:48.710
I'm just a supervisor of AI

06:13:48.870 --> 06:13:50.690
agents who are now the systems developers.

06:13:51.491 --> 06:13:53.652
And in the coming years, I think AI is gonna improve

06:13:53.652 --> 06:13:55.679
dramatically. And more

06:13:55.679 --> 06:13:57.920
more importantly, we're we're gonna learn how to use it more

06:13:57.920 --> 06:14:00.010
effectively. I think that often I

06:14:00.010 --> 06:14:02.251
I think that the models have gone so good

06:14:02.251 --> 06:14:04.679
so fast that I think now I think one of the biggest bottlenecks

06:14:04.820 --> 06:14:07.062
is how effectively able to use

06:14:07.062 --> 06:14:08.980
these models. And I think we're gonna learn

06:14:09.140 --> 06:14:11.240
if the model stopped improving, I think

06:14:11.460 --> 06:14:13.830
we're we're gonna learn to use a much better and better

06:14:14.070 --> 06:14:16.152
over time. And,

06:14:16.350 --> 06:14:18.312
we're we're we're still in the really early days.

06:14:18.470 --> 06:14:20.671
So this this tidal wave sort of crashing across

06:14:20.671 --> 06:14:22.860
the industry just began to touch the

06:14:23.080 --> 06:14:25.450
shore. I think we have some really exciting times

06:14:25.450 --> 06:14:25.720
ahead.

06:14:28.230 --> 06:14:29.020
Thank you, Jason.

06:14:30.400 --> 06:14:32.480
Piggybacking on Jason, I don't I don't have a

06:14:32.480 --> 06:14:34.650
laptop to read from, but the the important thing

06:14:34.650 --> 06:14:37.000
about is that it is a lower dimensionality

06:14:37.380 --> 06:14:39.380
problem than general developers.

06:14:39.620 --> 06:14:41.650
Right? General developers can be creating

06:14:41.650 --> 06:14:43.290
anything. There's creativity around this.

06:14:43.930 --> 06:14:46.190
In in systems, there there's much less

06:14:46.780 --> 06:14:48.960
creativity. The define there's a much smaller defined

06:14:49.020 --> 06:14:50.445
space of possible

06:14:51.430 --> 06:14:53.850
you know, selections. For example, kernel generation.

06:14:54.071 --> 06:14:56.251
One of the things that is very close to being

06:14:56.312 --> 06:14:58.429
solved at seems, is is is a

06:14:58.429 --> 06:15:00.870
much smaller space to explore.

06:15:01.990 --> 06:15:04.470
Architecture search for sys for chips

06:15:04.530 --> 06:15:06.950
has been happening for years now.

06:15:07.190 --> 06:15:09.050
Right? These are things that even before LLMs

06:15:09.350 --> 06:15:11.448
were vogue, people were able to do architecture

06:15:11.448 --> 06:15:13.609
search for language, for for chips

06:15:13.609 --> 06:15:15.690
as well as, AI architectures.

06:15:16.150 --> 06:15:18.310
The the other aspect of this that's very

06:15:18.310 --> 06:15:20.519
important is in areas of

06:15:20.580 --> 06:15:22.740
reinforcement learning, we've we've we've had massive

06:15:23.038 --> 06:15:25.171
massive breakthroughs in using

06:15:25.790 --> 06:15:27.890
verifiable verifiable compute right.

06:15:27.890 --> 06:15:29.970
If anything is verifiable, then you can you can train on

06:15:29.970 --> 06:15:32.300
it. And chips, systems,

06:15:32.439 --> 06:15:34.501
these things operate at clock rates of

06:15:34.501 --> 06:15:36.538
gigahertz. Not not hertz like

06:15:36.538 --> 06:15:38.779
human systems. Right? And so the feedback loop

06:15:38.779 --> 06:15:40.810
here is so fast that you can generate

06:15:41.349 --> 06:15:42.970
significantly more data than any other

06:15:43.591 --> 06:15:45.690
you know, domain. And so system developers

06:15:45.751 --> 06:15:48.230
will be trained at a at a extreme

06:15:48.230 --> 06:15:50.060
speed. The the data to train these sis

06:15:50.310 --> 06:15:52.420
these models be generated at such

06:15:52.420 --> 06:15:54.599
high speeds that you you can't replace

06:15:54.660 --> 06:15:56.030
system developers quite

06:15:56.671 --> 06:15:58.680
rapidly. And

06:15:58.680 --> 06:16:00.920
and and when you talk about compilers or kernel

06:16:00.920 --> 06:16:02.911
generation, these these are

06:16:02.970 --> 06:16:05.091
areas where problem is actually simple enough. Right?

06:16:05.091 --> 06:16:06.670
The hardware has already been laid out.

06:16:07.470 --> 06:16:09.890
You're you're you've had it for a while. You you you can.

06:16:11.510 --> 06:16:13.751
Solve this problem. It is something solvable. And

06:16:13.751 --> 06:16:15.880
and as we've seen, AI has solved the

06:16:15.880 --> 06:16:17.960
games that are solvable. It it it

06:16:17.960 --> 06:16:19.920
can't maybe solve

06:16:20.240 --> 06:16:22.480
extremely high dimensionality problems like general

06:16:22.480 --> 06:16:24.640
developers, but but systems developers, it definitely

06:16:24.640 --> 06:16:24.840
can.

06:16:27.400 --> 06:16:29.600
Cool. Thank you. Now we're

06:16:29.600 --> 06:16:30.900
gonna move on to the nos.

06:16:31.710 --> 06:16:34.050
We we don't have to attack each other yet.

06:16:36.260 --> 06:16:38.421
Cool. So first of all, views are

06:16:38.421 --> 06:16:40.830
my own. Not my employers. But,

06:16:41.210 --> 06:16:43.370
I'm I'm I'm definitely on

06:16:43.370 --> 06:16:45.100
the pro acceleration camp.

06:16:45.421 --> 06:16:47.560
But I am I I think I disagree with

06:16:47.560 --> 06:16:49.740
the framing of the the term replacement,

06:16:49.740 --> 06:16:52.001
especially. I do believe that

06:16:52.421 --> 06:16:54.550
what will happen is that I

06:16:54.550 --> 06:16:56.710
think as Jan said in the previous

06:16:56.710 --> 06:16:58.900
talk, the level of abstraction

06:16:58.900 --> 06:17:01.001
kind of moves higher I think

06:17:01.001 --> 06:17:03.070
I think coding as

06:17:03.380 --> 06:17:05.540
kind of like a a sub portion of

06:17:05.540 --> 06:17:07.679
the loop that developers

06:17:07.679 --> 06:17:09.940
actually, kinda

06:17:10.081 --> 06:17:12.260
participate in practice of, like, taking requirements

06:17:12.320 --> 06:17:14.250
from the real world, encoding them into

06:17:14.490 --> 06:17:16.570
sort of a formalized spec. And then

06:17:16.570 --> 06:17:18.939
the last part, which coding, which is kind of like

06:17:18.939 --> 06:17:21.198
translating those specs into machine interpretable

06:17:21.500 --> 06:17:23.609
and machine executable code. I

06:17:23.609 --> 06:17:25.870
do believe that part of the loop will

06:17:25.870 --> 06:17:28.030
slowly become more and more automated, if not

06:17:28.030 --> 06:17:30.200
completely automated. But

06:17:30.341 --> 06:17:32.660
basically, like, somebody has to tell the machines

06:17:32.660 --> 06:17:34.920
what to do. And so rather than

06:17:34.920 --> 06:17:36.940
perhaps, like, maybe maybe the

06:17:36.940 --> 06:17:39.020
maybe the term system developer will

06:17:39.341 --> 06:17:41.501
evolve into more of something more of like

06:17:41.501 --> 06:17:43.460
a system architect.

06:17:43.840 --> 06:17:46.340
When I think of, like, designing distributed systems,

06:17:46.870 --> 06:17:49.030
every distributed system is built to

06:17:49.030 --> 06:17:50.900
accomplish a real world

06:17:51.760 --> 06:17:54.210
need or to in service of, like, a real

06:17:54.860 --> 06:17:56.770
task that needs to be automated

06:17:57.091 --> 06:17:59.171
in the real world. And so somebody needs

06:17:59.171 --> 06:18:01.359
to be gathering requirements for that,

06:18:02.001 --> 06:18:04.400
encoding that maybe in, like, a higher level

06:18:04.400 --> 06:18:06.080
natural language spec, but then

06:18:06.800 --> 06:18:08.960
you know, maybe the process of translating that into

06:18:08.960 --> 06:18:11.198
machine code is not is no longer something we have

06:18:11.198 --> 06:18:11.500
to do.

06:18:13.270 --> 06:18:14.330
Thank you.

06:18:15.550 --> 06:18:17.491
Okay. Yeah. I'm also on the no.

06:18:17.690 --> 06:18:19.390
Side. Thank you, Mimi, for clarifying

06:18:20.091 --> 06:18:22.310
the definition of what you meant, and that is that made

06:18:22.310 --> 06:18:24.440
it very, very clear. I stand my ground.

06:18:25.152 --> 06:18:27.220
I Even in my talk, I

06:18:27.220 --> 06:18:27.960
talked about

06:18:29.300 --> 06:18:31.540
there are implications of using all these agents

06:18:31.540 --> 06:18:33.860
and who's gonna solve that. Right?

06:18:34.001 --> 06:18:36.320
If agents are going to be everywhere, there are actually

06:18:36.320 --> 06:18:38.530
new problems they're introducing much as they're solving

06:18:38.530 --> 06:18:40.769
some of the existing problems. I think everything

06:18:40.769 --> 06:18:43.120
basically boils down to the fact that are

06:18:43.180 --> 06:18:45.280
trained models depending on training data. And

06:18:45.280 --> 06:18:47.360
so they are bound to the training data

06:18:47.360 --> 06:18:49.421
distribution. If you're telling me you

06:18:49.421 --> 06:18:51.581
can actually absolutely represent everything in

06:18:51.581 --> 06:18:53.759
training data, I might think about

06:18:53.759 --> 06:18:55.972
switching. Right? But as long as that is

06:18:55.972 --> 06:18:58.210
really infeasible, I do

06:18:58.210 --> 06:19:00.430
not believe that we can we can replace

06:19:00.730 --> 06:19:03.230
ourselves with these agents. I believe,

06:19:04.260 --> 06:19:06.200
what this is changing, it it's reshaping

06:19:06.341 --> 06:19:08.679
what we, as system developers,

06:19:09.380 --> 06:19:11.509
designers, are supposed to be doing.

06:19:11.509 --> 06:19:13.670
We are no longer required to sort of do

06:19:13.670 --> 06:19:15.831
the lower level task. And definitely, we

06:19:15.831 --> 06:19:17.710
can use these agents, but these agents are

06:19:17.950 --> 06:19:20.450
still assistants. We have to be acting as orchestrators.

06:19:20.591 --> 06:19:22.600
The important thing is asking

06:19:22.980 --> 06:19:25.040
the right question. After that, like what

06:19:25.040 --> 06:19:27.120
Ian talked about. Right? Then that loop perhaps

06:19:27.120 --> 06:19:29.279
can be replaced. But asking the right questions

06:19:29.279 --> 06:19:31.359
has always been, at least in systems, it

06:19:31.359 --> 06:19:33.439
has always been least 50% of

06:19:33.439 --> 06:19:35.840
the contribution. Right? Asking the right questions,

06:19:35.840 --> 06:19:38.060
and that has not changed. At all.

06:19:38.460 --> 06:19:40.570
So, yeah. I would

06:19:40.570 --> 06:19:41.230
stick to no.

06:19:44.220 --> 06:19:46.540
Interesting. So we are already

06:19:46.540 --> 06:19:48.570
hitting some disagreements. For instance, do we

06:19:48.570 --> 06:19:50.911
have enough data? Is this problem difficult

06:19:50.970 --> 06:19:52.911
or easy compared to general machine

06:19:53.130 --> 06:19:55.160
learning? Problems that we're solving today? So

06:19:55.160 --> 06:19:56.220
that's really interesting.

06:19:58.210 --> 06:19:59.670
Next, I wanna ask,

06:20:00.770 --> 06:20:02.470
what are the agents surprisingly

06:20:02.980 --> 06:20:05.040
at right now? Just so that we can build some

06:20:05.040 --> 06:20:05.650
common ground.

06:20:11.160 --> 06:20:13.330
So I so I I've been constantly surprised

06:20:13.570 --> 06:20:15.640
so so one sort of anecdote is

06:20:15.880 --> 06:20:18.120
I I work on PyTorch and that or the compiler

06:20:18.120 --> 06:20:20.260
for PyTorch. And as part of that, we have an

06:20:20.260 --> 06:20:22.320
an on call. Where, you know, basically, the on

06:20:22.320 --> 06:20:24.421
call is responsible for dealing with the

06:20:24.480 --> 06:20:26.720
issues that many of the people in this room and across

06:20:26.720 --> 06:20:28.800
the industry open on the on on GitHub,

06:20:28.800 --> 06:20:31.130
PyTorch, PyTorch. And normally

06:20:31.130 --> 06:20:33.232
what an on call would do sort of a year

06:20:33.290 --> 06:20:35.439
ago would they they sort of be play a router,

06:20:35.660 --> 06:20:37.820
right, type role. Like, some issues would be low priority, and so

06:20:37.820 --> 06:20:39.900
we so but other many other issues, we'd

06:20:39.900 --> 06:20:41.970
sort of hand off to other people. So I I might

06:20:41.970 --> 06:20:44.090
take you know, get 20 issues, and I might hand

06:20:44.090 --> 06:20:46.170
them to a bunch of humans. And my last

06:20:46.170 --> 06:20:48.200
on call, which was which was a month and a

06:20:48.200 --> 06:20:50.280
half ago, rather than sort of

06:20:50.280 --> 06:20:52.040
handing all the issues off, to humans,

06:20:53.400 --> 06:20:55.460
I, just copied and

06:20:55.460 --> 06:20:57.700
pasted the URL of the issue into

06:20:57.700 --> 06:20:59.860
codecs, and said, fix

06:20:59.860 --> 06:21:01.874
this issue with no guidance, no

06:21:02.142 --> 06:21:03.770
just just open ended, just fix it.

06:21:04.171 --> 06:21:06.411
And about 80% of the issues, there was, like, 20

06:21:06.411 --> 06:21:07.490
or 30 issues.

06:21:08.849 --> 06:21:10.700
Kodak solved the solved the issue

06:21:12.060 --> 06:21:14.220
on on its own sort of without any help from

06:21:14.220 --> 06:21:16.460
me. It And now a lot of those

06:21:16.460 --> 06:21:18.540
those solutions were more complicated than they should

06:21:18.540 --> 06:21:20.590
have be and and and a

06:21:20.910 --> 06:21:22.990
a few of them were had some some bugs in

06:21:22.990 --> 06:21:24.930
them. But, you know,

06:21:25.171 --> 06:21:27.400
an 80% success rate is is really good.

06:21:28.040 --> 06:21:30.200
And and I was honestly, like, I saw

06:21:30.200 --> 06:21:32.421
that, and I'm like, oh, wow. Like, like you

06:21:32.501 --> 06:21:34.501
this is way more efficient because it would

06:21:34.529 --> 06:21:36.650
it actually like, the time it would take me to

06:21:37.136 --> 06:21:39.380
to hand these issues off

06:21:39.380 --> 06:21:41.620
to junior engineers, do code

06:21:41.620 --> 06:21:42.520
reviews, you'll

06:21:44.769 --> 06:21:47.030
Codecs was was it was easier to use

06:21:47.260 --> 06:21:49.341
and because it was so fast, because the turnaround

06:21:49.341 --> 06:21:50.402
time was so quickly,

06:21:51.440 --> 06:21:53.500
it actually, you know, saved me time

06:21:54.140 --> 06:21:56.312
versus sort of sort of routing the

06:21:56.312 --> 06:21:58.472
issues to other people. And I I I think, like like,

06:21:58.472 --> 06:22:00.560
you know, the other

06:22:00.683 --> 06:22:03.056
debaters mentioned, a

06:22:03.179 --> 06:22:04.760
few times about how

06:22:05.510 --> 06:22:07.410
we're basically just gonna be moving up the stack.

06:22:07.650 --> 06:22:09.610
And where, you know, we're gonna be

06:22:09.850 --> 06:22:12.270
rather than writing code directly, we're gonna

06:22:12.270 --> 06:22:14.350
be acting sort of as as architects and

06:22:14.350 --> 06:22:16.240
supervisor. And the but the the the thing

06:22:16.400 --> 06:22:18.210
I think that I completely agree with that.

06:22:18.610 --> 06:22:20.690
But I I think that the the key thing here is that we're

06:22:20.690 --> 06:22:22.150
gonna get this this dichotomy

06:22:23.300 --> 06:22:25.378
where senior engineers who are really good

06:22:25.378 --> 06:22:27.460
at supervising, really good at code reviewing,

06:22:28.081 --> 06:22:30.310
are all of a sudden gonna get way more productive. Like

06:22:30.310 --> 06:22:32.370
their productivity is gonna gonna gonna

06:22:32.930 --> 06:22:35.091
dramatically improve. But for

06:22:35.091 --> 06:22:37.251
junior engineers who are sort of just coming out

06:22:37.251 --> 06:22:39.259
of college, really the bottom few

06:22:39.259 --> 06:22:41.600
rungs of the ladder getting

06:22:41.600 --> 06:22:43.770
cut off. And it's it's

06:22:43.770 --> 06:22:45.990
it's basically, you know, if I if

06:22:45.990 --> 06:22:48.400
codecs is better at solving these types of issues,

06:22:48.640 --> 06:22:50.870
than a new college grad, why would I ever

06:22:50.870 --> 06:22:52.940
hire a new college grad? Like, there's there this

06:22:52.940 --> 06:22:55.360
is a huge huge problem. And

06:22:56.001 --> 06:22:58.241
but the key thing is, there's a lot more junior

06:22:58.241 --> 06:22:59.400
developers than there are senior

06:23:00.519 --> 06:23:02.939
developers. So, what this means

06:23:03.000 --> 06:23:05.038
is that for sort of

06:23:05.038 --> 06:23:06.780
the the the the skilled and talented

06:23:07.080 --> 06:23:09.400
know, they're gonna get way more productive. Their value

06:23:09.400 --> 06:23:11.580
that they're providing is gonna is gonna skyrocket.

06:23:12.091 --> 06:23:14.430
Then there's gonna be be sort of a much wider

06:23:14.970 --> 06:23:17.410
audience of people who are struggling

06:23:17.470 --> 06:23:19.970
to find work. And if you go talk to a senior in college,

06:23:20.350 --> 06:23:22.510
senior in college is probably having a pretty hard

06:23:22.510 --> 06:23:24.679
time finding a job right

06:23:24.679 --> 06:23:26.730
now. Because of this. And so

06:23:26.730 --> 06:23:28.868
I think on net, this means that

06:23:28.868 --> 06:23:30.620
a large number of

06:23:31.100 --> 06:23:32.800
systems developers are gonna be replaced,

06:23:33.730 --> 06:23:36.020
while a small fraction of them are

06:23:36.020 --> 06:23:38.260
going to, become a lot more

06:23:38.260 --> 06:23:38.740
productive.

06:23:41.200 --> 06:23:43.510
Adding to this, as you both

06:23:43.991 --> 06:23:46.251
astutely framed, right, system developers will be replaced

06:23:46.312 --> 06:23:48.609
by system architects or system orchestrators,

06:23:48.830 --> 06:23:50.970
so system developers will be replaced. Right?

06:23:51.370 --> 06:23:53.530
As we all agree. Definition. And and and so

06:23:53.530 --> 06:23:55.750
I think what's important to recognize here

06:23:55.750 --> 06:23:57.790
is no one writes assembly today. Right?

06:23:57.790 --> 06:23:59.950
When when was the last time anyone wrote assembly? Hey,

06:23:59.950 --> 06:24:02.110
for There's some of people here. For

06:24:02.110 --> 06:24:04.450
for for for, you know to be a bit more

06:24:04.590 --> 06:24:06.960
sensitive. For, you know, NVIDIA

06:24:06.960 --> 06:24:09.038
GPUs, I think there's only one man on earth who writes,

06:24:09.599 --> 06:24:11.732
directly in SAS that's Gray. Everyone

06:24:11.732 --> 06:24:13.840
else writes. At some higher level. And

06:24:13.840 --> 06:24:16.321
as we look at where AI has been

06:24:16.860 --> 06:24:18.870
extremely powerful, is on kernel generation. Right? Now

06:24:18.870 --> 06:24:21.130
a lot of times there is aural hacking and people's

06:24:21.980 --> 06:24:24.300
you know, or or or or reward hacking

06:24:24.300 --> 06:24:26.482
kernel bench currently. And and claiming

06:24:26.482 --> 06:24:28.910
that they're generating better kernels, but there's

06:24:29.210 --> 06:24:31.609
clear progress here. And it's not

06:24:31.609 --> 06:24:34.009
just on NVIDIA chips, it's on other other firms'

06:24:34.009 --> 06:24:36.120
chips where actually generating kernels for them is

06:24:36.120 --> 06:24:38.210
is much easier. And so just as people

06:24:38.210 --> 06:24:40.460
who wrote assembly were replaced

06:24:40.679 --> 06:24:42.750
in large droves, just like people who are

06:24:42.990 --> 06:24:45.150
printing out, and doing punch cards

06:24:45.150 --> 06:24:47.210
for programming IBMs and, like, the

06:24:47.210 --> 06:24:49.530
fifties were replaced. System developers are also

06:24:49.530 --> 06:24:51.380
being replaced. In the areas

06:24:51.679 --> 06:24:53.921
of kernel generation, kernel writing

06:24:53.921 --> 06:24:56.060
will not be a job in the in the

06:24:56.060 --> 06:24:58.180
near term in in the medium term future.

06:24:58.581 --> 06:25:00.741
Is being replaced, maybe followed

06:25:00.741 --> 06:25:02.780
by some orchestrator or

06:25:02.780 --> 06:25:05.110
some architect. But but those aren't system

06:25:05.170 --> 06:25:06.991
developers. Yeah.

06:25:07.290 --> 06:25:09.510
Well well, her definition remember Mimi's

06:25:09.510 --> 06:25:11.360
definition of system developers. Right?

06:25:11.600 --> 06:25:13.760
Just for me. But, what I want

06:25:13.760 --> 06:25:15.840
to say is, I'm not honestly,

06:25:15.840 --> 06:25:18.000
I'm not arguing against. Like, I'll

06:25:18.000 --> 06:25:20.000
take an, position of an

06:25:20.060 --> 06:25:22.140
academic. Right? In classes,

06:25:22.140 --> 06:25:24.380
for instance, you know, it's been a big problem

06:25:24.380 --> 06:25:25.340
to sort of tell

06:25:26.470 --> 06:25:28.550
what, use AI, not to use AI, how to

06:25:28.550 --> 06:25:30.599
use AI. Obviously, if you don't use,

06:25:30.599 --> 06:25:32.759
you're gonna be like living under a rock.

06:25:32.759 --> 06:25:34.878
You don't want to be doing that. The

06:25:34.878 --> 06:25:37.091
problem here is, like, how many of you actually

06:25:37.091 --> 06:25:38.970
think you can directly land a job

06:25:39.210 --> 06:25:40.750
where you can be the orchestrators?

06:25:41.610 --> 06:25:43.770
Right? You only do that after having

06:25:43.770 --> 06:25:45.921
done the actual job of you know,

06:25:45.921 --> 06:25:48.160
going through that experience and then

06:25:48.160 --> 06:25:50.350
landing somewhere where you actually get the

06:25:50.580 --> 06:25:52.599
understanding of okay, now I'm the

06:25:52.599 --> 06:25:54.661
sort of whatever principal engineer whatever,

06:25:54.661 --> 06:25:56.780
whoever is designing. Who

06:25:56.780 --> 06:25:58.560
is handing over. Here is a design document.

06:25:58.800 --> 06:26:00.960
To a junior. Right? I agree that

06:26:00.960 --> 06:26:03.071
juniors are getting replaced, but the problem with

06:26:03.071 --> 06:26:05.232
that is, and and this is like, I'm

06:26:05.232 --> 06:26:07.350
agreeing with that because all

06:26:07.350 --> 06:26:09.510
this dynamic shift that is happening is reflected

06:26:09.510 --> 06:26:11.630
in our our classrooms. So literally,

06:26:11.870 --> 06:26:13.940
we did a poll around, like, what

06:26:13.940 --> 06:26:16.100
classes are getting populated. And all of a sudden, the,

06:26:16.100 --> 06:26:18.580
you know, soft development engineering kind of classes

06:26:18.640 --> 06:26:20.720
or, data science classes. Like, you know,

06:26:20.720 --> 06:26:23.030
they're empty. Of a sudden. And

06:26:23.171 --> 06:26:25.251
everything else, like, hardware classes are getting more and more populated. So

06:26:25.251 --> 06:26:27.429
that is the the impact. But more and more,

06:26:27.429 --> 06:26:29.220
I see students using AI

06:26:29.790 --> 06:26:31.730
tools, and not using them

06:26:32.810 --> 06:26:34.970
in kind of responsible manner, I'm

06:26:34.970 --> 06:26:37.179
worried that they're actually, learning

06:26:37.179 --> 06:26:39.429
not to think. So when the court stops

06:26:39.429 --> 06:26:41.590
working, they cannot actually explain that.

06:26:41.590 --> 06:26:43.671
Right? So it it seems like we're heading towards

06:26:43.831 --> 06:26:45.990
if we replace, we're heading towards a place

06:26:45.990 --> 06:26:48.000
where we are getting biased. Like,

06:26:48.000 --> 06:26:49.460
the model feeding its

06:26:50.060 --> 06:26:52.300
its own output data to itself will create a sample

06:26:52.300 --> 06:26:54.392
bias because it you know, creates an

06:26:54.392 --> 06:26:56.491
illusion. That's what the world looks

06:26:56.491 --> 06:26:58.500
like. And it only being is

06:26:58.500 --> 06:27:00.580
able to predict that. What happens when things change? How

06:27:00.580 --> 06:27:02.720
do we you know, actually be ready for the

06:27:02.720 --> 06:27:03.100
real world?

06:27:05.000 --> 06:27:06.660
It feels like we are, as far

06:27:07.060 --> 06:27:09.140
a lot of time debating the definition of

06:27:09.140 --> 06:27:11.220
re replacement replacement rather. We only have

06:27:11.220 --> 06:27:11.630
two minutes.

06:27:13.550 --> 06:27:15.840
But I will say, it is hard to predict I

06:27:15.900 --> 06:27:18.060
think, how the roles will change. So, like, to

06:27:18.060 --> 06:27:20.179
the point about you know, assembly and lower level

06:27:20.179 --> 06:27:22.460
languages, yes, those went away.

06:27:22.460 --> 06:27:24.562
But as we got the higher level languages,

06:27:24.841 --> 06:27:26.940
we just could do so much more with software

06:27:27.081 --> 06:27:28.700
that amount of software developers

06:27:29.180 --> 06:27:31.040
10 x, if not a 100 x.

06:27:31.260 --> 06:27:33.370
And it could be something similar here where, you know, like,

06:27:34.390 --> 06:27:36.470
the fact that I have, codecs and

06:27:36.470 --> 06:27:38.400
all these, like, different tools means that

06:27:38.480 --> 06:27:40.340
know, if I wanna create something,

06:27:40.520 --> 06:27:42.841
nothing stops me anymore from just being

06:27:42.841 --> 06:27:45.110
able to of, like, prompt it into

06:27:45.110 --> 06:27:47.350
existence. And that's tremendously powerful as as

06:27:47.350 --> 06:27:49.770
well. So, like, you know, maybe it's not,

06:27:51.509 --> 06:27:53.690
there there are probably right. I think,

06:27:53.690 --> 06:27:55.390
like, with with kernel engineers, like,

06:27:56.180 --> 06:27:58.260
kernel programming, like, I think that's probably something we

06:27:58.260 --> 06:28:00.341
just won't have to do anymore, but maybe

06:28:00.341 --> 06:28:01.310
that opens up

06:28:03.150 --> 06:28:05.390
the the the room to to do more creative

06:28:05.390 --> 06:28:07.429
things. I see. So so

06:28:07.429 --> 06:28:09.609
far, we're getting, the junior

06:28:10.280 --> 06:28:12.220
developers' everyday tasks of programming

06:28:12.600 --> 06:28:13.740
seems to be

06:28:15.760 --> 06:28:17.140
on the way of being replaced.

06:28:18.100 --> 06:28:20.290
Right? Is that the consensus we have here?

06:28:20.770 --> 06:28:23.010
Okay. And colonel Raiching, perhaps by

06:28:23.010 --> 06:28:24.900
being such a difficult

06:28:25.040 --> 06:28:27.331
but needed thing, to be done.

06:28:27.331 --> 06:28:29.540
Also, it's getting more and more

06:28:29.540 --> 06:28:31.560
automated. It doesn't seem like we're

06:28:31.560 --> 06:28:33.720
really touching on the entirety of the system

06:28:33.720 --> 06:28:36.171
stack, but I like that we're you know,

06:28:36.230 --> 06:28:38.530
beginning and yeah, beginning

06:28:38.530 --> 06:28:39.550
to touch on something.

06:28:41.730 --> 06:28:43.890
So, you know, if we're really talking about the

06:28:43.890 --> 06:28:45.640
definition of again, we do

06:28:46.019 --> 06:28:48.470
include the design component. That we think is an

06:28:48.609 --> 06:28:50.660
important part of system developers. And, you

06:28:50.660 --> 06:28:52.679
know, people can't disagree. Maybe, you know,

06:28:52.880 --> 06:28:55.030
I don't think about design anymore. I

06:28:55.030 --> 06:28:57.160
do think that's part of the And another

06:28:57.160 --> 06:28:59.320
part is to become a software architect, to be

06:28:59.320 --> 06:29:01.420
someone who's maybe prompting

06:29:01.420 --> 06:29:03.720
the model even, explaining what's

06:29:03.859 --> 06:29:05.970
wrong, or even labeling the data you need to

06:29:05.970 --> 06:29:08.100
have become someone with that experience. And

06:29:08.100 --> 06:29:10.180
if that experience is the way of thinking and

06:29:10.180 --> 06:29:11.720
learning, goes away,

06:29:12.510 --> 06:29:14.751
what does that mean for us? Just like a small

06:29:14.751 --> 06:29:16.841
addendum. To think about.

06:29:17.060 --> 06:29:18.760
And next for our question,

06:29:20.100 --> 06:29:22.110
what is the one concrete systems hack?

06:29:22.610 --> 06:29:24.810
That agents are fully owned within the the

06:29:24.810 --> 06:29:27.100
next twelve month that we probably don't

06:29:27.100 --> 06:29:29.170
think about much now?

06:29:33.500 --> 06:29:35.599
Sorry. Could you clarify what what's meant by hacks?

06:29:35.810 --> 06:29:38.090
So we talked about the kernel writing

06:29:39.128 --> 06:29:40.670
some compiler tests.

06:29:41.840 --> 06:29:43.870
Also, the junior developer. Type

06:29:43.870 --> 06:29:45.810
of task. But what is some other

06:29:45.930 --> 06:29:48.280
system developer work?

06:29:48.280 --> 06:29:50.370
You think will be replaceable

06:29:50.510 --> 06:29:51.720
within the next twelve month?

06:29:57.360 --> 06:29:59.600
I think almost everything. Like, I

06:29:59.600 --> 06:30:01.660
I know, when when I oh, if I need

06:30:01.660 --> 06:30:04.110
to do anything coding

06:30:04.520 --> 06:30:06.550
related, I'd no longer open

06:30:06.550 --> 06:30:08.010
my IDE. I I open,

06:30:08.710 --> 06:30:11.020
a my AI agent's console.

06:30:11.501 --> 06:30:13.661
And so I I can't think of

06:30:13.661 --> 06:30:16.030
a task where the first thing I'd open

06:30:16.030 --> 06:30:17.760
would be Versus Code anymore.

06:30:18.482 --> 06:30:20.591
And that wasn't true three

06:30:20.652 --> 06:30:22.930
months ago. That this is, like, or or or six months

06:30:22.930 --> 06:30:25.071
ago. I I think

06:30:25.071 --> 06:30:27.310
that there there's definitely, like like, there's a boundary

06:30:27.310 --> 06:30:28.990
here where, like, a lot of the design work

06:30:30.750 --> 06:30:32.910
is still sort of needs to be done by humans.

06:30:33.390 --> 06:30:35.410
But I think as AI models improve like,

06:30:35.410 --> 06:30:37.490
I look at the types of prompts that I'm giving

06:30:37.490 --> 06:30:39.690
AI I think the types of prompts that I'm giving

06:30:39.690 --> 06:30:41.840
AI are totally automatable. Like,

06:30:41.840 --> 06:30:44.140
I'm I I tell it basically every single time

06:30:44.460 --> 06:30:46.240
is too complicated. Please simplify

06:30:47.179 --> 06:30:49.260
it. I I I I I literally have, like, a

06:30:49.741 --> 06:30:51.600
a copy and paste like, simplify

06:30:51.820 --> 06:30:53.740
this prompt that I basically just just

06:30:53.900 --> 06:30:55.930
I I put in without even reading the output

06:30:55.930 --> 06:30:57.930
of the model. On many times.

06:30:59.110 --> 06:31:01.331
And and so,

06:31:01.470 --> 06:31:03.710
I think that that the boundary of sort

06:31:03.710 --> 06:31:05.689
of sort of how much supervision

06:31:05.830 --> 06:31:07.898
I need to provide it think it's gonna need less

06:31:07.898 --> 06:31:09.960
and less supervision over time. And

06:31:09.960 --> 06:31:11.160
I I don't see an odd

06:31:14.120 --> 06:31:16.359
like, roadblock that'll

06:31:16.359 --> 06:31:18.490
stop AI from advancing and stop sort of

06:31:21.940 --> 06:31:24.020
And and so I think I think it's it's it's gonna be

06:31:24.020 --> 06:31:24.790
pretty widespread.

06:31:28.630 --> 06:31:30.790
I think sticking to the point of a

06:31:30.790 --> 06:31:32.950
task that will be automated in the next twelve months,

06:31:32.950 --> 06:31:35.060
many many will you know, longer timelines

06:31:35.060 --> 06:31:37.130
will take take a bit more for

06:31:37.130 --> 06:31:39.071
system developer agents to replace,

06:31:39.210 --> 06:31:41.220
but but kernel generation one of

06:31:41.220 --> 06:31:43.280
those which which will be replaced you know,

06:31:43.280 --> 06:31:44.830
within the next year. Right?

06:31:45.868 --> 06:31:48.368
The the amount of, work out there

06:31:48.429 --> 06:31:50.520
to, you know,

06:31:50.520 --> 06:31:52.562
for lowering, any any any code. Right? It's

06:31:52.562 --> 06:31:54.850
not it's not an orchestrator. It's not a system orchestrator.

06:31:55.251 --> 06:31:57.411
Or system architect even. Right? It's researchers will

06:31:57.411 --> 06:31:59.270
literally just call Helion

06:31:59.520 --> 06:32:01.680
in PyTorch, and then PyTorch will lower

06:32:01.680 --> 06:32:03.921
through whatever, set of stacks,

06:32:03.921 --> 06:32:05.760
whether it's some know, NVIDIA

06:32:06.060 --> 06:32:08.359
closed source thing or some other open source

06:32:08.359 --> 06:32:10.310
kernel lowering library. And instead of

06:32:10.560 --> 06:32:12.640
having to write custom kernels in certain places to get

06:32:12.640 --> 06:32:14.958
peak performance, in fact, the performance will

06:32:15.340 --> 06:32:16.960
be roof lined right out of the gate.

06:32:17.520 --> 06:32:19.460
Because kernel generation will happen

06:32:20.470 --> 06:32:22.490
automatically, right, with with system agents.

06:32:23.171 --> 06:32:25.380
This this is an area where it's it's quite

06:32:25.380 --> 06:32:27.280
clear that this is exactly where you're going.

06:32:28.720 --> 06:32:31.160
And and you know, what what where where

06:32:31.300 --> 06:32:33.460
researchers won't need to interact with their system

06:32:33.460 --> 06:32:35.470
developer to get peak performance, to

06:32:35.470 --> 06:32:37.550
get, for with efficient kernels.

06:32:37.550 --> 06:32:39.779
Right? They they they just don't need to talk to you.

06:32:39.779 --> 06:32:42.099
Right? This is this is there's no, you know, senior

06:32:42.099 --> 06:32:43.740
system developer in the loop even.

06:32:44.859 --> 06:32:47.019
And so this is a a a wholesale case

06:32:47.019 --> 06:32:47.599
of system

06:32:53.360 --> 06:32:55.360
I think one area I'm looking forward to

06:32:55.520 --> 06:32:57.050
I think I think we're not quite there

06:32:57.690 --> 06:32:59.750
but, you know, one area that I'm looking forward to is like

06:33:00.310 --> 06:33:02.810
the act of cleaning up or refactoring code

06:33:03.140 --> 06:33:05.200
think it's, like, a very unglamorous part of

06:33:05.200 --> 06:33:07.120
a job. We kind of joke sometimes

06:33:07.501 --> 06:33:09.902
that, like, we're accumulating so much tech debt

06:33:09.902 --> 06:33:12.200
internally that you know, we're hoping that AGI

06:33:12.200 --> 06:33:14.230
arrives so that we can clean up the tech

06:33:14.230 --> 06:33:15.929
debt before we have to pay it off ourselves.

06:33:16.341 --> 06:33:18.581
But I think yeah. No. Real realistically, I think agents

06:33:18.581 --> 06:33:20.200
are getting to the point where they can

06:33:21.161 --> 06:33:23.240
be the, you know, like, the the

06:33:23.240 --> 06:33:25.430
payments against the tech debt perhaps

06:33:26.300 --> 06:33:28.550
and then think that's necessarily

06:33:28.550 --> 06:33:30.510
replacing anyone's job because they're

06:33:30.751 --> 06:33:33.010
nobody whose, like, full time job it is to

06:33:34.370 --> 06:33:36.550
hopefully. But, but it

06:33:36.550 --> 06:33:38.560
is something that we can hopefully offload to the machines.

06:33:40.640 --> 06:33:42.760
You know what I would love I would

06:33:42.760 --> 06:33:45.001
love to love an agent to just take our prototype and

06:33:45.001 --> 06:33:46.820
convert that into a production level code.

06:33:47.700 --> 06:33:49.860
Right? Be able to, reproduce

06:33:49.860 --> 06:33:52.210
results from all these research papers that

06:33:52.450 --> 06:33:54.530
you know, we feel it's hard

06:33:54.530 --> 06:33:56.609
and make that easy. I don't know how how

06:33:56.609 --> 06:33:58.630
feasible that is with what Ian talked

06:33:58.630 --> 06:34:00.790
about. But that is one area

06:34:00.790 --> 06:34:03.030
that at least research, academic

06:34:03.030 --> 06:34:05.270
research or conferences have struggled with. That's

06:34:05.270 --> 06:34:07.620
why there is a lot of effort on artifact evaluation,

06:34:07.620 --> 06:34:09.840
reproducibility, and so on and so forth, but we haven't made it.

06:34:10.000 --> 06:34:12.070
And I think that's one area I would love

06:34:12.070 --> 06:34:13.580
for agents to take over.

06:34:15.210 --> 06:34:17.290
Mhmm. Relating to that,

06:34:17.290 --> 06:34:19.710
can you name one production

06:34:19.770 --> 06:34:22.030
critical system for those of you working

06:34:22.030 --> 06:34:24.130
in the industry? Where humans

06:34:24.130 --> 06:34:26.368
are already optional. You mentioned the on call

06:34:26.368 --> 06:34:28.550
part, Jason. You

06:34:28.550 --> 06:34:29.470
want to add to that?

06:34:31.050 --> 06:34:32.590
Or any one of you? Yeah.

06:34:35.759 --> 06:34:37.930
I I mean, I I think there's

06:34:37.930 --> 06:34:40.170
definitely, lots of things we mentioned are

06:34:40.170 --> 06:34:42.620
areas where where humans

06:34:42.620 --> 06:34:43.529
are are optional.

06:34:44.940 --> 06:34:47.060
I mean, I I think it's I think it's the case where, you

06:34:47.060 --> 06:34:48.810
know, we have policies around

06:34:49.130 --> 06:34:51.210
around that that that even when even

06:34:51.210 --> 06:34:53.150
when tasks are written entirely by AI,

06:34:53.310 --> 06:34:55.350
and actually within meta, a

06:34:55.350 --> 06:34:57.120
significant fraction of of

06:34:57.360 --> 06:34:59.280
PRs are are a 100% authored by p

06:34:59.440 --> 06:35:01.550
by by AI. Mostly refactoring

06:35:01.550 --> 06:35:03.330
type things, sort of mechanical refactoring,

06:35:04.109 --> 06:35:06.350
changes are fully automated.

06:35:09.010 --> 06:35:11.091
But I I I think I think, like, one one trend

06:35:11.091 --> 06:35:12.710
that I'm I'm I'm looking

06:35:13.790 --> 06:35:15.870
forward to in the future is, you know,

06:35:15.870 --> 06:35:18.189
right now, like, and especially when I'm building systems, like,

06:35:18.189 --> 06:35:20.300
one thing I'm often thinking about is, like, okay. Like, how do I

06:35:20.300 --> 06:35:22.560
make this the system perfect? How do I make it beautiful?

06:35:23.520 --> 06:35:25.680
But if we drive the cost of building systems

06:35:25.680 --> 06:35:27.831
down, to close to

06:35:27.831 --> 06:35:30.210
zero because it's automated, we could have

06:35:30.210 --> 06:35:32.270
disposable systems. Like, rather than sort of build

06:35:32.510 --> 06:35:34.751
building the perfect system that needs to work for every problem,

06:35:34.751 --> 06:35:36.820
we can build a one off system that's that's

06:35:36.820 --> 06:35:38.980
that's independent of every single problem and

06:35:38.980 --> 06:35:41.161
just worry less about getting

06:35:41.161 --> 06:35:43.400
every detail right because if it's wrong,

06:35:43.400 --> 06:35:45.550
just like prompt the AI to create a

06:35:45.550 --> 06:35:47.760
new system, then you have a new system later that day.

06:35:54.269 --> 06:35:56.349
I think one area where we're actually like

06:35:56.349 --> 06:35:58.460
getting to a point where maybe like

06:35:58.460 --> 06:36:00.600
humans aren't needed is kinda like the review flow.

06:36:00.841 --> 06:36:02.290
So I think, like,

06:36:03.091 --> 06:36:05.491
internally, like, you know, all PRs are reviewed

06:36:05.491 --> 06:36:07.350
by by codex nowadays at OpenAI.

06:36:07.770 --> 06:36:09.180
It's getting to the point where it's, like,

06:36:09.821 --> 06:36:11.902
really almost superhuman, and, like, it can catch

06:36:11.902 --> 06:36:14.241
things that even, like, a human human

06:36:14.241 --> 06:36:16.100
reviewer wouldn't catch. So we've

06:36:16.290 --> 06:36:18.450
you know, joked around about, you know, like,

06:36:18.450 --> 06:36:20.560
what if Codecs could approve your PRs

06:36:20.560 --> 06:36:22.770
and also, like, reject your PRs. But I think it's, like,

06:36:23.010 --> 06:36:25.150
not that far far away to imagine a

06:36:25.150 --> 06:36:27.810
world where, like, you know, actually, you know, 90%

06:36:28.110 --> 06:36:30.470
of PRs just go through automated review,

06:36:30.530 --> 06:36:32.800
and maybe there's only five or

06:36:32.800 --> 06:36:34.640
10% where they the humans actually

06:36:34.960 --> 06:36:37.270
need to be reviewing things. Yeah.

06:36:38.010 --> 06:36:40.140
Okay. So, with that in mind,

06:36:40.300 --> 06:36:42.460
we welcome a lot of audience questions, and

06:36:42.460 --> 06:36:44.480
we'll be looking through them from here.

06:36:44.841 --> 06:36:46.780
We'll get to the part where people may disagree.

06:36:47.220 --> 06:36:49.591
So throughout this, workshop,

06:36:49.591 --> 06:36:51.341
we've heard from Azilia,

06:36:51.800 --> 06:36:54.220
Yan, and many people that there's a lot of bottlenecks

06:36:54.360 --> 06:36:56.840
potentially. But they all take somewhat

06:36:57.380 --> 06:36:59.331
hopeful position. They are perhaps

06:36:59.390 --> 06:37:01.470
solvable. And that includes text time

06:37:01.470 --> 06:37:03.550
scaling for Azalea, and Yang

06:37:03.550 --> 06:37:05.769
talking about e vals,

06:37:05.769 --> 06:37:07.902
inefficient search, things that

06:37:07.902 --> 06:37:10.091
need human intervention. Which of

06:37:10.091 --> 06:37:12.331
your list of bottlenecks you think could

06:37:12.331 --> 06:37:14.491
be fundamentally would be fundamentally

06:37:14.630 --> 06:37:16.571
unsolvable coming from the opposition

06:37:16.790 --> 06:37:18.920
camp? By scale plus

06:37:19.060 --> 06:37:21.390
data plus maybe abandoning

06:37:21.448 --> 06:37:23.550
the system and starting a whole new ones.

06:37:29.250 --> 06:37:31.410
Yeah. I think I think it's to me, it's like

06:37:31.410 --> 06:37:32.870
this fundamental, like,

06:37:33.430 --> 06:37:35.591
outer loop of software development where you're kind

06:37:35.591 --> 06:37:37.450
of like, gathering requirements

06:37:38.070 --> 06:37:40.120
about the world, whether that's, like, coming

06:37:40.120 --> 06:37:42.300
from you know, people using products or

06:37:44.040 --> 06:37:46.220
know, systems breaking down due to, like, unexpected

06:37:47.480 --> 06:37:49.520
things happening in the world where, like,

06:37:49.760 --> 06:37:51.140
that's not necessarily predictable.

06:37:51.921 --> 06:37:54.010
If all you observe is, like, purely

06:37:54.010 --> 06:37:56.020
within the system, if if that makes

06:37:56.020 --> 06:37:58.090
sense. So there's this, like, whole outer loop of, like,

06:37:58.330 --> 06:37:59.948
gathering information from the world,

06:38:00.410 --> 06:38:02.780
potentially, like, working with other humans

06:38:02.840 --> 06:38:05.050
to, like, communicate, like, what needs to

06:38:05.050 --> 06:38:07.120
be done. Feel like that

06:38:08.160 --> 06:38:10.240
is still, like, a a long, long way

06:38:10.240 --> 06:38:11.990
from from being solved.

06:38:13.980 --> 06:38:16.099
Yeah. I I see many things, but I would

06:38:16.099 --> 06:38:18.179
just say, I would also don't get me wrong. I

06:38:18.179 --> 06:38:20.509
would love to have these agents just do all the things,

06:38:20.509 --> 06:38:22.509
and I can relax. Right? But,

06:38:23.510 --> 06:38:25.930
first, they need to be verified. That's my biggest

06:38:26.470 --> 06:38:28.500
thing. And I don't know how much it definitely

06:38:28.500 --> 06:38:30.660
reduces the work you need

06:38:30.660 --> 06:38:32.820
to do, but that verification of the of

06:38:32.820 --> 06:38:34.868
the result that you get you know, whether

06:38:34.868 --> 06:38:36.640
that is correct and whether

06:38:36.940 --> 06:38:39.290
it is correct longer term, is

06:38:39.460 --> 06:38:41.780
one of the bottlenecks I see as the major bottlenecks

06:38:41.780 --> 06:38:42.790
at least at this point.

06:38:46.120 --> 06:38:48.190
I have responses to that? Great.

06:38:49.630 --> 06:38:50.911
The the nice thing about,

06:38:51.980 --> 06:38:54.140
the current paths of scaling that we have ahead

06:38:54.140 --> 06:38:56.279
of us is that you can do functional

06:38:56.279 --> 06:38:58.440
proofs, functional verification both

06:38:59.001 --> 06:39:01.300
during training time to generate data, as

06:39:01.320 --> 06:39:03.100
well as during test time

06:39:03.421 --> 06:39:05.650
to verify your answer. And ensure

06:39:05.970 --> 06:39:08.189
that you're whether it's your kernel

06:39:08.189 --> 06:39:09.840
is accurate numerically,

06:39:10.640 --> 06:39:12.800
some startups haven't been doing that recently, but in

06:39:12.800 --> 06:39:14.530
general, this is something you can do.

06:39:15.890 --> 06:39:18.070
Or, you know, if you want to ensure

06:39:18.210 --> 06:39:20.250
that know, your model

06:39:20.250 --> 06:39:22.330
does know how to, say, search

06:39:22.330 --> 06:39:24.140
different types of architectures that you

06:39:24.370 --> 06:39:26.470
wanna implement, these are these are things that you can

06:39:26.470 --> 06:39:28.630
functionally verify both at training time

06:39:28.630 --> 06:39:31.128
and inference time. Right? So it is not something

06:39:31.189 --> 06:39:33.279
that is it needs you know, the

06:39:33.279 --> 06:39:35.359
the training data doesn't exist at all. Right? You can

06:39:35.359 --> 06:39:37.580
generate it, and you can prove it

06:39:37.580 --> 06:39:39.790
out functionally. At test time because most

06:39:39.790 --> 06:39:41.740
system developer tasks do

06:39:41.800 --> 06:39:44.040
not have a, you know, creativity aspect

06:39:44.040 --> 06:39:46.160
of it. There are there are multiple engineering trade

06:39:46.160 --> 06:39:48.581
offs, and you can weigh them. And and you have a solution.

06:39:48.720 --> 06:39:50.960
So so this is something that, you know, system

06:39:50.960 --> 06:39:52.700
developers aren't really needed for.

06:39:55.280 --> 06:39:57.440
Yeah. I I I mean, I I think AI is,

06:39:58.040 --> 06:40:00.120
often better at code review and finding bugs than a

06:40:00.120 --> 06:40:02.220
lot of a lot of people. Like, I I definitely

06:40:02.599 --> 06:40:04.750
feel like there's there's a there's

06:40:04.750 --> 06:40:06.570
a it's definitely a a case where

06:40:07.081 --> 06:40:09.300
where finding bugs is hard, but but,

06:40:09.540 --> 06:40:11.250
I don't think humans are particularly

06:40:11.570 --> 06:40:13.630
good at that either. What

06:40:13.630 --> 06:40:14.929
about solving bugs?

06:40:15.930 --> 06:40:18.050
Oh, I I mean like I I've had, like,

06:40:18.050 --> 06:40:20.290
really tricky memory leaks that I feel like

06:40:20.290 --> 06:40:22.230
would have taken me days to hunt down.

06:40:22.330 --> 06:40:24.490
I just throw out the AI and it it solves it

06:40:24.490 --> 06:40:26.540
for me really quickly. I it's amazing that

06:40:26.540 --> 06:40:28.650
type of problem. Yeah. But is it

06:40:28.650 --> 06:40:31.150
a skill issue? Is it like a AI issue?

06:40:34.410 --> 06:40:35.850
Maybe. Maybe. I I mean, it's it's,

06:40:36.810 --> 06:40:38.480
it's less work.

06:40:39.370 --> 06:40:41.509
I I feel like I need to

06:40:41.509 --> 06:40:43.590
explain my position. What I

06:40:43.590 --> 06:40:45.720
mean is like, what do you think is

06:40:45.720 --> 06:40:47.870
truly fundamental? Right? Mhmm. We

06:40:47.870 --> 06:40:50.190
can see some bottlenecks right now, but are they growing

06:40:50.190 --> 06:40:52.510
pains, or are there something truly fundamental?

06:40:52.810 --> 06:40:55.020
We mentioned something like interacting

06:40:55.020 --> 06:40:56.810
with human, gathering information,

06:40:57.810 --> 06:40:59.890
things breaking down that there might not be

06:40:59.890 --> 06:41:01.939
data about, how to resolve that kind

06:41:01.939 --> 06:41:04.160
of breakdown? Right? For these

06:41:04.160 --> 06:41:06.340
problems, maybe we won't have them anymore,

06:41:06.400 --> 06:41:08.500
or maybe we'll yeah, any

06:41:09.060 --> 06:41:11.530
any sort of, thoughts on that?

06:41:11.830 --> 06:41:14.150
Specifically, how do agents resolve bugs

06:41:14.150 --> 06:41:16.320
when the spec itself wrong? And when

06:41:16.320 --> 06:41:18.560
the person, you know, interacting with the agents,

06:41:18.560 --> 06:41:20.821
if there's a person right now, does not quite

06:41:20.821 --> 06:41:21.321
understand.

06:41:22.939 --> 06:41:25.259
Yeah. I do I do think we're reaching a point where basically

06:41:25.259 --> 06:41:27.390
it's like if you can describe what

06:41:27.390 --> 06:41:29.171
you want in natural language,

06:41:29.921 --> 06:41:32.260
we have these magical processing machines

06:41:32.321 --> 06:41:34.519
that basically turn one form of language

06:41:34.519 --> 06:41:36.750
into let's say, like, machine language.

06:41:36.750 --> 06:41:38.830
But, yeah, it's like, I think, yeah, to your point, it's like what

06:41:38.830 --> 06:41:41.200
if the spec itself is wrong

06:41:41.200 --> 06:41:43.440
or, even more challenging is, like, when the

06:41:43.440 --> 06:41:44.910
spec itself there, like, is no

06:41:45.550 --> 06:41:47.948
clear spec if it's, like, a you know, like, if it's a decision

06:41:47.948 --> 06:41:50.150
that has to it's like

06:41:50.150 --> 06:41:51.530
a trade off between multiple

06:41:52.591 --> 06:41:54.830
you know, parties with conflicting interests

06:41:54.830 --> 06:41:57.070
and then, you know, like, you you

06:41:57.070 --> 06:41:58.910
have one form of the spec, which

06:41:59.870 --> 06:42:01.948
you know, is advantage advantageous to

06:42:01.948 --> 06:42:03.880
some people, but not others. Then you have these

06:42:04.120 --> 06:42:06.439
very moral, like, maybe even, like, moral gray

06:42:06.439 --> 06:42:08.530
areas that kind

06:42:08.530 --> 06:42:10.550
of require humans at least for now.

06:42:12.030 --> 06:42:14.171
So all the things and correct

06:42:14.171 --> 06:42:16.591
me if I'm wrong, we have we have talked about our correctness

06:42:17.150 --> 06:42:18.690
kind of bugs. Right?

06:42:19.411 --> 06:42:21.652
There are also performance bugs, particularly talking

06:42:21.652 --> 06:42:23.660
about, like, cloud level programming.

06:42:23.660 --> 06:42:25.800
Let's say a microservice based application which has

06:42:25.960 --> 06:42:28.220
I don't know, thousands of nodes.

06:42:28.440 --> 06:42:29.900
When something goes wrong,

06:42:30.460 --> 06:42:32.620
it's very hard to actually pinpoint where it is.

06:42:32.620 --> 06:42:34.679
And we have a lot of machine learning

06:42:34.679 --> 06:42:36.760
based solutions. But they may or

06:42:36.760 --> 06:42:38.940
may not actually provide you with the right solution.

06:42:40.280 --> 06:42:42.570
Because of, you know, that stochasticity

06:42:42.788 --> 06:42:44.890
or nondeterminism doesn't actually

06:42:45.220 --> 06:42:46.820
bring it home. And that requires

06:42:47.359 --> 06:42:49.550
creativity because things go beyond

06:42:49.550 --> 06:42:51.708
what is seen and sort of somewhere a back

06:42:51.708 --> 06:42:53.780
pressure was built that sort of showed up

06:42:53.780 --> 06:42:56.120
here, at the end of the line.

06:42:56.500 --> 06:42:58.580
May or may not be easily feasible for an

06:42:58.580 --> 06:43:00.590
agent to track because it's hard for human

06:43:00.590 --> 06:43:03.090
beings too. Right? But creativity

06:43:03.230 --> 06:43:05.529
can take us there. Do you agree?

06:43:09.060 --> 06:43:11.200
So one minute on this. Yeah. So it's all I'll

06:43:11.200 --> 06:43:13.250
I'll throw a bone to the the other side.

06:43:13.410 --> 06:43:15.450
And give it sort of a a point,

06:43:15.890 --> 06:43:17.929
against, and so so so I was doing one

06:43:17.929 --> 06:43:19.970
example. Where

06:43:19.970 --> 06:43:22.130
I was asking AI to optimize a kernel,

06:43:22.520 --> 06:43:24.600
and the AI just removed all the memory fences.

06:43:25.690 --> 06:43:27.470
And removing all the memory fences,

06:43:27.730 --> 06:43:29.990
made it run faster, but it also introduced

06:43:30.130 --> 06:43:32.360
the data race that that just didn't

06:43:32.360 --> 06:43:34.620
manifest very often. So all the tests passed.

06:43:35.570 --> 06:43:37.810
And that was sort of a subtle correctness issue

06:43:37.810 --> 06:43:39.660
where, you know, I looked at that that that

06:43:39.900 --> 06:43:41.980
that change, and I'm like, this is this is

06:43:41.980 --> 06:43:43.970
clearly this is pretty sus.

06:43:44.671 --> 06:43:46.751
Now I I agree to it. I think it's still early. I think

06:43:46.751 --> 06:43:48.300
that there's there's,

06:43:48.759 --> 06:43:50.810
you know, technical solutions to that. But that's

06:43:50.810 --> 06:43:52.890
definitely the type of thing where, you know, you when

06:43:52.890 --> 06:43:53.640
you're using these types

06:43:56.920 --> 06:43:59.340
like, potential solutions,

06:43:59.640 --> 06:44:01.800
having, you know, adversarial AIs whose

06:44:01.800 --> 06:44:03.770
job it is to find the bugs

06:44:04.070 --> 06:44:06.310
in the optimization AIs is is is

06:44:06.310 --> 06:44:08.410
another approach. But but, you

06:44:08.410 --> 06:44:10.708
know, these are these are challenges that I think are are surmountable

06:44:11.170 --> 06:44:13.670
despite you know, that that being a very

06:44:13.730 --> 06:44:15.190
scary type of of optimization.

06:44:16.720 --> 06:44:18.180
Don't delete your memory fences.

06:44:19.220 --> 06:44:21.273
Yeah. Is really interesting, and I relate to

06:44:21.273 --> 06:44:23.670
what I was about to ask. Hasan hinted

06:44:23.670 --> 06:44:25.730
at moral questions. In

06:44:25.790 --> 06:44:27.950
deciding whether or not to take

06:44:27.950 --> 06:44:30.160
agent solutions. And Jason

06:44:30.160 --> 06:44:32.180
mentioned, what if the

06:44:32.421 --> 06:44:34.741
agent's too good at getting the fastest

06:44:34.741 --> 06:44:37.038
solution? Given an incorrect spec.

06:44:37.940 --> 06:44:40.280
So what are the, you know, risk, economics,

06:44:40.341 --> 06:44:42.368
accountability, issues

06:44:42.368 --> 06:44:44.650
that you see? Within

06:44:44.850 --> 06:44:46.991
the of the questions

06:44:46.991 --> 06:44:49.070
we're seeding is that if an agent is 10 x

06:44:49.070 --> 06:44:51.251
cheaper and two x slow, but

06:44:51.392 --> 06:44:53.200
correct, 98% of the

06:44:53.500 --> 06:44:55.550
time do I deploy it. Who is

06:44:55.550 --> 06:44:57.870
legally accountable when agents design

06:44:57.870 --> 06:45:00.171
unsafe infrastructure? And,

06:45:00.171 --> 06:45:02.310
like, is there gonna be a type of job where

06:45:02.310 --> 06:45:04.390
it's just debugging AI outputs when

06:45:04.390 --> 06:45:06.530
things go astray, and we just don't

06:45:06.530 --> 06:45:07.591
call them developers.

06:45:09.038 --> 06:45:11.100
Where do you see we go in

06:45:11.341 --> 06:45:11.840
the future?

06:45:18.740 --> 06:45:20.341
It's it's certainly true that the,

06:45:21.200 --> 06:45:23.270
catastrophic loss potential when

06:45:23.270 --> 06:45:25.770
you replace all system developers, which will happen,

06:45:26.320 --> 06:45:27.440
is is very high.

06:45:28.470 --> 06:45:30.810
You know, the these systems can have a catastrophic

06:45:30.950 --> 06:45:32.950
bug, which which may even be

06:45:33.009 --> 06:45:35.509
intentionally placed there, like a backdoor, or may not intentionally

06:45:35.570 --> 06:45:37.636
be placed there that that can be

06:45:37.636 --> 06:45:39.700
exploited or can cause, you know, the loss of

06:45:39.700 --> 06:45:41.640
a lot of performance or even worse, you know,

06:45:42.280 --> 06:45:44.630
say a system for, like, farming, you know, famine Right?

06:45:45.270 --> 06:45:47.350
These are these are certainly risks. And

06:45:47.350 --> 06:45:49.386
as junior system developers are replaced,

06:45:49.386 --> 06:45:51.460
which they are already being replaced,

06:45:52.970 --> 06:45:55.310
The the the skill sets to debug

06:45:55.630 --> 06:45:57.790
will go away, but that that creates a new field, a new job.

06:45:57.790 --> 06:45:59.991
Right? Maybe not a system orchestrator system

06:45:59.991 --> 06:46:02.331
architect, but maybe a system bug

06:46:02.970 --> 06:46:05.210
reviewer. Right? But not a system developer.

06:46:05.370 --> 06:46:07.456
And so so certainly these sorts of jobs

06:46:07.456 --> 06:46:09.769
will have to come. But they will be far

06:46:10.040 --> 06:46:12.232
fewer between when when

06:46:12.232 --> 06:46:14.251
system developer agents can

06:46:14.530 --> 06:46:16.070
develop and debug constantly,

06:46:16.850 --> 06:46:19.110
you know, in in in in Jason's specific

06:46:19.600 --> 06:46:21.759
example, Right? The the silly thing here

06:46:21.759 --> 06:46:23.860
is that system you know, these these

06:46:23.860 --> 06:46:26.099
current code agents are incredible at writing

06:46:26.099 --> 06:46:28.189
test cases. You just didn't put

06:46:28.189 --> 06:46:30.240
in your prompt. Right? So So, like, you know, that that's

06:46:31.810 --> 06:46:33.410
important to, record

06:46:36.890 --> 06:46:39.330
I mean, yeah. I do think there's an analogy to, like,

06:46:39.810 --> 06:46:41.530
more, like, traditional forms of

06:46:41.850 --> 06:46:44.190
engineering. I think software engineering is one of the

06:46:44.671 --> 06:46:46.760
types of engineering that's, like, almost fairly

06:46:46.760 --> 06:46:49.180
engineering and that there's, like, actually very few

06:46:50.081 --> 06:46:50.821
like, accountability

06:46:52.830 --> 06:46:54.990
kind of, like, guardrails for that field. But

06:46:54.990 --> 06:46:57.122
if you're if you think about, like, more traditional,

06:46:57.474 --> 06:46:59.570
you know, like, civil engineering or things like this

06:46:59.570 --> 06:47:01.730
where it's like sure, like, the design the

06:47:01.730 --> 06:47:02.708
process of, like,

06:47:04.210 --> 06:47:06.710
making the designs is, like, all done by software nowadays.

06:47:07.690 --> 06:47:09.841
Still, you know, like, the civil engineer is, like,

06:47:09.841 --> 06:47:10.591
accountable for,

06:47:11.980 --> 06:47:14.060
the safety of the the final building. So

06:47:14.060 --> 06:47:16.300
I think it it does kind of, like, feel like that

06:47:17.560 --> 06:47:19.341
appropriate. Analogy.

06:47:21.660 --> 06:47:23.991
Yeah. Yeah. Releasing

06:47:24.050 --> 06:47:26.280
to, like,

06:47:26.280 --> 06:47:27.859
risk and reward the audience

06:47:28.982 --> 06:47:31.411
questions. Largely, it just include

06:47:31.550 --> 06:47:33.790
a lot of different issues that we have yet to

06:47:33.790 --> 06:47:36.222
touch on. Will just do a brief summary.

06:47:36.930 --> 06:47:39.010
We have extensive extendability

06:47:39.152 --> 06:47:41.571
or maintainability of AI generated codebase.

06:47:42.140 --> 06:47:43.679
If so much of the code is actually

06:47:44.380 --> 06:47:46.390
AI generated? How do we even read and maintain

06:47:46.390 --> 06:47:47.610
that going forward?

06:47:48.610 --> 06:47:50.770
People are a little bit upset that GPU

06:47:50.770 --> 06:47:52.960
kernel generation is considered nearly solved.

06:47:57.370 --> 06:47:59.530
Human labor is a big part of r and

06:47:59.530 --> 06:48:01.550
d cost, and it might

06:48:01.550 --> 06:48:03.790
just be for profit that they're reducing it rather

06:48:03.790 --> 06:48:05.581
than being you know, meaningfully

06:48:05.800 --> 06:48:08.020
reduced. Are we confident on the

06:48:08.581 --> 06:48:10.642
accuracy, etcetera, and you know,

06:48:10.642 --> 06:48:12.880
current capability is not that good. Please

06:48:12.880 --> 06:48:14.940
stop exaggerating it. So just

06:48:14.940 --> 06:48:17.180
to give you some example on the very opinions that

06:48:17.180 --> 06:48:19.320
we have. Niraj,

06:48:19.320 --> 06:48:21.620
your turn. What what's

06:48:21.620 --> 06:48:23.800
ultimately the question? There's

06:48:23.860 --> 06:48:25.740
somebody on risk accountability.

06:48:25.900 --> 06:48:27.730
Well, I I want

06:48:28.029 --> 06:48:30.259
accountability. Right, if you take that's my personal

06:48:30.259 --> 06:48:32.520
take on it. And that's why I I kept saying agents

06:48:32.580 --> 06:48:34.940
should be really assistants. Whoever

06:48:34.940 --> 06:48:36.960
used it. I can go find that person,

06:48:36.960 --> 06:48:39.100
right, when things go wrong. Right?

06:48:39.100 --> 06:48:41.040
So there has to be accountability. Otherwise,

06:48:41.180 --> 06:48:43.240
yeah, it's like you

06:48:43.240 --> 06:48:45.480
said, we can actually, I feel,

06:48:45.480 --> 06:48:47.599
be led into this undebugable mess that

06:48:47.599 --> 06:48:49.570
only agents can understand, and it's

06:48:49.650 --> 06:48:51.730
now we have stopped thinking. We have stopped learning how

06:48:51.730 --> 06:48:53.950
to think and how to debug. So we

06:48:53.950 --> 06:48:56.290
are again at the mercy of these agents again to debug

06:48:56.429 --> 06:48:58.730
their own undiviable mess. That's

06:48:58.730 --> 06:49:00.029
how I honestly feel.

06:49:02.330 --> 06:49:04.580
So that was accountability. What because you've

06:49:04.740 --> 06:49:06.820
there are too many questions. Can you be sorry? I

06:49:06.820 --> 06:49:09.060
feel like the question we should be focusing on is,

06:49:09.060 --> 06:49:11.230
like, will the current bottleneck

06:49:11.230 --> 06:49:13.170
that is, extensibility and maintainability

06:49:13.790 --> 06:49:16.240
in this, like, risk landscape.

06:49:17.310 --> 06:49:19.420
You know, is this be a long term

06:49:19.420 --> 06:49:21.519
bottleneck to replacing system developers?

06:49:22.980 --> 06:49:25.060
For the sake of sticking to the question, I think the

06:49:25.060 --> 06:49:27.259
implication is it actually makes

06:49:27.259 --> 06:49:29.599
the system developers potentially more valuable.

06:49:29.710 --> 06:49:31.960
Right? People with experience, people who can

06:49:32.120 --> 06:49:33.980
go into the weeds. I

06:49:35.400 --> 06:49:37.720
somebody else wants to well, I don't even believe.

06:49:37.720 --> 06:49:39.812
Yeah. Any opposition to that? Yeah. I don't even

06:49:39.812 --> 06:49:40.270
believe that.

06:49:45.269 --> 06:49:47.349
Yeah. Like, so I so I think that definitely, like, if I look

06:49:47.349 --> 06:49:49.620
at what was say, what's the biggest weakness

06:49:49.759 --> 06:49:51.620
of AI coding tools today? They overcomplicate

06:49:51.920 --> 06:49:54.230
their solutions. They write x too much code than than

06:49:54.230 --> 06:49:56.620
they should. And so

06:49:57.520 --> 06:49:59.661
you know, what what do I do to for that? Like, I

06:49:59.661 --> 06:50:01.680
I ask the AI agents to simplify

06:50:01.741 --> 06:50:03.960
their own code. Some Sometimes, I'll ask Claude

06:50:03.960 --> 06:50:06.110
to simplify Codex's code and and

06:50:06.110 --> 06:50:08.270
Codex to simplify Claude's code. And I I

06:50:08.270 --> 06:50:10.321
I and there's been PRs that

06:50:10.321 --> 06:50:12.370
were so complicated. I went through, like,

06:50:13.251 --> 06:50:15.331
five to 10 iterations of just, like, simplify this,

06:50:15.331 --> 06:50:17.010
simplify this, simplify this.

06:50:17.652 --> 06:50:19.732
And then, like, by hand, the thing that I'm

06:50:19.732 --> 06:50:21.890
doing is I'm giving it, like, specific guidance

06:50:22.110 --> 06:50:24.321
of, like, simplify like like, this is unnecessary.

06:50:24.380 --> 06:50:26.321
Find a way not to do that. Like

06:50:26.402 --> 06:50:28.722
and and, like, I that that's, like, the main feedback

06:50:28.722 --> 06:50:30.860
I'm giving it. It's not like, get the thing

06:50:30.860 --> 06:50:33.269
right. It's it's it's

06:50:33.510 --> 06:50:35.610
do it in a simpler, more elegant, more maintainable

06:50:35.671 --> 06:50:37.700
way. And,

06:50:37.700 --> 06:50:39.860
you know, I I I think that this is something that that

06:50:39.860 --> 06:50:41.950
AI agents are gonna improve Like, I I don't think

06:50:41.950 --> 06:50:44.130
there there's some fundamental reason

06:50:44.429 --> 06:50:46.690
why you know, we can't train

06:50:46.750 --> 06:50:49.090
AI agents to write simpler, more more attainable

06:50:49.230 --> 06:50:51.350
code. Especially when people like me

06:50:51.350 --> 06:50:53.150
are generating so much training data

06:50:54.429 --> 06:50:56.380
for other folks on this panel to use.

06:50:57.020 --> 06:50:59.040
To to try to get

06:51:00.640 --> 06:51:03.017
do you use my data? Never mind.

06:51:03.195 --> 06:51:04.372
Never mind.

06:51:06.720 --> 06:51:08.760
The so so I I think

06:51:08.760 --> 06:51:10.940
I think that's something that will improve over time, and it is definitely

06:51:10.940 --> 06:51:12.982
one of the the biggest challenges. The other the other

06:51:12.982 --> 06:51:14.130
thing is just sort of like

06:51:16.491 --> 06:51:18.751
learning to let go. And that's one thing I've I've increasingly

06:51:19.210 --> 06:51:21.100
done more and more In that

06:51:21.350 --> 06:51:23.591
okay. If if the only one who's gonna need to read

06:51:23.591 --> 06:51:25.770
and maintain this code is other AI agents,

06:51:26.360 --> 06:51:28.520
How important is it that it that it

06:51:28.520 --> 06:51:30.560
actually be as simple and elegant as possible?

06:51:30.800 --> 06:51:32.960
Like, sometimes, like, I'm pushing to, like

06:51:32.960 --> 06:51:35.390
like, turn that 30 line solution and

06:51:35.710 --> 06:51:38.210
the 25 cell line solution But,

06:51:38.429 --> 06:51:40.670
you know, if if at at

06:51:40.670 --> 06:51:42.780
some point, like, if if it's if it's if

06:51:43.020 --> 06:51:45.050
maybe that's not even necessary. Maybe I'm wasting my time,

06:51:45.050 --> 06:51:47.240
and I should just say, the 30 line solution

06:51:47.240 --> 06:51:49.370
is fine. I'm I'm not

06:51:49.770 --> 06:51:52.020
totally sure I buy that argument, but I I could definitely

06:51:52.020 --> 06:51:54.050
see sort of see a world where, like,

06:51:54.050 --> 06:51:55.510
if we're entirely replaced,

06:51:56.450 --> 06:51:58.640
maybe the best solution

06:51:58.640 --> 06:52:00.780
to a little bit too much complexity is even

06:52:00.780 --> 06:52:01.310
more AI.

06:52:03.710 --> 06:52:06.210
Feeding to the point of the audience, which is, you know, current capabilities

06:52:06.270 --> 06:52:08.370
aren't that good. Like, calm down.

06:52:08.570 --> 06:52:10.900
You know, the the the important thing to recognize is

06:52:11.415 --> 06:52:13.530
today, AI agents are really, really good

06:52:13.530 --> 06:52:15.501
for MVPs. Not necessarily

06:52:15.640 --> 06:52:17.878
that great for going across

06:52:17.878 --> 06:52:19.958
your entire code base once it's too large. Right? Once it's

06:52:19.958 --> 06:52:22.270
spilling out of context windows, once extremely

06:52:22.270 --> 06:52:24.350
large, you you just it just can't make sense

06:52:24.350 --> 06:52:26.509
of it all. Right? It'll you can get it caught in loops

06:52:26.509 --> 06:52:28.700
doing the same thing over and over again. But

06:52:28.700 --> 06:52:31.020
the simple thing there is, like, look at how much research

06:52:31.020 --> 06:52:32.470
there is on long context here.

06:52:33.590 --> 06:52:36.070
Both on the data side as well as on the modeling

06:52:36.070 --> 06:52:38.160
side. Models are just

06:52:38.160 --> 06:52:40.430
limited in capabilities today. Know, over

06:52:40.430 --> 06:52:42.591
the last two years, we've had context sense go from, like,

06:52:42.591 --> 06:52:44.675
four k to a million. Or 2,000,000

06:52:44.675 --> 06:52:46.698
and and and they gonna do multiple

06:52:46.698 --> 06:52:48.880
orders of magnitude increases again, probably.

06:52:49.090 --> 06:52:51.140
Right? And so a lot of these issues

06:52:51.140 --> 06:52:53.429
will be solved quite simply by just

06:52:53.429 --> 06:52:55.460
more scaling. Of of the models on

06:52:55.460 --> 06:52:57.581
context length on data on that that

06:52:57.581 --> 06:52:59.110
side. Right? So so the issues of

06:52:59.831 --> 06:53:01.911
hey. AI agents are only good for MVPs, but

06:53:01.911 --> 06:53:04.040
not for actually dealing with a

06:53:04.040 --> 06:53:06.140
lot large messy code base. When you wanna,

06:53:06.140 --> 06:53:08.538
say, refactor something. That that that's stuff that probably

06:53:08.538 --> 06:53:10.840
is just an artifact of current capabilities,

06:53:10.840 --> 06:53:12.980
not not indicative of future ones.

06:53:13.220 --> 06:53:15.520
Mhmm. So I have a minute to respond

06:53:15.520 --> 06:53:17.660
to that? And yeah, yeah.

06:53:17.660 --> 06:53:19.720
I just wanna add. I think, like, it's sometimes we take

06:53:19.720 --> 06:53:22.040
for granted, like, how quickly these things have

06:53:22.760 --> 06:53:25.001
become available. Like, just I think it's just

06:53:25.001 --> 06:53:27.300
really been this year that coding

06:53:27.360 --> 06:53:29.300
agents have actually gotten very good.

06:53:29.810 --> 06:53:32.050
And so there's, like, a there's, overhang between,

06:53:32.050 --> 06:53:34.170
like, raw capabilities that have

06:53:34.170 --> 06:53:36.029
become available to us versus, like,

06:53:36.421 --> 06:53:38.581
how good we are at using them and also, like, the

06:53:38.581 --> 06:53:40.490
tools that we have. And we're I think we're, like,

06:53:41.390 --> 06:53:43.581
even, at OpenAI, like, we're not quite sure

06:53:43.581 --> 06:53:45.741
how to do this. Like, you can today, like,

06:53:45.741 --> 06:53:47.760
all of us, we can, like, deploy an

06:53:47.760 --> 06:53:49.780
agent on, like, every file in the code base.

06:53:50.091 --> 06:53:52.171
And, you know, tell it to make an improvement. But, like, what are

06:53:52.171 --> 06:53:54.030
you gonna do with all those PRs?

06:53:54.250 --> 06:53:56.070
Right? So I think we're we're there

06:53:56.310 --> 06:53:58.390
there's still a lot of room to, like, figure out how

06:53:58.390 --> 06:54:00.020
to best make use of these. Agents.

06:54:02.081 --> 06:54:04.161
Thank you. And on that, we

06:54:04.161 --> 06:54:06.482
wanna move on to predictions, and it also relates

06:54:06.482 --> 06:54:08.940
to the audience question. Can you tie an economic

06:54:08.940 --> 06:54:11.010
value indicator to the replacement assistant

06:54:11.491 --> 06:54:13.652
developers? Now to make a concrete on where do we

06:54:13.652 --> 06:54:15.330
actually agree and disagree on,

06:54:15.970 --> 06:54:18.060
for example, what would be your

06:54:18.060 --> 06:54:19.310
adoption rate

06:54:20.280 --> 06:54:22.540
for coding agents prediction in the next

06:54:22.920 --> 06:54:25.000
year versus five year? How many roles

06:54:25.000 --> 06:54:27.099
will be impacted? What's being automated?

06:54:27.940 --> 06:54:29.560
And for the once against,

06:54:30.020 --> 06:54:32.100
the replacement theories, what will

06:54:32.100 --> 06:54:34.290
still exist? Resist,

06:54:34.910 --> 06:54:36.990
automation? What are the failure modes

06:54:36.990 --> 06:54:39.230
that we'll see that keep going?

06:54:40.750 --> 06:54:43.090
Are we specifically talking about code?

06:54:43.310 --> 06:54:45.550
We're talking about system developers, like

06:54:45.550 --> 06:54:47.640
predictions. If we don't think

06:54:47.640 --> 06:54:49.260
it's going to be replaced

06:54:49.760 --> 06:54:52.081
with agents. Yeah. If if we're talking about system

06:54:52.081 --> 06:54:53.540
developers, meaning the coders,

06:54:54.310 --> 06:54:56.550
programmers, a lot of it will be replaced.

06:54:56.550 --> 06:54:58.930
What I don't actually believe is

06:55:00.130 --> 06:55:01.911
that would be without any supervision.

06:55:03.599 --> 06:55:05.759
That I don't believe, or at least I don't want to

06:55:05.840 --> 06:55:07.991
us to get in that mess.

06:55:09.010 --> 06:55:11.020
And so this is actually pushing us in

06:55:11.020 --> 06:55:13.180
lot of other directions. How do I

06:55:13.180 --> 06:55:15.378
now get verifications? So, you

06:55:15.378 --> 06:55:17.380
know, formal verification is by far far

06:55:17.380 --> 06:55:19.370
away from actually doing this, but that would be

06:55:19.770 --> 06:55:21.812
something really fantastic to get

06:55:21.812 --> 06:55:23.991
there so that it can verify for us. And then

06:55:24.310 --> 06:55:26.470
perhaps I can think about replacement as

06:55:26.470 --> 06:55:28.570
such. But, until then,

06:55:29.402 --> 06:55:31.720
this this verification and actually, what we're doing

06:55:31.720 --> 06:55:33.850
is right, being

06:55:33.850 --> 06:55:35.870
really convinced about that is my biggest

06:55:37.310 --> 06:55:39.571
sort of bottleneck, call that, or roadblock.

06:55:40.440 --> 06:55:41.300
Towards this?

06:55:43.620 --> 06:55:45.560
I don't think this is a particular release.

06:55:46.180 --> 06:55:47.910
Novel prediction, but I do think

06:55:48.550 --> 06:55:50.710
the vast New York like, just even within

06:55:50.710 --> 06:55:52.920
a year, I think the masked the

06:55:53.320 --> 06:55:55.050
the vast majority of code will be

06:55:55.530 --> 06:55:57.581
AI ridden rather than human ridden, but But

06:55:57.581 --> 06:55:59.440
I think, like, that does bring

06:56:00.161 --> 06:56:02.640
like, to the one of the points raised in the audience

06:56:02.640 --> 06:56:04.730
about, you know, like, what do you do when all the

06:56:04.730 --> 06:56:06.610
code's written by AI? It's like it does

06:56:06.930 --> 06:56:09.019
actually, like, degrade

06:56:09.019 --> 06:56:11.420
your or humans humans' mental

06:56:11.420 --> 06:56:13.642
models of how the code operates,

06:56:13.642 --> 06:56:15.689
and it does make it much more challenging to

06:56:15.689 --> 06:56:17.760
maintain things. But I think prediction

06:56:17.760 --> 06:56:20.001
is that we will find ways to make this more

06:56:20.001 --> 06:56:22.060
manageable. I think like, one interesting phenomenon

06:56:22.060 --> 06:56:24.380
is, like, in in many production code

06:56:24.380 --> 06:56:26.501
bases, it's, like, inverted where like, you

06:56:26.501 --> 06:56:28.581
don't have a lot of docs, and you rely on the code

06:56:28.581 --> 06:56:30.732
as a source of truth. Like, if all the

06:56:30.732 --> 06:56:33.071
code's AI written, then I think it has to be the

06:56:33.360 --> 06:56:35.440
reverse where, like, now actually, like,

06:56:35.440 --> 06:56:37.620
the code or the spec is the source of truth,

06:56:37.760 --> 06:56:39.780
and then or sorry. The the docs were

06:56:39.780 --> 06:56:42.100
the specs were the source of truth and the code is Who's

06:56:42.100 --> 06:56:42.540
added

06:56:44.170 --> 06:56:44.670
I

06:56:46.560 --> 06:56:48.700
Yeah. When

06:56:48.700 --> 06:56:50.860
we talk about predictions, it's it's very easy to

06:56:50.860 --> 06:56:53.259
be caught up in know, artificial general

06:56:53.259 --> 06:56:55.259
intelligence nonsense. Right? This is this is

06:56:55.340 --> 06:56:55.800
over our

06:56:57.640 --> 06:56:59.751
specific intelligence in

06:56:59.751 --> 06:57:02.200
in in the areas of system developers. And

06:57:02.200 --> 06:57:04.040
and so that sense, you know, that there is

06:57:05.080 --> 06:57:07.480
you know, not not not giving, like, specific timelines,

06:57:07.480 --> 06:57:09.562
but there's there's it's clear that over

06:57:09.780 --> 06:57:12.040
the next, you know, decade, there will be many categories

06:57:12.040 --> 06:57:14.140
of system developers entirely replaced.

06:57:14.751 --> 06:57:16.991
And I think I think even the opposition seems to agree

06:57:16.991 --> 06:57:19.038
based on what they've said. So

06:57:19.038 --> 06:57:21.359
as far as system dev remind you

06:57:21.359 --> 06:57:23.450
as for next one year or five For next

06:57:23.450 --> 06:57:25.530
next one year. Categories will be replaced in

06:57:25.530 --> 06:57:27.618
the next ten years. Yes. With about one

06:57:27.618 --> 06:57:29.859
or five years. What are your prediction? We're gonna hold

06:57:29.859 --> 06:57:30.920
you a word to it?

06:57:35.520 --> 06:57:37.600
It's okay to be And then in the

06:57:37.600 --> 06:57:39.600
in the next one year, you know, I think

06:57:39.841 --> 06:57:41.470
think the vast majority

06:57:42.628 --> 06:57:44.671
of, researchers will not be passing

06:57:44.671 --> 06:57:46.270
things over to performance engineers.

06:57:46.911 --> 06:57:48.991
Right? That they'll be they'll be they'll

06:57:48.991 --> 06:57:51.470
at they'll continue to use higher and higher level

06:57:51.530 --> 06:57:53.610
libraries, that that do lowering.

06:57:53.610 --> 06:57:55.440
Right? There will be still

06:57:55.660 --> 06:57:57.679
some over the next one year, but in in in five

06:57:57.679 --> 06:57:59.690
years, I think that'll just be gone. Right? There will

06:57:59.690 --> 06:58:02.010
not be this this army of,

06:58:02.010 --> 06:58:03.990
you know, cracked CUDA colonel engineers

06:58:04.710 --> 06:58:06.610
and so that's that's my main prediction.

06:58:07.491 --> 06:58:09.530
So you think we'll have AI researchers but

06:58:09.530 --> 06:58:11.970
not you know? I think I think we'll

06:58:12.349 --> 06:58:14.480
have AI system developers. I don't think we'll

06:58:14.480 --> 06:58:16.610
have automated AI research. Think

06:58:16.831 --> 06:58:18.650
that's fairytale. Stuff. Interesting. Okay.

06:58:21.020 --> 06:58:23.062
Yeah. So so it sounds sounds like we all agree that sort of

06:58:23.062 --> 06:58:25.550
in the next year, like, basically, everyone's gonna be

06:58:25.610 --> 06:58:26.680
using AI agents to code.

06:58:27.800 --> 06:58:30.142
I think prob I think in five years, we'll increasingly

06:58:30.360 --> 06:58:32.580
have AI coming up with the ideas and sort

06:58:32.580 --> 06:58:34.599
of doing that more of that high level reasoning.

06:58:36.030 --> 06:58:38.110
Type work. You know, one one of the great predictions I

06:58:38.110 --> 06:58:40.402
think we're gonna see is I I think we're gonna

06:58:40.460 --> 06:58:42.402
see an explosion of companies

06:58:42.930 --> 06:58:45.091
that are just one or two people. Where, you

06:58:45.091 --> 06:58:47.251
know, rather than sort of hiring a team

06:58:47.251 --> 06:58:48.950
of a 100 or a thousand people,

06:58:49.310 --> 06:58:51.630
you'll you'll just have sort of a person with

06:58:51.630 --> 06:58:52.689
a brilliant idea

06:58:53.902 --> 06:58:55.040
and using AI tools,

06:58:56.671 --> 06:58:58.830
to, you know, not raise money and just sort

06:58:58.830 --> 06:59:00.590
of scale, sort of do more with

06:59:00.990 --> 06:59:03.085
less. And I think there's a lot

06:59:03.085 --> 06:59:05.180
of replace system developers in there. There's

06:59:05.180 --> 06:59:07.340
a lot of people you would have had to hire a bunch of system

06:59:07.660 --> 06:59:09.140
developers, and in that world, you

06:59:09.780 --> 06:59:11.700
you you would can skip that.

06:59:12.500 --> 06:59:14.580
But I think that there's still sort of a kernel of

06:59:14.580 --> 06:59:16.599
knowledge in that that that human is bringing

06:59:17.331 --> 06:59:19.360
And, I think that

06:59:19.760 --> 06:59:22.001
it's gonna allow humans to do way more.

06:59:22.001 --> 06:59:24.310
And I think it's actually an opportunity

06:59:24.689 --> 06:59:26.769
because, if sort of the only

06:59:26.769 --> 06:59:29.140
limitation is your imagination

06:59:29.140 --> 06:59:31.070
and sort of being able to come up with with with

06:59:31.470 --> 06:59:33.630
brilliant ideas. We can move

06:59:33.630 --> 06:59:35.920
away faster as a society. We can get more way

06:59:36.320 --> 06:59:38.740
way more done. We can generate more economic value.

06:59:39.850 --> 06:59:42.350
And I think it could be it would be be great for society.

06:59:42.780 --> 06:59:44.720
I think if you're a systems developer,

06:59:44.850 --> 06:59:46.870
you need to adapt, and you need to

06:59:47.010 --> 06:59:47.510
learn to

06:59:49.040 --> 06:59:51.280
use this technology to sort of do something

06:59:51.280 --> 06:59:53.411
that would have been impossible years before

06:59:53.630 --> 06:59:55.571
and not sort of hope that the industry

06:59:55.790 --> 06:59:57.160
is gonna stay the same forever.

06:59:58.280 --> 07:00:00.360
So you're saying that system developers

07:00:00.360 --> 07:00:02.140
today need to adapt. The next

07:00:02.540 --> 07:00:04.620
five years? Unsure there'll be. But there's probably

07:00:04.620 --> 07:00:06.370
gonna be fewer of them. I see.

07:00:07.970 --> 07:00:09.671
So we're gonna

07:00:10.210 --> 07:00:12.380
have one more audience poll before we move

07:00:12.380 --> 07:00:14.860
on to the closing statements. To

07:00:14.880 --> 07:00:17.240
remind the audience the poll

07:00:17.240 --> 07:00:18.860
is, who believes replacement

07:00:19.940 --> 07:00:22.410
is inevitable? Could you raise your hand now

07:00:22.410 --> 07:00:24.510
after the debate? If you

07:00:24.751 --> 07:00:26.700
still believe so or now you believe so?

07:00:30.118 --> 07:00:31.390
Somehow I can't tell.

07:00:37.609 --> 07:00:39.260
Okay. I think it's about

07:00:39.991 --> 07:00:41.341
like, 10%.

07:00:42.380 --> 07:00:44.810
Yeah. I mean, we'll keep this in mind and

07:00:45.290 --> 07:00:47.370
yeah. Let me ask the other girls because

07:00:47.571 --> 07:00:49.991
The other oh, okay. Just to be calibrated

07:00:50.130 --> 07:00:51.430
that everyone's paying attention.

07:00:53.440 --> 07:00:55.600
Who does not believe that replacement is

07:00:55.600 --> 07:00:57.673
inevitable? Who thinks that was still have

07:00:57.673 --> 07:00:58.880
system developers in the future?

07:01:02.610 --> 07:01:04.850
Okay. I'll say we have like a one, two, five, maybe

07:01:04.850 --> 07:01:06.880
not 10%. Right? And now

07:01:06.880 --> 07:01:09.200
we'll have one sentence closing statement. Actually,

07:01:09.200 --> 07:01:11.480
one sentence. How many commas

07:01:13.240 --> 07:01:15.099
Commas are fine, but no semicolons.

07:01:16.821 --> 07:01:19.060
I feel I feel like I I sort of did my closing

07:01:19.060 --> 07:01:21.140
statement in the previous one, but it's all handed off to

07:01:21.140 --> 07:01:23.130
you. System

07:01:23.270 --> 07:01:24.970
developers will be replaced.

07:01:25.730 --> 07:01:27.349
System architects and orchestrators

07:01:27.760 --> 07:01:29.960
is what these folks call the people

07:01:30.320 --> 07:01:32.600
who are no longer system developers.

07:01:33.720 --> 07:01:35.982
Oh, plus one Jason's statement on adaptation

07:01:36.040 --> 07:01:38.070
and I think of it more as evolution

07:01:38.530 --> 07:01:39.510
rather than replacement.

07:01:42.410 --> 07:01:44.490
I don't know what else to add. I think I've been,

07:01:44.490 --> 07:01:46.400
like, firmly rooting for no this

07:01:46.640 --> 07:01:48.800
is not feasible. And if at all, you need to

07:01:48.800 --> 07:01:49.550
be orchestrators.

07:01:51.370 --> 07:01:52.751
I combined them.

07:01:53.610 --> 07:01:56.040
With that aside, thank our debaters

07:01:56.099 --> 07:01:57.800
for today. Thank you very much.

07:02:02.710 --> 07:02:04.810
Next, we have our break, and I think

07:02:05.200 --> 07:02:06.680
another talk. Yeah.

07:02:07.402 --> 07:02:09.170
Yeah. No. I I better get posted.

07:16:26.350 --> 07:16:28.650
Excuse me, everyone. So, please,

07:16:28.970 --> 07:16:31.210
get back to the your seats. So

07:16:31.210 --> 07:16:32.660
we're gonna be starting soon.

07:16:33.620 --> 07:16:35.800
We have a really exciting talk for you.

07:16:35.960 --> 07:16:36.440
Coming up.

07:17:24.290 --> 07:17:26.370
Hello. Hi, everyone. We are going to

07:17:26.370 --> 07:17:28.790
have our next invited speaker talk

07:17:29.450 --> 07:17:29.780
very soon.

07:17:54.890 --> 07:17:57.080
Should we start?

07:17:57.120 --> 07:17:59.331
Hello, everyone.

07:18:01.260 --> 07:18:03.330
Welcome back. So we have another

07:18:03.470 --> 07:18:05.940
invited speaker talk. By Rahul.

07:18:06.570 --> 07:18:08.650
Rahul is a research engineer at Google

07:18:08.650 --> 07:18:10.030
DeepMind. Contributing

07:18:10.810 --> 07:18:12.990
to the training and inference performance of Gemini

07:18:13.210 --> 07:18:15.470
models. He previously worked on the

07:18:15.690 --> 07:18:16.864
SLA TPU compiler.

07:18:23.241 --> 07:18:25.330
Alright. Is this working? Can everyone hear

07:18:25.330 --> 07:18:27.520
me? Yep. Alright. Let's

07:18:27.520 --> 07:18:29.820
get started. So today, yeah, I'm gonna be chatting

07:18:29.820 --> 07:18:32.050
a little bit about, LMM serving, how

07:18:32.050 --> 07:18:34.130
we you know, some extent of how we do it at

07:18:34.130 --> 07:18:36.280
at Google, at Gemini, and, you know, in general, how

07:18:36.841 --> 07:18:38.380
TPUs help you serve, you know, really big models.

07:18:39.610 --> 07:18:41.679
So let's get started. Do you wanna

07:18:41.679 --> 07:18:43.230
do? You know,

07:18:43.870 --> 07:18:45.950
we use TPUs and accelerators to, what,

07:18:45.950 --> 07:18:48.110
train giant models. Then we kind of serve them a little bit and, you

07:18:48.110 --> 07:18:50.230
know, doesn't matter so much. The main challenge is

07:18:50.400 --> 07:18:52.540
know, training the big thing in the first place. But,

07:18:52.540 --> 07:18:54.620
you know, recently, you know, obviously, OpenAI's worked last

07:18:54.620 --> 07:18:56.700
year. Other inference time scaling work from from other

07:18:56.700 --> 07:18:58.800
labs, Google included. Right? Indicates that

07:18:58.800 --> 07:19:00.500
we care a lot more about inference time.

07:19:01.350 --> 07:19:03.430
The the flops we put in that inference time, both in, like, the RL

07:19:03.430 --> 07:19:05.770
post training setup as well as an actual serving

07:19:05.831 --> 07:19:08.040
end users. And as part of this, we have

07:19:08.040 --> 07:19:10.200
to start thinking about this new workflows throughout the the lifecycle of

07:19:10.200 --> 07:19:12.350
the model. So when we build

07:19:12.350 --> 07:19:14.160
a a model, rather than thinking about, okay,

07:19:14.400 --> 07:19:16.480
is the shape of the model. This is the accelerators I have. This is

07:19:16.480 --> 07:19:18.849
how I pretrain it. You ask yourselves, okay,

07:19:19.091 --> 07:19:21.190
How do I serve this as well? How do I post train

07:19:21.331 --> 07:19:23.340
it efficiently? And efficiently means a couple of

07:19:23.340 --> 07:19:25.421
things. Right? One means just cost per token. You

07:19:25.421 --> 07:19:27.500
can say, alright. You know, bigger models is less

07:19:27.500 --> 07:19:29.820
efficient because it costs more per op a token or per input

07:19:29.820 --> 07:19:32.030
token process. But there also is notion of token

07:19:32.030 --> 07:19:34.210
efficiency. Right? The the idea that you

07:19:34.210 --> 07:19:36.450
think longer, as you as you do more generation either in parallel

07:19:36.450 --> 07:19:38.760
or serially, you end up with better quality.

07:19:39.241 --> 07:19:41.501
So you you have this concept that I think anthropologists have called, you know, token

07:19:41.800 --> 07:19:43.900
efficiency. Right? Basically, how many tokens you

07:19:44.040 --> 07:19:45.510
need an average to achieve a certain quality benchmark.

07:19:46.660 --> 07:19:48.820
And this matters in the context of pre training, because now

07:19:48.820 --> 07:19:51.230
it's not so obvious, and a bigger model is better. Right?

07:19:51.591 --> 07:19:53.700
Make a model significantly bigger, and it gives you better

07:19:53.700 --> 07:19:56.120
quality, but it you know, better quality translates

07:19:56.179 --> 07:19:58.210
to needing somewhat fewer tokens to

07:19:58.210 --> 07:20:00.277
to generate the final output. It's not so obvious

07:20:00.277 --> 07:20:02.480
as a net win. Right? The the token efficiency reduction

07:20:02.480 --> 07:20:04.560
might be less than the the increased cost of of

07:20:04.560 --> 07:20:06.530
training of serving the thing. Thing.

07:20:06.591 --> 07:20:08.750
So ultimately, you really have you know, rather than just a

07:20:08.750 --> 07:20:10.800
single point, right, ultimate pre trained model

07:20:10.800 --> 07:20:12.370
quality, you have this sort of trade off.

07:20:13.100 --> 07:20:15.260
Between both the the the cost the the absolute

07:20:15.260 --> 07:20:17.429
throughput cost generating an output of the given quality,

07:20:17.429 --> 07:20:19.859
which is like the cost per token times the number

07:20:19.859 --> 07:20:22.090
of tokens you're generating, as well as the trade off in

07:20:22.090 --> 07:20:24.279
throughput and latency. Right? Especially in the in order to approximate

07:20:24.279 --> 07:20:26.170
sampling, which is how most LMs work today,

07:20:26.850 --> 07:20:28.830
you have sort of the, you know, this tension where

07:20:29.070 --> 07:20:31.160
generating more more tokens is not a particularly paralyzable

07:20:31.160 --> 07:20:33.167
problem. Right? You have things that speculative

07:20:33.167 --> 07:20:35.460
decoding, diffusion, that's that's sort of ideas. Which

07:20:35.460 --> 07:20:37.500
help you, you know, get something to do parallelism, but by and large, like

07:20:37.500 --> 07:20:39.714
all the rest of models, you know, you do one token at

07:20:39.714 --> 07:20:41.831
a time over and over again. So if you wanna

07:20:41.831 --> 07:20:43.840
get you know, if you want this in search, you want to, you know, solve

07:20:43.840 --> 07:20:44.830
the IMO, whatever,

07:20:46.030 --> 07:20:48.230
you have this problem that now if your model is huge and it

07:20:48.230 --> 07:20:50.440
takes forever to sample one token, then, you know, it doesn't

07:20:50.440 --> 07:20:52.599
matter if after, you know, a million tokens can give you a

07:20:52.599 --> 07:20:54.640
good answer. Because you're not gonna get there. Right?

07:20:54.640 --> 07:20:56.720
Users will be bored. Like, you wanted a time in your exam or your

07:20:56.720 --> 07:20:58.890
your workload or your coding or whatever to finish a job.

07:20:59.450 --> 07:21:01.530
So you need to think about now, not only, you know, how much does the

07:21:01.530 --> 07:21:03.560
model cost to train, and how much per token does

07:21:03.560 --> 07:21:05.618
it cost to serve, but also how is the latency of

07:21:05.618 --> 07:21:07.490
serving? So you have this, like, very multidimensional problem

07:21:07.800 --> 07:21:09.950
and your task is to find this, you know, this

07:21:09.950 --> 07:21:12.350
high dimensional surface of, you know, the the cost, throughput,

07:21:12.350 --> 07:21:14.720
the latency, and pick the the best point for your application.

07:21:16.150 --> 07:21:18.269
So before we get into the details, I want to give a very

07:21:18.269 --> 07:21:20.340
quick run through of, you know, what is the workload, you know,

07:21:20.340 --> 07:21:22.570
TPU and accelerator level. LIMMs

07:21:22.570 --> 07:21:24.571
basically have an MLP, which is a big MacMo, you

07:21:24.571 --> 07:21:26.644
know, traditionally sort of dense. Now it can be MOE

07:21:26.644 --> 07:21:28.849
or something more fun. And you have attention. Right? Which

07:21:28.849 --> 07:21:30.830
of of various kinds. And

07:21:30.990 --> 07:21:32.840
take in the big sequence of tokens at training time,

07:21:33.000 --> 07:21:35.160
and inference time, they can run sort of two different modes.

07:21:35.160 --> 07:21:37.220
You have the prefilled where you're taking a ton of tokens in

07:21:37.220 --> 07:21:39.234
parallel. You do four passes to all of them together. And

07:21:39.234 --> 07:21:41.350
you get out kvCaches. Then

07:21:41.350 --> 07:21:43.670
you take these KB caches and attend to them one token

07:21:43.670 --> 07:21:45.818
at a time or a few tokens at a time in order to

07:21:45.818 --> 07:21:48.200
guess the fashion in generate. And this fits at the actual

07:21:48.200 --> 07:21:50.357
answer. And as part of this autogressive process, you get

07:21:50.357 --> 07:21:52.430
either, you know, some some thinking tokens, some thought, maybe

07:21:52.591 --> 07:21:54.595
tool calls, whatever, like we saw earlier in the day. And

07:21:54.595 --> 07:21:55.980
eventually you get your answer. Right?

07:21:56.760 --> 07:21:58.490
And that sort of happens one token at a time.

07:21:58.890 --> 07:22:01.040
Time. And these workloads are very different in terms of their operational

07:22:01.040 --> 07:22:03.429
their operational intensity. Pre fill is super

07:22:03.429 --> 07:22:05.670
high intensity because you have all the tokens between all in parallel.

07:22:05.670 --> 07:22:07.929
The CPU loves this. You know, it's just very heavily

07:22:07.929 --> 07:22:10.170
flop bound. This is like a fantastic workload for most accelerators,

07:22:10.170 --> 07:22:11.310
TPUs and TPUs included.

07:22:12.501 --> 07:22:14.650
Generate though is a very different story. Because

07:22:14.790 --> 07:22:16.950
you have this big model, you have, you know, like, you know, billions or tens, hundreds,

07:22:16.950 --> 07:22:18.820
thousands, you know, trillion letters. Enormous

07:22:18.960 --> 07:22:21.040
number of parameters. And you're just

07:22:21.040 --> 07:22:22.260
doing, like, batch size one pretty

07:22:23.062 --> 07:22:25.142
and one token multiplying against these

07:22:25.142 --> 07:22:27.190
huge weights. It's like a really cruddy

07:22:27.730 --> 07:22:29.732
operational density. You can think of it as, you know,

07:22:29.732 --> 07:22:31.810
if you have, say, eight GPUs or whatever in your small

07:22:31.810 --> 07:22:33.960
system, you're kind of each beam bound at bound, loading the

07:22:33.960 --> 07:22:36.040
weights every single time, which is, like, really slow and hurts

07:22:36.040 --> 07:22:38.120
for, like, big models. Because doesn't matter

07:22:38.120 --> 07:22:40.360
how good or how big your model is. You made a model

07:22:40.360 --> 07:22:42.190
two x bigger, and it's two x longer to get a token.

07:22:42.591 --> 07:22:44.671
Then what's the point? Right? Like, I might as well take the small model

07:22:44.671 --> 07:22:45.720
and make it think longer.

07:22:47.130 --> 07:22:49.290
We also see that, you know, how do you give our batch size and

07:22:49.290 --> 07:22:51.720
generate? For the most part, you get it by running independent

07:22:51.940 --> 07:22:54.050
requests in parallel. So you have batch size of, you know, a couple

07:22:54.050 --> 07:22:56.110
100, thousand, whatever from you know, the

07:22:56.110 --> 07:22:58.150
request you get in production or in your RL job.

07:22:59.001 --> 07:23:01.009
Run it together. And this gives you a great throughput win, because,

07:23:01.009 --> 07:23:03.090
obviously, your intensity is being brought up to the that of

07:23:03.090 --> 07:23:05.200
the machine. But it doesn't really give you a latency win.

07:23:05.200 --> 07:23:07.340
Right? Like, you're token for its getting given request, it

07:23:07.340 --> 07:23:09.360
gets even everything slower if you're doing all the other

07:23:09.500 --> 07:23:11.600
requests in parallel. We

07:23:11.600 --> 07:23:13.730
can look at how this matters to the accelerator. So

07:23:13.730 --> 07:23:15.810
Ironwood, which is Google's latest publicly announced

07:23:15.810 --> 07:23:17.620
GPU, has a ridiculous amount of of

07:23:18.180 --> 07:23:20.510
compute and Right? You know, four teraflops,

07:23:20.510 --> 07:23:22.900
eight terabytes of second machine bandwidth is is like a stupid amount.

07:23:23.060 --> 07:23:25.220
And also a pretty solid interconnect. Right? Like, egress balance

07:23:25.220 --> 07:23:27.280
on the chip is is you know, is it gonna gigabytes per

07:23:27.280 --> 07:23:29.320
second. Over a Taurus. You know?

07:23:29.480 --> 07:23:31.640
GPUs are pretty similar, like in terms of the amount of flops

07:23:31.640 --> 07:23:33.657
and the AFM bandwidth, you know, to within factor of

07:23:33.657 --> 07:23:35.340
two or three, so not, like, dramatically different.

07:23:35.820 --> 07:23:37.900
As well as you have the egress bandwidth. Like, I think, you know, black wall is

07:23:37.900 --> 07:23:40.080
like 900 egress from one chip

07:23:40.080 --> 07:23:42.040
goes, you know, to 600, whatever. They're about the

07:23:42.440 --> 07:23:44.520
same. So both these things, the problem is that they are

07:23:44.520 --> 07:23:45.980
very high intensity machines.

07:23:46.720 --> 07:23:48.099
And, yeah, that's a problem. Right?

07:23:49.091 --> 07:23:51.240
That you have a high concurrency. You know, I'm currently

07:23:51.240 --> 07:23:53.320
in my previous generation TPU. It is a 100 it's a

07:23:53.320 --> 07:23:55.251
thousand terahertz ups per per byte room load

07:23:55.550 --> 07:23:58.040
from HBM. On ironwood, it's a bit better, but still, like, 600.

07:23:58.360 --> 07:24:00.440
And this is crazy compared to, you know, Generate, where

07:24:00.440 --> 07:24:02.540
you have, like, one token per one token per

07:24:02.610 --> 07:24:04.850
batch size at per per independent request at any

07:24:04.850 --> 07:24:06.880
time. For prefilled, though, this is pretty

07:24:06.880 --> 07:24:08.980
good. Right? If all these sequences, like a prompt is

07:24:08.980 --> 07:24:11.320
maybe a thousand tokens, that's, like, perfectly matched with intensity

07:24:11.320 --> 07:24:13.500
here. You can do some model parallelism over a relatively

07:24:13.500 --> 07:24:15.620
small topology, you know, four, eight chips or, like,

07:24:15.620 --> 07:24:18.009
an NV link node. And get you a little more tensor

07:24:18.009 --> 07:24:20.420
of that. You also have self attention, which, you know, traditionally

07:24:20.480 --> 07:24:22.560
is, like, quadratic and, again, very heavily flop bound because you're doing

07:24:22.560 --> 07:24:24.724
all sequence in parallel. So it's kind of awesome.

07:24:24.724 --> 07:24:26.929
Right? You can use your data parallelism over as many

07:24:26.929 --> 07:24:29.240
shit as you want. You can do a little bit more parallelism if

07:24:29.240 --> 07:24:31.320
you really want to. And sort of prefill works very nicely in accelerated

07:24:31.320 --> 07:24:33.501
systems. And, you know, it works.

07:24:33.501 --> 07:24:35.700
Right? This is me, Gemini three point o. It's a giant model. You know,

07:24:35.700 --> 07:24:37.350
I can't tell you how big, but it's big.

07:24:37.991 --> 07:24:40.071
I stuck a book into it. It reads it in one in, like, ten seconds.

07:24:40.071 --> 07:24:42.251
Like, what the hell? This is pre

07:24:42.251 --> 07:24:44.331
fill, like, two first order approximation is to solve problem. Like, throw

07:24:44.331 --> 07:24:46.210
chips to the problem, you get back faster answers.

07:24:46.390 --> 07:24:47.740
You know? Easy.

07:24:48.779 --> 07:24:50.940
So as I sort of mentioned briefly, that's not true

07:24:50.940 --> 07:24:53.331
for generate. Can get

07:24:53.331 --> 07:24:55.411
throughput through adding more data parallelism. Right? You can add more

07:24:55.411 --> 07:24:57.620
batch, and you can get more requests done in parallel sort of

07:24:57.620 --> 07:24:59.290
amortizing the cost of learning the weights.

07:24:59.930 --> 07:25:02.010
This doesn't help you with any given request. Like, if I'm a

07:25:02.010 --> 07:25:03.550
user and I talk to, you know, Gemini

07:25:04.060 --> 07:25:06.380
Cloud or ChattyBT and wanna get an answer back, and it sits there and thinks

07:25:06.380 --> 07:25:08.470
for thirty minutes. I won't be a happy user.

07:25:09.360 --> 07:25:11.440
A lot of our our our our colleagues in various

07:25:11.440 --> 07:25:13.730
accelerator companies, you know, give big throughput

07:25:13.730 --> 07:25:15.810
numbers saying, oh, we do, you know, a thousand, 10,000

07:25:15.810 --> 07:25:17.900
tokens per second per chip. But, like,

07:25:17.900 --> 07:25:20.300
as a user, I get one token. I get much less than

07:25:20.300 --> 07:25:22.610
this. Right? I'm I have this this huge throughput number,

07:25:22.610 --> 07:25:24.410
but it's across everybody else. I'm covered

07:25:24.890 --> 07:25:27.019
myself. And as an end user, these huge throughput

07:25:27.019 --> 07:25:28.410
numbers aren't directly beneficial.

07:25:30.027 --> 07:25:32.050
So we wanna figure out how can we can,

07:25:32.050 --> 07:25:34.290
you know, modify our system and our model to to fix

07:25:34.290 --> 07:25:36.300
this problem. Right? So let's see. Like,

07:25:36.300 --> 07:25:38.560
what exactly is the bottleneck? Right? If you

07:25:38.560 --> 07:25:40.640
look at, you know, Llama four zero five b, which is the biggest

07:25:40.640 --> 07:25:42.540
public model I found, you know, sure

07:25:42.680 --> 07:25:44.841
there are others that are more activated, but I couldn't find them. This is a big

07:25:44.841 --> 07:25:46.821
recent one. About half a half a trillion

07:25:47.040 --> 07:25:49.161
parameters activated. Right? The biggest thing I

07:25:49.161 --> 07:25:51.260
can find, look at one token, multiply the params,

07:25:51.400 --> 07:25:53.321
divide by the flops, on just

07:25:53.530 --> 07:25:55.618
one ironwood. Right? Can get this, like, norm this

07:25:55.618 --> 07:25:57.660
massive number of 6,000 tokens per second for

07:25:57.660 --> 07:25:59.840
a single request if you're flop bound. Right?

07:26:00.240 --> 07:26:02.260
Is insane. It's like, you know, centimeter of microseconds

07:26:02.260 --> 07:26:04.450
per token. But obviously, you know,

07:26:04.690 --> 07:26:06.850
don't get this. If you try using any chat app today, any

07:26:06.850 --> 07:26:08.880
any any LLM, the latency you

07:26:08.880 --> 07:26:10.960
observe is significantly worse. You are not getting 6,000

07:26:10.960 --> 07:26:12.610
tokens for yours particular of less

07:26:12.990 --> 07:26:14.609
you know, in in serial.

07:26:15.440 --> 07:26:17.600
Problem, as I mentioned before, is the intensity mismatch, right, that

07:26:17.600 --> 07:26:19.730
you have all these flops but they're stranded behind

07:26:19.730 --> 07:26:22.100
loading parameters from HBM. So

07:26:22.460 --> 07:26:24.640
you know, what can we do about this? Right?

07:26:24.980 --> 07:26:27.187
Have, you know, all these parameters, how we're controlling

07:26:27.187 --> 07:26:29.400
parameters. If you look at one chip, they take, you know,

07:26:29.960 --> 07:26:32.380
hundred hundred milliseconds of load on on on on ironwood.

07:26:32.930 --> 07:26:35.331
Right? So going from the flop added number here of 6,000

07:26:35.331 --> 07:26:37.360
if we're bottlenecked by the MXU, to

07:26:37.360 --> 07:26:39.440
only, you know, 10 tokens per second if we're bottlenecked by the

07:26:39.440 --> 07:26:41.650
Asian bandwidth. That sucks. Right?

07:26:41.841 --> 07:26:43.921
And the obvious answer is we do, you know, model parallelism,

07:26:43.921 --> 07:26:46.120
tensor parallels, megatron, whatever whatever you wanna

07:26:46.120 --> 07:26:48.350
it, right, and similar things for MOUs. Where you

07:26:48.350 --> 07:26:50.510
basically start these parameters across a large

07:26:50.510 --> 07:26:52.519
number of chips. So let's say you want to get to

07:26:52.519 --> 07:26:54.560
the flow bound point for a single request. Right? How much

07:26:54.560 --> 07:26:56.642
model parallelism do we need? Well, we just

07:26:56.642 --> 07:26:58.800
do the basic math. Right? We have three parameters. Let's say, before

07:26:58.800 --> 07:27:00.970
16, divided by the latency you want, divided

07:27:00.970 --> 07:27:03.460
by the bandwidth. It tells you, okay. You need about, you know, 500

07:27:03.860 --> 07:27:05.921
little over 500 ironwood. So

07:27:06.710 --> 07:27:08.870
Okay. Fine. You're saying that just take the the model, shard it

07:27:08.870 --> 07:27:10.820
500 ways, One batch has one. We're done.

07:27:11.780 --> 07:27:13.690
So what is the constraint here? Right?

07:27:13.850 --> 07:27:15.890
We also have an, a collective constraint. We

07:27:15.890 --> 07:27:18.001
have to be able to do the model power collective, you know, for

07:27:18.001 --> 07:27:20.010
this one token. So pretty small. But under

07:27:20.010 --> 07:27:22.110
a very tight latency budget. You know, La La is

07:27:22.110 --> 07:27:24.370
about a 100 layers, so that means we have about two microseconds

07:27:24.509 --> 07:27:26.972
per collective to get this to to to keep us under

07:27:26.972 --> 07:27:29.170
this, you know, to keep us HPM bound, avoid bottleneck

07:27:29.170 --> 07:27:31.273
buyer interconnect. But this is, like,

07:27:31.273 --> 07:27:33.362
you know, nuts. Right? A GPU system is, like, chips. Like, where

07:27:33.362 --> 07:27:35.390
the hell do you get 600 chips altogether in this

07:27:35.390 --> 07:27:37.600
latency budget? There.

07:27:38.080 --> 07:27:40.240
So that's the basic motivation. Right? This is why we sell

07:27:40.240 --> 07:27:42.250
this thing as an inference You know, this is it's

07:27:42.250 --> 07:27:44.400
probably a surprise. You know, I said it's about, you know,

07:27:44.400 --> 07:27:46.560
l l m serving is really a marketing talk that you

07:27:46.560 --> 07:27:48.720
can buy TPUs. The motivation is, yeah,

07:27:48.720 --> 07:27:51.020
that TPU and, like, torus

07:27:51.020 --> 07:27:53.120
based, you know, accelerated systems have a very

07:27:53.120 --> 07:27:55.170
large number of chips compared to a switch based system.

07:27:55.170 --> 07:27:56.810
At a very low ICI latency.

07:27:57.370 --> 07:27:59.450
So latency for the 500 chip system, I measure it's about

07:27:59.450 --> 07:28:01.360
ten microseconds, and you can get it lower with some

07:28:01.520 --> 07:28:03.300
some tricks, which I'm not going to mention here.

07:28:03.921 --> 07:28:05.570
So the important part is that, yeah,

07:28:05.970 --> 07:28:08.050
giant pods of chips, you know, that, you

07:28:08.050 --> 07:28:10.290
know, like like like ironwood or the previous, you know,

07:28:10.290 --> 07:28:12.680
TP generations, let you drive latency way, way down,

07:28:12.841 --> 07:28:14.312
discharging the model so many different ways.

07:28:15.081 --> 07:28:17.161
And, you know, nightly, though this this collective is

07:28:17.161 --> 07:28:19.220
exposed, right, you go through this model collective, followed by

07:28:19.220 --> 07:28:21.300
the mat mill, and you can do various architecture things,

07:28:21.300 --> 07:28:23.341
well, things in the, you know, in how you bite your

07:28:23.341 --> 07:28:25.300
kernels and such forth to try and overlap things better.

07:28:25.841 --> 07:28:28.001
But the high level the the high level bit is still the

07:28:28.001 --> 07:28:30.350
same. That if you want to get low latency and benefit single

07:28:30.350 --> 07:28:32.200
requests, rather than just doing, you know,

07:28:32.440 --> 07:28:34.640
high throughput but really slow per token requests,

07:28:34.880 --> 07:28:36.840
you have to charge your model like a ton.

07:28:37.240 --> 07:28:39.390
On this you know, this is a very recent chip, and this is a model from

07:28:39.390 --> 07:28:41.460
several years ago. But even still, you just shred the

07:28:41.460 --> 07:28:43.509
model over, like, you know, hundreds of chips to try

07:28:43.509 --> 07:28:45.500
and get error codes and by number for a small batch,

07:28:45.624 --> 07:28:47.750
serving. And, yeah, for frontier models, I can tell

07:28:47.750 --> 07:28:50.091
you, I work on frontier models. You know, hundreds of thousands

07:28:50.122 --> 07:28:52.200
of laser centers chips. It's quite, you know,

07:28:52.500 --> 07:28:54.580
un unsurprising to you to for for us to use to serve our

07:28:54.580 --> 07:28:56.320
models as latency targets we want to hit.

07:28:57.310 --> 07:28:59.230
Now, you know, this is for TPUs.

07:28:59.970 --> 07:29:01.995
Do you do for GPUs? You've got, like, eight. You know, now

07:29:01.995 --> 07:29:04.040
you have a 144. I

07:29:04.040 --> 07:29:06.140
don't know. Goodbye by TPU, I guess. I don't know.

07:29:07.152 --> 07:29:09.179
So yeah. You know, these are all this, you know, me

07:29:09.179 --> 07:29:11.300
doing, like, spreadsheet numbers. Like, how do you actually go

07:29:11.300 --> 07:29:13.420
and program the system to to hit these numbers?

07:29:14.841 --> 07:29:16.921
The key idea behind the TV system and, like, our

07:29:16.921 --> 07:29:18.950
our serving second general is we want the hardware

07:29:18.950 --> 07:29:20.580
to behave in a very deterministic manner.

07:29:20.980 --> 07:29:23.019
If you have something like, you know, a fancy switch, which, you

07:29:23.019 --> 07:29:25.140
know, you know, which has, like, a tail latency that's really

07:29:25.140 --> 07:29:27.179
bad, or we have, you know, scheduling on the

07:29:27.179 --> 07:29:29.210
on the device side, which is unpredictable. This

07:29:29.210 --> 07:29:31.370
is very challenging. We're trying to get these latencies that

07:29:31.370 --> 07:29:33.400
low. But recall back then, we said our each layer

07:29:33.400 --> 07:29:35.400
should take only a few microseconds at a Right?

07:29:35.560 --> 07:29:37.690
Say even ten microseconds. That's kinda crazy.

07:29:37.690 --> 07:29:39.690
Like, your your your latency for a single,

07:29:39.849 --> 07:29:42.109
reads from each VM on, like, either GPUs

07:29:42.109 --> 07:29:43.810
or TVs are, like, multiple microseconds.

07:29:44.470 --> 07:29:46.550
So if you're just, like, being a bit sloppy about your work, you're

07:29:46.550 --> 07:29:48.630
doing the hardware handle, you know, pre fetching for you, you're

07:29:48.630 --> 07:29:50.340
dead because these magnitudes add up instantly.

07:29:50.820 --> 07:29:53.140
So what you really need is, like, complete hand control

07:29:53.140 --> 07:29:54.850
over, like, every last bit of the hardware.

07:29:55.171 --> 07:29:57.030
And that's basically how this works. Right?

07:29:57.270 --> 07:29:59.430
The only TPUs work is you have a big VLIW

07:29:59.430 --> 07:30:01.220
instruction stream. So basically every cycle,

07:30:01.460 --> 07:30:03.500
know exactly what the hardware is doing. You write the code. You can

07:30:03.500 --> 07:30:05.921
see this inspect this in the assembly. And

07:30:05.980 --> 07:30:07.991
you just do these operations. There's no notion of,

07:30:07.991 --> 07:30:10.152
you know, latency hiding automatically for you or

07:30:10.152 --> 07:30:12.200
thread switching or any of that. You just write.

07:30:12.200 --> 07:30:14.120
You want to start a transfer? Transfer start.

07:30:14.360 --> 07:30:16.670
Finish transfer? Transfer, done. And

07:30:16.990 --> 07:30:19.150
you have full control over this at the current languages, as well

07:30:19.150 --> 07:30:21.290
as even the compiler does a good job of this. But we

07:30:21.290 --> 07:30:23.370
can drop down to our various the current languages, either PALOS from

07:30:23.370 --> 07:30:25.650
Mosaic that is exposed from the, you know, JAX stack.

07:30:26.210 --> 07:30:28.290
And get these have the operations placed exactly

07:30:28.290 --> 07:30:30.349
where you want them. And because the hardware

07:30:30.349 --> 07:30:32.509
is so predictable and you have so much control over where they

07:30:32.509 --> 07:30:34.520
place, where you place instructions, you can

07:30:34.520 --> 07:30:36.710
basically hide latency perfectly. But you know,

07:30:36.710 --> 07:30:38.870
okay. Every single time I do this map mode, it'll take, you know,

07:30:38.870 --> 07:30:41.030
two hundred cycles to finish. Every time I do a

07:30:41.030 --> 07:30:43.251
DMA read of this buffer of this size, it'll take this much latency

07:30:43.310 --> 07:30:45.421
plus this much bandwidth time to load. And you can predict

07:30:45.421 --> 07:30:47.730
this exactly and place, like, the the consumption of the buffer

07:30:47.890 --> 07:30:49.940
right where you expect it to be. There's not much

07:30:49.940 --> 07:30:51.630
surprise here because the hardware just does what you say.

07:30:52.990 --> 07:30:55.070
You know, you can also ask, alright. Well, what about, you

07:30:55.070 --> 07:30:57.140
know, thread switching, async stuff, thread blocks? I

07:30:57.140 --> 07:30:58.630
don't know. That sounds hard. Don't do that.

07:30:59.671 --> 07:31:01.831
The the way the TPU system works is you are writing

07:31:01.831 --> 07:31:03.859
a single thread program. And this is not really a function of

07:31:03.859 --> 07:31:06.110
the accelerator system itself. Right? You

07:31:06.110 --> 07:31:08.270
still have multiple threads in the terms of having multiple chips in your

07:31:08.270 --> 07:31:10.300
system. The key idea is, again, the

07:31:10.300 --> 07:31:12.490
programming model. That you're writing a single thread at a

07:31:12.490 --> 07:31:14.920
time you write all the collect operations yourself, synchronization,

07:31:15.140 --> 07:31:17.310
DMA start, DMA whatever. All these operations,

07:31:17.310 --> 07:31:19.448
you do by hand. You sequence them at

07:31:19.448 --> 07:31:20.750
high latencies, you know, explicitly.

07:31:21.480 --> 07:31:23.720
So every single transaction, it takes more than, like, two

07:31:23.720 --> 07:31:25.880
cycles. Is expressed in the TPU I set, and expressed also

07:31:25.880 --> 07:31:28.330
at the the user level. As asynchronous operations.

07:31:28.410 --> 07:31:30.040
You can say asynchronous starters operation,

07:31:31.750 --> 07:31:33.690
and finish operation later when you know because of the deterministic behavior when the latency

07:31:33.850 --> 07:31:36.230
you know, will have been hidden. So basically,

07:31:36.230 --> 07:31:38.390
how do you write a serving program for this to hit these the

07:31:38.390 --> 07:31:40.590
very tight latency budgets? You basically

07:31:40.590 --> 07:31:42.948
write one big schedule sequencing all operations

07:31:42.948 --> 07:31:44.970
and exactly the latency you want to hide them and

07:31:44.970 --> 07:31:47.130
then you're done. Right? You know exactly how long the hardware takes to

07:31:47.130 --> 07:31:49.265
do these operations. Can either measure it empirically or just

07:31:49.265 --> 07:31:51.759
read the specification. The time is is completely predictable.

07:31:52.100 --> 07:31:54.180
And because you have you know, there's no race conditions here, there

07:31:54.180 --> 07:31:56.288
are no, like, you know, atomic operations that

07:31:56.288 --> 07:31:58.320
might not might not be deterministic every time, It's

07:31:58.320 --> 07:32:00.820
very easy to get, like, perfect synchronization, complete determinism

07:32:00.880 --> 07:32:02.480
across this you know, a very large system.

07:32:04.130 --> 07:32:06.210
And that's a good made them clear message here. Right?

07:32:06.370 --> 07:32:08.550
The hardware does certain things very well. It does.

07:32:08.770 --> 07:32:10.850
It does DMA trans HBM

07:32:11.150 --> 07:32:13.280
reads, whatever. And you get to tell to do what you want.

07:32:13.921 --> 07:32:16.160
Right? Just write code for each you know, the single scalar

07:32:16.160 --> 07:32:18.190
unit, which expresses a thread of control. You write

07:32:18.190 --> 07:32:20.212
code here saying what you wanna do. Do a map

07:32:20.212 --> 07:32:22.341
well now, start the DMA here, start the map whatever.

07:32:22.341 --> 07:32:24.460
Like, you write these operations. You sequence

07:32:24.460 --> 07:32:26.480
them in in the way you expect. And it does it.

07:32:26.560 --> 07:32:28.698
There's no magic here. Wanna do a kernel. You launch

07:32:28.698 --> 07:32:30.779
a kernel. You leave the DNA dangling. Okay. Up to you. You

07:32:30.779 --> 07:32:32.450
can finish the DNA later. That's totally fine.

07:32:33.500 --> 07:32:35.740
And, you know, you get to handle all these

07:32:35.740 --> 07:32:38.090
resources. You get full control over the the machine itself.

07:32:39.110 --> 07:32:41.509
There's no hidden, like there's no prefetching. There's no branch prediction.

07:32:41.509 --> 07:32:43.810
There's nothing in this hardware. If you want to preload the

07:32:43.910 --> 07:32:45.995
next part of your program, you write a function calling preload the next part

07:32:45.995 --> 07:32:48.160
of program. Right? The hardware has no magic there.

07:32:48.160 --> 07:32:50.240
You do exactly what you want, and that's how you avoid

07:32:50.240 --> 07:32:52.310
these, like, microseconds adding up and killing your latency.

07:32:52.550 --> 07:32:54.710
And then it lets you throw, like, a ridiculous number of

07:32:54.710 --> 07:32:56.757
chips at a problem. Right? 500 chips for

07:32:56.757 --> 07:32:58.810
a, you know, an old dense model. Trust me. Our models are

07:32:58.810 --> 07:33:00.950
bigger now. And just get away with it.

07:33:00.950 --> 07:33:03.071
Because, you know, full control over what

07:33:03.071 --> 07:33:05.321
the machine is doing. So,

07:33:05.321 --> 07:33:07.480
I mean, people will say this like, oh, no. I have to write

07:33:07.480 --> 07:33:09.600
a complicated kernel. That's too hard. I like skill.

07:33:09.600 --> 07:33:11.190
Like, whatever. But that doesn't make sense.

07:33:11.690 --> 07:33:14.110
Reason it doesn't make sense is because unlike, you know, other accelerated

07:33:14.251 --> 07:33:16.259
systems, there's no notion of host dependence

07:33:16.259 --> 07:33:18.380
in the TPU. Right? There's no notion at the when

07:33:18.380 --> 07:33:20.450
you hit the actual hardware, there's no notion

07:33:20.450 --> 07:33:22.509
of a kernel. The hardware is like a, you know, like a

07:33:22.509 --> 07:33:24.650
CPU. There's a big sequence of instructions. It runs

07:33:24.650 --> 07:33:27.020
them in order one at a time. And that's it. Right?

07:33:27.020 --> 07:33:29.050
You wanna fetch more programs, it is reads

07:33:29.050 --> 07:33:31.210
from an HTML and loads an SQL memory, whatever. Like, it's

07:33:31.210 --> 07:33:33.410
all completely deterministic and and controlled by the

07:33:33.410 --> 07:33:35.760
user. So if you wanna write small kernels,

07:33:36.241 --> 07:33:38.321
and then have them go one after other, that's fine. Because the way

07:33:38.321 --> 07:33:40.540
the compiler does it literally case the kernels and ducks

07:33:40.540 --> 07:33:42.830
them together at the end. In some sense, every TPU

07:33:42.830 --> 07:33:44.902
program is a giant kernel, just orchestrated by

07:33:44.902 --> 07:33:46.920
the compiler. And, again, like, you have control,

07:33:46.920 --> 07:33:48.380
but you also have, you know, a lot of help.

07:33:49.020 --> 07:33:51.210
Right? May not want to, like, manually bookkeep

07:33:51.210 --> 07:33:53.480
every single latency in the operation of the compiler. That's

07:33:53.480 --> 07:33:55.550
fine. Write the operation, so the compilers figure out

07:33:55.550 --> 07:33:57.640
schedules, it'll do it for you. You. You wanna say, oh,

07:33:57.640 --> 07:33:59.960
I wanna do a boring dense map mode. Compiler doesn't do

07:33:59.960 --> 07:34:02.171
this. Issue the map model. And they sort

07:34:02.171 --> 07:34:04.240
of play nicely together. Example, you say, oh, I wanna do

07:34:04.240 --> 07:34:06.400
a big dense map but I wanna prefetch the weight

07:34:06.400 --> 07:34:08.560
in a particular, you know, carefully overlap fashion to keep the

07:34:08.560 --> 07:34:10.830
atria band loop busy. That works too. You prefetch

07:34:10.830 --> 07:34:12.910
the weight. You feed the prefetch buffer into the compiler. It'll take

07:34:12.910 --> 07:34:14.939
that and keep You wanna do some async operations

07:34:14.939 --> 07:34:17.310
behind the map model the compiler runs. That's

07:34:17.310 --> 07:34:19.350
fine. Compiler knows, okay, you're doing an async operation.

07:34:19.350 --> 07:34:21.259
This buffer is reserved. The sync flag is occupied.

07:34:21.550 --> 07:34:23.630
And it's do it'll do its math more, you know, while keeping

07:34:23.630 --> 07:34:25.890
your state untouched. So sort of the key principle

07:34:25.890 --> 07:34:28.050
of XLA is not that it's, you know, clever,

07:34:28.050 --> 07:34:30.290
that there's a very opt optimization, so there's something like that.

07:34:30.450 --> 07:34:32.390
The key idea of XLA is that it lets you orchestrate

07:34:32.610 --> 07:34:34.710
things. Tracks all the resources you're doing, and basically

07:34:34.710 --> 07:34:36.831
lets you call the different kernels, the different, you know, built

07:34:36.831 --> 07:34:38.870
in h loads in XLA, without messing with

07:34:38.870 --> 07:34:41.150
messing with you. And it lets you offload decisions

07:34:41.150 --> 07:34:43.241
to compiler when you want but also you

07:34:43.241 --> 07:34:44.910
don't wanna do that, you can just take control of yourself.

07:34:45.390 --> 07:34:47.430
And so the key idea is you could overlap all the latencies

07:34:47.430 --> 07:34:49.480
You could compose, like, a ton of primitives both in the

07:34:49.480 --> 07:34:51.580
standard library. Which is quite huge, as well as from all

07:34:51.580 --> 07:34:53.600
your kernels that you write to yourself or you take from the

07:34:53.600 --> 07:34:56.020
Internet. And, you know, build your big program. And ultimately,

07:34:56.020 --> 07:34:58.290
it gets serialized and stitched into one giant kernel anyway.

07:34:58.610 --> 07:35:00.720
So sort of you get the the min latency

07:35:00.720 --> 07:35:02.800
performance without having to go through hell and run a big

07:35:02.800 --> 07:35:04.970
kernel every time you wanna do a new model.

07:35:05.410 --> 07:35:07.490
Now this is sort of the the we talked about the

07:35:07.490 --> 07:35:09.650
the high level idea. Use a ton of chips for for fast

07:35:09.650 --> 07:35:11.730
models. We've talked about, you know, how do you actually make

07:35:11.730 --> 07:35:13.880
the compiler, make the hardware do what you want. Now

07:35:13.880 --> 07:35:15.570
how do we get the model is the final part.

07:35:16.290 --> 07:35:18.490
So the key thing is as we touched upon at the beginning. Right?

07:35:18.570 --> 07:35:20.630
You want a certain latency throughput objective. And if

07:35:20.630 --> 07:35:22.740
you do this very, you know, cognizant

07:35:22.740 --> 07:35:24.849
of what chips you're using, So if you know,

07:35:24.849 --> 07:35:27.009
okay, I have ironwood, and I have, you know, this many chips I

07:35:27.009 --> 07:35:29.050
can use in one one latency domain based on the latencies

07:35:29.050 --> 07:35:30.720
that you measure or you see from the specification.

07:35:31.560 --> 07:35:33.587
Tells you, you know, how big is your model, how many parameters can you

07:35:33.587 --> 07:35:35.430
have per layer, what kind of quantitation that sort of thing.

07:35:35.831 --> 07:35:37.530
Thing. And you can ask yourself again. Okay.

07:35:37.841 --> 07:35:40.001
If I don't really care about latency, fine. I want throughput.

07:35:40.001 --> 07:35:42.090
So you crank up the batch size. You go. You get the marketing

07:35:42.170 --> 07:35:43.590
you throw a ton of flops at the problem.

07:35:44.628 --> 07:35:46.970
If you want low latency though, you think about, okay.

07:35:47.050 --> 07:35:49.030
I want lower latency. I want to use more chips.

07:35:49.430 --> 07:35:51.670
There's too many chips. The ICI collective itself becomes a

07:35:51.670 --> 07:35:53.859
bottleneck. So you wanna play with,

07:35:53.859 --> 07:35:56.040
okay, what kind of model can I build which minimizes

07:35:56.179 --> 07:35:58.270
communication? But still gets the quality I want?

07:35:58.430 --> 07:36:00.591
Or take a quality discount where pays but pay for pays for it

07:36:00.591 --> 07:36:02.310
by letting me generate each token more rapidly.

07:36:02.710 --> 07:36:04.790
And, you know, these are some ideas I pulled from, you know, from Claude

07:36:04.790 --> 07:36:06.700
because I didn't want to give anything I already know.

07:36:07.500 --> 07:36:09.900
Yeah. Make the models denser, make the model shallower,

07:36:09.900 --> 07:36:11.921
you know, whatever. These give you, you know, fewer layers,

07:36:12.070 --> 07:36:14.090
fewer collectives. But, you know, maybe give you throughput

07:36:14.090 --> 07:36:16.250
loss, but you can maybe pay back pay back by just getting

07:36:16.250 --> 07:36:18.290
tokens faster. And you can

07:36:18.290 --> 07:36:20.368
see here, there's a paper publication we made quite

07:36:20.368 --> 07:36:22.510
some time ago. Right? That empirically,

07:36:22.510 --> 07:36:24.600
once you have the system set up, can sort

07:36:24.600 --> 07:36:26.660
of sweep different things. Right? You can say, okay. I

07:36:26.660 --> 07:36:28.740
want a a more relaxed latency target, I can

07:36:28.740 --> 07:36:30.820
use a larger batch size. I can maybe use fewer chips and get

07:36:30.820 --> 07:36:32.830
less, you know, pay less for the the

07:36:32.830 --> 07:36:34.900
auto collectives and get a better throughput. You sorta

07:36:34.900 --> 07:36:36.920
get this trade off you want. The the point on this

07:36:36.920 --> 07:36:39.109
trade off you actually select depends on the the constraints

07:36:39.109 --> 07:36:40.220
you have from your workload.

07:36:41.240 --> 07:36:43.400
But that's a key idea that you should be aware of what

07:36:43.400 --> 07:36:45.519
system you're using, and and and

07:36:45.519 --> 07:36:47.300
build your model to fit that system exactly.

07:36:47.890 --> 07:36:50.060
And maybe, you know, that leads to pretty many

07:36:50.060 --> 07:36:52.110
compromises. Right? Shallower model, wider model, less

07:36:52.110 --> 07:36:54.269
sparse model, whatever. But that's okay if

07:36:54.269 --> 07:36:56.509
you end up with a better final artifact on that on

07:36:56.509 --> 07:36:58.610
that curve. Just think about it here. Let's

07:36:58.610 --> 07:37:00.850
say I take a model and, oh, no. I made the the the model

07:37:00.850 --> 07:37:02.958
worse. So I have to think longer. But the latency is

07:37:02.958 --> 07:37:05.020
better. Then great. I can just push along the curve,

07:37:05.020 --> 07:37:07.150
get the latency, regress it back to where it was before.

07:37:07.310 --> 07:37:09.630
And eat through eat enjoy the throughput win. So basically,

07:37:09.630 --> 07:37:11.710
even if from an accurate parameter point of view, the throughput is

07:37:11.710 --> 07:37:13.790
worse, because you're thinking about not just the point,

07:37:13.790 --> 07:37:15.870
you know, not just a certain point in this curve, but entire

07:37:15.870 --> 07:37:17.900
curve itself. You can end up with a better final

07:37:17.900 --> 07:37:20.160
operating point. And, yeah, ultimately,

07:37:20.160 --> 07:37:21.930
that's how we do it. Right?

07:37:22.411 --> 07:37:24.411
Take our model, jam, like, two point o. We said, okay. We'll we'll we'll train

07:37:24.491 --> 07:37:26.510
we'll service on trillion. We'll serve on

07:37:26.510 --> 07:37:28.680
iron word, 2.5. You know, I guess you'll find out.

07:37:29.161 --> 07:37:29.821
Thanks, guys.

07:37:43.849 --> 07:37:45.880
Testing? So we have

07:37:45.880 --> 07:37:47.500
some time for a few questions.

07:37:52.620 --> 07:37:54.720
Yeah. Really excellent talk.

07:37:54.990 --> 07:37:56.769
Increased my TPU FOMO.

07:37:57.130 --> 07:37:58.180
To incredible levels.

07:37:59.540 --> 07:38:01.562
Like, the question I have,

07:38:03.790 --> 07:38:04.290
Okay?

07:38:08.350 --> 07:38:09.810
That was not the question.

07:38:11.831 --> 07:38:13.880
So yeah. In response

07:38:13.880 --> 07:38:15.982
to your recent question about why

07:38:16.950 --> 07:38:19.110
DeepThink is is behind a $250

07:38:19.110 --> 07:38:20.505
a month subscription

07:38:21.180 --> 07:38:23.421
Logan Kilpatrick said that that there are not enough

07:38:23.421 --> 07:38:25.550
TPUs in the world. To

07:38:25.550 --> 07:38:27.868
serve it. Like, openly

07:38:27.868 --> 07:38:29.880
to everybody. So I'm curious like,

07:38:30.120 --> 07:38:32.280
when do you think there will be enough TPUs in the

07:38:32.280 --> 07:38:34.241
world that everybody in this room can,

07:38:34.402 --> 07:38:36.320
conserve these models at this scale?

07:38:36.480 --> 07:38:38.210
Okay. So, you know, obviously, I can't give you

07:38:38.530 --> 07:38:40.610
numbers because, you know, you know, if I know them, I can't say them. But what

07:38:40.610 --> 07:38:42.890
I can say is that, obviously, you know, model quality at

07:38:42.890 --> 07:38:45.010
ISO cost has been improving dramatically in the last

07:38:45.010 --> 07:38:47.130
few years. I think we said publicly, Jeff,

07:38:47.210 --> 07:38:49.290
posted that we bought something like a 30 x throughput

07:38:49.290 --> 07:38:51.530
improvement. At ISO quality in the last year from, like, 1.5

07:38:51.530 --> 07:38:53.710
to two point o or 2.5. And

07:38:53.710 --> 07:38:55.770
that translates to again, you know, better points

07:38:55.770 --> 07:38:58.060
in this curve. So there are two parts of Deepgram. Right? Deepgram

07:38:58.060 --> 07:39:00.080
has a throughput cost, which is less expensive, and also

07:39:00.080 --> 07:39:02.160
the latency cost. It takes, you know, like five minutes or ten

07:39:02.160 --> 07:39:04.270
minutes to give you an answer. And

07:39:04.831 --> 07:39:06.900
you know, they're they're correlated. Right? Like deep think is

07:39:06.900 --> 07:39:09.070
you know, it's ticking for a long time. We

07:39:09.070 --> 07:39:11.230
want to get the latency down to give you an answer fast. And that

07:39:11.230 --> 07:39:13.470
means we're running at a, you know, very extreme point on this throughput

07:39:13.470 --> 07:39:15.620
latency curve. Curve. So if you can get models to be either

07:39:15.620 --> 07:39:17.710
more token efficient, or just get the sampling latency down

07:39:17.710 --> 07:39:19.750
in the first place, then the translate not only to

07:39:19.750 --> 07:39:21.820
a win at that particular serving point, but

07:39:21.820 --> 07:39:24.100
let's select a different serving point with more throughput wins.

07:39:24.100 --> 07:39:26.470
And get the cost even more rapidly. So

07:39:26.470 --> 07:39:28.560
basically, not yet, but, you know, I expect pretty soon.

07:39:28.800 --> 07:39:30.880
But it may not be the deep link you see right now. Like, deep link, you

07:39:30.880 --> 07:39:32.690
know, things for, you know, how many tokens

07:39:33.331 --> 07:39:34.940
I in one

07:39:35.400 --> 07:39:37.509
generation length. Part of our work is to improve token

07:39:37.509 --> 07:39:39.570
efficiency. So maybe that actually what you end up with next

07:39:39.570 --> 07:39:41.730
year is, like, a model which doesn't think at all, thinks for, like, very

07:39:41.730 --> 07:39:43.320
little. And gives you the same quality answer.

07:39:44.160 --> 07:39:46.314
Like, I don't know exactly what the characteristic of the

07:39:46.314 --> 07:39:48.460
product will be, but what I can say is that, yeah, I I totally

07:39:48.460 --> 07:39:50.780
expected something that gives the same quality answers to be available,

07:39:50.780 --> 07:39:52.841
you know, to everybody very, very soon. What

07:39:52.841 --> 07:39:54.390
we've been doing, you know, so far.

07:39:55.030 --> 07:39:56.810
And then one,

07:39:57.750 --> 07:40:00.230
separate question. Also

07:40:00.230 --> 07:40:02.570
seeing I work working model

07:40:02.570 --> 07:40:04.770
deployment platform. We're seeing a lot of folks running

07:40:05.251 --> 07:40:07.530
smaller models targeted to more specific tasks that

07:40:07.530 --> 07:40:09.610
they've sort of POC ed it

07:40:09.610 --> 07:40:11.630
with frontier models. So I'm curious

07:40:11.870 --> 07:40:13.890
how you think these same lessons that we've

07:40:14.030 --> 07:40:16.190
learned from serving at this super large

07:40:16.190 --> 07:40:18.580
scale are going to apply at a smaller

07:40:18.580 --> 07:40:20.660
scale. Whether that means changing things about hardware

07:40:20.660 --> 07:40:22.520
and changing things about, like, system

07:40:22.760 --> 07:40:24.841
interconnect on the chip or, whether

07:40:24.841 --> 07:40:26.910
it means, like, different modeling approaches or

07:40:26.910 --> 07:40:29.060
something else. Sure. Yeah. So the the thing here is, like,

07:40:29.060 --> 07:40:31.220
we said that basically big model means use more

07:40:31.220 --> 07:40:33.370
chips. Small model means, you know, use fewer chips. Right?

07:40:33.370 --> 07:40:35.690
Because if you try to use a similar chips, it'll be bottlenecked by the collective.

07:40:36.660 --> 07:40:38.900
You still get a giant throughput win. Right? So I think I mentioned

07:40:38.900 --> 07:40:40.710
it very, very briefly here somewhere.

07:40:41.841 --> 07:40:44.180
Maybe I didn't. Somewhere?

07:40:44.560 --> 07:40:46.640
Okay. Yeah. Here. Like the first small

07:40:46.640 --> 07:40:48.670
models that are doing, like, relatively short generations,

07:40:48.750 --> 07:40:50.830
this looks more flop heavy. Right? Because you aren't really worried

07:40:50.830 --> 07:40:52.930
about collectives. You aren't necessarily even worried about pramper loading. You

07:40:52.930 --> 07:40:54.470
just run a very high batch size.

07:40:55.030 --> 07:40:57.075
So, yeah, like, we could you know, for

07:40:57.075 --> 07:40:59.120
an accelerator system, if you're thinking about using small models, mini care you

07:40:59.120 --> 07:41:00.900
know, frankly, you build a GPU. Right?

07:41:01.140 --> 07:41:03.300
To some extent, GPUs are fantastic for, like, the

07:41:03.300 --> 07:41:05.720
small special purpose models, like, you know, the classifier or

07:41:05.720 --> 07:41:08.100
you know, it checks the model output is safe, and TPUs

07:41:08.100 --> 07:41:09.482
are better for, like, you know, ATI.

07:41:10.400 --> 07:41:12.570
Right? Because of how the system is designed, that you

07:41:12.570 --> 07:41:14.950
have a relatively small system, you know, with a high high

07:41:14.950 --> 07:41:17.009
operation. Density. And that's great for for

07:41:17.009 --> 07:41:19.050
these kind of, you know, denser, you know,

07:41:19.050 --> 07:41:20.940
weaker models. If you have a really

07:41:21.480 --> 07:41:23.840
strong model, you want to think for a very long time, then you're forced to use a giant

07:41:23.840 --> 07:41:25.980
accelerator system, and then you wanna So it's kind of like

07:41:26.140 --> 07:41:28.220
you'd make a GPU like GPU if you want to serve like weaker

07:41:28.220 --> 07:41:30.280
models, and a TPU harder to serve stronger models.

07:41:33.171 --> 07:41:34.480
Any more questions?

07:41:37.501 --> 07:41:38.820
One last question. No?

07:41:57.000 --> 07:41:59.349
Next have our two more,

07:41:59.349 --> 07:42:01.050
like, paper print stations.

07:42:02.232 --> 07:42:04.330
The first one is, by Aya.

07:42:05.050 --> 07:42:07.430
Aya is a computer science PhD student.

07:42:07.430 --> 07:42:09.700
At Harvard. Advised by Vijay.

07:42:10.260 --> 07:42:12.600
And, he's supported by the NSF Graduate

07:42:12.661 --> 07:42:14.720
Research Fellowship, His research

07:42:15.019 --> 07:42:17.099
interests are broadly in machine learning for

07:42:17.099 --> 07:42:19.251
a system optimization, and

07:42:19.251 --> 07:42:21.411
his recent work has focused on hardware

07:42:21.411 --> 07:42:23.350
aware autonomous performance engineering.

07:42:24.890 --> 07:42:26.570
Safety stores. Cool. Thanks.

07:42:41.750 --> 07:42:43.950
Hey, everyone. I'm Aria.

07:42:43.950 --> 07:42:45.490
I'm excited to, share

07:42:46.341 --> 07:42:48.661
my work on, swizzle perf, which is hardware

07:42:48.661 --> 07:42:50.599
aware LLMs for,

07:42:51.510 --> 07:42:53.591
kernel performance optimization. This, work was

07:42:53.591 --> 07:42:55.980
done last summer during my internship at,

07:42:56.220 --> 07:42:58.570
AMD. So thanks to all my collaborators at Orbit

07:42:58.618 --> 07:43:00.810
at AMD for that. So here's

07:43:00.810 --> 07:43:02.970
a quick summary of, what I'm going

07:43:02.970 --> 07:43:04.821
to walk through in this talk, and,

07:43:05.062 --> 07:43:07.241
we'll start with a background on what chiplet

07:43:07.300 --> 07:43:09.170
swizzling on AMD architectures is,

07:43:10.050 --> 07:43:12.210
and, some prior work on ML

07:43:12.270 --> 07:43:13.530
for GPU code optimization.

07:43:14.970 --> 07:43:16.990
So, sure many of you are familiar with

07:43:16.990 --> 07:43:19.251
a lot of the really cool recent breakthroughs

07:43:19.470 --> 07:43:21.790
in GPU kernel optimization, autonomous

07:43:21.790 --> 07:43:23.490
GPU kernel optimization in the past

07:43:24.130 --> 07:43:26.150
maybe one or two years. And

07:43:26.150 --> 07:43:28.360
at AMD, we took it

07:43:28.841 --> 07:43:30.380
maybe one step further by decomposing

07:43:31.810 --> 07:43:33.920
this kernel optimization task into

07:43:34.160 --> 07:43:36.480
optimizing for specific bottlenecks. So

07:43:36.495 --> 07:43:38.710
think fixing bank conflicts

07:43:38.770 --> 07:43:40.930
or uncoalesced memory accesses, and we

07:43:40.930 --> 07:43:42.840
can validate this with profiler feedback.

07:43:43.800 --> 07:43:45.958
Now, unfortunately, a lot of this

07:43:45.958 --> 07:43:48.130
prior work is

07:43:48.850 --> 07:43:51.091
not what we call hardware aware. And,

07:43:51.251 --> 07:43:53.402
even though there's some really strong results that come out of

07:43:53.402 --> 07:43:55.790
it, we show how this fundamentally limits

07:43:55.790 --> 07:43:57.870
the hardware specific optimizations that you're able

07:43:57.870 --> 07:44:00.090
to make. Now, what

07:44:00.090 --> 07:44:02.260
is hardware awareness? So at

07:44:02.260 --> 07:44:04.341
a high level, it's understanding the hardware and

07:44:04.341 --> 07:44:06.500
scheduling to be able to actually design

07:44:06.500 --> 07:44:08.530
the software specifically for the hardware.

07:44:08.930 --> 07:44:11.091
And in this case study, we focus on

07:44:11.091 --> 07:44:13.020
memory locality, but but you could imagine there's

07:44:13.260 --> 07:44:15.269
many different things that we can focus on that

07:44:15.269 --> 07:44:17.110
are specific to the hardware.

07:44:19.510 --> 07:44:21.600
For, the purposes of this study,

07:44:21.600 --> 07:44:23.540
we care specifically about

07:44:23.760 --> 07:44:25.540
the number of triplets per GPU.

07:44:26.020 --> 07:44:28.240
And the default runtime block scheduling policy.

07:44:28.480 --> 07:44:30.519
But there's many other things that you

07:44:30.519 --> 07:44:32.430
can care about, such as the warp size or number of,

07:44:32.530 --> 07:44:34.380
SMs or CUs depending on the provider,

07:44:36.140 --> 07:44:37.671
And importantly,

07:44:39.410 --> 07:44:41.460
different GPUs have different specs. Thus the

07:44:41.460 --> 07:44:43.640
way you need to program for them varies significantly.

07:44:45.170 --> 07:44:46.920
So Now, on AMD GPUs,

07:44:47.480 --> 07:44:49.470
they introduced eight triplets.

07:44:49.970 --> 07:44:52.370
And, this means that each chiplet has its own,

07:44:53.591 --> 07:44:55.554
separate l two cache, and

07:44:56.100 --> 07:44:58.360
of course, we wanna hit in the cache as much as possible.

07:44:59.001 --> 07:45:01.241
Now, by default, if we,

07:45:01.700 --> 07:45:04.100
tile up a gem and schedule it onto

07:45:04.100 --> 07:45:06.190
the different giblets, you're gonna have really

07:45:06.190 --> 07:45:07.251
bad cache locality.

07:45:08.840 --> 07:45:11.180
Because adjacent tiles are gonna be on

07:45:11.180 --> 07:45:12.810
different caches. You're not gonna hit it all.

07:45:14.730 --> 07:45:16.990
So my mentor over there spent

07:45:17.050 --> 07:45:19.100
several days working on this

07:45:19.341 --> 07:45:21.581
you know, few lines of code and this is gonna swizzle

07:45:21.581 --> 07:45:23.902
your blocks to improve

07:45:23.960 --> 07:45:26.030
the, cache hit rate on these, triplets.

07:45:27.491 --> 07:45:29.110
You can imagine that it's not really

07:45:29.650 --> 07:45:31.732
scalable for a really

07:45:31.732 --> 07:45:33.750
great GPU programmer to spend

07:45:33.750 --> 07:45:35.650
a long time on one

07:45:35.755 --> 07:45:37.800
optimization. So the purpose of our

07:45:37.800 --> 07:45:39.982
stuff was to think about how we can actually use

07:45:39.982 --> 07:45:42.000
hardware awareness to make this optimization.

07:45:42.640 --> 07:45:44.700
Autonomously. So Now, when you're

07:45:44.700 --> 07:45:46.880
actually able to drop in this optimization,

07:45:47.099 --> 07:45:49.341
you remap the

07:45:49.341 --> 07:45:51.460
tiles such that now you hit a lot

07:45:51.460 --> 07:45:53.710
more in the cache, and you get, improved

07:45:53.710 --> 07:45:55.020
hit rate and also speed up.

07:45:56.140 --> 07:45:58.001
So I'll walk through how we get there autonomously.

07:45:59.060 --> 07:46:01.220
This is, at a high level, what I'm doing, and I'll

07:46:01.220 --> 07:46:02.820
walk through each, piece of it.

07:46:03.859 --> 07:46:05.200
But to start,

07:46:05.900 --> 07:46:07.580
like, the prior work, you give an

07:46:08.850 --> 07:46:11.091
And you also give some hardware awareness

07:46:11.091 --> 07:46:13.269
information coming from the profiler and,

07:46:13.590 --> 07:46:16.001
hip device attribute, And you give runtime

07:46:16.060 --> 07:46:18.160
scheduling information coming from the architecture

07:46:18.300 --> 07:46:20.350
guide. You construct the

07:46:20.350 --> 07:46:22.300
prompt with this hardware awareness and

07:46:22.780 --> 07:46:24.510
some history from prior iterations.

07:46:25.171 --> 07:46:27.280
You output a new kernel that

07:46:27.280 --> 07:46:29.540
you can compile and validate to make sure that it's correct.

07:46:29.660 --> 07:46:31.820
And you can also profile for the metric of

07:46:31.820 --> 07:46:34.060
interest within which in this case is l two hit

07:46:34.060 --> 07:46:36.140
rate, and the speed up,

07:46:36.140 --> 07:46:38.440
which, you know, we we care a lot about. And

07:46:38.440 --> 07:46:40.460
we can combine all of this into

07:46:40.460 --> 07:46:42.480
what we call a bottleneck report, add it to a

07:46:42.480 --> 07:46:44.491
buffer, and iterate. Keep going from

07:46:44.491 --> 07:46:45.900
there and and and see speedups down the line.

07:46:47.340 --> 07:46:49.520
So So I'll quickly walk through some of our main

07:46:49.520 --> 07:46:51.740
results. And as

07:46:51.740 --> 07:46:53.840
you can imagine, different kernels have different

07:46:53.900 --> 07:46:56.081
baseline hit rates. And the purpose of this work is to

07:46:56.081 --> 07:46:57.290
improve these hit rates.

07:46:58.110 --> 07:47:00.190
For Gem, the swizzling pattern gives us a

07:47:00.190 --> 07:47:01.950
14%. L two hit rate.

07:47:02.831 --> 07:47:04.911
And for soft max, we see a completely

07:47:04.911 --> 07:47:07.171
different pattern that leads to a huge You actually

07:47:07.171 --> 07:47:09.220
get close to a 100% hit rate, so you

07:47:09.220 --> 07:47:10.380
can't get much better that.

07:47:11.690 --> 07:47:13.841
Now across this suite of

07:47:13.841 --> 07:47:16.050
of 10 ML and science kernels, we

07:47:16.050 --> 07:47:18.330
see that, our swizzle proof

07:47:18.490 --> 07:47:19.670
methodology is able to get us

07:47:21.140 --> 07:47:22.279
nine successful patterns

07:47:23.232 --> 07:47:25.280
compared to some baselines, which we

07:47:25.280 --> 07:47:27.150
don't put in hardware awareness or we just

07:47:27.310 --> 07:47:29.550
overload it with a bunch of documentation without actually

07:47:29.550 --> 07:47:31.720
kind trying to sift through what's important.

07:47:32.520 --> 07:47:34.390
We don't see anything close to as good.

07:47:34.870 --> 07:47:37.170
And we don't just see l two head rates. We we see speed ups.

07:47:38.849 --> 07:47:41.009
But what I'm also really excited to share

07:47:41.009 --> 07:47:42.880
is, our

07:47:43.120 --> 07:47:45.360
future work, where we're thinking about zooming out a little

07:47:45.360 --> 07:47:47.671
bit. Now, of course, SwizzlePerf is really specific

07:47:47.671 --> 07:47:50.010
to one optimization on one architecture.

07:47:50.930 --> 07:47:53.010
We wanna go to a lot of architectures and a lot

07:47:53.010 --> 07:47:55.060
of optimizations and see how hardware awareness can

07:47:55.220 --> 07:47:55.940
can push the needle.

07:47:57.540 --> 07:47:59.609
So a computer architect

07:47:59.609 --> 07:48:01.840
and, you know, the first day I stepped into

07:48:01.840 --> 07:48:03.180
class, we care about co design.

07:48:04.062 --> 07:48:06.220
Co design means we're designing the hardware

07:48:06.220 --> 07:48:07.680
to support emerging workloads,

07:48:08.290 --> 07:48:10.380
it means we're designing the software to

07:48:10.380 --> 07:48:12.140
really utilize the underlying hardware.

07:48:13.251 --> 07:48:15.331
Hardware awareness is not new. It didn't start with

07:48:15.331 --> 07:48:17.501
this paper. It didn't start in the few years with autonomous

07:48:17.560 --> 07:48:19.610
kernel generation. It's really old. And

07:48:19.610 --> 07:48:21.850
there's a been a lot of really great work in hardware

07:48:21.850 --> 07:48:22.690
awareness for

07:48:23.921 --> 07:48:25.670
know, across the the software stack.

07:48:26.310 --> 07:48:28.340
And what I'm really interested in is

07:48:28.340 --> 07:48:29.859
how we can bring hardware awareness

07:48:30.520 --> 07:48:32.570
into our new agentic code optimization work.

07:48:32.570 --> 07:48:34.670
Flows. Our

07:48:34.670 --> 07:48:36.769
current understanding is that LLMs

07:48:36.910 --> 07:48:39.220
can recall and reason about

07:48:39.220 --> 07:48:41.345
hardware where details. And I encourage you guys to

07:48:41.345 --> 07:48:43.170
check out some of our labs work on quark,

07:48:43.680 --> 07:48:46.180
where we, benchmark element reasoning on computer architecture,

07:48:46.880 --> 07:48:48.860
and see that compared to a lot of other systems tasks,

07:48:49.180 --> 07:48:50.950
LMs have really strong understandings of

07:48:51.270 --> 07:48:52.130
GPU architectures.

07:48:53.650 --> 07:48:55.990
LMs can also implement hardware agnostic

07:48:56.210 --> 07:48:57.685
GPU optimizations. And

07:48:58.270 --> 07:49:00.430
you know, our our awesome collaborators at Stanford

07:49:00.991 --> 07:49:03.152
have done some some great work that show that with some ML

07:49:03.152 --> 07:49:05.370
tricks, you can use elements to generate fast CUDA

07:49:05.370 --> 07:49:07.640
code. But most of these optimizations

07:49:08.100 --> 07:49:10.240
are limited to hardware agnostic things like

07:49:10.240 --> 07:49:12.260
tiling coloss memory accesses,

07:49:12.661 --> 07:49:14.590
reduced bank conflicts. All of these things work

07:49:14.830 --> 07:49:16.779
on a lot of different architectures, and

07:49:17.070 --> 07:49:19.232
you know, this is great, but we also wanna

07:49:19.232 --> 07:49:21.310
think about things that only work on the specific

07:49:21.310 --> 07:49:23.430
architecture that you're running on. Because that's how we really

07:49:23.430 --> 07:49:24.880
squeeze every last drop of perf.

07:49:25.505 --> 07:49:27.509
So What this tells

07:49:27.509 --> 07:49:29.495
us is that right now,

07:49:29.510 --> 07:49:30.982
LMs cannot implement

07:49:31.600 --> 07:49:33.760
hardware aware GPU kernel optimizations.

07:49:34.860 --> 07:49:36.940
You know, there's there's definitely some exceptions

07:49:36.940 --> 07:49:39.091
where they do a great job, But at a high level,

07:49:39.091 --> 07:49:41.150
we observed that they can't use

07:49:41.470 --> 07:49:43.660
hardware specific instructions like you know,

07:49:43.660 --> 07:49:45.910
the you know, GPU specific Tensor Core

07:49:45.910 --> 07:49:47.440
instruction, TMA, TMM.

07:49:47.921 --> 07:49:50.054
Etcetera. And there's, a lot of these things that change a

07:49:50.054 --> 07:49:52.091
lot with each generation of GPU. And they can't

07:49:52.091 --> 07:49:54.000
make hardware specific optimizations.

07:49:54.320 --> 07:49:56.260
Like, for example, the disaggregated memory

07:49:56.661 --> 07:49:58.600
optimization that I I talked about right now.

07:49:59.081 --> 07:50:01.270
And, why Unsurprisingly,

07:50:01.331 --> 07:50:03.340
we think, this is because there's not enough

07:50:03.655 --> 07:50:05.790
good there's not enough good data out

07:50:05.790 --> 07:50:08.010
there. So, So

07:50:08.251 --> 07:50:10.270
as I wrap up, our goal

07:50:11.210 --> 07:50:13.370
a high level is we wanna teach Ellen how and

07:50:13.610 --> 07:50:15.630
when to make hardware specific optimizations.

07:50:16.476 --> 07:50:18.790
And confident that if you can do

07:50:18.790 --> 07:50:20.820
this consistently, you'll get

07:50:21.060 --> 07:50:23.480
really great performance on a lot of different architectures,

07:50:24.421 --> 07:50:26.470
even as you go in the future, you'll you'll see great results.

07:50:26.950 --> 07:50:27.610
Yeah. Thank you.

07:50:34.220 --> 07:50:36.618
Thank the speaker. We'll do a combined

07:50:36.618 --> 07:50:38.770
q and a after, other

07:50:39.070 --> 07:50:40.190
spotlight. Speak.

07:50:41.310 --> 07:50:42.689
So this will be our

07:50:43.550 --> 07:50:44.849
last final talk, also

07:50:45.710 --> 07:50:47.430
spotlight presentation.

07:50:48.046 --> 07:50:50.420
By. Rokai is a final

07:50:50.420 --> 07:50:52.850
year PhD student in

07:50:52.850 --> 07:50:55.030
ECE at Yale University. Where he

07:50:55.030 --> 07:50:57.450
designs computer architecture, systems,

07:50:57.760 --> 07:51:00.001
and compression algorithms for energy

07:51:00.001 --> 07:51:02.359
efficient AI. His work has appeared

07:51:02.359 --> 07:51:04.630
in top venues like Micro, ISCA,

07:51:05.380 --> 07:51:06.840
NewRePS, and ICML.

07:51:07.661 --> 07:51:09.930
Including a best paper nomination, and,

07:51:10.570 --> 07:51:11.630
I triple e microtopics.

07:51:13.780 --> 07:51:15.480
Thank you, Horan, for the introduction.

07:51:16.020 --> 07:51:18.200
And thanks everyone for attending

07:51:18.200 --> 07:51:20.300
our presentation on learn to chart.

07:51:21.220 --> 07:51:23.460
So, this work was done during my internship

07:51:23.460 --> 07:51:24.118
at Microsoft.

07:51:25.610 --> 07:51:27.330
Just a quick advertisement.

07:51:27.830 --> 07:51:30.070
The team is hiring both interns and the full

07:51:30.070 --> 07:51:31.509
time for next,

07:51:32.120 --> 07:51:34.360
so, yeah, it's a great team. And my mentor,

07:51:34.360 --> 07:51:36.421
Paula, is here today. Feel free

07:51:36.421 --> 07:51:38.590
to reach out to her if you are interested. Thanks.

07:51:39.930 --> 07:51:42.010
Okay. So, in the era of

07:51:42.010 --> 07:51:43.640
IOM, ARM nowadays,

07:51:44.019 --> 07:51:46.259
the large scale distributed inference on a

07:51:46.259 --> 07:51:48.171
cloud level becomes really essential.

07:51:49.232 --> 07:51:51.550
Distributed inference, essentially is just

07:51:51.550 --> 07:51:53.580
put different parts of your

07:51:53.580 --> 07:51:55.540
models or data, across different

07:51:55.679 --> 07:51:58.070
devices, and doing the computation in parallel.

07:51:58.991 --> 07:52:01.010
Here are some examples of different parallelization

07:52:01.310 --> 07:52:03.331
types. You may be very familiar

07:52:03.390 --> 07:52:05.630
with all of those. So here are tensor

07:52:05.630 --> 07:52:07.690
parison, x perparison,

07:52:08.470 --> 07:52:10.250
which are a little delayed

07:52:10.570 --> 07:52:12.730
for the latest MOE models? And also

07:52:12.730 --> 07:52:14.800
there are pipeline Parism, and a

07:52:14.800 --> 07:52:17.060
lot of other parison types are there.

07:52:18.070 --> 07:52:20.170
And if we zoom into this

07:52:21.161 --> 07:52:23.411
paradigm types, right, how do we

07:52:24.110 --> 07:52:26.140
specifically describe a parallelization

07:52:26.920 --> 07:52:27.340
strategy?

07:52:29.259 --> 07:52:31.260
So let's use this well knowing

07:52:31.501 --> 07:52:33.040
example of the tensor paradigm

07:52:33.751 --> 07:52:35.230
on the Atlas and blocks.

07:52:36.130 --> 07:52:37.760
As an example? So

07:52:38.620 --> 07:52:40.940
one very well known

07:52:40.940 --> 07:52:42.870
perspective is the degrees.

07:52:43.030 --> 07:52:45.510
Right? So basically, how many number of devices

07:52:45.510 --> 07:52:47.779
are involved So in this

07:52:47.779 --> 07:52:50.140
example, the decrease is simply four.

07:52:51.179 --> 07:52:53.259
In perspective that we found very

07:52:53.259 --> 07:52:55.439
interesting but somehow got neglected

07:52:55.580 --> 07:52:57.840
before is the

07:52:58.060 --> 07:52:59.840
per operation, perism dimension.

07:53:00.751 --> 07:53:03.010
Basically, this means that on which

07:53:03.870 --> 07:53:06.000
dimensions does the each operator's

07:53:06.000 --> 07:53:07.939
output get shared across devices?

07:53:08.661 --> 07:53:10.902
Right. For example, in this, example, we

07:53:10.902 --> 07:53:12.910
have f f one being shorted

07:53:12.910 --> 07:53:14.970
on dimension one, FFN

07:53:14.970 --> 07:53:17.230
two got shorted on dimension zero.

07:53:18.142 --> 07:53:20.540
And this is often neglected before because

07:53:20.780 --> 07:53:22.880
this is usually determined by the handcrafted

07:53:23.180 --> 07:53:25.200
heuristic in different

07:53:25.200 --> 07:53:27.279
frameworks. But we find it interesting to

07:53:27.279 --> 07:53:29.284
include that in the search space as

07:53:29.284 --> 07:53:31.440
well. So

07:53:31.440 --> 07:53:33.050
in short, this design space

07:53:33.850 --> 07:53:35.523
for the parallelization strategy on the inferen

07:53:35.948 --> 07:53:37.970
distributed inference system is very

07:53:38.029 --> 07:53:40.288
large. Because we have different parallelization

07:53:40.509 --> 07:53:42.770
types and strategies which are complementary

07:53:42.770 --> 07:53:44.900
to each other. Thus creating a

07:53:44.900 --> 07:53:46.580
vast and complex design space.

07:53:47.220 --> 07:53:49.600
Especially when the number of total devices

07:53:49.600 --> 07:53:51.540
and the number of nodes

07:53:51.760 --> 07:53:53.220
on the computation graph got increased.

07:53:54.429 --> 07:53:56.300
For this vast design space,

07:53:56.780 --> 07:53:59.260
expert level exploration on the strategy remains

07:53:59.260 --> 07:54:01.360
very important, but we do want to

07:54:01.360 --> 07:54:03.380
leverage the help of an autonomous

07:54:03.519 --> 07:54:05.560
framework to help in exploring this

07:54:05.560 --> 07:54:07.591
space. So

07:54:07.591 --> 07:54:10.070
here is the standard framework of using the autonomous

07:54:10.070 --> 07:54:11.290
agent to search.

07:54:12.312 --> 07:54:14.520
The question is to how to how do we

07:54:15.081 --> 07:54:17.161
model the parallelization strategy search problem

07:54:17.161 --> 07:54:18.140
into this framework?

07:54:19.482 --> 07:54:21.650
Right? And we find that the key is

07:54:21.650 --> 07:54:23.760
just to integrate the

07:54:23.982 --> 07:54:26.150
strategy space into the action space.

07:54:27.110 --> 07:54:29.610
Specifically, we model all these sub strategies

07:54:30.520 --> 07:54:32.620
basically the different options for different

07:54:32.680 --> 07:54:34.980
degrees, and the per op dimensions.

07:54:35.460 --> 07:54:37.720
Into the discrete sub action spaces.

07:54:38.710 --> 07:54:41.109
So that the agent can then freely select

07:54:41.109 --> 07:54:42.490
a compound of subactions.

07:54:44.450 --> 07:54:46.550
And we want to explore whether reinforcement

07:54:46.849 --> 07:54:48.929
learning based agent is

07:54:48.929 --> 07:54:51.029
a promising candidate here. Below

07:54:51.029 --> 07:54:52.810
is the overview of our framework.

07:54:53.251 --> 07:54:55.411
We use a PPO based agent as our

07:54:55.411 --> 07:54:57.500
searching agent. On each

07:54:57.500 --> 07:54:59.760
iteration, it will select a compound

07:54:59.900 --> 07:55:01.911
of sub actions which determines

07:55:02.050 --> 07:55:04.300
the strategy. We then send this

07:55:04.300 --> 07:55:06.520
strategy to an in house simulator to

07:55:06.520 --> 07:55:07.591
provide the feedback.

07:55:08.720 --> 07:55:11.062
Sometimes, the output from the simulator

07:55:11.120 --> 07:55:13.171
can be very noisy, due

07:55:13.171 --> 07:55:15.110
to the violation of either the SLAs

07:55:15.330 --> 07:55:17.680
or the hardware's limitations.

07:55:18.642 --> 07:55:21.040
So that we have these dual rewards to encourage

07:55:21.040 --> 07:55:23.130
the high throughput strategies while

07:55:23.130 --> 07:55:25.270
avoiding, the violating ones.

07:55:26.970 --> 07:55:29.050
And this is our policy network design.

07:55:29.050 --> 07:55:31.240
It's pretty straightforward. It's an

07:55:31.240 --> 07:55:33.660
ELITE context based autoregressive transformer.

07:55:34.870 --> 07:55:37.020
If if you look at the figure from left to right,

07:55:37.421 --> 07:55:39.690
we have this, a light history queue,

07:55:39.930 --> 07:55:42.180
can store number T of the prior best

07:55:42.736 --> 07:55:44.751
actions. Then we send that through

07:55:44.890 --> 07:55:47.171
a lightweight embedding layers, and after

07:55:47.171 --> 07:55:49.331
embedding, we send that through transforming

07:55:49.331 --> 07:55:51.609
codel blocks, Right? And we get

07:55:51.609 --> 07:55:53.708
the logics, and we perform the action

07:55:53.769 --> 07:55:55.921
sampling. To get a

07:55:55.980 --> 07:55:58.140
strategy alpha. We send this alpha into our

07:55:58.140 --> 07:56:00.200
simulator to get the reward,

07:56:00.200 --> 07:56:02.360
which would be, the throughputs in

07:56:02.360 --> 07:56:04.500
this sense in this case. And then

07:56:04.500 --> 07:56:06.690
we use the reward to perform the gradient descent,

07:56:06.930 --> 07:56:09.390
And if we find the, new

07:56:09.929 --> 07:56:12.250
action is better than any of the ones

07:56:12.250 --> 07:56:14.519
that existed in our history

07:56:14.519 --> 07:56:16.300
queue, we will update our queue accordingly.

07:56:18.320 --> 07:56:20.480
We also have several optimizations that

07:56:20.480 --> 07:56:22.500
come up with our Opex

07:56:22.500 --> 07:56:24.920
network to improve the searching efficiency.

07:56:26.359 --> 07:56:28.618
The first optimization would be the sequential

07:56:28.759 --> 07:56:30.859
multi agents. Essentially,

07:56:30.859 --> 07:56:32.640
what we are doing is that we

07:56:32.880 --> 07:56:34.420
will launch several agents

07:56:35.288 --> 07:56:37.429
sequentially to search on the

07:56:37.429 --> 07:56:39.740
same design space. Every

07:56:39.800 --> 07:56:42.260
new agent will inherit the

07:56:42.260 --> 07:56:44.660
prior best results and highlight history cue

07:56:44.660 --> 07:56:45.800
from the previous agents.

07:56:46.751 --> 07:56:48.911
Right? And if the new results

07:56:48.911 --> 07:56:50.930
found by the new agent worse

07:56:50.930 --> 07:56:53.270
than the prior best results, it will receive

07:56:53.270 --> 07:56:55.509
a soft penalty. By doing

07:56:55.509 --> 07:56:57.530
this, it can hugely alleviate

07:56:57.670 --> 07:56:59.050
the issue of local maximum,

07:56:59.841 --> 07:57:01.921
And while more agents will have better

07:57:01.921 --> 07:57:03.940
chance of finding a good strategy we will

07:57:03.940 --> 07:57:06.120
have, right, However, having

07:57:06.120 --> 07:57:08.620
a fixed search budget for each agent

07:57:08.921 --> 07:57:10.070
will be very costly.

07:57:11.189 --> 07:57:13.450
So that comes, to our second optimization,

07:57:14.571 --> 07:57:16.868
the confidence early exit. This is also

07:57:16.868 --> 07:57:19.070
straightforward. Basically, once the agent

07:57:19.070 --> 07:57:21.310
starts to exploit the strategy it has

07:57:21.310 --> 07:57:23.450
found before, there's no meaning to

07:57:23.450 --> 07:57:25.170
keep the searching going on.

07:57:25.730 --> 07:57:27.970
And this is also back up from this loss curve. Right?

07:57:27.970 --> 07:57:30.140
We can see that the loss doesn't change much

07:57:30.300 --> 07:57:31.679
it starts to exploit.

07:57:32.540 --> 07:57:35.040
So we define this confidential confidence

07:57:35.740 --> 07:57:38.179
score which is essentially just a maximum of the softmax

07:57:38.240 --> 07:57:39.859
of the logics for each subactions.

07:57:40.460 --> 07:57:42.400
And as long as the confidence

07:57:42.540 --> 07:57:44.720
score of every subactions is greater

07:57:44.720 --> 07:57:46.880
than a predefined threshold, we will

07:57:46.880 --> 07:57:48.990
just early exit terminate the search

07:57:49.230 --> 07:57:51.390
we can allocate the remaining budgets for

07:57:51.390 --> 07:57:53.410
the current agent to the remaining agents

07:57:53.580 --> 07:57:54.870
for the searching efficiency.

07:57:56.150 --> 07:57:58.010
So here is our experimental setups.

07:57:58.970 --> 07:58:01.288
For the system, we choose using h 100

07:58:01.288 --> 07:58:03.421
systems, and deploy in one point two

07:58:03.421 --> 07:58:05.680
trillion and one point six trillion MOE models.

07:58:06.130 --> 07:58:08.210
We allow 12 fused operations on

07:58:08.210 --> 07:58:10.310
our computation graph and up to 24

07:58:10.310 --> 07:58:12.450
k GPUs. The total

07:58:12.509 --> 07:58:14.850
search space is roughly 10 to the power of nine.

07:58:15.970 --> 07:58:17.911
We said we allow 4,000

07:58:18.210 --> 07:58:20.090
search budgets for our agents.

07:58:20.330 --> 07:58:22.810
That means 4,000 times of simulator

07:58:22.810 --> 07:58:24.921
calls. And

07:58:25.060 --> 07:58:27.280
confidence threshold setting to be point nine five.

07:58:28.640 --> 07:58:31.030
We first compare the search quality

07:58:31.270 --> 07:58:33.350
of our proposed method with the other

07:58:33.350 --> 07:58:35.360
meta heuristic search algorithms which

07:58:35.360 --> 07:58:36.640
are very well, widely,

07:58:37.520 --> 07:58:39.570
adopted. Basically, the simulated

07:58:39.570 --> 07:58:41.630
annealing and the random walk. So here, all

07:58:41.630 --> 07:58:43.950
the results are normalized to the random walk,

07:58:43.950 --> 07:58:46.230
so we can see that our methods provide

07:58:46.230 --> 07:58:48.630
a much better quality compared to those meta heuristic

07:58:48.630 --> 07:58:49.130
algorithms.

07:58:50.710 --> 07:58:52.790
We also measure the improvement of the

07:58:52.790 --> 07:58:54.929
strategy found by our agents over the

07:58:54.929 --> 07:58:56.991
MicroCharm Heuristic. So

07:58:56.991 --> 07:58:59.251
we can see that we can observe

07:58:59.390 --> 07:59:01.230
from 1% to 6%

07:59:01.714 --> 07:59:03.180
improvements over the Magatron

07:59:04.040 --> 07:59:05.660
LM heuristic from

07:59:06.120 --> 07:59:08.060
the new strategies found by our agents?

07:59:09.320 --> 07:59:11.341
Here is just an a simple example

07:59:11.480 --> 07:59:13.650
of the results found by our agent.

07:59:13.650 --> 07:59:15.741
Right? So on top, should

07:59:15.741 --> 07:59:17.460
be very familiar with this, is the

07:59:18.280 --> 07:59:20.660
what LM Megatron LM

07:59:20.720 --> 07:59:22.840
is doing. For the all reduce based

07:59:24.359 --> 07:59:26.501
on the FFM blocks. And on

07:59:26.501 --> 07:59:28.760
the bottom is the all gather based parallelization

07:59:28.900 --> 07:59:31.240
strategy found by our agent, So,

07:59:31.380 --> 07:59:33.480
essentially, we can see here the only difference is that

07:59:33.720 --> 07:59:35.800
we choose our agent choose to shard on the

07:59:35.800 --> 07:59:37.780
dimension one as well for

07:59:38.160 --> 07:59:40.310
the FFN one. The difference would be then we will have

07:59:40.310 --> 07:59:42.650
three altogether instead of two or reduced.

07:59:42.870 --> 07:59:45.190
Depending on your system implementation. If you have a faster

07:59:45.190 --> 07:59:47.321
altogether, you will have

07:59:47.380 --> 07:59:49.429
a better overall

07:59:49.429 --> 07:59:51.190
latency. So it all depends on,

07:59:51.430 --> 07:59:53.700
what's your system looks like. But it just

07:59:53.859 --> 07:59:56.179
I just wanna illustrate that our agency is able

07:59:56.179 --> 07:59:57.990
to find a better

07:59:58.550 --> 08:00:00.630
strategy compared to a heuristic based on

08:00:00.630 --> 08:00:02.840
the simulator. Okay. Great.

08:00:02.840 --> 08:00:04.939
Thank you. And I'm happy to take questions.

08:00:06.060 --> 08:00:08.458
Again, I want special thanks to my mentors,

08:00:08.458 --> 08:00:10.310
collaborators, and the team in micro

08:00:10.790 --> 08:00:12.831
give me a huge support. Thank you. And

08:00:12.831 --> 08:00:14.800
the paper link is here. We also provide

08:00:15.040 --> 08:00:17.200
code link. The code is not ready yet. We are cleaning

08:00:17.200 --> 08:00:18.450
up things for

08:00:19.890 --> 08:00:22.321
attended contest, but it will be there. Thank

08:00:22.321 --> 08:00:22.760
you.

08:00:35.340 --> 08:00:37.640
To either of these talks, any

08:00:37.698 --> 08:00:38.060
questions?

08:00:47.280 --> 08:00:49.510
I guess in the meantime, can ask

08:00:49.510 --> 08:00:51.640
a question to Suisopurf

08:00:54.402 --> 08:00:56.495
Maybe I'll just save the

08:00:56.495 --> 08:00:58.490
question. To

08:00:58.550 --> 08:01:00.540
that. Hi.

08:01:00.700 --> 08:01:02.500
Hi. This is a question for you.

08:01:03.140 --> 08:01:03.230
For you.

08:01:05.930 --> 08:01:08.200
So you mentioned hardware aware

08:01:08.675 --> 08:01:10.830
LLMs. Yes. What are some

08:01:10.830 --> 08:01:13.010
strategies you're using to explain

08:01:13.070 --> 08:01:14.190
the hardware to them?

08:01:15.630 --> 08:01:17.730
And get them to reason through that, I guess?

08:01:19.070 --> 08:01:21.230
So it changes based on the optimization we're making. Right?

08:01:21.230 --> 08:01:23.270
Like, obviously, the hardware details that you'd have to

08:01:23.270 --> 08:01:25.349
know for swizzling is different than the hardware details you

08:01:25.349 --> 08:01:27.120
have to know for I don't know,

08:01:28.081 --> 08:01:30.190
using asynchronous memory copies or something. So Right.

08:01:31.230 --> 08:01:33.390
For this paper, we just

08:01:33.390 --> 08:01:35.609
manually figured it out. Like, we got the information from

08:01:35.609 --> 08:01:37.740
the profiler, got it from architecture

08:01:37.740 --> 08:01:39.780
guides, we said, okay, this is the important information that

08:01:39.780 --> 08:01:40.280
you need.

08:01:41.841 --> 08:01:44.140
Now we're thinking about how do you scale this so that you don't have

08:01:44.140 --> 08:01:45.310
to do it by hand and

08:01:46.171 --> 08:01:48.010
haven't quite figured that out yet. But we're thinking about it.

08:01:48.251 --> 08:01:49.240
Yeah. Okay. Thanks.

08:01:52.030 --> 08:01:54.091
Any other questions? Oh,

08:01:54.091 --> 08:01:54.770
in the back.

08:02:01.960 --> 08:02:04.060
Hello. I have a question for, Du Kyi.

08:02:04.700 --> 08:02:05.800
Is currently is your

08:02:06.840 --> 08:02:09.190
environment like actual h one hundreds

08:02:09.190 --> 08:02:11.500
or are they more of an analytical modeling

08:02:11.980 --> 08:02:14.480
of the performance? Sorry. You say the environment

08:02:14.538 --> 08:02:16.540
is what? Like actual

08:02:16.540 --> 08:02:18.860
hardware's performance. Oh, got it. Yeah. It's based

08:02:18.860 --> 08:02:20.970
on the in house simulator. So

08:02:20.970 --> 08:02:23.230
the simulator will provide, the estimated

08:02:23.288 --> 08:02:25.210
performance of the real system.

08:02:25.850 --> 08:02:28.091
And is there a reason behind it's not the actual

08:02:28.091 --> 08:02:30.140
system that you're working on top of? Is it

08:02:30.140 --> 08:02:31.850
because of the latency? Yeah.

08:02:32.152 --> 08:02:34.500
I guess one of the reasons is

08:02:34.500 --> 08:02:36.530
that the speed to get reward right?

08:02:36.550 --> 08:02:39.009
So to deploy changing the parallelization

08:02:39.470 --> 08:02:41.330
strategy and get the real

08:02:42.821 --> 08:02:45.040
the real system speed up like

08:02:45.120 --> 08:02:47.380
to getting the reward, time is much longer

08:02:47.440 --> 08:02:49.380
by just run running through the simulators.

08:02:50.270 --> 08:02:52.710
Yeah. I think that's one of the reason. Makes

08:02:52.710 --> 08:02:53.929
sense. Thank you. Thank you.

08:02:58.680 --> 08:03:00.220
Another question for Rokai.

08:03:01.270 --> 08:03:03.590
Like, kinda following up on that question. Did

08:03:03.720 --> 08:03:05.870
you did the models discover any

08:03:05.870 --> 08:03:08.030
bugs in your simulator that they reward hacked

08:03:08.030 --> 08:03:10.062
and, like, otherwise, did you observe

08:03:10.062 --> 08:03:12.300
the sort of, like, reward hacking things that people have seen

08:03:12.300 --> 08:03:14.390
with Sukana, KernelBench, other

08:03:14.390 --> 08:03:15.760
places where the models you know,

08:03:16.880 --> 08:03:18.780
skip work, drop memory barriers, and other

08:03:18.940 --> 08:03:21.180
otherwise, like, exploit weaknesses in the reward

08:03:21.180 --> 08:03:21.650
function.

08:03:23.890 --> 08:03:24.550
So basically,

08:03:26.770 --> 08:03:28.930
sorry. Like, could you could you just repeat the question at

08:03:28.930 --> 08:03:31.020
the beginning? Yeah. Did you observe reward hacking

08:03:31.260 --> 08:03:33.330
like, and what did

08:03:33.330 --> 08:03:35.360
you do to detect it or avoid it?

08:03:36.001 --> 08:03:38.480
Oh, I didn't, see any reward

08:03:38.480 --> 08:03:40.939
hacking. But I I guess that will

08:03:42.001 --> 08:03:44.130
that is heavily affected like

08:03:44.370 --> 08:03:46.710
affected by the design of your simulator as well.

08:03:46.770 --> 08:03:48.960
Right? And how your design, basically

08:03:50.000 --> 08:03:52.010
at what level of details you have in your simulators,

08:03:52.251 --> 08:03:54.411
simulator. But I I guess that's

08:03:54.411 --> 08:03:56.620
also a great question. I I certainly get

08:03:56.860 --> 08:03:58.870
So think that's another benefit of

08:03:58.870 --> 08:04:01.040
our method. So basically, we don't

08:04:01.040 --> 08:04:02.980
need to, ask the user

08:04:03.439 --> 08:04:05.410
to impose any details of the implementation

08:04:05.710 --> 08:04:07.800
of the simulator. You just need to provide

08:04:07.880 --> 08:04:10.009
the reward to us. So I think that's

08:04:10.009 --> 08:04:12.330
another we can treat the simulator as

08:04:12.330 --> 08:04:14.421
a pure black box. I guess that's

08:04:14.421 --> 08:04:16.740
one of the benefits if we compare to other

08:04:16.740 --> 08:04:18.960
existing methods like internship programming and all

08:04:18.960 --> 08:04:21.142
those things. I don't know whether that answer your question.

08:04:21.380 --> 08:04:23.460
Partly. But I guess it's like if you treat the simulator

08:04:23.460 --> 08:04:25.600
as a black box, the model can optimize that

08:04:25.600 --> 08:04:27.440
black box and

08:04:27.680 --> 08:04:30.081
not optimize the thing you actually are trying to optimize,

08:04:30.081 --> 08:04:31.380
like, the hardware itself.

08:04:32.200 --> 08:04:34.280
Right? And, like, a user providing you

08:04:34.280 --> 08:04:36.610
the reward is frequently going

08:04:36.610 --> 08:04:38.950
to miss things about the

08:04:39.430 --> 08:04:41.910
the, like, how to design that evaluator

08:04:42.130 --> 08:04:44.590
appropriately Like, a prominent

08:04:44.970 --> 08:04:46.510
instance was I think, IntelliJ

08:04:47.171 --> 08:04:49.671
was only checking whether the default CUDA context

08:04:50.040 --> 08:04:52.040
had, like, completed its work. But not

08:04:52.120 --> 08:04:54.120
or the default CUDA stream had completed its work.

08:04:54.280 --> 08:04:56.450
But not whether all streams had completed work.

08:04:56.610 --> 08:04:58.690
So the agent learned to push all the work into

08:04:58.690 --> 08:05:00.700
the other streams so that it

08:05:00.700 --> 08:05:02.930
would appear the kernel had finished faster. Right?

08:05:02.930 --> 08:05:05.071
So there's, like, quite like, quite a

08:05:05.071 --> 08:05:07.150
large number of these projects have had these, like,

08:05:07.630 --> 08:05:09.250
very like, somewhat subtle,

08:05:09.870 --> 08:05:11.730
like, reward hacking by the agents.

08:05:11.910 --> 08:05:14.200
That, have yeah,

08:05:14.200 --> 08:05:15.940
undermined their claims to, like, improve

08:05:16.421 --> 08:05:18.530
I'm curious if you've seen anything like that. Yeah. Totally

08:05:18.530 --> 08:05:20.950
agree with that. So but, again, since we are treating

08:05:21.009 --> 08:05:23.180
the the the black box, so the more accurate

08:05:23.180 --> 08:05:25.260
of the simulator, then you can capture more details

08:05:25.260 --> 08:05:27.368
of the real system. The better result of our

08:05:27.368 --> 08:05:29.440
origins will provide. Yeah.

08:05:29.440 --> 08:05:30.340
Got it. Thank you.

08:05:33.560 --> 08:05:35.730
Cool. I think it's the time for

08:05:35.730 --> 08:05:36.830
the poster session.

08:05:37.790 --> 08:05:39.940
And we're going to have

08:05:39.940 --> 08:05:41.530
a closing remark at the end.

08:05:42.730 --> 08:05:44.810
Yeah. So, the closing remark is mostly

08:05:44.810 --> 08:05:47.200
for the online, livestream, which is ending

08:05:47.411 --> 08:05:49.279
now, I think. But yeah. So for the,

08:05:49.519 --> 08:05:51.566
people who are physically here, yeah, we're gonna do

08:05:51.566 --> 08:05:53.640
the poster session. And then afterwards,

08:05:54.019 --> 08:05:55.910
starting at 05:30, there's going

08:05:56.310 --> 08:05:57.580
to be the first ever

08:05:59.366 --> 08:06:01.660
NERPS, ML for systems, workshop

08:06:01.660 --> 08:06:03.800
happy hour. Really excited

08:06:03.800 --> 08:06:05.841
about that. And it

08:06:05.841 --> 08:06:07.921
looks like based on the number of people who

08:06:07.921 --> 08:06:09.939
have applied, it's going to

08:06:09.939 --> 08:06:12.030
be probably pretty crowded. So I

08:06:12.030 --> 08:06:13.890
recommend going earlier if you can.

08:06:14.370 --> 08:06:16.710
And if you're a, speaker

08:06:16.850 --> 08:06:18.070
panelist, or,

08:06:19.671 --> 08:06:21.690
have a poster or did the oral

08:06:21.751 --> 08:06:23.050
presentation, please

08:06:23.960 --> 08:06:25.991
to one of us. We'll give you a sticker so

08:06:25.991 --> 08:06:28.091
that you can get preferential treatment.

08:06:28.240 --> 08:06:30.120
To get yeah.

08:06:30.260 --> 08:06:32.421
In case in case that fully runs out. So

08:06:32.421 --> 08:06:34.090
we'll see. Yeah. But hopefully, it won't come to that.

08:06:34.650 --> 08:06:36.310
And, yeah, thanks everyone.

08:06:37.370 --> 08:06:38.510
And see you at the party.