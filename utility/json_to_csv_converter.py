import csv
import json
from typing import List, Dict, Any, Type
from pydantic import BaseModel
import pandas as pd


def json_to_csv_with_pydantic(
    json_data: str,
    pydantic_model: Type[BaseModel],
    csv_filename: str,
    remove_duplicates: bool = True,
    filter_empty: bool = True,
    preserve_all_data: bool = False,
    encoding: str = "utf-8",
) -> Dict[str, Any]:
    """
    Convert JSON data to CSV using Pydantic model schema as column structure.

    Args:
        json_data: Raw JSON string from browser-use output
        pydantic_model: Pydantic model class that defines the structure
        csv_filename: Output CSV filename
        remove_duplicates: Whether to remove duplicate rows
        filter_empty: Whether to filter out empty/N/A values
        preserve_all_data: If True, keeps all data with minimal filtering
        encoding: File encoding (default: utf-8)

    Returns:
        Dictionary with statistics about the conversion
    """
    try:
        # Parse the JSON data using the Pydantic model
        parsed_data = pydantic_model.model_validate_json(json_data)

        # Extract the list of items (assuming the model has a list field)
        items = []
        for field_name in pydantic_model.model_fields.keys():
            field_value = getattr(parsed_data, field_name)
            if isinstance(field_value, list):
                items = field_value
                break

        if not items:
            raise ValueError("No list field found in the Pydantic model")

        # Get field names from the first item's model
        if items:
            item_model = type(items[0])
            fieldnames = list(item_model.model_fields.keys())
        else:
            raise ValueError("No items found to extract field names")

        # Remove duplicates if requested
        unique_items = []
        seen_items = set()

        for item in items:
            # Create a unique identifier for each item
            item_values = tuple(getattr(item, field) for field in fieldnames)

            if remove_duplicates:
                if item_values not in seen_items:
                    seen_items.add(item_values)
                    unique_items.append(item)
            else:
                unique_items.append(item)

        # Filter empty/invalid items if requested
        valid_items = []
        if preserve_all_data:
            # When preserving all data, only remove completely empty rows
            valid_items = unique_items
            print(
                f"üîí Preserve all data mode: keeping all {len(unique_items)} unique items"
            )
        elif filter_empty:
            for item in unique_items:
                # Less aggressive filtering - only filter if ALL fields are empty/N/A
                all_empty = True
                for field in fieldnames:
                    value = getattr(item, field)
                    if (
                        value
                        and str(value).strip()
                        and str(value).strip().upper() != "N/A"
                    ):
                        all_empty = False
                        break

                if not all_empty:  # Keep item if at least one field has valid data
                    valid_items.append(item)
        else:
            valid_items = unique_items

        # Write to CSV
        with open(csv_filename, "w", newline="", encoding=encoding) as csvfile:
            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

            # Write header
            writer.writeheader()

            # Write data rows
            for item in valid_items:
                row_data = {}
                for field in fieldnames:
                    row_data[field] = getattr(item, field)
                writer.writerow(row_data)

        # Calculate statistics
        stats = {
            "total_items": len(items),
            "unique_items": len(unique_items),
            "valid_items_written": len(valid_items),
            "duplicates_removed": len(items) - len(unique_items),
            "empty_filtered": len(unique_items) - len(valid_items),
            "csv_filename": csv_filename,
            "fieldnames": fieldnames,
        }

        return stats

    except Exception as e:
        raise Exception(f"Error converting JSON to CSV: {str(e)}")


def json_to_csv_fallback(
    raw_text: str,
    csv_filename: str,
    expected_columns: List[str],
    delimiter: str = "|",
    encoding: str = "utf-8",
) -> Dict[str, Any]:
    """
    Fallback method to convert raw text output to CSV when JSON parsing fails.

    Args:
        raw_text: Raw text output from browser-use
        csv_filename: Output CSV filename
        expected_columns: List of expected column names
        delimiter: Delimiter used in the raw text (default: '|')
        encoding: File encoding (default: utf-8)

    Returns:
        Dictionary with statistics about the conversion
    """
    lines = raw_text.strip().split("\n")
    csv_data = []

    # Add header
    csv_data.append(expected_columns)

    # Process each line
    for line in lines:
        if line.strip() and delimiter in line:
            row = [cell.strip() for cell in line.split(delimiter)]
            row = [cell for cell in row if cell]  # Remove empty cells

            if row and len(row) >= len(expected_columns):
                # Take only the expected number of columns
                csv_data.append(row[: len(expected_columns)])

    # Remove duplicates
    unique_rows = []
    seen_rows = set()

    for row in csv_data[1:]:  # Skip header
        row_tuple = tuple(row)
        if row_tuple not in seen_rows:
            seen_rows.add(row_tuple)
            unique_rows.append(row)

    # Filter empty rows
    valid_rows = []
    for row in unique_rows:
        if all(cell and cell.strip() and cell.strip().upper() != "N/A" for cell in row):
            valid_rows.append(row)

    # Write to CSV
    final_data = [csv_data[0]] + valid_rows  # Header + valid rows

    with open(csv_filename, "w", newline="", encoding=encoding) as csvfile:
        writer = csv.writer(csvfile)
        writer.writerows(final_data)

    stats = {
        "total_rows": len(csv_data) - 1,  # Exclude header
        "unique_rows": len(unique_rows),
        "valid_rows_written": len(valid_rows),
        "duplicates_removed": len(csv_data) - 1 - len(unique_rows),
        "empty_filtered": len(unique_rows) - len(valid_rows),
        "csv_filename": csv_filename,
        "fieldnames": expected_columns,
    }

    return stats


# Example usage function
def convert_browser_use_output_to_csv(
    raw_result: str,
    pydantic_model: Type[BaseModel],
    csv_filename: str,
    preserve_all_data: bool = False,
) -> Dict[str, Any]:
    """
    Main function to convert browser-use output to CSV.
    Tries JSON parsing first, falls back to text parsing if needed.

    Args:
        raw_result: Raw result string from browser-use
        pydantic_model: Pydantic model defining the data structure
        csv_filename: Output CSV filename
        preserve_all_data: If True, preserves all data with minimal filtering
    """
    try:
        # Try JSON parsing first
        stats = json_to_csv_with_pydantic(
            json_data=raw_result,
            pydantic_model=pydantic_model,
            csv_filename=csv_filename,
            preserve_all_data=preserve_all_data,
        )

        print(f"‚úÖ Successfully converted JSON to CSV: {csv_filename}")
        print(f"üìä Statistics:")
        print(f"   Total items: {stats['total_items']}")
        print(f"   Unique items: {stats['unique_items']}")
        print(f"   Valid items written: {stats['valid_items_written']}")
        print(f"   Duplicates removed: {stats['duplicates_removed']}")
        print(f"   Empty/invalid filtered: {stats['empty_filtered']}")

        return stats

    except Exception as e:
        print(f"‚ö†Ô∏è  JSON parsing failed: {e}")
        print("üîÑ Attempting fallback text parsing...")

        # Extract field names from Pydantic model
        fieldnames = list(pydantic_model.model_fields.keys())
        if hasattr(pydantic_model, "__annotations__"):
            # If it's a container model, get the inner model's fields
            for field_name, field_info in pydantic_model.model_fields.items():
                if hasattr(field_info.annotation, "__args__"):
                    inner_type = field_info.annotation.__args__[0]
                    if hasattr(inner_type, "model_fields"):
                        fieldnames = list(inner_type.model_fields.keys())
                        break

        fallback_filename = csv_filename.replace(".csv", "_fallback.csv")

        stats = json_to_csv_fallback(
            raw_text=raw_result,
            csv_filename=fallback_filename,
            expected_columns=fieldnames,
        )

        print(f"‚úÖ Fallback conversion completed: {fallback_filename}")
        print(f"üìä Statistics:")
        print(f"   Total rows: {stats['total_rows']}")
        print(f"   Unique rows: {stats['unique_rows']}")
        print(f"   Valid rows written: {stats['valid_rows_written']}")
        print(f"   Duplicates removed: {stats['duplicates_removed']}")
        print(f"   Empty/invalid filtered: {stats['empty_filtered']}")

        return stats
