{"sessions": [{"Title": "1st Workshop on Multilingual Data Quality Signals", "Abstract": "Recent research has shown that large language models (LLMs)\nnot only need large quantities of data, but also need data\nof sufficient quality. Ensuring data quality is even more\nimportant in a multilingual setting, where the amount of\nacceptable training data in many languages is limited.\nIndeed, for many languages even the fundamental step of\nlanguage identification remains a challenge, leading to\nunreliable language labels and thus noisy datasets for\nunderserved languages.\n\nIn response to these challenges, we will be holding the\nfirst Workshop on Multilingual Data Quality Signals (WMDQS)\nin tandem with COLM. We invite the submission of long and\nshort research papers related to data quality in\nmultilingual data.\n\nEven though most previous work on data quality has been\ntargeted at LLM development, we believe that research in\nthis area can also benefit other research communities in\nareas such as web search, web archiving, corpus linguistics,\ndigital humanities, political sciences and beyond. We\ntherefore encourage submissions from a wide range of\ndisciplines.\n\nWMDQS will also include a shared task on language\nidentification for web text. We invite participants to\nsubmit novel systems which address current problems with\nlanguage identification for web text. We will provide a\ntraining set of annotated documents sourced from Common\nCrawl to aid development.", "Speakers": "Sebastian Nagel (Common Crawl Foundation); Julia Kreutzer (Cohere Labs); David Ifeoluwa Adelani (McGill University and Mila)"}, {"Title": "COLM 2025 Workshop on AI Agents: Capabilities and Safety", "Abstract": "Over the past decades, researchers across artificial intelligence, linguistics, and human-computer interaction have pursued the goal of making machines understand and respond to natural language. Recent breakthroughs in large language models have accelerated progress, powering virtual assistants and enabling more complex interactions. Yet, many challenges remain around trust, control, and real-world effectiveness.\n\nAI Agents: Capabilities and Safety will be a one-day workshop, co-located with COLM 2025 in Montreal, Canada. This workshop will convene experts and practitioners to explore the next frontier of AI agents. We aim to examine the evolving capabilities of agents, from reasoning and planning to tool use and embodiment, while also confronting the urgent need for safety, reliability, and alignment as these systems are deployed in real-world, high-stakes contexts.\n\nThe workshop will include keynotes, oral presentations, posters, and panel sessions. In keynote talks, senior technical leaders from industry and academia will share insights on the latest developments in the field. We would like to encourage researchers and students to share their prospects and latest discoveries. There will also be a panel discussion with noted conversational AI leaders focused on the state of the field, future directions, and open problems across academia and industry.", "Speakers": "Yoshua Bengio (Universit\u00e9 de Montr\u00e9al & Mila); Diyi Yang (Stanford University); Huan Sun (Ohio State University); Vinay Rao (Anthropic); Graham Neubig (Carnegie Mellon University & All hands); Muhao Chen (University of California, Davis); Jo\u00e3o Sedoc (New York Univerisity); Chi Wang (Google Deepmind & AG2)"}, {"Title": "Welcome to the First Workshop on Bridging NLP and Public Opinion Research!", "Abstract": "NLPOR aims to strengthen the emerging connection between NLP and public opinion research (POR);\n\njoining forces to improve both data collection and human-facing technology.\n\nAdvances in NLP and Large Language Models (LLMs) are driving changes in how we engage with and interpret human-generated data. The NLP community is increasingly interested in LLM evaluation (e.g., Sun et al., 2023, R\u00f6ttger et al., 2024), language model pretraining research (Gao et al., 2020, Soldaini et al., 2024), data-centric approaches to data collection (Gebru et al., 2021, Paullada et al., 2021), data ethics (Koch et al., 2021) and biases (Gallegos et al., 2024). Progress in quantifying data quality (Swayamdipta et al., 2020) and mitigating biases (Bender & Friedman, 2018, Srivastava 2020) will require rigorously defined methods and robust measures. Public opinion research (POR), the science of collecting and analyzing high-quality data from and about humans, can help move this research forward. Closer collaboration between the two fields has immense potential for mutual benefit, combining the rigorous data collection methods from public opinion research with the speed and insights of NLP tools.\n\n* How POR can improve NLP: As LLMs and generative AI broadly are integrated into human-facing applications, it is increasingly critical that these models are grounded in high-quality, representative data. Public opinion researchers specialize in designing instruments to capture accurate, representative data from human subjects. NLP researchers can draw on this expertise to improve the quality of data used to train, fine-tune, and evaluate LLMs (Durmus et al., 2024), which can increase model performance (Kern et al., 2023). POR's focus on data accuracy, the cognitive response process, and bias mitigation can help improve the performance and fairness of LLMs.\n* How NLP can improve POR: NLP also offers tools and techniques that can make collection of public opinion data more efficient and more accurate. For example, LLMs can extract information from free text responses  (He et al., 2024), serve as interviewers (Xiao et al., 2020, Kim et al., 2019), write survey questions (Jansen et al., 2023), and impute missing data (Callegaro and Yang, 2018). LLMs can also answer survey questions, serving as synthetic responses to test questionnaires (Jordan et al., 2022, Argyle et al., 2023). The careful and responsible integration of NLP into POR workflows can enhance data quality, efficiency, and analysis.\n\nNLPOR is timely and critical as LLMs increasingly influence public discourse, decision-making, and social science research. By fostering collaboration, this workshop will help both fields tackle key challenges in data ethics, bias mitigation, and methodological transparency in the era of LLMs.", "Speakers": "Lora Aroyo (Google Research); David M. Rothschild (Microsoft Research)"}, {"Title": "ORIGen 2025", "Abstract": "Workshop on Optimal Reliance and Accountability in Interactions with Generative LMs", "Speakers": "Malihe Alikhani (Northeastern University); Andreas Vlachos (University of Cambridge); Bertram Malle (Brown University); Q. Vera Liao (University of Michigan/MSR FATE); Matthias Scheutz (Tufts University); Jesse Thomason (University of Southern California); Diyi Yang (Stanford University); Matthew Marge (DARPA)"}, {"Title": "First Workshop on the Interplay of Model Behavior and Model Internals", "Abstract": "Language models have grown increasingly powerful at performing complex tasks, motivating the study of their behavior and internals. However, distinct research communities often pursue these two objectives in isolation. As a result, we lack robust and standardized interpretability methods to assess LM behavior in complex, real-world scenarios comprehensively. This workshop promotes research and discussion on the interplay between behavior and model internals to address this gap. We aim to explore how understanding internal mechanisms can enhance our knowledge of complex model behaviors, and vice versa.", "Speakers": "Mor Geva (N/A); John Hewitt (N/A); Anna Ivanova (N/A); Aaron Mueller (N/A)"}, {"Title": "LM4Sci Workshop", "Abstract": "The workshop on Large Language Modeling for Scientific Discovery - exploring the intersection of artificial intelligence and scientific discovery.\n\nWelcome the Workshop on LLM for Scientific Discovery: Reasoning, Assistance, and Collaboration, (LM4Sci), a forum for researchers, practitioners, and stakeholders working at the intersection of artificial intelligence and scientific discovery.\n\nSignificant advancements in Large Language Models (LLMs) have spurred interest in using these frontier AI models to assist researchers in various scientific tasks, such as:\n\nIdea Generation & Brainstorming\n\nAccelerating the research ideation process through AI-assisted brainstorming and concept exploration.\n\nLiterature Review & Synthesis\n\nSearching, synthesizing literature reviews and enabling literature-based question-answering.\n\nData Analysis & Discovery\n\nUsing AI for data-driven discovery and complex scientific data analysis.\n\nResearch Pipeline Automation\n\nEnd-to-end research pipeline including experiment execution, ML engineering, and paper generation.", "Speakers": "N/A"}, {"Title": "MELT Workshop @ COLM 2025", "Abstract": "Recent advances in large language models (LLMs) have led to a paradigm shift in the economy, society, and daily life, demonstrating remarkable capabilities for complex tasks, such as reasoning and multimodality. However, **these innovations remain largely centered around English and are unevenly distributed across languages, cultures, and values**.\n\nThat is, linguistic and cultural inclusion is lacking in language models, mainly due to an imbalance of language sources used for training. While there are more than 7,000 languages in the world, most NLP research sheds light on a handful of high-resource languages (Blasi et al., 2022; Qin et al., 2024; Zhu et al., 2024). For instance, only 7% and 17% of non-English tokens are incorporated into the pre-training data of GPT-3 (Brown et al., 2020) and BLOOM (Scao et al., 2022), respectively. Further, as we often rely on translation from English-centric datasets (Jin et al., 2024) that predominantly reflect Western cultures (Ahuja et al., 2023) to build multilingual benchmarks, these models naturally uphold social values close to North American and European cultures and neglect a wide array of languages and cultures (Held et al., 2023; AlKhamissi et al., 2024; Tao et al., 2024; Xu et al., 2024; Masoud et al., 2025).\n\nAs LLMs are increasingly deployed in diverse, region- and culture-sensitive domains worldwide, the limitations of this linguistic and cultural imbalance become more pronounced, and their implications more consequential (Yang et al., 2025; Qadri et al., 2025a; Qadri et al., 2025b). These models often exhibit significant biases, reduced performance, and a lack of cultural sensitivity when applied to underrepresented languages and diverse sociocultural settings (Foroutan et al., 2023; Liu et al., 2024; Kwok et al., 2024; Romanou et al., 2025).\n\nAddressing these limitations is essential to ensure that LLMs provide equal access to information, resources, and services to all populations equitably, regardless of language or cultural background. Our workshop aims to **meet the ultimate demand to enhance diversity and inclusion in language models**. We will disseminate the state-of-the-art research on building multilingual and multicultural LLMs, fostering international cross-disciplinary collaborations, and exploring the fundamental theoretical and methodological challenges in this field.", "Speakers": "Shamsuddeen Hassan Muhammad (Imperial College London & Google DeepMind); Julia Kreutzer (Cohere Labs); Pedro Ortiz Suarez (Common Crawl Foundation); Monojit Choudhury (Mohamed bin Zayed University of Artificial Intelligence); Shixiang Shane Gu (Google DeepMind)"}, {"Title": "First Workshop on NLP 4 Democracy @ COLM 2025", "Abstract": "This workshop brings together NLP researchers, social scientists, and democracy practitioners to explore how language technologies can support and challenge democratic values. We focus on a three guiding questions:\n\n1. How can we apply NLP techniques to study democracy?\n2. How can we build LLM-based systems to empower citizens and improve democratic systems?\n3. What threats does AI (particularly LLMs) pose to democracy, and how can we mitigate such threats?\n\nAs language technologies become more powerful and pervasive, they are increasingly shaping how democracies function\u2014and how they falter.\n\nIn recent years, NLP and computational social science have helped us understand how political language influences public opinion and policy. Researchers have analyzed how politicians and the media use rhetoric to shape narratives, and how citizens express attitudes toward policies, parties, and social movements on social media. NLP has also been used to detect threats to democracy, such as extremism, disinformation, propaganda, censorship, and suppression.\n\nWith the rise of large language models (LLMs), we are entering a new phase. LLMs promise to reshape how we study political text and how we design systems to support democratic engagement. A particularly exciting frontier is deliberation\u2014the process by which people engage in thoughtful dialogue to solve problems, share perspectives, and make collective decisions. LLMs have shown early promise in improving conversations between people with opposing political views and even in reducing belief in conspiracy theories. These developments open up possibilities for building tools that support more inclusive, informed, and respectful public discourse.\n\nSeveral of our invited talks will focus on the role LLMs might play in enhancing civic dialogue, facilitating deliberation in diverse communities, and making democratic decision-making more accessible.\n\nAt the same time, these technologies pose serious risks. LLMs can be used to produce highly persuasive disinformation, manipulate public narratives, and erode trust in democratic institutions. Governments and tech companies alike are experimenting with AI in ways that raise concerns about surveillance, censorship, and political bias. Already, real-world actors have deployed AI to monitor political opponents, spread fabricated content, and influence elections.", "Speakers": "Thomas Costello (Carnegie Mellon University); Lisa P. Argyle (Brigham Young University); Jillian fisher (University of Washington); Alexander Spangher (University of Southern California)"}, {"Title": "Pragmatic Reasoning in Language Models", "Abstract": "We produce language based on our understanding of how context contributes to meaning and deliberate on the choice of utterances and interpretations that helps us collaborate and engage in social interactions. While recent large language models (LLMs) have shown impressive performance on a variety of language-related tasks, could these models be considered as true pragmatic language users?\n\nThe 1st Workshop on Pragmatic Reasoning in Language Models (PragLM) aims to stimulate research on LLMs as pragmatically competent language users. We invite contributions that will forward the discussion of understanding and improvement of LLMs' capability to generate natural language flexibly and efficiently across contexts, with relations to research on the cognitive and linguistic processes supporting effective, context-sensitive communication. Our interdisciplinary theme brings together researchers in NLP, computational pragmatics, cognitive science, and other fields.", "Speakers": "Vera Demberg (Saarland University); Michael Franke (University of T\u00fcbingen); Daniel Fried (Carnegie Mellon University); Jennifer Hu (Harvard University / Johns Hopkins University)"}, {"Title": "RAM 2: Reasoning, Attention & Memory \u2013 10 Years On", "Abstract": "Ten years ago\u2026 in Montreal 2015, the RAM workshop took place to bring together the burgeoning field covering the \u201cinterplay of reasoning, attention and memory\u201d, just before Transformers were invented \u2013 but when many of the components to get there had just been published and were in place. The workshop included many speakers who are still prominent in pushing these directions today: Yoshua Bengio, Kyunghyun Cho, J\u00fcrgen Schmidhuber, Sainbayar Sukhbaatar, Ilya Sutskever, and more. See the historical website for more details.\n\nTen years later\u2026 we are hosting RAM 2 in the same location in Montreal, with a two-fold purpose. Firstly, as a retrospective and analysis of what has happened in the last 10 years. We are inviting presenters from the first workshop to this end, as well as to add their current perspectives. Hence secondly, and more importantly, we will bring together the field to discuss new trends and future directions for the next 10 years \u2013 which is further enabled by inviting new speakers, panelists and poster presenters discussing these fresh ideas.\n\nWhy does this make sense? The RAM topic is as important as ever, and has gone on to dominate the field.\n\nOverall, we highlight that the workshop is most concerned with methods that aim to explore the interplay between these three aspects.", "Speakers": "Yoshua Bengio (Univ. of Montreal); Kyunghyun Cho (NYU & Prescient Design); Yejin Choi (Stanford & NVIDIA); Azalia Mirhoseini (Stanford); Juergen Schmidhuber (KAUST); Sainbayar Sukhbaatar (Meta); Jason Wei (OpenAI)"}, {"Title": "The First Workshop on Test-time Scaling and Reasoning Models (ScalR @ COLM 2025)", "Abstract": "The workshop aims to spark discussion around test-time scaling, specifically--but not limited to-- reasoning. By bringing together researchers working on reasoning, planning, alignment, and agentic systems, SCALR aims to build a community around this rapidly evolving area.", "Speakers": "Aviral Kumar (Carnegie Mellon University); Xuezhi Wang (Google DeepMind); Nathan Lambert (Allen Institute for AI); Lewis Tunstall (HuggingFace); Azalia Mirhoseini (Stanford University); Eric Wallace (OpenAI)"}, {"Title": "Social Simulation with LLMs  @ COLM 2025", "Abstract": "In an era where digital interactions shape our social fabric, leveraging LLMs for social simulation offers unique lens to understand and influence complex societal dynamics. Numerous recent works have demonstrated the potential of LLMs in modeling complex decision-making processes [1, 5], emulating human-like interactions [3, 5],  and tracking social evolution [4, 7].  For instance, [1, 5, 8]  showcase scalable frameworks for simulating intricate electoral and societal dynamics. Complementary research, such as [4, 8], further illustrates how varying worldviews and cultural trajectories can be captured using state-of-the-art LLMs.  These advancements in social simulation\u2014whether modeling entire societies or social media ecosystems\u2014provide unprecedented opportunities to understand human collective behavior.  However, these capabilities also raise important ethical, social, and policy considerations such as social identity biases [2] and the risk of misrepresenting or flattening identity dynamics through LLM-driven simulations [6].\n\nOur workshop invites researchers from  researchers, practitioners, and thought leaders to explore innovative social simulation techniques, serving as a meeting point for diverse communities spanning agent-based modeling, social psychology, and ethical technology design. Participants will share insights on emergent social behaviors, LLM interaction patterns, and ethical implications while collaboratively addressing the field's key challenges.", "Speakers": "Alexander Sasha Vezhnevets (Google DeepMind); Alice Oh (Korea Advanced Institute of Science & Technology (KAIST)); Joon Sung Park (Stanford University); Maarten Sap (Carnegie Mellon University (CMU)); Saadia Gabriel (University of California, Los Angeles (UCLA)); Zhijing Jin (University of Toronto (UofT))"}, {"Title": "Third Workshop on Socially Responsible Language Modelling Research (SoLaR) 2025", "Abstract": "The Socially Responsible Language Modelling Research (SoLaR) workshop at\nCOLM 2025 is an interdisciplinary gathering that aims to foster responsible\nand ethical research in the field of language modeling. Recognizing the\nsignificant risks and harms [33-37] associated with the development,\ndeployment, and use of language models, the workshop emphasizes the need for\nresearchers to focus on addressing these risks starting from the early\nstages of development. The workshop brings together experts and\npractitioners from various domains and academic fields with a shared\ncommitment to promoting fairness, equity, accountability, transparency, and\nsafety in language modeling research.", "Speakers": "Lama Ahmad (Trustworthy AI Team at OpenAI); David Duvenaud (Associate Professor at University of Toronto); Mandy Wang (Professor at Princeton University)"}, {"Title": "The First Workshop on the Application of LLM Explainability to Reasoning and Planning", "Abstract": "Enabling large language models (LLMs) to **reason** (e.g., arithmetic reasoning, symbolic reasoning, commonsense reasoning, etc.) and **plan** (e.g., path-finding, tool use, web navigation, computer use, etc.) has been a popular topic in the past few years. Despite the exciting achievement, there have also been growing concerns about the safety and trustworthiness of these LLM applications, due to our large \u201cunknowns\u201d on how LLMs achieve these capabilities and where they could fail. On the other hand, **LLM explainability** (broadly including any research explaining or interpreting LLMs) has also attracted increasing attention, but existing research has mostly focused on simplified tasks and hardly yields insights that can be directly applied to realistic reasoning and planning tasks. This discrepancy has consequently raised doubts about the practical meaning of LLM explainability research.\n\nIn this workshop, we aim to bring together researchers from various perspectives to discuss the potential and practical applications of model explainability to advancing LLM reasoning and planning. Specifically, the workshop welcomes submissions on the following topics (non-exclusively):", "Speakers": "Greg Durrett (New York University); Yonatan Belinkov (Technion); Ana Marasovi\u0107 (University of Utah); Jennifer Wortman Vaughan (Microsoft Research); Zining Zhu (Stevens Institute of Technology); Mark Riedl (Georgia Institute of Technology); Huan Sun (The Ohio State University)"}, {"Title": "Visions of Language Modeling Workshop", "Abstract": "Language modeling (LM) has progressed at a breakneck pace, with industry often leading the way through greater access to resources. Academia, however, can remain at the forefront by focusing on forward-looking research that may shape the future of our field. This workshop will bring together researchers from diverse backgrounds to explore critical questions about what language modeling could look like in a decade: which techniques will define state-of-the-art systems, and what challenges that are out of scope today will become relevant? By casting our gaze beyond near-term goals, we aim to spark discussions that help researchers identify new directions and shape the research landscape of tomorrow.", "Speakers": "Ari Holtzman (N/A); Nicholas Tomlin (N/A); Melanie Sclar (N/A); Jennifer Hu (N/A); Robin Netzorg (N/A); Chenglei Si (N/A); William Merrill (N/A); Saadia Gabriel (N/A); Malihe Alikhani (N/A); Lionel Wong (N/A); Tanya Goyal (N/A); A. Feder Cooper (N/A); Celine Lee (N/A); Simran Arora (N/A); Daphne Ippolito (N/A); Kiant\u00e9 Brantley (N/A)"}, {"Title": "XTempLLMs 2025", "Abstract": "Large language models (LLMs) have been used for a variety of time-sensitive applications such as temporal reasoning (Fatemi et al., 2024), forecasting (Jin et al., 2023) and planning (Meng et al., 2024). In addition, there has been a growing number of interdisciplinary works that use LLMs for cross-temporal research in several domains, including social science (Zhou et al., 2024), psychology (Bodro\u017ea et al., 2023), cognitive science (Huet et al., 2025), environmental science (Tian et al., 2024) and clinical studies (He et al., 2024). However, LLMs are hindered in their understanding of time due to many different reasons, including temporal biases and knowledge conflicts in pretraining and RAG data but also a fundamental limitation in LLM tokenization that fragments a date into several meaningless subtokens. Such inadequate understanding of time would lead to inaccurate reasoning, forecasting and planning, and time-sensitive findings that are potentially misleading.\n\nOur workshop looks for (i) **cross-temporal work in the NLP community**  and (ii)  **interdisciplinary work that relies on LLMs for cross-temporal studies**. See call for papers for more details. Reference papers are available here.", "Speakers": "Jose Camacho Collados (Cardiff University); Ali Emami (Brock University); Alexis Huet (Huawei Technologies); Bahare Fatemi (Google Research); Vivek Gupta (Arizona State University)"}]}