date,time,type,title,url,speaker,end_time,abstract,overview,matched_team,recommendation_reason,focus_area
"Sunday, Nov 30, 2025",11:00,Workshop,NeurIPS 2025 Workshop on Socially Responsible and Trustworthy Foundation Models,https://neurips.cc/virtual/2025/workshop/127834,,18:00,"The Socially Responsible and Trustworthy Foundation Models (ResponsibleFM) Workshop at NeurIPS 2025 Mexico City is envisioned as a vital interdisciplinary forum dedicated to advancing ethical, inclusive, and socially conscious research practices in the rapidly evolving field of foundation models, including language models and multimodal models. As foundation models are tremendously reshaping human communication, decision-making, and societal infrastructures, there is a growing recognition of the profound impacts these systems can have, both positive and negative, on individuals and communities. In particular, previous research has documented a wide range of risks and harms associated with foundation models, including but not limited to bias and discrimination, misinformation propagation, privacy violations, environmental concerns, and unintended social consequences.","Overview: The ResponsibleFM Workshop is an interdisciplinary forum focused on advancing ethical, inclusive, and socially responsible research in foundation models, including language and multimodal models. It aims to address fairness, accountability, transparency, and safety throughout model development and deployment, proactively tackling ethical and social risks. The workshop brings together researchers, practitioners, ethicists, policy-makers, and affected communities to catalyze methods and best practices ensuring foundation model research serves the common good. The event is scheduled to be held in hybrid mode, both virtually and at the Hilton Mexico City Reforma, during NeurIPS 2025. | Research Interests: Defining & Measuring Trustworthiness, Techniques to Enhance Trustworthiness, Deployment & Social Good, Datasets & Benchmarks, Interdisciplinary Perspectives & Governance",可信; 多伦多云,可信: 该BU关注在多模态环境中推断用户意图及动态适应，与Workshop中跨学科提升模型透明度、公平性和安全性的主题高度契合，有助推动可信机制和风; 多伦多云: Session探讨基础模型的可信性与社会责任，正好衔接多伦多云对可信AI和安全执行的需求，有助强化模型评估与多源反馈机制。,可信: 多模态意图识别; 不确定性量化; 记忆与上下文管理; 自我学习与防护; 风险监测与干预; 多伦多云: 模型可信度提升; 伦理与社会责任; 多模态基础模型; 模型评测指标; 多方反馈优化
"Sunday, Nov 30, 2025",11:00,Workshop,NeurIPS 2025 Workshop on Embodied and Safe-Assured Robotic Systems,https://neurips.cc/virtual/2025/workshop/127833,,18:00,"This workshop focuses on advancing safe and quality-assured embodied robotic systems. Embodied systems—including autonomous robots, self-driving vehicles, robotic arms, and humanoid robots—are increasingly deployed in safety-critical real-world scenarios. Ensuring their trustworthiness—encompassing safety, reliability, and predictable behavior—remains a pressing challenge. Despite notable progress in perception, reasoning, and control, many AI-based robotic systems still operate as “black boxes,” often exhibiting unpredictable behaviors. Failures can emerge from complex sensorimotor interactions, adversarial inputs, or novel environments, leading to safety incidents and diminished user trust.",,温哥华云; 可信; 多伦多云,温哥华云: 我们关注强化学习在实际约束下的数据效率和安全适应性，而该Workshop聚焦机器人系统的安全性与可预测行为，正好补充了解决复杂环境中的安全和; 可信: 该Session聚焦于机器人系统的安全和行为可预测性，正好契合BU对不确定性量化和风险干预的需求，有助推进自学习策略中安全性和异常检测技术发; 多伦多云: 本Session强调机器人系统的安全和可预测性，与多伦多云BU中对AI代理安全性验证、行为可信度及在线改进机制的需求高度契合，有助优化安全关,温哥华云: 强化学习数据效率; 安全感知控制; 多模态信用分配; 仿真与在线反馈融合; 安全在线自适应; 可信: 不确定性量化; 异常检测机制; 多模态意图推断; 在线自学习安全; 风险感知干预; 多伦多云: 安全保障机制; 行为可信验证; 在线学习与优化; 多步规划验证; 回滚与事务控制
"Sunday, Nov 30, 2025",11:00,Workshop,NeurIPS2025 Workshop Research Development AI Mexico,https://neurips.cc/virtual/2025/workshop/127832,,18:00,"The Research Development of AI in Mexico: Main Applications workshop seeks to showcase, strengthen, and connect the most impactful developments in Artificial Intelligence (AI) and Data Science emerging from Mexico and the broader Latin American region. Over the past four decades, Mexico has cultivated a robust research community in AI through pioneering contributions in areas such as computational intelligence, autonomous robotics, fuzzy systems, and natural language processing, led by institutions including CIC–IPN, INAOE, UNAM, ITESM, CINVESTAV, and Universidad Veracruzana.Today, the region is undergoing a strategic transformation, shifting from foundational research to the development of applied AI technologies addressing real-world needs in healthcare, education, agriculture, smart cities, cybersecurity, and sustainability. This evolution has been further propelled by increased access to open data, advances in computing infrastructure, and growing collaborations between academia, government, and industry.Despite these advances, Latin America faces distinctive challenges in the development and deployment of AI. These include limited funding, underrepresentation in global AI initiatives, digital inequality, and the need for responsible, inclusive, and culturally relevant AI systems. Additionally, emerging concerns related to AI ethics, algorithmic bias, and regulatory frameworks must be addressed proactively to ensure equitable and trustworthy technology adoption.This workshop aims to create a forum for researchers, students, practitioners, and policymakers to engage in meaningful dialogue about the current landscape and future directions of AI in Mexico and Latin America. By promoting interdisciplinary collaboration, the workshop will highlight impactful case studies, emerging research trajectories, and opportunities for cross-border cooperation, while fostering a shared vision for AI that is ethical, sustainable, and aligned with regional priorities.",,,,
"Sunday, Nov 30, 2025",11:00,Workshop,Vision Language Models: Challenges of Real World Deployment,https://neurips.cc/virtual/2025/workshop/127831,,18:00,"Vision language models (VLMs) have demonstrated remarkable capabilities in integrating visual perception with natural language understanding, powering applications such as multimodal assistants, robotics, autonomous systems, and accessibility tools. However, their real-world deployment faces significant challenges in efficiency, scalability, and reliability. This workshop will bring together researchers and practitioners from academia and industry to highlight cutting-edge research, systems-level optimizations, and evaluation methodologies that are often overlooked yet pivotal for robust real-world integration. Efficiency, robustness, and reliability will be emphasized as core design principles, essential to advancing VLMs from experimental systems to dependable deployed technologies. By convening researchers at the intersection of multimodal learning, efficient inference and training, robustness and uncertainty estimation, and large-scale systems design, the workshop aims to establish concrete pathways toward building VLMs that can operate reliably under practical constraints. We hope this workshop will serve as a venue for exchanging insights on model design, efficiency techniques, and robustness evaluation that bridge the gap between research and real-world systems.","Overview: The VLM4RWD NeurIPS 2025 Workshop focuses on the challenges of deploying Vision Language Models (VLMs) in real-world applications. It aims to bring together researchers and practitioners to discuss cutting-edge research, systems-level optimizations, and evaluation methodologies essential for the efficient, scalable, and reliable deployment of VLMs. The workshop emphasizes the importance of efficiency, robustness, and reliability in advancing VLMs from experimental systems to dependable technologies. | Research Interests: Data pipelines for efficient multimodal learning, Accelerating VLMs inference, Compression and distillation for VLMs deployment, Sparse, modular, and retrieval-augmented VLM architectures, Efficient training for complex reasoning tasks, Robust training and evaluation of VLMs, Benchmarks for deployment-oriented evaluation, Mitigating hallucination and improving multimodal grounding, Agentic VLMs for real-world integration",温哥华云; 海思; 诺亚,温哥华云: 云BU面临多模态强化学习中数据效率和稳定优化挑战，该Session聚焦真实场景下VLM高效推理与鲁棒性，正好提供解决多模态在线学习和系统容错; 海思: 海思BU关注多模态模型的异构编码同步和内存带宽优化，这与会中强调的系统级效率和低延时响应策略高度契合，有助于提升实际部署中VLM的性能和稳定; 诺亚: 诺亚BU关注高效管理超长多模态序列与推理，和会议强调的VLM高效推理与可靠性高度契合，可借鉴会议优化多模态输入与自主计算调度方法。,温哥华云: 多模态强化学习; 高效推理技术; 鲁棒训练策略; 系统级优化; 部署评测指标; 海思: 多模态编码同步; 内存与带宽优化; 低延时推理; 系统级调度策略; 模型压缩与加速; 诺亚: 多模态长序列推理; 高效注意力机制; 动态计算分配; 多模态输入分层处理; VLM鲁棒性评估
"Monday, Dec 1, 2025",06:00,Workshop,Centering Low-Resource Languages and Cultures in the Age of Large Language Models,https://neurips.cc/virtual/2025/workshop/127830,,15:00,"Large Language Models (LLMs) have transformed NLP research and applications, yet they are still predominantly trained on high-resource, globally dominant languages. This imbalance leads to poor performance and limited applicability for low-resource languages, which are rich in tone, morphology, and cultural meaning. As a result, current AI systems risk reinforcing linguistic inequality, cultural erasure, and lack of accessibility in critical domains like education and healthcare.This workshop aims to reframe language technology by centering low-resource languages, cultures, and epistemologies in the age of LLMs. We seek to bring together researchers, linguists, developers, healthcare professionals, and technologists to share insights and develop strategies for building inclusive, culturally grounded, and linguistically robust language models. The workshop emphasizes collaboration across disciplines and regions to ensure both technical advancement and social relevance.Key areas of focus include developing LLM architectures tailored to low-resource linguistic features, ethical and community-centered dataset collection, and multilingual benchmarks designed specifically for underrepresented languages. We also highlight the importance of healthcare and medical machine translation to support equitable access to information and improve public health outcomes. Ultimately, this workshop aims to advance responsible AI innovation that empowers low-resource language communities and shapes a more inclusive future for global language technologies.","Overview: The CLRLC-LLMs Workshop at NeurIPS 2025 focuses on the role of Large Language Models (LLMs) in promoting multilingual and culturally inclusive AI. It addresses the underrepresentation of low-resource languages and cultures in AI research and development. The workshop aims to reframe language technology discussions by centering low-resource languages and cultures in AI innovation. It brings together researchers, linguists, developers, and technologists to share insights, challenges, and strategies for developing LLMs that support linguistic diversity and cultural identity, particularly in Africa and parts of Asia. The workshop promotes global collaboration, scalable data creation, model alignment, and ethical evaluation of LLMs, inspiring interdisciplinary research across linguistics, computer science, and engineering. | Research Interests: LLMs for low-resource languages, Culturally grounded NLP and multilingual evaluation, Dataset creation, curation, and benchmarks for African, Indigenous, and marginalized languages, Cross-linguistic transfer and model adaptation, Ethical, social, and policy dimensions of language technologies, Community-based and participatory AI approaches, Speech, ASR, and translation in low-resource settings, Human–AI interaction for linguistic and cultural inclusivity, Cognitive or sociolinguistic perspectives on AI and language, Healthcare and Biomedical Machine Translation for Low-Resource Languages",,,
"Monday, Dec 1, 2025",06:00,Workshop,NORA: The First Workshop on Knowledge Graphs & Agentic Systems Interplay,https://neurips.cc/virtual/2025/workshop/127827,,15:00,"Agents have experienced significant growth in recent years, largely due to the rapid technological advancements of Large Language Models (LLMs). Although these agents benefit from LLMs' advanced generation proficiency, they still suffer from catastrophic forgetting and a limited context window size compared to the agents' needs in terms of contextual information.  Knowledge Graphs (KGs) are a powerful paradigm for structuring and managing connected pieces of information while unlocking deeper insights than traditional methods. Their value is immense for tasks that require context, integration, and reasoning. However, this power comes at the cost of significant upfront and ongoing investment in construction, curation, and specialized expertise.   The first version of this workshop aims at analyzing and discussing emerging and novel practices, ongoing research and validated or deployed innovative solutions that showcase the growing synergy between LLMs agents and KGs.","Overview: The NORA 2025 workshop focuses on the interplay between Knowledge Graphs (KGs) and agentic systems, particularly those enhanced by Large Language Models (LLMs). The workshop aims to explore emerging practices, research efforts, and innovative solutions that demonstrate the synergy between LLM agents and KGs. It addresses the challenges faced by agents, such as catastrophic forgetting and limited context window size, and highlights the potential of KGs in structuring and managing information for improved reasoning and integration. | Research Interests: Agentic and Knowledgeable Systems with Small Language Models, Agentic Information Extraction and Retrieval, Agentic KG Construction & Enrichment, Agents for Complex Reasoning over KGs, Agents and KGs for private and proactive personal assistants & Personalisation, Augmenting Agents with External Knowledge, Collaborative Agents for Knowledge Computing and Serving, Context Engineering enhanced by KGs, Efficient Reinforcement Learning for better performance, Graph Retrieval Augmented Generation in Agentic systems, KGs serving agents’ memories: episodic, semantic, and procedural, Multi-Lingual & Multi-modal integrations, On-Device or Hybrid (Device-Cloud) systems combining Agents and KGs, Personalisation via Agents and KGs, Personas and digital twins enabled by Agents and KGs, Theoretical and experimental analysis of close and open Domain applications scenarios",可信; 中软; 多伦多云,可信: 该Session强调利用知识图增强语言模型代理的记忆和推理能力，与可信BU中多模态意图识别和长期状态管理技术密切相关，有助提升代理的在线自学; 中软: 这个Workshop探讨了知识图谱辅助解决多轮对话中的长期记忆和信息整合问题，正好契合中软对统一长上下文存储和状态一致性的需求，同时关注异步; 多伦多云: 多伦多云关注AI Agent的可靠性和持续学习，NORA工作坊探讨知识图谱对Agent记忆与推理能力的增强，能为构建稳健、具备长序列计划和上,可信: 知识图增强记忆管理; 多模态意图识别; 长期状态保持机制; 在线自学习安全; 风险感知与干预; 中软: 统一长上下文记忆模型; 知识图谱构建与增强; 智能体异步调度机制; 多模态信息融合; 安全隐私保护; 多伦多云: 知识图谱与Agent记忆; 上下文窗口扩展技术; Agent的持续学习机制; Agent行动可信保障; 多步计划执行分析
"Monday, Dec 1, 2025",06:00,Workshop,7th International Workshop on Large Scale Holistic Video Understanding: Toward Video Foundation Models,https://neurips.cc/virtual/2025/workshop/127828,,15:00,"This workshop aims to advance the field of video understanding by fostering discussions around holistic and generalist video foundation models. Building upon the Holistic Video Understanding (HVU) initiative and dataset introduced in 2019, we have successfully organized eight HVU workshops and tutorials at top-tier venues such as CVPR and ICCV, uniting researchers, practitioners, and students from around the world. These efforts have played a central role in moving the community beyond narrow action recognition tasks toward multi-faceted, semantic, and generalist video understanding.With the emergence of large-scale foundation models and video large language models (Video-LLMs), the landscape of video understanding is rapidly evolving. These models enable unified reasoning across spatial, temporal, and multimodal dimensions, yet introduce new challenges in scalability, efficiency, interpretability, and responsible deployment.The HVU Workshop 2025 will provide a platform to explore these frontiers, discussing topics such as multimodal representation learning, long-context reasoning, evaluation of general-purpose video systems, efficient adaptation and scaling laws, and the ethical and societal implications of video AI. Our goal is to bring together a diverse and inclusive community to define the next chapter of holistic, generalist, and responsible video understanding.",,诺亚; CBG; 海思,诺亚: 这个Workshop聚焦多模态视频中高效的长序列推理和多维度语义理解，正好契合BU对100k+ tokens的稳定注意力机制和层次化帧页编码; CBG: 该会议聚焦大规模视频基础模型，正好覆盖了CBG对复杂视频动态解析与3D一致性的需求，能带来多模态表示和长时序关系的最新技术思路。; 海思: 海思BU关注高效处理长上下文和多模态信息，这正好契合工作坊讨论的视频多模态表征和长距离推理，能借鉴视频大模型的规模化与效率策略提升终端性能。,诺亚: 长序列推理; 多模态表示学习; 层次化帧编码; 计算资源调度; 多模态跨模态注意力; CBG: 多模态视频表示学习; 长时序推理; 高效模型适应与扩展; 视频大语言模型; 视频AI伦理与责任; 海思: 多模态表示学习; 长距离上下文推理; 模型效率与扩展; 统一潜空间设计; 实时低延迟处理
"Monday, Dec 1, 2025",06:00,Workshop,First Workshop on LLM Persona Modeling,https://neurips.cc/virtual/2025/workshop/127829,,15:00,"Large language models (LLMs) are increasingly used to simulate human-like personas for applications in research, education, healthcare, and interactive AI systems. While such persona modeling creates opportunities for interdisciplinary innovation, it raises challenges around authenticity, consistency, bias, and ethical deployment. This workshop brings together perspectives from AI, psychology, cognitive science, and human–computer interaction to advance robust methods, standardized evaluation frameworks, and responsible practices for persona modeling in LLMs. Through invited talks, panels, posters, and discussions, the event will chart a roadmap for interdisciplinary collaboration and future research in this emerging area.","Overview: The PersonaLLM workshop, part of NeurIPS 2025, focuses on the modeling of personas using large language models (LLMs). It aims to explore the creation of human-like personas with consistent traits and behaviors, leveraging LLMs' adaptability across diverse contexts. The workshop seeks to provide an interdisciplinary forum for discussing the conceptualization, evaluation, and ethical implications of LLM persona modeling, with applications in marketing, social science, product development, and healthcare. | Research Interests: LLM persona modeling, Instruction-following capabilities of LLMs, Personification and human-like personas, Interdisciplinary approaches in AI, psychology, and cognitive science, Evaluation methods for persona consistency and effectiveness, Social and ethical implications of anthropomorphism in LLMs, Technical innovations in LLMs, Real-world applications of LLM personas",,,
"Tuesday, Dec 2, 2025",09:30,Tutorial,"Efficient Transformers: State of the art in pruning, sparse attention, and transformer funneling",https://neurips.cc/virtual/2025/128790,,12:00,"""Transformer architectures consume the lionshare of computational budgets associated with today’s most powerful language and vision models, making research into greater computational efficiency a hot and essential direction. Our proposed tutorial surveys the bleeding edge of three complementary research threads that together comprise a significant part of the current industrial toolkit for achieving computational efficiency in Transformers: (1) pruning, the structured or unstructured removal of weights, layers and heads; (2) sparse attention & routing, including block, sliding-window, locality-sensitive hashing; and (3) funneling, which pools intermediate representations to shorten sequences through depth. We will then feature an expert industrial and academic panel of speakers from Caltech, MIT, Anthropic, Google Deepmind, and Microsoft, hearing about the latest trends seen in top industrial labs. Attendees will leave with actionable recipes for building sub-10 B-parameter models that match or exceed dense baselines on language, vision and multi-modal benchmarks.The tutorial targets researchers and practitioners who build or deploy Transformer models and assumes familiarity with basic deep-learning concepts but not with any specific efficiency method. All slides and publication materials will be released under a permissive license.""",,海思; 诺亚; 计算,海思: 海思关注长上下文的稀疏注意和计算加速，该讲座中的稀疏注意和融合中间表示技术直接契合，实现更高效模型部署。多路径解码和量化技巧也有助提升性能同; 诺亚: Noah BU关注内存管理与长序列推理，该教程覆盖稀疏注意力与序列压缩技术，有助提升模型对超长序列的高效处理能力。借鉴工业界最新实践，能帮助; 计算: 这场教程深入讲解了Transformer计算效率提升的核心技术，与计算BU面对的模型—硬件协同优化及计算资源约束紧密关联，助力预见未来计算负,海思: 稀疏注意机制; 剪枝技术; 序列融合技术; 多路径解码; 权重量化; 诺亚: 稀疏注意力机制; 序列压缩与漏斗结构; 内存写入与驱逐策略; 多模态长序列编码; 工业前沿效率方法; 计算: Transformer剪; 稀疏注意力机制; 序列压缩技术; 模型-硬件共设计; 计算资源优化
"Tuesday, Dec 2, 2025",09:30,Tutorial,"Geospatial Foundation Models: Overview, Application and Benchmarking",https://neurips.cc/virtual/2025/128793,,12:00,"Geospatial foundation models (GeoFMs) are a class of large-scale deep learning models, typically based on the transformer architecture, that are pre-trained on vast, diverse datasets of Earth Observation data to learn a general, transferable understanding of the Earth’s surface. These models help address long-standing challenges in Earth Observation by dramatically reducing the need for manually labeled data, handling vast and diverse data streams (e.g., optical, SAR, multispectral, LiDAR), and enabling robust performance across time, space, and sensor types. In this tutorial, we will give an overview of the recent advancements in GeoFMs, highlighting the main challenges in developing these models and differences from foundation models developed for other domains. We will also show practical examples of fine-tuning and inferencing GeoFMs for different downstream tasks using the TerraTorch open-source framework, which facilitates the use to publicly available GeoFMs such as SatMAE, Prithvi-EO, DOFA, Galileo and TerraMind. Finally, we will introduce best practices for systematic and reproducible benchmarking of GeoFMs using the TerraTorch Iterate plug-in and its integration with GEO-Bench.",,,,
"Tuesday, Dec 2, 2025",09:30,Tutorial,From Tuning to Guarantees: Statistically Valid Hyperparameter Selection,https://neurips.cc/virtual/2025/128794,,12:00,"""The performance and reliability of modern machine learning systems depend critically on hyperparameter selection. Whether tuning a large language model, configuring a vision pipeline, or deploying AI in safety-critical environments, the choice of hyperparameters is decisive. Current tuning strategies such as grid or random search and Bayesian optimization are powerful for empirical optimization but they do not provide statistical guarantees on the reliability of the selected configuration after deployment. This gap becomes critical when models must satisfy strict performance, safety, or fairness requirements.This tutorial introduces a rigorous and practical framework that treats hyperparameter selection as a statistical testing problem. By constructing valid p- or e-values for candidate configurations and applying multiple hypothesis testing (MHT) procedures, practitioners can control deployment risk with finite-sample guarantees. We begin with the Learn-Then-Test (LTT) methodology for average-risk control and build up to multiple key extensions, such as controlling the quantile risk using quantile LTT (QLTT), multi-objective optimization through Pareto Testing (PT), incorporating prior information through the concept of reliability graphs, and data-efficient selection through adaptive LTT (aLTT). Throughout the tutorial, we emphasize conceptual clarity, plain-language explanations of assumptions, and hands-on demonstrations with minimal, reproducible notebooks.Attendees will gain a drop-in toolkit for augmenting existing tuning workflows with statistically valid selection. They will learn how to formalize relevant risk functions, generate valid evidence, choose appropriate error-rate controls (FWER/FDR), and navigate the trade-offs between statistical conservatism and power under limited data. No prior expertise in multiple hypothesis testing is required.""",,多伦多云,多伦多云: 多伦多云关注AI代理的可靠性和持续学习，而本教程提供统计验证的超参数选择方法，有助于严格控制模型性能和安全风险，实现更可信的部署与优化。,多伦多云: 超参数统计验证; 多重假设检验; 风险控制机制; 模型性能保障; 有限样本理论
"Tuesday, Dec 2, 2025",13:30,Tutorial,How to Build Agents to Generate Kernels for Faster LLMs (and Other Models!),https://neurips.cc/virtual/2025/128792,,16:00,"The compute demanded by modern AI has been exploding since 2016; the FLOPs used to train frontier models have grown at a rate of 2.4x per year [0], and the inference side is growing even faster—already an estimated 80% of total AI electricity use [1]. Large language models and other deep networks rely on highly tuned GPU kernels to achieve state-of-the-art performance; these efficient kernels directly translate to cost and energy savings. In this 2.5-hour in-person tutorial, we demonstrate how LLM-powered agents can generate and optimize GPU kernels for CUDA, HIP/ROCm, and Triton. We begin with a unified primer on GPU‐programming fundamentals and common tooling (memory hierarchy, occupancy, profilers), then introduce an agentic loop: prompt engineering, compiler/profiler feedback as tools, iterative kernel refinement, correctness validation, and automated benchmarking. We will provide additional benchmarking examples on HIP and Triton, on top of Stanford’s KernelBench that covers CUDA [2], KernelBot as reliable source of human curated dataset for heterogenous GPU code [3], and show how to turn runtime and profiler metrics into reward signals that drive kernel optimizations. On top of this loop, we build an inference-scaling framework in which the LLM proposes candidate kernels, compiles them, measures latency/throughput/energy, and feeds those signals back as rewards. By combining test-time scaling techniques the agent iteratively discovers increasingly accurate and efficient kernels. Attendees will compare generated code against expert kernels, inspect wins and losses. By the end, participants will walk away with a reproducible pipeline for LLM-driven GPU‐kernel optimization.",,海思; 计算,海思: 本次教程聚焦用LLM智能生成GPU核函数，实现性能与能效双提升，正好契合海思在加速大模型时需降低计算和内存瓶颈的需求。; 计算: 这场教程技术聚焦GPU核生成与优化，直接提升推理效率和能耗控制，契合计算BU对硬件—软件协同设计和动态负载预测的需求。通过实战演练，掌握循环,海思: GPU核函数自动生成; 性能与能效优化; 内存层次结构管理; 异构加速框架; 自动化性能反馈循环; 计算: GPU编程基础; 自动化内核优化; 性能与能耗评测; 基于LLM的代码生成; 编译器与分析工具
"Tuesday, Dec 2, 2025",13:30,Tutorial,"Positional Encoding: Past, Present, and Future",https://neurips.cc/virtual/2025/128797,,16:00,"""Positional Encoding is a foundational yet often opaque component of Transformerarchitectures, underpinning how self-attention mechanisms capture sequence orderin language, vision, and multimodal models. Despite its centrality to the successof modern LLMs, and other attention-reliant architectures, the mathematical in-tuition behind positional encoding remains challenging and inaccessible to manyresearchers and practitioners. This workshop aims to demystify positional encodingby bridging formal theory with intuitive understanding and practical experimen-tation. Through a series of guided lectures, visual demonstrations, and hands-oncoding sessions, participants will explore the operational principles behind ef-fective positional representations, the evolution of key methods (from sinusoidaland learned embeddings to rotary and relative encodings), and open challengesthat motivate current research directions. We will also provide open-source codeimplementations, mathematical visualizations, and collaborative ideation sessionsfor fostering new positional encoding concepts. By easing the barrier to entry forthis mathematically intensive, yet crucial topic, the workshop seeks to foster deeperunderstanding, interdisciplinary exchange, and novel contributions to the future ofPositional Encoding, and Transformer design""",,诺亚,诺亚: 诺亚业务关注长序列推理与跨模态处理，精准理解和设计位置编码有助于优化序列表示和计算效率，提升模型的长距离依赖捕捉和多模态融合能力。,诺亚: 位置编码原理; 长序列注意力机制; 跨模态序列处理; 可扩展编码方法; 代码实现与实验
"Tuesday, Dec 2, 2025",13:30,Tutorial,Science of Trustworthy Generative Foundation Models,https://neurips.cc/virtual/2025/128795,,16:00,"We are living through a moment that once belonged to science fiction: generative foundation models can write, reason, design, diagnose, and increasingly, decide. They are no longer just predicting the next word — they are shaping knowledge, influencing choices, and becoming collaborators in science, medicine, education, and daily life. But here's the tension: as their capabilities accelerate, our ability to trust them has not kept pace.Trustworthiness can't remain a ""patch after the failure"" or a moral hope layered on top of engineering. It must evolve into a science—a discipline as rigorous as the one that created these models in the first place. In this tutorial, we explore what that science looks like: how we understand model behaviors, measure and stress-test trust, and design systems that earn it. We'll build the foundations together, then step into the frontier—where models begin to exhibit human-like cognitive behaviors that inspire wonder, but also demand responsibility and new forms of alignment.This session is an invitation: to move beyond building models that impress us, toward building models we can trust with what matters.",,多伦多云; 可信; 中软,多伦多云: 该BU强调AI代理的可信执行与持续优化，会议内容聚焦于构建严谨的信任度科学，有助于解决代理行为的信任验证和多步骤决策的责任分配问题。; 可信: 这场教程深入探讨了如何科学地评估和设计可信生成模型，正好契合构建可信智能体中对不确定性量化和行为校准的核心需求。; 中软: 多轮对话与异步编排对模型的信任和一致性提出了高要求，本教程针对模型行为理解与信任测试，助力构建可靠的分布式AI系统。,多伦多云: 模型行为理解; 信任度测量与压力测试; 事务语义保障; 在线持续改进; 长程决策信用分配; 可信: 不确定性量化方法; 多模态意图识别; 模型行为解释; 在线自我学习机制; 风险监控与干预; 中软: 模型行为理解; 信任度量与测试; 异步编排机制; 多轮长上下文管理; 安全隐私防护
"Wednesday, Dec 3, 2025",08:30,Invited Talk,The Oak Architecture: A Vision of SuperIntelligence from Experience,https://neurips.cc/virtual/2025/invited-talk/129132,Rich Sutton,09:30,"As AI has become a huge industry, to an extent it has lost its way. What is needed to get us back on track to true intelligence? We need agents that learn continually. We need world models and planning. We need knowledge that is high-level and learnable. We need to meta-learn how to generalize. The Oak architecture is one answer to all these needs. It is a model-based RL architecture with three special features: 1) all of its components learn continually, 2) each learned weight has a dedicated step-size parameter that is meta-learned using online cross-validation, and 3) abstractions in state and time are continually created in a five-step progression: Feature Construction, posing a SubTask based on the feature, learning an Option to solve the subtask, learning a Model of the option, and Planning using the option’s model (the FC-STOMP progression). The Oak architecture is rather meaty; in this talk we give an outline and point to the many works, prior and contemporaneous, that are contributing to its overall vision of how superintelligence can arise from an agent’s experience.",,可信; 多伦多云; 诺亚,可信: Oak架构重视持续学习与元学习，正好对应可信BU对持续适应与在线策略改进的需求，特别是通过分层抽象和模型规划提升内存与上下文管理能力。; 多伦多云: 这个Oak框架强调持续学习和元学习，正好对应多伦多云关注的在线改进循环和连续学习机制，能增强智能体的稳健性和泛化能力。; 诺亚: Oak架构强调持续学习和高级抽象，与BU在多模态和长序列推理中需要的适应性计算策略高度契合，特别是其元学习和规划机制有助提升长期推理的准确性,可信: 持续学习机制; 元学习步长调整; 分层状态与时间抽象; 模型规划策略; 安全适应与风险控制; 多伦多云: 持续学习能力; 元学习参数调优; 抽象层次建模; 模型规划策略; 在线改进反馈; 诺亚: 持续学习机制; 元学习调参; 状态与时间抽象; 模型规划策略; 多模态长序列推理
"Wednesday, Dec 3, 2025",14:30,Invited Talk,Are We Having the Wrong Nightmares About AI?,https://neurips.cc/virtual/2025/invited-talk/129136,Zeynep Tufekci,15:30,"Though seemingly opposite, doom and optimism regarding generative AI's spectacular rise both center on AGI or even superintelligence as a pivotal moment. But generative AI operates in a distinct manner from human intelligence, and it’s not a less intelligent human on a chip slowly getting smarter anymore than cars were mere horseless carriages. It must be understood on its own terms. And even if Terminator isn’t coming to kill us or superintelligence isn’t racing to save us, generative AI does bring profound challenges, well-beyond usual worries such as employment effects.  Technology facilitates progress by transforming the difficult into easy, the rare into ubiquitous, the scarce into abundant, the manual into automated, and the artisan into mass-produced. While potentially positive long-term, these inversions are extremely destabilizing during the transition, shattering the correlations and assumptions of our social order that relied on superseded difficulties as mechanisms of proof, filtering, sorting and signaling. For example, while few would dispute the value of the printing press or books, their introduction led to such destructive upheaval that the resulting religious wars caused proportionally more deaths than all other major wars and pandemics since combined. Historically, a new technology's revolutionary impact comes from making what's already possible and desired cheap, easy, fast, and large-scale, not from outdated or ill-fitting benchmarks that technologists tend to focus on. As such, Artificial Good-Enough Intelligence can unleash chaos and destruction long before, or if ever, AGI is reached. Existing AI is good enough to blur or pulverize our existing mechanisms of proof of accuracy, effort, veracity, authenticity, sincerity, and even humanity. The tumult from such a transition will require extensive technological, regulatory, and societal effort to counter. But the first step to getting started is having the right nightmares.",,,,
"Thursday, Dec 4, 2025",08:30,Invited Talk,The Art of (Artificial) Reasoning,https://neurips.cc/virtual/2025/invited-talk/129134,Yejin Choi,09:30,"Scaling laws suggest that “more is more” — brute-force scaling of data and compute leads to stronger AI capabilities. However, despite rapid progress on benchmarks, state-of-the-art models still exhibit ""jagged intelligence,"" indicating that current scaling approaches may have limitations in terms of sustainability and robustness. Additionally, while the volume of papers on arXiv continues to grow rapidly, our scientific understanding of artificial intelligence hasn't kept pace with engineering advances, and the current literature presents seemingly contradictory findings that can be difficult to reconcile. In this talk, I will discuss key insights into the strengths and limitations of LLMs, examine when reinforcement learning succeeds or struggles in reasoning tasks, and explore methods for enhancing reasoning capabilities in smaller language models to help them close the gap against their larger counterparts in specific domains.",,诺亚,诺亚: 这个专题深入探讨了在有限计算资源下如何通过改进模型结构和训练策略增强小模型的推理能力，正好契合我们BU对长序列多模态推理的高效架构设计与鲁棒,诺亚: 长序列推理架构; 稀疏与线性注意力; 动态计算分配; 持久内存管理; 跨模态输入策略
"Thursday, Dec 4, 2025",14:30,Invited Talk,"On the Science of “Alien Intelligences”: Evaluating Cognitive Capabilities in Babies, Animals, and AI",https://neurips.cc/virtual/2025/invited-talk/129137,Melanie Mitchell,15:30,"Today’s generative AI systems—termed by some researchers as “alien intelligences”—have exceeded human performance on many benchmarks meant to test humanlike cognitive capabilities.  However, these systems still struggle in unhumanlike ways on real-world tasks requiring these capabilities.  This disconnect may be due in part to neglect in the AI community of well-founded experimental protocols for evaluating cognition.  In this talk I will summarize several recommendations on experimental methods from developmental and comparative psychology—fields that study the “alien intelligences” of babies and non-human animals—and demonstrate the application of such methods in two case studies of cognitive abilities in LLMs: analogical reasoning and visual abstraction.",,,,
"Friday, Dec 5, 2025",08:30,Invited Talk,From Benchmarks to Problems - A Perspective on Problem Finding in AI,https://neurips.cc/virtual/2025/invited-talk/129135,Kyunghyun Cho,09:30,"During the past 15 years or so, I have worked on a series of seemingly distinct but eventually related problems, including machine learning algorithms, generative modeling with neural networks, machine translation, language modeling, medical imaging, a bit of healthcare, protein modeling and a bit of drug discovery. I chose to work on some of these problems intentionally, while it was pure serendipity that I worked on some others. It was only in hindsight that these seemingly different problems turned out to be closely related to each other from both technical, social and personal perspectives. In this talk, I plan to do my own retrospective on my own choices, be them intentional or not, on these problems and share with you my thoughts what our own discipline, which is sometimes called computer science, data science, machine learning or artificial intelligence, is.",,,,
"Friday, Dec 5, 2025",14:30,Invited Talk,Demystifying depth: Principles of learning in deep neural networks,https://neurips.cc/virtual/2025/invited-talk/129133,Andrew Saxe,15:30,"Deep neural networks have revolutionized artificial intelligence, yet their inner workings remain poorly understood. This talk presents mathematical analyses of the nonlinear dynamics of learning in several solvable deep network models, offering theoretical insights into the role of depth. These models reveal how learning algorithms, data structure, initialization schemes, and architectural choices interact to produce hidden representations that afford complex generalization behaviors. A recurring theme across these analyses is a neural race: competing pathways within a deep network vie to explain the data, with an implicit bias toward shared representations. These shared representations in turn shape the network’s capacity for systematic generalization, multitasking, and transfer learning. I will show how such principles manifest across diverse architectures—including feedforward, recurrent, and linear attention networks. Together, these results provide analytic foundations for understanding how environmental statistics, network architecture, and learning dynamics jointly structure the emergence of neural representations and behavior.",,,,
