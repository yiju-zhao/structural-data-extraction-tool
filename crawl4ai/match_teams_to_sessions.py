#!/usr/bin/env python3
"""
Session-Team Matching Script using OpenAI GPT-5
Matches NeurIPS 2025 sessions to research teams based on their interests

Features:
- Uses GPT-5-mini with Pydantic structured output
- Type-safe parsing with Pydantic models
- Deduplicates matched teams automatically
- Limits to top 3 most relevant teams per session, sorted by relevance
"""

import os
import json
import pandas as pd
from openai import OpenAI
from pydantic import BaseModel
from typing import List, Dict, Tuple
import time
import argparse
from dotenv import load_dotenv
load_dotenv()

# Configuration
RESEARCH_INTEREST_FILE = "research_interest.md"
SESSIONS_CSV_FILE = "neurips_2025_sessions_SanDiego_detail.csv"
OUTPUT_CSV_FILE = "neurips_2025_sessions_SanDiego_matched_v5.csv"
OUTPUT_REVIEW_FILE = "neurips_2025_sessions_SanDiego_matched_v3_review.csv"


# Pydantic models for structured output
class MatchedTeam(BaseModel):
    """Single matched team with focus and reason"""
    bu: str          # Team BU name
    focus: str       # Team's focus area
    reason: str      # Recommendation reason


class MatchResult(BaseModel):
    """Result containing list of matched teams"""
    matched_teams: List[MatchedTeam]


class ReviewDecision(BaseModel):
    """Review decision on whether rematch is needed"""
    needs_rematch: bool       # Whether rematch is needed
    review_notes: str         # Reason for the decision


class MatchScore(BaseModel):
    """Three-dimensional matching score for a session-team pair"""
    keyword_score: float      # Keyword matching score (0-10)
    directness_score: float   # Problem-solving directness score (0-10)
    relevance_score: float    # Technical relevance strength score (0-10)
    total_score: float        # Weighted average total score (0-10)
    score_reasoning: str      # Detailed reasoning for the scores


# Huawei BU background information (from web research)
BU_CONTEXT_INFO = {
    "å­˜å‚¨": "åä¸ºäº‘ä¸‰å¤§æ ¸å¿ƒä¸šåŠ¡ä¹‹ä¸€ï¼ˆé€šç®—ã€æ™ºç®—ã€å­˜å‚¨ï¼‰ï¼Œè´Ÿè´£æ•°æ®ä¸­å¿ƒå­˜å‚¨ç³»ç»Ÿã€äº‘å­˜å‚¨è§£å†³æ–¹æ¡ˆçš„ç ”å‘å’Œä¼˜åŒ–ï¼ŒåŒ…æ‹¬AIè®­ç»ƒæ¨ç†ä¸­çš„æ•°æ®è®¿é—®ã€å­˜å‚¨æ¶æ„åˆ›æ–°ç­‰ã€‚",

    "CBG": "Consumer Business Groupï¼ˆæ¶ˆè´¹è€…ä¸šåŠ¡éƒ¨é—¨ï¼‰ï¼Œè´Ÿè´£åä¸ºæ™ºèƒ½æ‰‹æœºã€å¹³æ¿ç”µè„‘ã€å¯ç©¿æˆ´è®¾å¤‡ã€æ™ºæ…§å±ç­‰ç»ˆç«¯äº§å“çš„ç ”å‘ã€ç”Ÿäº§å’Œé”€å”®ï¼Œè‡´åŠ›äºå…¨åœºæ™¯æ™ºæ…§ç”Ÿæ´»ä½“éªŒã€‚",

    "DCN": "Data Communication Networkï¼ˆæ•°æ®é€šä¿¡ç½‘ç»œéƒ¨é—¨ï¼‰ï¼Œè´Ÿè´£æ•°æ®ä¸­å¿ƒç½‘ç»œæ¶æ„è®¾è®¡ä¸ä¼˜åŒ–ï¼ŒåŒ…æ‹¬Spine-Leafæ¶æ„ã€VXLANã€SDNã€æ•°æ®ä¸­å¿ƒäº’è”ã€ç½‘ç»œå®‰å…¨ç®¡æ§ç­‰æŠ€æœ¯çš„ç ”å‘å’Œéƒ¨ç½²ã€‚",

    "æµ·æ€": "åä¸ºé›†æˆç”µè·¯è®¾è®¡å…¬å¸ï¼Œä¸­å›½æœ€å¤§çš„æ— æ™¶åœ†å‚åŠå¯¼ä½“è®¾è®¡å…¬å¸ï¼Œä¸»è¦äº§å“åŒ…æ‹¬éº’éºŸç³»åˆ—ç§»åŠ¨å¤„ç†å™¨ã€AIèŠ¯ç‰‡ç­‰ï¼Œè¦†ç›–æ— çº¿é€šä¿¡ã€æ™ºèƒ½è§†è§‰ã€æ™ºèƒ½åª’ä½“ç­‰é¢†åŸŸçš„èŠ¯ç‰‡è®¾è®¡ã€‚",

    "è®¡ç®—": "è´Ÿè´£åä¸ºæ˜‡è…¾ï¼ˆAscendï¼‰AIèŠ¯ç‰‡å’ŒAtlas AIè®¡ç®—è§£å†³æ–¹æ¡ˆçš„ç ”å‘ï¼Œä¸“æ³¨AIè®¡ç®—åŸºç¡€è®¾æ–½ã€é«˜æ€§èƒ½è®¡ç®—æ¶æ„ã€AIè®­ç»ƒæ¨ç†åŠ é€Ÿç­‰æ ¸å¿ƒæŠ€æœ¯åˆ›æ–°ã€‚",

    "æ¸©å“¥åäº‘": "Huawei Cloud Vancouverç ”ç©¶å›¢é˜Ÿï¼Œä¸“æ³¨å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMsï¼‰çš„æˆæœ¬ä¼˜åŒ–ã€å¾®è°ƒæ¨ç†æŠ€æœ¯ã€è´Ÿè´£ä»»AIï¼ˆæ•°æ®/æ¨¡å‹æ°´å°ã€è”é‚¦å­¦ä¹ ï¼‰ä»¥åŠLLMsåœ¨è¿ç­¹å­¦ã€åˆ†ææ•°æ®åº“ç­‰é¢†åŸŸçš„å®é™…åº”ç”¨ã€‚",

    "å¤šä¼¦å¤šäº‘": "Huawei Cloudåˆ†å¸ƒå¼è°ƒåº¦å’Œæ•°æ®å¼•æ“å®éªŒå®¤ï¼Œä¸“æ³¨AI AgentæŠ€æœ¯ç ”ç©¶ï¼ŒåŒ…æ‹¬å¤šæ™ºèƒ½ä½“ç³»ç»Ÿï¼ˆMulti-Agentï¼‰ã€Agentç¼–æ’ï¼ˆAgentic Orchestrationï¼‰ã€Agentå®‰å…¨æ€§ä»¥åŠGenAIäº‘æœåŠ¡æŠ€æœ¯åˆ›æ–°ã€‚",

    "è¯ºäºš": "åä¸ºè¯ºäºšæ–¹èˆŸå®éªŒå®¤ï¼Œä»äº‹äººå·¥æ™ºèƒ½åŸºç¡€ç ”ç©¶ï¼Œä¸»è¦æ–¹å‘åŒ…æ‹¬å¤§æ¨¡å‹è‡ªæ¼”è¿›ã€å¼ºåŒ–å­¦ä¹ ï¼ˆRLHFï¼‰ã€LLM-based agentã€æ·±åº¦å¼ºåŒ–å­¦ä¹ ã€å¤šæ™ºèƒ½ä½“ç³»ç»Ÿä»¥åŠå†³ç­–æ¨ç†ç­‰å‰æ²¿AIæŠ€æœ¯ç ”ç©¶ã€‚",
}


def get_bu_context(bu_name: str) -> str:
    """
    Get BU background information from predefined dictionary

    Args:
        bu_name: Name of the BU

    Returns:
        Background information about the BU
    """
    return BU_CONTEXT_INFO.get(bu_name, "è¯¥BUæš‚æ— èƒŒæ™¯ä¿¡æ¯")


def parse_research_interests(file_path: str) -> List[Dict[str, str]]:
    """
    Parse the research interest markdown file and extract team profiles

    Returns:
        List of dicts with keys: 'bu', 'focus', 'challenges'
    """
    teams = []

    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    # Skip header lines and parse the table
    data_started = False
    for line in lines:
        line = line.strip()

        # Skip until we find the table separator
        if '|:----' in line or '|----' in line:
            data_started = True
            continue

        if not data_started or not line.startswith('|'):
            continue

        # Parse table row
        parts = [p.strip() for p in line.split('|')[1:-1]]  # Remove empty first and last

        if len(parts) >= 3:
            bu = parts[0]
            focus = parts[1]
            challenges = parts[2]

            if bu and focus:  # Make sure it's not empty
                teams.append({
                    'bu': bu,
                    'focus': focus,
                    'challenges': challenges
                })

    return teams


def create_prompt(session: Dict, teams: List[Dict[str, str]], review_mode: bool = False, old_matches: str = "") -> str:
    """
    Create a unified prompt for matching or reviewing matches

    Args:
        session: Dict containing session information
        teams: List of team profiles
        review_mode: If True, create review prompt; otherwise matching prompt
        old_matches: Original matched teams (used in review mode)

    Returns:
        Formatted prompt string
    """
    # Build team profiles section
    team_profiles = []
    for i, team in enumerate(teams, 1):
        profile = f"{i}. BU: {team['bu']}\n"
        # Add BU background context if available
        if 'context' in team and team['context']:
            profile += f"   BUèƒŒæ™¯: {team['context']}\n"
        profile += f"   å…³æ³¨æ–¹å‘: {team['focus']}\n"
        profile += f"   éš¾é¢˜: {team['challenges']}"
        team_profiles.append(profile)

    teams_text = "\n\n".join(team_profiles)

    # Build session information
    session_info = f"""
Session Information:
- Title: {session.get('title', 'N/A')}
- Type: {session.get('type', 'N/A')}
- Date & Time: {session.get('date', 'N/A')} {session.get('time', 'N/A')}
- Abstract: {session.get('abstract', 'N/A')}
- Overview: {session.get('overview', 'N/A')}
"""

    # Detect information completeness
    abstract = str(session.get('abstract', 'N/A'))
    overview = str(session.get('overview', 'N/A'))
    has_abstract = abstract and abstract.strip() and abstract not in ('N/A', 'nan', 'None', '')
    has_overview = overview and overview.strip() and overview not in ('N/A', 'nan', 'None', '')
    only_title = not has_abstract and not has_overview

    # Create prompt based on mode
    if review_mode:
        # Review mode: stricter evaluation of existing matches
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„ç ”ç©¶å…´è¶£åŒ¹é…å®¡æ ¸ä¸“å®¶ã€‚ä½ æ­£åœ¨è¿›è¡Œ**REVIEWå®¡æ ¸æ¨¡å¼**ï¼Œéœ€è¦é‡æ–°è¯„ä¼°ä¹‹å‰çš„åŒ¹é…ç»“æœã€‚

{session_info}

ç ”ç©¶å›¢é˜Ÿä¿¡æ¯ï¼š
{teams_text}

**åŸå§‹åŒ¹é…ç»“æœ**ï¼š{old_matches if old_matches else 'æ— åŒ¹é…'}

---

**REVIEWå®¡æ ¸ä»»åŠ¡**ï¼š

ä½ çš„ä»»åŠ¡æ˜¯ä»¥**æ›´é«˜çš„æ ‡å‡†**é‡æ–°å®¡æ ¸è¿™ä¸ªSessionï¼Œåˆ¤æ–­æ˜¯å¦åº”è¯¥åŒ¹é…å›¢é˜Ÿã€‚

**å®¡æ ¸åŸåˆ™ï¼ˆæåº¦ä¸¥æ ¼ï¼‰**ï¼š

1. **è´¨ç–‘ä¼˜å…ˆ**ï¼šå‡è®¾åŸå§‹åŒ¹é…å¯èƒ½å­˜åœ¨é”™è¯¯ï¼Œé‡æ–°ä»é›¶å¼€å§‹è¯„ä¼°
2. **è¯æ®å¯¼å‘**ï¼šåªæœ‰Sessionä¸­æœ‰**æ˜ç¡®çš„æŠ€æœ¯è¯æ®**æ‰èƒ½åŒ¹é…
3. **å®ç¼ºæ¯‹æ»¥**ï¼šä¸ç¡®å®šæ—¶é€‰æ‹©ä¸åŒ¹é…ï¼Œè¿‡åº¦åŒ¹é…æ¯”é—æ¼æ›´ç³Ÿç³•
4. **æ·±åº¦éªŒè¯**ï¼šä¸è¦è¢«è¡¨é¢çš„å…³é”®è¯ç›¸ä¼¼æ€§è¯¯å¯¼

---

**å®¡æ ¸æ­¥éª¤**ï¼š

**ç¬¬ä¸€æ­¥ï¼šSessionæŠ€æœ¯å†…æ ¸æå–**
- Sessionçš„æ ¸å¿ƒæŠ€æœ¯ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆæå–3-5ä¸ªå…³é”®æŠ€æœ¯æœ¯è¯­ï¼‰
- Sessionè§£å†³çš„å…·ä½“æŠ€æœ¯é—®é¢˜æ˜¯ä»€ä¹ˆï¼Ÿ
- Sessionæå‡ºçš„æŠ€æœ¯æ–¹æ³•/ç®—æ³•æ˜¯ä»€ä¹ˆï¼Ÿ

**ç¬¬äºŒæ­¥ï¼šå›¢é˜Ÿéœ€æ±‚ä¸¥æ ¼åŒ¹é…**
å¯¹äºæ¯ä¸ªå€™é€‰å›¢é˜Ÿï¼š
1. å›¢é˜Ÿ"éš¾é¢˜"ä¸­çš„å…·ä½“æŠ€æœ¯æœ¯è¯­æ˜¯ä»€ä¹ˆï¼Ÿ
2. Sessionçš„æŠ€æœ¯ç‚¹æ˜¯å¦**ç›´æ¥å‘½ä¸­**å›¢é˜Ÿçš„æŠ€æœ¯æœ¯è¯­ï¼Ÿ
3. å¦‚æœæ˜¯å®è§‚éœ€æ±‚ï¼ˆå¦‚"ç†è§£è¶‹åŠ¿"ï¼‰ï¼ŒSessionæ˜¯å¦æä¾›äº†**æˆ˜ç•¥çº§æ´å¯Ÿ**ï¼Ÿ

**ç¬¬ä¸‰æ­¥ï¼šä¸¥æ ¼ç­›é€‰**
- âœ… åŒ¹é…æ¡ä»¶ï¼ˆå¿…é¡»æ»¡è¶³ï¼‰ï¼š
  * å…·ä½“æŠ€æœ¯æœ¯è¯­å®Œå…¨å¯¹åº” OR å®è§‚æˆ˜ç•¥æ´å¯Ÿæ˜ç¡®
  * Sessionèƒ½ç›´æ¥è§£å†³å›¢é˜Ÿçš„æ ¸å¿ƒæŠ€æœ¯é—®é¢˜
  * æŠ€æœ¯å…³è”åº¦é«˜ä¸”æ˜ç¡®

- âŒ ä¸åŒ¹é…æ¡ä»¶ï¼ˆä»»ä¸€å³æ’é™¤ï¼‰ï¼š
  * åªæ˜¯å¤§é¢†åŸŸç›¸å…³ï¼Œä½†æŠ€æœ¯ç»†èŠ‚ä¸ç¬¦
  * Sessionæåˆ°çš„æŠ€æœ¯ä¸å›¢é˜Ÿéš¾é¢˜æ˜¯"å¹³è¡ŒæŠ€æœ¯"ï¼ˆåŒé¢†åŸŸä½†ä¸åŒé—®é¢˜ï¼‰
  * å…³è”åº¦æ¨¡ç³Šæˆ–éœ€è¦"è„‘è¡¥"æ‰èƒ½å»ºç«‹è”ç³»
  * åªæœ‰é—´æ¥æˆ–æ½œåœ¨çš„å¸®åŠ©

**ç¬¬å››æ­¥ï¼šæœ€ç»ˆå†³ç­–**
- æœ€å¤šåŒ¹é…3ä¸ªæœ€ç›¸å…³çš„å›¢é˜Ÿ
- å¦‚æœæ²¡æœ‰é«˜åº¦ç›¸å…³çš„å›¢é˜Ÿï¼Œè¿”å›ç©ºæ•°ç»„ `[]`
- å®å¯æ¼æ‰ä¸€ä¸ªæ­£ç¡®åŒ¹é…ï¼Œä¹Ÿä¸è¦åŠ å…¥ä¸€ä¸ªé”™è¯¯åŒ¹é…

---

{'**ã€æåº¦ä¿å®ˆåŒ¹é…ã€‘å½“å‰sessionä¿¡æ¯ä¸è¶³ï¼ˆåªæœ‰æ ‡é¢˜ï¼‰**ï¼š' if only_title else ''}
{'''- ä»…å½“æ ‡é¢˜ä¸­æ˜ç¡®åŒ…å«å›¢é˜Ÿå…³æ³¨çš„å…·ä½“æŠ€æœ¯æœ¯è¯­æ—¶æ‰åŒ¹é…
- ä¸è¦æ ¹æ®æ ‡é¢˜æ¨æµ‹å¯èƒ½çš„å†…å®¹æˆ–åšä»»ä½•è”æƒ³
- æ ‡é¢˜ç¬¼ç»Ÿæˆ–å®½æ³› â†’ ç›´æ¥ä¸åŒ¹é…
- åªåšæœ€å°çš„ã€åˆç†çš„çŒœæµ‹ï¼Œç¦æ­¢è¿‡åº¦è”æƒ³''' if only_title else ''}

---

**æ¨èç†ç”±æ’°å†™è¦æ±‚**ï¼š
- æ ¼å¼ï¼šSessionè®¨è®ºçš„[Sessionæ ¸å¿ƒæŠ€æœ¯ç‚¹]ï¼Œå¯é‡ç‚¹å…³æ³¨å…¶åœ¨[å›¢é˜ŸæŠ€æœ¯éš¾é¢˜]ä¸­çš„[å…·ä½“åº”ç”¨æ–¹å‘æˆ–ç®—æ³•]
- è¦æ±‚ï¼š50-80å­—ï¼Œç²¾ç‚¼è‡ªç„¶ï¼Œæ˜ç¡®æŒ‡å‡ºæŠ€æœ¯è¿æ¥ç‚¹

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
    "matched_teams": [
        {{
            "bu": "å›¢é˜ŸBUåç§°",
            "focus": "è¯¥å›¢é˜Ÿçš„å…³æ³¨æ–¹å‘",
            "reason": "æ¨èç†ç”±"
        }}
    ]
}}
"""
    else:
        # Matching mode: standard three-step analysis
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„ç ”ç©¶å…´è¶£åŒ¹é…ä¸“å®¶ã€‚è¯·åˆ†æä»¥ä¸‹NeurIPS 2025ä¼šè®®sessionï¼Œåˆ¤æ–­å“ªäº›ç ”ç©¶å›¢é˜Ÿåº”è¯¥å‚åŠ è¿™ä¸ªsessionã€‚

{session_info}

ç ”ç©¶å›¢é˜Ÿä¿¡æ¯ï¼š
{teams_text}

è¯·ä¸¥æ ¼éµå¾ªä»¥ä¸‹ä¸‰æ­¥åˆ†ææ³•ï¼Œä¸ºç»™å®šçš„SessionåŒ¹é…æœ€å¤š3ä¸ªæœ€ç›¸å…³çš„å›¢é˜Ÿã€‚æ ¸å¿ƒåŸåˆ™æ˜¯"å®ç¼ºæ¯‹æ»¥"ï¼Œåªé€‰æ‹©é«˜åº¦ç›¸å…³çš„åŒ¹é…ã€‚

---

**ç¬¬ä¸€æ­¥ï¼šè§£æ„Session - æ·±å…¥æŠ€æœ¯å†…æ ¸**

1.  **æ ¸å¿ƒè®®é¢˜ (What)ï¼š** ç²¾ç‚¼æ€»ç»“Sessionçš„æ ¸å¿ƒæŠ€æœ¯ä¸»é¢˜ã€‚å®ƒåˆ°åº•åœ¨è®²ä»€ä¹ˆï¼Ÿ
2.  **ç›®æ ‡é—®é¢˜ (Why)ï¼š** Sessionè¯•å›¾è§£å†³æˆ–ä¼˜åŒ–çš„å…·ä½“æŠ€æœ¯æŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Ÿ
3.  **æŠ€æœ¯æ–¹æ¡ˆ (How)ï¼š** Sessionæå‡ºäº†ä»€ä¹ˆå…·ä½“çš„æ–¹æ³•ã€æ¨¡å‹ã€ç®—æ³•æˆ–ç³»ç»Ÿè®¾è®¡ï¼Ÿè¯·åˆ—å‡ºå…³é”®æŠ€æœ¯æœ¯è¯­ã€‚
4.  **åº•å±‚åŸç† (First Principle)ï¼š** è¿™äº›æŠ€æœ¯æ–¹æ¡ˆçš„æœ¬è´¨æ˜¯ä»€ä¹ˆï¼Ÿå®ƒæ˜¯åœ¨å“ªä¸ªåŸºç¡€å±‚é¢ï¼ˆå¦‚è®¡ç®—ã€å­˜å‚¨ã€é€šä¿¡ã€ç®—æ³•ï¼‰ä¸Šè¿›è¡Œäº†åˆ›æ–°ï¼Ÿ

---

**ç¬¬äºŒæ­¥ï¼šæ‹†è§£å›¢é˜Ÿéœ€æ±‚ - èšç„¦æ ¸å¿ƒç“¶é¢ˆ**

é’ˆå¯¹å€™é€‰çš„æ¯ä¸ªå›¢é˜Ÿï¼Œè¿›è¡Œå¦‚ä¸‹åˆ†æï¼š

1.  **ä¸šåŠ¡å®šä½ (Business Context)ï¼š** å¿«é€Ÿå®šä½è¯¥å›¢é˜Ÿçš„ä¸šåŠ¡é¢†åŸŸå’Œæ ¸å¿ƒèŒè´£ã€‚
2.  **æ ¸å¿ƒéš¾é¢˜ (Core Problem)ï¼š** ä»"éš¾é¢˜"æè¿°ä¸­ï¼Œæç‚¼å‡º1-2ä¸ªæœ€æ ¸å¿ƒçš„æŠ€æœ¯æŒ‘æˆ˜ã€‚
3.  **æŠ€æœ¯æœ¬è´¨ (Technical Essence)ï¼š** è¿™ä¸ªéš¾é¢˜çš„åº•å±‚æŠ€æœ¯é€»è¾‘æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆè®¡ç®—æ•ˆç‡ã€å†…å­˜ç“¶é¢ˆã€ç®—æ³•ç²¾åº¦ã€ç³»ç»Ÿè°ƒåº¦ç­‰ï¼‰
4.  **å…³é”®æœ¯è¯­ (Keywords)ï¼š** â€œéš¾é¢˜â€ä¸­å‡ºç°äº†å“ªäº›å¿…é¡»å…³æ³¨çš„å…·ä½“æŠ€æœ¯æœ¯è¯­ï¼Ÿ
5.  **éš¾é¢˜åˆ†ç±» (Problem Type)ï¼š** æ ¹æ®éš¾é¢˜æ€§è´¨ï¼Œå°†å…¶æ˜ç¡®å½’ç±»ä¸ºï¼š
    *   **Aç±»ï¼šå…·ä½“æŠ€æœ¯å®ç°**
    *   **Bç±»ï¼šå®è§‚æˆ˜ç•¥è®¤çŸ¥**

---

**ç¬¬ä¸‰æ­¥ï¼šæ‰§è¡ŒåŒ¹é… - ä¾æ®åº•å±‚é€»è¾‘**

1.  **é€‰æ‹©åŒ¹é…æ ‡å‡†ï¼š**
    *   å¦‚æœå›¢é˜Ÿéš¾é¢˜è¢«åˆ†ç±»ä¸º **Aç±»**ï¼šSessionæ˜¯å¦**ç›´æ¥**è§£å†³äº†å›¢é˜Ÿçš„**å…³é”®æœ¯è¯­**æ‰€æŒ‡å‘çš„é—®é¢˜ï¼Œå¹¶èƒ½æä¾›**å…·ä½“ã€å¯æ“ä½œ**çš„æ€è·¯ï¼Ÿ
    *   å¦‚æœå›¢é˜Ÿéš¾é¢˜è¢«åˆ†ç±»ä¸º **Bç±»**ï¼šSessionæ‰€æ­ç¤ºçš„è¶‹åŠ¿ï¼Œæ˜¯å¦èƒ½å¸®åŠ©å›¢é˜Ÿ**é¢„åˆ¤**æŠ€æœ¯æ¼”è¿›ï¼Œå¹¶æä¾›é«˜é˜¶çš„**æˆ˜ç•¥æ€§æ´å¯Ÿ**ï¼Ÿ

2.  **ç”ŸæˆåŒ¹é…ç»“æœï¼š**
    *   æŒ‰ç›¸å…³åº¦ä»é«˜åˆ°ä½æ’åºï¼Œæœ€å¤šè¾“å‡º3ä¸ªåŒ¹é…å›¢é˜Ÿã€‚
    *   è‹¥æ— é«˜åº¦ç›¸å…³çš„å›¢é˜Ÿï¼Œåˆ™è¿”å›ç©ºæ•°ç»„ `[]`ã€‚
    *   ä¸ºæ¯ä¸ªåŒ¹é…çš„å›¢é˜Ÿï¼Œæä¾›`bu`, `focus`, å’Œ`reason`ã€‚

3.  **æ’°å†™æ¨èç†ç”± (Reason)ï¼š**
    *   **æ ¼å¼ï¼š** Sessionè®¨è®ºçš„[Sessionæ ¸å¿ƒæŠ€æœ¯ç‚¹]ï¼Œå¯é‡ç‚¹å…³æ³¨å…¶åœ¨[å›¢é˜ŸæŠ€æœ¯éš¾é¢˜]ä¸­çš„[å…·ä½“åº”ç”¨æ–¹å‘æˆ–ç®—æ³•]ã€‚
    *   **è¦æ±‚ï¼š** 50-80å­—ï¼Œè¯­è¨€ç²¾ç‚¼è‡ªç„¶ï¼Œæ˜ç¡®æŒ‡å‡ºæŠ€æœ¯è¿æ¥ç‚¹ï¼Œç»™äºˆæ¸…æ™°çš„å…³æ³¨å»ºè®®ã€‚

---
{'**ã€æåº¦ä¿å®ˆåŒ¹é…ã€‘å½“å‰sessionä¿¡æ¯ä¸è¶³ï¼ˆåªæœ‰æ ‡é¢˜ï¼‰**ï¼š' if only_title else ''}
{'''- ä»…å½“æ ‡é¢˜ä¸­æ˜ç¡®åŒ…å«å›¢é˜Ÿå…³æ³¨çš„å…·ä½“æŠ€æœ¯æœ¯è¯­æ—¶æ‰åŒ¹é…
- ä¸è¦æ ¹æ®æ ‡é¢˜æ¨æµ‹å¯èƒ½çš„å†…å®¹æˆ–åšä»»ä½•è”æƒ³
- æ ‡é¢˜ç¬¼ç»Ÿæˆ–å®½æ³›ï¼ˆå¦‚"AI Advances"ã€"Future of ML"ã€"Recent Progress"ç­‰ï¼‰â†’ ç›´æ¥ä¸åŒ¹é…
- æ ‡é¢˜åªæåˆ°å¤§é¢†åŸŸï¼ˆå¦‚"Computer Vision"ã€"NLP"ã€"Robotics"ï¼‰æ²¡æœ‰å…·ä½“æŠ€æœ¯ â†’ ä¸åŒ¹é…
- ä¿¡å¿ƒä¸è¶³æ—¶ â†’ ä¸åŒ¹é…
- åªåšæœ€å°çš„ã€åˆç†çš„çŒœæµ‹ï¼Œç¦æ­¢è¿‡åº¦è”æƒ³''' if only_title else ''}
"""

    return prompt


def score_match(session: Dict, team: Dict[str, str], client: OpenAI) -> Dict:
    """
    Score a session-team match using three dimensions (0-10 scale each)

    Args:
        session: Dict containing session information
        team: Single team profile dict
        client: OpenAI client instance

    Returns:
        Dict with keyword_score, directness_score, relevance_score, total_score, score_reasoning
    """
    # Build session information
    session_info = f"""
Session Information:
- Title: {session.get('title', 'N/A')}
- Type: {session.get('type', 'N/A')}
- Abstract: {session.get('abstract', 'N/A')}
- Overview: {session.get('overview', 'N/A')}
"""

    # Build team information
    team_info = f"""
Team Information:
- BU: {team['bu']}
- BU Background: {team.get('context', 'N/A')}
- Focus: {team['focus']}
- Challenge/Problem: {team['challenges']}
"""

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯åŒ¹é…è¯„åˆ†ä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹Sessionå’Œå›¢é˜Ÿçš„åŒ¹é…åº¦è¿›è¡Œä¸‰ç»´åº¦æ‰“åˆ†ï¼ˆ0-10åˆ†åˆ¶ï¼‰ã€‚

{session_info}

{team_info}

---

**è¯„åˆ†ä»»åŠ¡**ï¼š

è¯·ä»ä»¥ä¸‹ä¸‰ä¸ªç»´åº¦å¯¹åŒ¹é…åº¦è¿›è¡Œæ‰“åˆ†ï¼ˆæ¯ä¸ªç»´åº¦0-10åˆ†ï¼‰ï¼š

**ç»´åº¦1ï¼šå…³é”®æœ¯è¯­åŒ¹é…åº¦ï¼ˆ0-10åˆ†ï¼‰**

è¯„åˆ†æ ‡å‡†ï¼š
1. æå–å›¢é˜Ÿ"éš¾é¢˜"ä¸­çš„å…³é”®æŠ€æœ¯æœ¯è¯­
2. æå–Sessionä¸­çš„æŠ€æœ¯æœ¯è¯­ï¼ˆä»title/abstract/overviewï¼‰
3. è®¡åˆ†æ–¹å¼ï¼š
   - æ ¸å¿ƒæœ¯è¯­å®Œå…¨åŒ¹é…ï¼š+3åˆ†/ä¸ª
   - ç›¸å…³æœ¯è¯­åŒ¹é…ï¼š+2åˆ†/ä¸ª
   - æ³›åŒ–æœ¯è¯­åŒ¹é…ï¼š+1åˆ†/ä¸ª
   - **å°é¡¶10åˆ†**

ç¤ºä¾‹ï¼š
- å›¢é˜Ÿéš¾é¢˜ï¼š"Ascend AIèŠ¯ç‰‡çš„CANNç®—å­ä¼˜åŒ–"
- Sessionï¼š"Efficient Operator Fusion for AI Accelerators"
- è¯„åˆ†ï¼šAI accelerator(+2), Operator(+3) = 5åˆ†

**ç»´åº¦2ï¼šé—®é¢˜è§£å†³ç›´æ¥æ€§ï¼ˆ0-10åˆ†ï¼‰**

è¯„åˆ†æ ‡å‡†ï¼š
- 9-10åˆ†ï¼šSessionçš„ä¸»è¦å†…å®¹**ç²¾ç¡®å‘½ä¸­**å›¢é˜Ÿéš¾é¢˜çš„æ ¸å¿ƒæŠ€æœ¯ç‚¹
- 7-8åˆ†ï¼šSessionå›ç­”äº†éš¾é¢˜çš„**ä¸»è¦æ–¹é¢**ï¼Œæœ‰æ˜ç¡®å¯æ“ä½œçš„æ€è·¯
- 5-6åˆ†ï¼šSessionæä¾›äº†**éƒ¨åˆ†ç›¸å…³**çš„è§£å†³æ€è·¯æˆ–æŠ€æœ¯å‚è€ƒ
- 3-4åˆ†ï¼šSessionçš„æ€è·¯å¯ä»¥**é—´æ¥åº”ç”¨**åˆ°å›¢é˜Ÿé—®é¢˜
- 1-2åˆ†ï¼šSessionåªåœ¨**æ¦‚å¿µå±‚é¢**ä¸é—®é¢˜ç›¸å…³
- 0åˆ†ï¼šSessionä¸å›¢é˜Ÿé—®é¢˜**å®Œå…¨ä¸ç›¸å…³**

**ç»´åº¦3ï¼šæŠ€æœ¯ç›¸å…³æ€§å¼ºåº¦ï¼ˆ0-10åˆ†ï¼‰**

è¯„åˆ†æ ‡å‡†ï¼š
- 9-10åˆ†ï¼šSessionå’Œå›¢é˜Ÿåœ¨**åŒä¸€æŠ€æœ¯æ ˆ**å·¥ä½œï¼ˆå¦‚éƒ½æ˜¯AscendèŠ¯ç‰‡ä¼˜åŒ–ï¼‰
- 7-8åˆ†ï¼šSessionå’Œå›¢é˜Ÿåœ¨**åŒä¸€æŠ€æœ¯é¢†åŸŸ**ï¼ˆå¦‚éƒ½æ˜¯AIæ¨ç†åŠ é€Ÿï¼‰
- 5-6åˆ†ï¼šæŠ€æœ¯æ–¹å‘ç›¸å…³ä½†**å­é¢†åŸŸä¸åŒ**ï¼ˆå¦‚è®­ç»ƒ vs æ¨ç†ï¼‰
- 3-4åˆ†ï¼š**åŒå¤§é¢†åŸŸä½†æŠ€æœ¯è·¯å¾„ä¸åŒ**ï¼ˆå¦‚GPUä¼˜åŒ– vs ASICä¼˜åŒ–ï¼‰
- 1-2åˆ†ï¼šåªåœ¨**AIå¤§é¢†åŸŸ**ç›¸å…³ï¼ŒæŠ€æœ¯ç»†èŠ‚å®Œå…¨ä¸åŒ
- 0åˆ†ï¼šå®Œå…¨ä¸åŒçš„æŠ€æœ¯é¢†åŸŸ

---

**æ€»åˆ†è®¡ç®—**ï¼š
æ€»åˆ† = (ç»´åº¦1 Ã— 0.3) + (ç»´åº¦2 Ã— 0.4) + (ç»´åº¦3 Ã— 0.3)
ï¼ˆä¿ç•™1ä½å°æ•°ï¼‰

**è¯„åˆ†ç†ç”±**ï¼š
ç”¨ä¸€å¥è¯ï¼ˆ80-150å­—ï¼‰è§£é‡Šæ‰“åˆ†ä¾æ®ï¼Œæ ¼å¼ä¸ºï¼š
"å…³é”®æœ¯è¯­ï¼š[åŒ¹é…åˆ°çš„æœ¯è¯­åŠåˆ†æ•°]ï¼›ç›´æ¥æ€§ï¼š[æ˜¯å¦ç›´æ¥è§£å†³é—®é¢˜åŠåˆ†æ•°]ï¼›ç›¸å…³æ€§ï¼š[æŠ€æœ¯æ ˆ/é¢†åŸŸå…³è”åº¦åŠåˆ†æ•°]"

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
    "keyword_score": X.X,
    "directness_score": X.X,
    "relevance_score": X.X,
    "total_score": X.X,
    "score_reasoning": "è¯„åˆ†ç†ç”±"
}}
"""

    try:
        # Use chat.completions.parse with Pydantic model for structured output
        completion = client.chat.completions.parse(
            model="gpt-5",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯åŒ¹é…è¯„åˆ†ä¸“å®¶ï¼Œæ“…é•¿é‡åŒ–è¯„ä¼°sessionä¸å›¢é˜Ÿéœ€æ±‚çš„åŒ¹é…åº¦ã€‚"},
                {"role": "user", "content": prompt}
            ],
            response_format=MatchScore,
        )

        # Get the parsed result directly from Pydantic model
        score_result = completion.choices[0].message.parsed

        return {
            "keyword_score": score_result.keyword_score,
            "directness_score": score_result.directness_score,
            "relevance_score": score_result.relevance_score,
            "total_score": score_result.total_score,
            "score_reasoning": score_result.score_reasoning
        }

    except Exception as e:
        print(f"Error scoring session '{session.get('title', 'N/A')}' for team '{team['bu']}': {str(e)}")
        # Return zero scores on error
        return {
            "keyword_score": 0.0,
            "directness_score": 0.0,
            "relevance_score": 0.0,
            "total_score": 0.0,
            "score_reasoning": f"è¯„åˆ†å‡ºé”™: {str(e)}"
        }


def review_match_decision(session: Dict, teams: List[Dict[str, str]], old_matches: str, client: OpenAI) -> Dict:
    """
    Review existing matches and decide if rematch is needed

    Args:
        session: Dict containing session information
        teams: List of team profiles
        old_matches: Original matched teams string (e.g., "BU1; BU2")
        client: OpenAI client instance

    Returns:
        Dict with keys: needs_rematch (bool), review_notes (str)
    """
    # Build team profiles section
    team_profiles = []
    for i, team in enumerate(teams, 1):
        profile = f"{i}. BU: {team['bu']}\n"
        if 'context' in team and team['context']:
            profile += f"   BUèƒŒæ™¯: {team['context']}\n"
        profile += f"   å…³æ³¨æ–¹å‘: {team['focus']}\n"
        profile += f"   éš¾é¢˜: {team['challenges']}"
        team_profiles.append(profile)

    teams_text = "\n\n".join(team_profiles)

    # Build session information
    session_info = f"""
Session Information:
- Title: {session.get('title', 'N/A')}
- Type: {session.get('type', 'N/A')}
- Date & Time: {session.get('date', 'N/A')} {session.get('time', 'N/A')}
- Abstract: {session.get('abstract', 'N/A')}
- Overview: {session.get('overview', 'N/A')}
"""

    # Detect information completeness
    abstract = str(session.get('abstract', 'N/A'))
    overview = str(session.get('overview', 'N/A'))
    has_abstract = abstract and abstract.strip() and abstract not in ('N/A', 'nan', 'None', '')
    has_overview = overview and overview.strip() and overview not in ('N/A', 'nan', 'None', '')
    only_title = not has_abstract and not has_overview

    prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„ç ”ç©¶å…´è¶£åŒ¹é…å®¡æ ¸ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ¤æ–­åŸå§‹åŒ¹é…æ˜¯å¦ç¬¦åˆæ ‡å‡†ã€‚

{session_info}

ç ”ç©¶å›¢é˜Ÿä¿¡æ¯ï¼š
{teams_text}

**åŸå§‹åŒ¹é…ç»“æœ**ï¼š{old_matches if old_matches else 'æ— åŒ¹é…'}

---

**å®¡æ ¸ä»»åŠ¡**ï¼š

åˆ¤æ–­åŸå§‹åŒ¹é…æ˜¯å¦ç¬¦åˆä»¥ä¸‹åŒ¹é…æ ‡å‡†ã€‚å¦‚æœä¸ç¬¦åˆï¼Œåˆ™éœ€è¦é‡æ–°åŒ¹é…ã€‚

**åŒ¹é…æ ‡å‡†æ£€æŸ¥æ¸…å•**ï¼ˆæç‚¼è‡ªmatchingæ ‡å‡†ï¼‰ï¼š

1. **Aç±»é—®é¢˜ï¼ˆå…·ä½“æŠ€æœ¯å®ç°ï¼‰æ£€æŸ¥**ï¼š
   - Sessionæ˜¯å¦**ç›´æ¥**è§£å†³äº†å›¢é˜Ÿçš„**å…³é”®æŠ€æœ¯æœ¯è¯­**æ‰€æŒ‡å‘çš„é—®é¢˜ï¼Ÿ
   - Sessionæ˜¯å¦æä¾›äº†**å…·ä½“ã€å¯æ“ä½œ**çš„æŠ€æœ¯æ€è·¯ï¼Ÿ

2. **Bç±»é—®é¢˜ï¼ˆå®è§‚æˆ˜ç•¥è®¤çŸ¥ï¼‰æ£€æŸ¥**ï¼š
   - Sessionæ˜¯å¦èƒ½å¸®åŠ©å›¢é˜Ÿ**é¢„åˆ¤**æŠ€æœ¯æ¼”è¿›ï¼Ÿ
   - Sessionæ˜¯å¦æä¾›äº†**æˆ˜ç•¥æ€§æ´å¯Ÿ**ï¼Ÿ

3. **ä¿å®ˆåŸåˆ™æ£€æŸ¥**ï¼š
   - å¦‚æœåªæœ‰æ ‡é¢˜ä¿¡æ¯ï¼Œæ ‡é¢˜æ˜¯å¦æ˜ç¡®åŒ…å«å›¢é˜Ÿçš„å…·ä½“æŠ€æœ¯æœ¯è¯­ï¼Ÿ
   - æ˜¯å¦é¿å…äº†å¤§é¢†åŸŸæ³›åŒ¹é…ï¼ˆå¦‚"AI"ã€"ML"ç­‰å®½æ³›æ¦‚å¿µï¼‰ï¼Ÿ

4. **ç›¸å…³æ€§æ£€æŸ¥**ï¼š
   - Sessionçš„æŠ€æœ¯ç‚¹ä¸å›¢é˜Ÿéš¾é¢˜æ˜¯å¦æ˜¯"å¹³è¡ŒæŠ€æœ¯"ï¼ˆåŒé¢†åŸŸä½†ä¸åŒé—®é¢˜ï¼‰ï¼Ÿ
   - å…³è”åº¦æ˜¯å¦æ˜ç¡®ï¼Œè¿˜æ˜¯éœ€è¦"è„‘è¡¥"æ‰èƒ½å»ºç«‹è”ç³»ï¼Ÿ

---

{'**ã€æåº¦ä¿å®ˆåŒ¹é…ã€‘å½“å‰sessionä¿¡æ¯ä¸è¶³ï¼ˆåªæœ‰æ ‡é¢˜ï¼‰**ï¼š' if only_title else ''}
{'''- ä»…å½“æ ‡é¢˜ä¸­æ˜ç¡®åŒ…å«å›¢é˜Ÿå…³æ³¨çš„å…·ä½“æŠ€æœ¯æœ¯è¯­æ—¶æ‰èƒ½åŒ¹é…
- æ ‡é¢˜ç¬¼ç»Ÿæˆ–å®½æ³› â†’ ä¸åº”åŒ¹é…
- åªåšæœ€å°çš„ã€åˆç†çš„æ¨æ–­''' if only_title else ''}

---

**åˆ¤æ–­é€»è¾‘**ï¼š
- å¦‚æœåŸå§‹åŒ¹é…**å®Œå…¨ç¬¦åˆ**ä»¥ä¸Šæ ‡å‡† â†’ needs_rematch = false
- å¦‚æœåŸå§‹åŒ¹é…**ä¸ç¬¦åˆ**æˆ–**å¯èƒ½è¿‡åº¦åŒ¹é…** â†’ needs_rematch = trueï¼Œå¹¶åœ¨review_notesä¸­è¯´æ˜åŸå› 

**review_notesè¦æ±‚**ï¼š
- å¦‚æœneeds_rematch = trueï¼šç”¨80-150å­—è¯´æ˜ä¸ºä»€ä¹ˆåŸåŒ¹é…ä¸ç¬¦åˆæ ‡å‡†ï¼ˆä¾‹å¦‚ï¼š"åŸåŒ¹é…è¿‡äºå®½æ³›ï¼ŒSessionè®¨è®ºXæŠ€æœ¯ï¼Œä½†å›¢é˜Ÿéœ€æ±‚YæŠ€æœ¯ï¼Œå±äºå¹³è¡ŒæŠ€æœ¯ä¸ç›´æ¥ç›¸å…³"ï¼‰
- å¦‚æœneeds_rematch = falseï¼šç®€è¦è¯´æ˜"åŒ¹é…ç¬¦åˆæ ‡å‡†"

è¯·è¿”å›JSONæ ¼å¼ï¼š
{{
    "needs_rematch": true/false,
    "review_notes": "åˆ¤æ–­ç†ç”±"
}}
"""

    try:
        # Use chat.completions.parse with Pydantic model for structured output
        completion = client.chat.completions.parse(
            model="gpt-5-nano",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„åŒ¹é…å®¡æ ¸ä¸“å®¶ã€‚æ ¹æ®åŒ¹é…æ ‡å‡†åˆ¤æ–­åŸå§‹åŒ¹é…æ˜¯å¦åˆæ ¼ï¼Œéœ€è¦æ—¶å»ºè®®é‡æ–°åŒ¹é…ã€‚"},
                {"role": "user", "content": prompt}
            ],
            response_format=ReviewDecision,
        )

        # Get the parsed result directly from Pydantic model
        review_result = completion.choices[0].message.parsed

        return {
            "needs_rematch": review_result.needs_rematch,
            "review_notes": review_result.review_notes
        }

    except Exception as e:
        print(f"Error reviewing session '{session.get('title', 'N/A')}': {str(e)}")
        # Default to not rematch on error
        return {
            "needs_rematch": False,
            "review_notes": f"å®¡æ ¸å‡ºé”™ï¼Œä¿ç•™åŸåŒ¹é…: {str(e)}"
        }


def match_session_to_teams(session: Dict, teams: List[Dict[str, str]], client: OpenAI, review_mode: bool = False, old_matches: str = "", review_feedback: str = "") -> Dict:
    """
    Use OpenAI API to match a session to relevant teams with Pydantic structured output

    Args:
        session: Session information dict
        teams: List of team profiles
        client: OpenAI client instance
        review_mode: If True, use stricter review prompt
        old_matches: Original matched teams (for review mode)
        review_feedback: Feedback from review process (if rematching based on review)

    Returns:
        Dict with matched teams information
    """
    # Choose prompt based on mode
    if review_mode:
        prompt = create_prompt(session, teams, review_mode=True, old_matches=old_matches)
        system_msg = "ä½ æ˜¯ä¸€ä¸ªæåº¦ä¸¥æ ¼çš„ç ”ç©¶å…´è¶£åŒ¹é…å®¡æ ¸ä¸“å®¶ã€‚åœ¨REVIEWæ¨¡å¼ä¸‹ï¼Œä½ å¿…é¡»ä»¥æ›´é«˜æ ‡å‡†é‡æ–°è¯„ä¼°ï¼Œè´¨ç–‘åŸå§‹åŒ¹é…ï¼Œåªä¿ç•™æœ‰æ˜ç¡®æŠ€æœ¯è¯æ®çš„åŒ¹é…ã€‚å®å¯æ¼æ‰ä¹Ÿä¸è¦è¿‡åº¦åŒ¹é…ã€‚"
    else:
        prompt = create_prompt(session, teams, review_mode=False)
        system_msg = "ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„ç ”ç©¶å…´è¶£åŒ¹é…ä¸“å®¶ã€‚Return data in the exact structured format requested. æ¨èç†ç”±å¿…é¡»ç²¾ç®€è‡ªç„¶ï¼ˆ50-80å­—ï¼‰ï¼Œä½¿ç”¨æµç•…çš„ä¸­æ–‡è¡¨è¾¾ï¼Œæ ¼å¼ä¸º'Sessionè®¨è®º[å…·ä½“æŠ€æœ¯ç‚¹]ï¼Œå¯é‡ç‚¹å…³æ³¨[å…·ä½“æ–¹å‘/æŠ€æœ¯/ç®—æ³•]'ï¼Œç»™å‡ºæ˜ç¡®çš„æŠ€æœ¯å…³æ³¨å»ºè®®ã€‚é‡è¦ï¼šå…ˆåˆ¤æ–­å›¢é˜Ÿéš¾é¢˜çš„æ€§è´¨â€”â€”å¦‚æœæ˜¯å…·ä½“æŠ€æœ¯æœ¯è¯­ï¼Œåˆ™ä¸¥æ ¼åŒ¹é…ï¼›å¦‚æœæ˜¯å®è§‚æˆ˜ç•¥éœ€æ±‚ï¼ˆå¦‚ç†è§£è¶‹åŠ¿ã€é¢„åˆ¤æ¼”è¿›ï¼‰ï¼Œåˆ™ä»å®è§‚å±‚é¢åŒ¹é…ã€‚"

    # Add review feedback if provided
    if review_feedback:
        feedback_section = f"""
ã€è¯„å®¡å»ºè®® - é‡æ–°åŒ¹é…è¯´æ˜ã€‘
åŸåŒ¹é…ç»è¿‡è¯„å®¡å‘ç°éœ€è¦è°ƒæ•´ï¼š
{review_feedback}

è¯·åŸºäºä»¥ä¸Šè¯„å®¡å»ºè®®å’Œä¸‹é¢çš„åŒ¹é…æ ‡å‡†ï¼Œé‡æ–°è¿›è¡Œæ›´ç²¾ç¡®çš„åŒ¹é…ã€‚

---

"""
        prompt = feedback_section + prompt

    try:
        # Use chat.completions.parse with Pydantic model for structured output
        completion = client.chat.completions.parse(
            model="gpt-5",
            messages=[
                {"role": "system", "content": system_msg},
                {"role": "user", "content": prompt}
            ],
            response_format=MatchResult,
        )

        # Get the parsed result directly from Pydantic model
        match_result = completion.choices[0].message.parsed

        # Convert Pydantic model to dict
        return {
            "matched_teams": [
                {
                    "bu": team.bu,
                    "focus": team.focus,
                    "reason": team.reason
                }
                for team in match_result.matched_teams
            ]
        }

    except Exception as e:
        print(f"Error matching session '{session.get('title', 'N/A')}': {str(e)}")
        return {"matched_teams": []}


def format_matched_results(matched_teams: List[Dict]) -> Tuple[str, str, str]:
    """
    Format matched teams into three strings for CSV columns

    Args:
        matched_teams: List of matched team dicts (already sorted by relevance)

    Returns:
        Tuple of (team_names, focuses, reasons) formatted with semicolon separators
    """
    if not matched_teams:
        return "", "", ""

    # Deduplicate teams while preserving order (first occurrence wins)
    seen_bus = set()
    unique_teams = []
    for team in matched_teams:
        bu = team['bu']
        if bu not in seen_bus:
            seen_bus.add(bu)
            unique_teams.append(team)

    # Limit to top 3 most relevant teams (already sorted by GPT-5)
    unique_teams = unique_teams[:3]

    team_names = "; ".join([team['bu'] for team in unique_teams])
    focuses = "; ".join([f"{team['bu']}: {team['focus']}" for team in unique_teams])
    reasons = "; ".join([f"{team['bu']}: {team['reason']}" for team in unique_teams])

    return team_names, focuses, reasons


def parse_old_matches(team_names_str: str, focuses_str: str, reasons_str: str) -> List[Dict]:
    """
    Parse old matches from CSV columns back to matched_teams format

    Args:
        team_names_str: String like "BU1; BU2; BU3"
        focuses_str: String like "BU1: focus1; BU2: focus2; BU3: focus3"
        reasons_str: String like "BU1: reason1; BU2: reason2; BU3: reason3"

    Returns:
        List of matched team dicts with keys: bu, focus, reason
    """
    if not team_names_str or pd.isna(team_names_str) or team_names_str.strip() == "":
        return []

    # Parse team names
    team_names = [name.strip() for name in str(team_names_str).split(';') if name.strip()]

    # Parse focuses into dict
    focuses_dict = {}
    if focuses_str and not pd.isna(focuses_str):
        for item in str(focuses_str).split(';'):
            if ':' in item:
                bu, focus = item.split(':', 1)
                focuses_dict[bu.strip()] = focus.strip()

    # Parse reasons into dict
    reasons_dict = {}
    if reasons_str and not pd.isna(reasons_str):
        for item in str(reasons_str).split(';'):
            if ':' in item:
                bu, reason = item.split(':', 1)
                reasons_dict[bu.strip()] = reason.strip()

    # Reconstruct matched_teams list
    matched_teams = []
    for bu in team_names:
        matched_teams.append({
            'bu': bu,
            'focus': focuses_dict.get(bu, ''),
            'reason': reasons_dict.get(bu, '')
        })

    return matched_teams


def scoring_pass(client: OpenAI, teams: List[Dict[str, str]], sessions_df: pd.DataFrame) -> None:
    """
    First pass: Score all session-team combinations and save to scored CSV

    Args:
        client: OpenAI client instance
        teams: List of team profiles with context
        sessions_df: DataFrame containing session information
    """
    # Define output file for scored results
    SCORED_OUTPUT = OUTPUT_CSV_FILE.replace('.csv', '_scored.csv')

    print("\n" + "=" * 80)
    print("SCORING PASS: Evaluating all session-team combinations")
    print("=" * 80)

    # Check if scored file exists (for resume capability)
    start_idx = 0
    if os.path.exists(SCORED_OUTPUT):
        print(f"\n[Notice] Scored file {SCORED_OUTPUT} already exists.")
        scored_df = pd.read_csv(SCORED_OUTPUT)
        start_idx = len(scored_df)
        if start_idx >= len(sessions_df) * len(teams):
            print(f"[Notice] All combinations already scored. Delete {SCORED_OUTPUT} to restart.")
            return
        print(f"[Notice] Resuming from combination {start_idx + 1}/{len(sessions_df) * len(teams)}")
    else:
        # Initialize scored CSV with header
        output_columns = list(sessions_df.columns) + [
            'å›¢é˜ŸBU', 'å…³é”®è¯å¾—åˆ†', 'ç›´æ¥æ€§å¾—åˆ†', 'ç›¸å…³æ€§å¾—åˆ†', 'æ€»åˆ†', 'è¯„åˆ†ç†ç”±'
        ]
        pd.DataFrame(columns=output_columns).to_csv(SCORED_OUTPUT, index=False, encoding='utf-8-sig')
        print(f"[Step 1] Created scored file: {SCORED_OUTPUT}")

    # Calculate all combinations
    total_combinations = len(sessions_df) * len(teams)
    current_combination = 0

    print(f"\n[Step 2] Scoring {len(sessions_df)} sessions Ã— {len(teams)} teams = {total_combinations} combinations...")
    print(f"Output will be saved to: {SCORED_OUTPUT}")

    # Iterate through all session-team combinations
    for session_idx, row in sessions_df.iterrows():
        session = row.to_dict()
        session_title = session.get('title', 'N/A')[:60]

        for team in teams:
            current_combination += 1

            # Skip if already processed (resume logic)
            if current_combination <= start_idx:
                continue

            team_bu = team['bu']
            print(f"\n[{current_combination}/{total_combinations}] Scoring: {session_title} Ã— {team_bu}")

            # Call score_match to get scores
            score_result = score_match(session, team, client)

            # Create output row
            output_row = row.to_dict()
            output_row['å›¢é˜ŸBU'] = team_bu
            output_row['å…³é”®è¯å¾—åˆ†'] = score_result['keyword_score']
            output_row['ç›´æ¥æ€§å¾—åˆ†'] = score_result['directness_score']
            output_row['ç›¸å…³æ€§å¾—åˆ†'] = score_result['relevance_score']
            output_row['æ€»åˆ†'] = score_result['total_score']
            output_row['è¯„åˆ†ç†ç”±'] = score_result['score_reasoning']

            # Append to CSV immediately
            pd.DataFrame([output_row]).to_csv(
                SCORED_OUTPUT,
                mode='a',
                header=False,
                index=False,
                encoding='utf-8-sig'
            )

            print(f"  âœ“ å…³é”®è¯:{score_result['keyword_score']} ç›´æ¥æ€§:{score_result['directness_score']} ç›¸å…³æ€§:{score_result['relevance_score']} â†’ æ€»åˆ†:{score_result['total_score']}")
            print(f"  ğŸ’¾ Saved to {SCORED_OUTPUT}")

            # Rate limiting
            time.sleep(0.3)

    # Summary
    print("\n" + "=" * 80)
    print("Scoring Complete!")
    print("=" * 80)
    print(f"Total combinations scored: {total_combinations}")
    print(f"\nScored results saved to: {SCORED_OUTPUT}")
    print("=" * 80)
    print("\nNext step: Run filtering pass with --filter flag")
    print(f"Example: python {__file__} --filter --min-score 6.0 --max-ratio 0.33")


def apply_allocation_constraints(scored_df: pd.DataFrame, max_ratio: float = 0.33) -> pd.DataFrame:
    """
    Apply 33% allocation constraint: each team can have at most max_ratio of total sessions

    Args:
        scored_df: DataFrame with scored matches (already filtered by min_score)
        max_ratio: Maximum allocation ratio per team (default 0.33 = 33%)

    Returns:
        Filtered DataFrame with allocation constraints applied
    """
    print("\n" + "=" * 80)
    print(f"Applying Allocation Constraints (max {max_ratio*100:.0f}% per team)")
    print("=" * 80)

    # Calculate max sessions per team
    total_sessions = scored_df['title'].nunique()
    max_per_team = int(total_sessions * max_ratio)

    print(f"\nTotal unique sessions: {total_sessions}")
    print(f"Max sessions per team: {max_per_team} ({max_ratio*100:.0f}%)")

    # Group by team and apply constraints
    constrained_rows = []
    team_stats = []

    for team_bu in scored_df['å›¢é˜ŸBU'].unique():
        team_df = scored_df[scored_df['å›¢é˜ŸBU'] == team_bu].copy()
        original_count = len(team_df)

        # Sort by total score descending
        team_df = team_df.sort_values('æ€»åˆ†', ascending=False)

        # Apply constraint
        if original_count > max_per_team:
            team_df = team_df.head(max_per_team)
            status = f"æˆªæ–­ ({original_count} â†’ {max_per_team})"
        else:
            status = f"æœªè¶…é™ ({original_count})"

        constrained_rows.append(team_df)
        team_stats.append({
            'team': team_bu,
            'original': original_count,
            'final': len(team_df),
            'status': status
        })

        print(f"[{team_bu}] {status}")

    # Combine all constrained teams
    result_df = pd.concat(constrained_rows, ignore_index=True)

    # Summary
    print("\n" + "=" * 80)
    print("Allocation Constraints Applied")
    print("=" * 80)
    print(f"Total matches before: {len(scored_df)}")
    print(f"Total matches after: {len(result_df)}")
    print(f"Filtered out: {len(scored_df) - len(result_df)}")

    return result_df


def filtering_pass(client: OpenAI, teams: List[Dict[str, str]], min_score: float = 6.0, max_ratio: float = 0.33) -> None:
    """
    Second pass: Filter by score, apply allocation constraints, and generate recommendations

    Args:
        client: OpenAI client instance
        teams: List of team profiles with context
        min_score: Minimum score threshold (default 6.0)
        max_ratio: Maximum allocation ratio per team (default 0.33)
    """
    # Define file paths
    SCORED_INPUT = OUTPUT_CSV_FILE.replace('.csv', '_scored.csv')
    FILTERED_OUTPUT = OUTPUT_CSV_FILE.replace('.csv', '_filtered.csv')

    print("\n" + "=" * 80)
    print(f"FILTERING PASS: Score threshold={min_score}, Max ratio={max_ratio*100:.0f}%")
    print("=" * 80)

    # Check if scored file exists
    if not os.path.exists(SCORED_INPUT):
        print(f"\nError: Scored file {SCORED_INPUT} not found.")
        print("Please run scoring pass first: python match_teams_to_sessions.py --score-only")
        return

    # Load scored results
    print(f"\n[Step 1] Loading scored results from {SCORED_INPUT}...")
    scored_df = pd.read_csv(SCORED_INPUT)
    print(f"  Loaded {len(scored_df)} scored combinations")

    # Step 2: Filter by minimum score
    print(f"\n[Step 2] Filtering by minimum score >= {min_score}...")
    high_score_df = scored_df[scored_df['æ€»åˆ†'] >= min_score].copy()
    print(f"  High-score matches: {len(high_score_df)} (filtered out {len(scored_df) - len(high_score_df)})")

    if len(high_score_df) == 0:
        print(f"\nNo matches found with score >= {min_score}. Adjust --min-score threshold.")
        return

    # Step 3: Apply allocation constraints
    print(f"\n[Step 3] Applying allocation constraints...")
    constrained_df = apply_allocation_constraints(high_score_df, max_ratio)

    # Step 4: Group by session and generate final matched teams format
    print(f"\n[Step 4] Generating recommendations for {len(constrained_df)} final matches...")

    # Initialize output CSV
    filtered_columns = ['title', 'type', 'date', 'time', 'location', 'abstract', 'overview',
                       'åŒ¹é…å›¢é˜Ÿ', 'å…³æ³¨æ–¹å‘', 'æ¨èç†ç”±', 'æ€»åˆ†']
    pd.DataFrame(columns=filtered_columns).to_csv(FILTERED_OUTPUT, index=False, encoding='utf-8-sig')

    # Group by session
    sessions_with_teams = constrained_df.groupby('title')

    total_sessions = len(sessions_with_teams)
    processed = 0

    for session_title, group_df in sessions_with_teams:
        processed += 1
        print(f"\n[{processed}/{total_sessions}] Processing: {session_title[:60]}...")

        # Get session info from first row
        session_row = group_df.iloc[0]
        session = session_row.to_dict()

        # Get teams for this session (sorted by score descending)
        matched_teams_info = []
        for _, row in group_df.sort_values('æ€»åˆ†', ascending=False).iterrows():
            team_bu = row['å›¢é˜ŸBU']
            # Find team details
            team_details = next((t for t in teams if t['bu'] == team_bu), None)
            if team_details:
                matched_teams_info.append({
                    'bu': team_bu,
                    'focus': team_details['focus'],
                    'score': row['æ€»åˆ†']
                })

        # Limit to top 3 teams per session
        matched_teams_info = matched_teams_info[:3]

        # Generate recommendation reasons using matching prompt
        # Create a list of matched teams to pass to match_session_to_teams
        teams_for_this_session = [next((t for t in teams if t['bu'] == tm['bu']), None)
                                  for tm in matched_teams_info]
        teams_for_this_session = [t for t in teams_for_this_session if t is not None]

        if teams_for_this_session:
            # Call match_session_to_teams to get detailed reasons
            match_result = match_session_to_teams(session, teams_for_this_session, client)
            matched_teams_with_reasons = match_result.get('matched_teams', [])

            # Format results
            team_names, focuses, reasons = format_matched_results(matched_teams_with_reasons)

            # Calculate average score for this session
            avg_score = group_df['æ€»åˆ†'].mean()

            # Create output row
            output_row = {
                'title': session.get('title'),
                'type': session.get('type'),
                'date': session.get('date'),
                'time': session.get('time'),
                'location': session.get('location'),
                'abstract': session.get('abstract'),
                'overview': session.get('overview'),
                'åŒ¹é…å›¢é˜Ÿ': team_names,
                'å…³æ³¨æ–¹å‘': focuses,
                'æ¨èç†ç”±': reasons,
                'æ€»åˆ†': f"{avg_score:.1f}"
            }

            # Append to CSV
            pd.DataFrame([output_row]).to_csv(
                FILTERED_OUTPUT,
                mode='a',
                header=False,
                index=False,
                encoding='utf-8-sig'
            )

            print(f"  âœ“ Matched {len(matched_teams_with_reasons)} team(s): {team_names}")
            print(f"  ğŸ’¾ Saved to {FILTERED_OUTPUT}")

        # Rate limiting
        time.sleep(0.5)

    # Summary
    print("\n" + "=" * 80)
    print("Filtering Complete!")
    print("=" * 80)
    print(f"Total sessions with matches: {total_sessions}")
    print(f"\nFiltered results saved to: {FILTERED_OUTPUT}")
    print("=" * 80)


def review_existing_matches(client: OpenAI, teams: List[Dict[str, str]], sessions_df: pd.DataFrame) -> None:
    """
    Review and update existing matches with incremental write to OUTPUT_REVIEW_FILE

    Args:
        client: OpenAI client instance
        teams: List of team profiles with context
        sessions_df: DataFrame containing session information
    """
    if not os.path.exists(OUTPUT_CSV_FILE):
        print(f"Error: Output file {OUTPUT_CSV_FILE} not found. Please run normal mode first.")
        return

    print("\n" + "=" * 80)
    print("REVIEW MODE: Re-evaluating existing matches (Incremental Write)")
    print("=" * 80)

    # Load existing results
    existing_df = pd.read_csv(OUTPUT_CSV_FILE)
    print(f"\nLoaded {len(existing_df)} existing matches from {OUTPUT_CSV_FILE}")

    # Check if review file exists (for resume capability)
    start_idx = 0
    if os.path.exists(OUTPUT_REVIEW_FILE):
        print(f"\n[Notice] Review file {OUTPUT_REVIEW_FILE} already exists.")
        reviewed_df = pd.read_csv(OUTPUT_REVIEW_FILE)
        start_idx = len(reviewed_df)
        if start_idx >= len(existing_df):
            print(f"[Notice] All sessions already reviewed. Delete {OUTPUT_REVIEW_FILE} to restart.")
            return
        print(f"[Notice] Resuming review from session {start_idx + 1}/{len(existing_df)}")
    else:
        # Initialize review CSV with header
        output_columns = list(existing_df.columns)
        pd.DataFrame(columns=output_columns).to_csv(OUTPUT_REVIEW_FILE, index=False, encoding='utf-8-sig')
        print(f"[Step 3] Created review file: {OUTPUT_REVIEW_FILE}")

    # Track statistics
    total_changed = 0
    total_rematched = 0
    total_confirmed = 0

    # Process each row from start_idx
    for idx in range(start_idx, len(existing_df)):
        row = existing_df.iloc[idx]
        session = row.to_dict()
        session_title = session.get('title', 'N/A')[:60]
        old_teams = str(row.get('åŒ¹é…å›¢é˜Ÿ', ''))

        print(f"\n[{idx+1}/{len(existing_df)}] Reviewing: {session_title}...")
        print(f"  Original: {old_teams if old_teams else 'None'}")

        # Step 1: Review decision - check if rematch is needed
        review_decision = review_match_decision(session, teams, old_teams, client)
        needs_rematch = review_decision['needs_rematch']
        review_notes = review_decision['review_notes']

        print(f"  Review: {'éœ€è¦é‡æ–°åŒ¹é…' if needs_rematch else 'åŒ¹é…ç¬¦åˆæ ‡å‡†'}")
        print(f"  ç†ç”±: {review_notes[:80]}...")

        # Step 2: Conditional rematch
        if needs_rematch:
            # Rematch with review feedback
            match_result = match_session_to_teams(
                session, teams, client,
                review_feedback=review_notes
            )
            matched_teams = match_result.get('matched_teams', [])
            print(f"  âœï¸  æ‰§è¡Œé‡æ–°åŒ¹é…...")
        else:
            # Keep original matches
            old_focuses = str(row.get('å…³æ³¨æ–¹å‘', ''))
            old_reasons = str(row.get('æ¨èç†ç”±', ''))
            matched_teams = parse_old_matches(old_teams, old_focuses, old_reasons)
            print(f"  âœ“  ä¿ç•™åŸåŒ¹é…")

        # Format new results
        new_team_names, new_focuses, new_reasons = format_matched_results(matched_teams)

        # Update row with new results
        output_row = row.to_dict()
        output_row['åŒ¹é…å›¢é˜Ÿ'] = new_team_names
        output_row['å…³æ³¨æ–¹å‘'] = new_focuses
        output_row['æ¨èç†ç”±'] = new_reasons

        # Append to review CSV immediately
        pd.DataFrame([output_row]).to_csv(
            OUTPUT_REVIEW_FILE,
            mode='a',
            header=False,
            index=False,
            encoding='utf-8-sig'
        )

        # Update statistics
        if needs_rematch:
            total_rematched += 1
            if new_team_names != old_teams:
                total_changed += 1
                print(f"  ç»“æœ: {new_team_names if new_team_names else 'None'}")
            else:
                print(f"  ç»“æœ: é‡æ–°åŒ¹é…åç»“æœç›¸åŒ")
        else:
            total_confirmed += 1

        print(f"  ğŸ’¾ Saved to {OUTPUT_REVIEW_FILE}")

        # Rate limiting
        time.sleep(0.5)

    # Summary
    print("\n" + "=" * 80)
    print("Review Complete!")
    print("=" * 80)
    print(f"Total sessions reviewed: {len(existing_df)}")
    print(f"  - Rematched (éœ€è¦é‡æ–°åŒ¹é…): {total_rematched}")
    print(f"  - Confirmed (ä¿ç•™åŸåŒ¹é…): {total_confirmed}")
    print(f"  - Actually changed: {total_changed}")
    print(f"\nReviewed results saved to: {OUTPUT_REVIEW_FILE}")
    print("=" * 80)


def main():
    """Main execution function"""
    # Parse command line arguments
    parser = argparse.ArgumentParser(description='Session-Team Matching System')
    parser.add_argument('--review', action='store_true',
                       help='Review and update existing matches')
    parser.add_argument('--score-only', action='store_true',
                       help='Run scoring pass only (evaluate all session-team combinations)')
    parser.add_argument('--filter', action='store_true',
                       help='Run filtering pass (filter by score and generate recommendations)')
    parser.add_argument('--min-score', type=float, default=6.0,
                       help='Minimum score threshold for filtering (default: 6.0)')
    parser.add_argument('--max-ratio', type=float, default=0.33,
                       help='Maximum allocation ratio per team (default: 0.33 = 33%%)')
    args = parser.parse_args()

    # Check for OpenAI API key
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("Error: OPENAI_API_KEY environment variable not set")
        print("Please set it using: export OPENAI_API_KEY='your-api-key'")
        return

    # Initialize OpenAI client
    client = OpenAI(api_key=api_key)

    print("=" * 80)
    print("Session-Team Matching System (Incremental Write Mode)")
    print("=" * 80)

    # Step 1: Parse research interests
    print("\n[Step 1] Parsing research interests...")
    teams = parse_research_interests(RESEARCH_INTEREST_FILE)
    print(f"Found {len(teams)} teams:")
    for team in teams:
        print(f"  - {team['bu']}: {team['focus']}")

    # Step 1.5: Add BU background context
    print("\n[Step 1.5] Adding BU background context...")
    for team in teams:
        team['context'] = get_bu_context(team['bu'])
        print(f"  - {team['bu']}: {team['context'][:50]}...")

    # Step 2: Load sessions CSV
    print(f"\n[Step 2] Loading sessions from {SESSIONS_CSV_FILE}...")
    sessions_df = pd.read_csv(SESSIONS_CSV_FILE)
    print(f"Found {len(sessions_df)} sessions")

    # Check mode and dispatch
    if args.score_only:
        # Run scoring pass only
        scoring_pass(client, teams, sessions_df)
        return
    elif args.filter:
        # Run filtering pass
        filtering_pass(client, teams, min_score=args.min_score, max_ratio=args.max_ratio)
        return
    elif args.review:
        # Run review mode
        review_existing_matches(client, teams, sessions_df)
        return

    # Step 3: Check if output file exists (for resume capability)
    start_idx = 0

    if os.path.exists(OUTPUT_CSV_FILE):
        print(f"\n[Notice] Output file {OUTPUT_CSV_FILE} already exists.")
        existing_df = pd.read_csv(OUTPUT_CSV_FILE)
        start_idx = len(existing_df)
        if start_idx >= len(sessions_df):
            print(f"[Notice] All sessions already processed. Delete {OUTPUT_CSV_FILE} to restart.")
            return
        print(f"[Notice] Resuming from session {start_idx + 1}/{len(sessions_df)}")
    else:
        # Initialize output CSV with header
        output_columns = list(sessions_df.columns) + ['åŒ¹é…å›¢é˜Ÿ', 'å…³æ³¨æ–¹å‘', 'æ¨èç†ç”±']
        pd.DataFrame(columns=output_columns).to_csv(OUTPUT_CSV_FILE, index=False, encoding='utf-8-sig')
        print(f"[Step 3] Created output file: {OUTPUT_CSV_FILE}")

    # Step 4: Match each session to teams and write incrementally
    print(f"\n[Step 4] Matching sessions to teams (writing incrementally)...")
    total_matches = 0

    for idx in range(start_idx, len(sessions_df)):
        row = sessions_df.iloc[idx]
        session = row.to_dict()
        session_title = session.get('title', 'N/A')[:60]

        print(f"  [{idx+1}/{len(sessions_df)}] Processing: {session_title}...")

        # Call OpenAI API
        match_result = match_session_to_teams(session, teams, client)
        matched_teams = match_result.get('matched_teams', [])

        # Format results
        team_names, focuses, reasons = format_matched_results(matched_teams)

        # Create output row
        output_row = row.to_dict()
        output_row['åŒ¹é…å›¢é˜Ÿ'] = team_names
        output_row['å…³æ³¨æ–¹å‘'] = focuses
        output_row['æ¨èç†ç”±'] = reasons

        # Append to CSV immediately
        pd.DataFrame([output_row]).to_csv(
            OUTPUT_CSV_FILE,
            mode='a',
            header=False,
            index=False,
            encoding='utf-8-sig'
        )

        if matched_teams:
            print(f"      âœ“ Matched {len(matched_teams)} team(s): {team_names}")
            print(f"      ğŸ’¾ Saved to CSV")
            total_matches += 1
        else:
            print(f"      - No matches")
            print(f"      ğŸ’¾ Saved to CSV")

        # Small delay to avoid rate limiting
        time.sleep(0.5)

    # Summary statistics
    print("\n" + "=" * 80)
    print("Matching Complete!")
    print("=" * 80)
    print(f"Total sessions processed: {len(sessions_df)}")
    print(f"Sessions with matches: {total_matches}")
    print(f"Sessions without matches: {len(sessions_df) - total_matches}")
    print(f"\nOutput saved to: {OUTPUT_CSV_FILE}")
    print("=" * 80)


if __name__ == "__main__":
    main()
