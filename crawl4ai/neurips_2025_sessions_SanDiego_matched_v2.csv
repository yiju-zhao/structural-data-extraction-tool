date,time,type,title,url,speaker,end_time,abstract,overview,匹配团队,关注方向,推荐理由
TUE 2 DEC,8:30 a.m.,Expo Talk Panel,Telling Stories at Scale: Multimodal ML in the Global Media Landscape,https://neurips.cc/virtual/2025/128652,,9:30 AM,"Netflix has long been recognized as a pioneer in personalization, leveraging member preference data to recommend the most engaging shows, movies, and games. In recent years, we have expanded our use of machine learning and data-driven approaches to support a wide array of upstream creative and operational workflows. In this talk, we will discuss how modern AI methods – contrastive learning, transformers, cross-modal retrieval, graph neural networks, etc. – are transforming the curation, localization, promotion, and launch of stories on a global scale. A distinctive aspect of our work is the integration of highly creative assets—text, images, video, and speech—alongside traditional tabular datasets. We will highlight unique challenges that arise at the intersection of multimedia, personalization, and web-scale products, and share how advanced ML/AI techniques are addressing these challenges to connect great stories with our worldwide audience.",,海思; 诺亚; 温哥华云,海思: 大语言模型和扩散模型的加速; 诺亚: 大模型长序列、多模态长序列; 温哥华云: Reinforcement fine tuning (RFT),海思: Session 直接涉及多模态大模型的加速和低延迟推理需求，讨论跨模态融合、统一特征空间、以及面向海量多模态内容的高效推理与部署，和团队在“多模态、低延迟、长上下文”方面的难题高度契合。具体包含对文本-图像-视频/语音等多模态数据的整合，以及在 web-scale 产品中提升模型推理和服务速度的方法。; 诺亚: Session 讨论的规模化、多模态 ML 在全球媒体生态中的应用，需要处理长视频和跨模态信息的上下文依赖。团队的研究方向聚焦于长序列多模态建模、记忆更新与跨模态一致性的挑战，与该 session 所涉及的面向大规模多模态内容的长序列建模和跨模态对齐理念直接相关。; 温哥华云: 会场强调把现代 ML 技术落地到生产、解决理论与生产之间的差距、提升样本效率与可扩展性。这与团队在 VLMs/LLMs 的强化学习微调、产线化部署、以及面向海量真实数据的高效策略微调的需求高度契合， session 提供的关于实际生产场景下的技术要点和经验对该难题有直接帮助。
TUE 2 DEC,8:30 a.m.,Expo Talk Panel,Data Scout: “From Prompt to Corpus: Accelerating Domain-Specific Data Collection with LLMGuided Discovery”,https://neurips.cc/virtual/2025/128656,,9:30 AM,"Training large language models for specialized disciplines such as advanced mathematics, molecular biology, or legal reasoning is limited by the scarcity of large, high quality, domain specific corpora. Most publicly available datasets are dominated by general purpose web text. When available, specialized data are fragmented across diverse sources such as preprints, conference papers, forums, lecture notes, and digitized books. No single source offers comprehensive real-world coverage across scientific domains. Consequently, scaling up authentic domain data remains a bottleneck: collecting a subset of relevant tokens often requires downloading and filtering hundreds of terabytes of raw web material, a process that is both time consuming and costly.x000Dx000DWe introduce Data Scout, a modular, LLM powered pipeline that turns a high-level user intent (e.g., “I need data for advanced mathematics”) into a vetted, list of seed URLs in minutes. The system first expands the original intent using an LLM that generates a hierarchical subgraph of related concepts; this taxonomy drives a diversified set of search queries that systematically cover the target domain while respecting known licensing signals. Candidate URLs are then filtered by the same LLM using chain of thought prompting based on topical relevance, licensing clarity, and crawlability. Our results show that that the list of selected candidate URLs when crawled can yield a high percentage of relevant pages (40%+) related to the user’s intended topic or query, compared to less than 1 percent in general web-scale corpora. Data Scout is available with both CLI and GUI front ends. By democratizing domain specific data acquisition, Data Scout enables researchers without dedicated crawling infrastructure to bootstrap large, high-fidelity corpora, accelerating the development of specialized LLMs across various niche domains fields.",,温哥华云; 存储; 诺亚,温哥华云: Reinforcement fine tuning (RFT); 存储: 向量 DB 对于存储系统的访问模式; 诺亚: 大模型长序列，多模态长序列,温哥华云: Data Scout 提供端到端领域特定数据获取管道，能够在短时间内产出高质量、许可清晰的领域语料，直接支持对 VLMs/LLMs 的 RLHF/微调，提升领域适应性和样本效率；讲座中关于意图扩展、分层子图、许可筛选等环节与这类数据获取需求存在直接技术对接点。; 存储: Data Scout 的数据获取与元数据管理（许可、数据源、覆盖范围、crawlability）将影响领域数据的存储、索引与检索需求；结合讲座中对大规模领域数据的组织、缓存和可抓取性的思考，能为向量数据库访问模式、MemOS 调度、数据布局优化等难题提供明确的技术对接点。; 诺亚: Data Scout 提供跨领域的长文本、跨源数据（论文、论坛、讲义、书籍等）来源，有助于丰富长序列/多模态模型的训练与评估语料，直接关联到诺亚在长序列推理与跨模态建模方面的研究需求。
TUE 2 DEC,8:30 a.m.,Expo Talk Panel,Multimodal Data Foundation at Industry-Scale,https://neurips.cc/virtual/2025/128659,,9:30 AM,"Pre-training is fundamental to foundation models, enabling them to acquire broad knowledge that gives rise to emerging capabilities at later training stages, and scaling is the key for pre-training. In this talk, we present a recipe for building and curating pre-training, multimodal image-text paired data from scratch on a global scale, enabling mutual benefits between English and non-English data. We would like to share our key observations and insights with the community on: (1) why scaling matters, including the foundational role of data and key principles to hold for scaling; (2) how to design simple yet scalable data algorithms that enable industry-scale data collection and training without data filters, serving both research and production needs; (3) how the scaling improves Meta’s products at conventional and frontier machine learning areas. Submission is facilitated by Cogs & Marvel but is entirely organized, executed, and implemented by Meta.",,存储; 诺亚,存储: 模型训练; 诺亚: 大模型长序列，多模态长序列,存储: Session 讲述的在全球规模上构建与整理用于预训练的多模态图像-文本数据，以及无需数据过滤的可扩展数据算法，直接回应存储团队在大规模训练数据输入阶段的 IO/内存访问模式与数据布局难题。其对数据管线、数据布局、预取与分片等可扩展数据处理策略的探讨，将为提升训练数据吞吐、降低存储与数据传输瓶颈提供具体思路。; 诺亚: 该Session聚焦在行业规模上的多模态数据基础与数据扩展能力，包括为预训练基础模型设计可扩展的数据获取/整理方法。对于诺亚团队的长序列、多模态文本视觉数据的训练与推理瓶颈（如内存/缓存管理、输入范式的长上下文处理）具有直接技术相关性：可提供面向长序列的高效数据管线设计、数据质量与规模平衡，以及跨语言多模态数据的规模化采集策略。
TUE 2 DEC,8:30 a.m.,Expo Talk Panel,Toward General Full Autonomy: Open Research Challenges in Scalable Self-Driving,https://neurips.cc/virtual/2025/128661,,9:30 AM,"Speaker: Ben Snyder, Director of AI Research for Autonomous Vehicles, General Motorsx000Dx000DGM’s driver-assistance systems now power millions of vehicles across North America, logging over half a billion hands-free miles with zero reported crashes—demonstrating the safety and scalability of autonomy at unprecedented consumer scale. Following GM’s acquisition of Cruise earlier this year, the combined teams now bring together a decade of experience to accelerate progress toward full, generalized autonomy. This talk will dive into the open research challenges ahead—from model architecture and the balance between imitation and reinforcement learning, to leveraging vision-language models for long-tail, common-sense reasoning, and advancing the training, deployment, and simulation infrastructure needed to scale truly generalized autonomy.",,CBG; 多伦多云; 诺亚,CBG: 3DAIGC; 多伦多云: AI Agent; 诺亚: 大模型长序列、多模态长序列,"CBG: Session discusses open challenges in generalized autonomy including model architecture, imitation vs reinforcement learning, vision-language models for long-tail common-sense reasoning, and scaling via training/deployment/simulation infrastructure. CBG's work directly maps to these needs: (1) fast and accurate segmentation of moving objects in video, (2) monocular-video-based 3D/4D sparse reconstruction (point cloud, pose), (3) video-driven camera trajectory generation, and (4) ensuring long video consistency. These topics align with the session's emphasis on perception, long-term reasoning, and scalable autonomous systems.; 多伦多云: Session focuses on scalable autonomy challenges including evaluation accuracy and coverage of AI agents, reliability of agent behaviors, and autonomous learning and continuous optimization post-deployment. 这与多伦多云的 AI Agent 研究方向高度契合，尤其是在提升评估准确性与覆盖范围、提高行为可靠性，以及上线后的自主学习与持续优化机制方面，能为自驾决策代理的鲁棒性、规划能力和长期性能提供具体的技术启发和实验设计思路。; 诺亚: Session explicitly mentions leveraging vision-language models for long-tail, common-sense reasoning. 诺亚的研究聚焦于大模型长序列和多模态长序列，覆盖长上下文处理、跨模态对齐与推理、以及推理中的记忆管理等，能够直接支撑自驾场景中的长时序感知、跨模态推理与一致性维护，提供实现长尾推理与跨模态协同的具体技术路径。"
TUE 2 DEC,8:30 a.m.,Expo Talk Panel,Beyond Benchmarks: Rethinking Reasoning in Language Models,https://neurips.cc/virtual/2025/128665,,9:30 AM,"Reasoning is often described as the next frontier for AI, but what does it really mean for a model to “reason”, and how should we measure it? Popular benchmarks like GSM8K suggest steady progress, yet controlled studies reveal that models can fail dramatically under small changes—such as swapping numbers or adding irrelevant details. Large Reasoning Models (LRMs), which generate explicit chains of thought, raise new optimism but also expose clear limits: they often underperform standard models on simple tasks, improve briefly at medium complexity, and then collapse on harder ones despite having unused compute. Crucially, reasoning is not the same as knowledge recall, tool use, or agent-like behavior. True reasoning involves solving novel problems, decomposing them into steps, generalizing to new contexts, recombining partial results, and finally generating novel hypotheses—capabilities current systems largely lack. Today’s evaluations, focused on final answers and contaminated benchmarks, risk giving a misleading sense of progress. This talk will provide a critical review of reasoning in language models, highlight why current evaluations can be deceptive, and emphasize that reasoning is not just about “what"" models answer, but “how"" they solve problems.",,多伦多云; 诺亚,多伦多云: AI Agent：评估的准确性与覆盖范围、推理能力、计划与多步执行能力的评估框架; 诺亚: 大模型长序列，多模态长序列,多伦多云: session 的核心是对推理能力的评估方法、过程级别的推理与解题步骤的分析，以及避免将最终答案作为衡量唯一标准。该内容直接对应你们在提升 AI Agent 评估的准确性与覆盖范围、划分评估子项以覆盖推理能力与领域知识的需求。通过引入解题过程分解、跨场景泛化与中间推理轨迹的一致性检查等要点，可以为你们的评估框架提供明确的流程级指标与测试用例，帮助提升 Agent 行为的可解释性、可重复性与鲁棒性。; 诺亚: session 强调对语言模型推理过程的批判性评估，以及如何衡量模型在解题中的“如何做”而非仅给出最终答案。对于长序列、多模态长序列任务，这种关注过程与步骤的评估视角尤为重要，因为长上下文下的推理需要对中间状态、跨模态信息的记忆与重组进行一致性检验。该 talk 的“分步解题、在新场景中的泛化、重新组合局部结果”等要点可直接用于设计面向长时间跨度推理的基准、任务设计和评测指标，帮助你们在多模态长序列中评估与提升推理能力。
TUE 2 DEC,8:30 a.m.,Expo Talk Panel,Foundational Generative Recommendations for E-Commerce,https://neurips.cc/virtual/2025/128667,,9:30 AM,"Modern commerce platforms face the challenge of delivering personalized recommendations across billions of items to users with diverse intents, temporal dynamics, and cold-start scenarios. We present a generative foundation model for commerce built on Hierarchical Sequential Transduction Units (HSTU) that integrates Liquid Foundation Models (LFM) for rich textual product representations and custom CUDA kernels developed in collaboration with Nvidia for efficient training and online serving. Our approach demonstrates that generative methods unlock substantial gains through three key innovations: (1) large-scale contrastive learning with hard negative sampling; (2) temporal mechanisms that fuse multi-scale time signals (session, day, season) with commerce-specific features; and (3) optimized training and inference kernels. By treating LFM embeddings as auxiliary features rather than end-to-end fine-tuning, we maintain modularity while capturing semantic relationships. While results are promising, significant challenges remain in handling non-stationary preferences, multi-objective optimization, and ensuring fairness in generative retrieval—we discuss our roadmap toward truly foundational commerce models that generalize across domains and market conditions.",,存储; 计算; 海思,存储: 单/多模型推理; 计算: 训推新范式; 海思: 大语言模型和扩散模型的加速,存储: 会话聚焦于面向商务场景的生成式检索模型（generative retrieval）与向量化产品表示（LFM）嵌入的使用，以及大规模对比学习和多时间尺度的特征融合；这直接涉及需要对向量数据库的存储访问模式、KV缓存加载/卸载、数据布局以及跨模型推理系统的内存调度等问题。对接 NVIDIA 的高效训练/在线推理内核也提供了可操作的优化思路，帮助解决向量检索与多模型推理中的存储/内存瓶颈。; 计算: 会话提出的三项创新（1) 大规模对比学习+硬负采样；2) 将会话/日/季等多尺度时间信号与商务特征融合的时序机制；3) 针对训练和推理的高效内核优化）构成新的训练/推理范式的具体例子。对关注训练推理新范式、理解与预测未来负载分布的团队尤为有价值，可为产学研如何进行负载建模、架构设计和系统优化提供可操作的思路。; 海思: 会话强调了面向文本产品表示的分层/模组化设计、将 LFM 嵌入作为辅助特征、以及在高效训练和在线服务中使用定制的 CUDA 内核等加速策略，这与贵司致力于大语言模型和扩散模型的低延迟实时推理和多模态条件生成的目标高度相关；可为实现端到端加速、降低推理延迟、以及统一多模态特征空间提供具体的实现线索。
TUE 2 DEC,8:30 a.m.,Expo Talk Panel,From Research to Markets: Applying Cutting-Edge Machine Learning in Quantitative Finance,https://neurips.cc/virtual/2025/128670,,9:30 AM,"In modern quantitative finance, the boundaries between academic machine learning research and real-world trading strategies are rapidly dissolving. In this talk, we will share how researchers at G-Research apply advances in deep learning, probabilistic modelling, reinforcement learning, and large-scale optimization to some of the most challenging problems in global financial markets. Our work transforms vast, noisy, and non-stationary data into predictive signals, demanding innovations that extend the state of the art in both theory and practice. This includes sequence modelling for high-frequency data, scalable Bayesian inference for uncertainty quantification, generative modelling for scenario analysis, and reinforcement learning for adaptive portfolio construction. Operating at the intersection of rigorous science and high-performance computing, we thrive in an environment where milliseconds matter and model generalization is paramount.x000Dx000DCome and see how cutting-edge machine learning meets the realities of global financial markets at G-Research.",,温哥华云; 诺亚,温哥华云: Reinforcement fine tuning; 诺亚: 大模型长序列，多模态长序列,温哥华云: 会话的核心议题之一是将强化学习应用于自适应投资组合构建，并强调如何缩短理论研究与生产部署之间的差距、提升样本效率、提升性能与可扩展性。对于贵 BU 的 Reinforcement fine tuning 关注，此次会话提供了在云端环境落地 RL 的具体场景、挑战与解决思路，如生产化阶段的策略微调、跨任务的样本效率提升，以及面向高吞吐量金融工作负载的分布式推理与优化策略。; 诺亚: 会话明确聚焦用于高频数据的序列建模与对不确定性进行量化的可扩展推理方法，以及情景分析的生成建模。这与贵公司在大模型长序列、多模态长序列方向的研究具有直接技术相关性：可借鉴在高频序列建模中的结构设计、记忆与缓存管理，以及在非平稳金融数据上的可扩展贝叶斯推断思路；此外，生成模型用于情景分析的能力也有助于评估和训练长序列模型在真实世界金融环境中的鲁棒性与稳健性。
TUE 2 DEC,8:30 a.m.,Expo Talk Panel,GRAID: Synthetic Data Generation with Geometric Constraints and Multi-Agentic Reflection for Harmful Content Detection,https://neurips.cc/virtual/2025/128668,,9:30 AM,"The rapid integration of LLMs for synthetic data generation offers a powerful solution for data scarcity, a critical issue for training machine learning models. However, the resulting data quality is often questionable, requiring costly and time-consuming manual review. This oversight is especially challenging in the crucial domain of trustworthy and safe AI.x000Dx000DA significant need here is the automated identification of adversarial and harmful inputs, a process known as red-teaming, to improve guardrailing systems before deployment. Developing an effective, fully automated red-teaming approach capable of generating diverse, out-of-domain harmful content has been a long-standing challenge.x000Dx000DTo address data scarcity in harmful text detection & the challenge of automated red-teaming, we introduce GRAID (Geometric & Reflective AI-Driven Data Augmentation), a novel, versatile, & dynamic multi-agent framework.x000Dx000DGRAID operates in two stages:x000D1) Geometric Generation: A constrained, fine-tuned LLM generates geometrically controlled examples to diversify content within the original embedding space.x000D2) Reflective Augmentation: A multi-agentic reflective process promotes stylistic diversity & uncovers difficult edge cases.x000Dx000DThis combination ensures both reliable coverage of the input space & nuanced exploration of harmful content. We demonstrate that augmenting a harmful text classification dataset with GRAID significantly improves the performance of downstream real-world guardrail models. Furthermore, GRAID captures data variability in new geometric domains while preserving data relationships.x000Dx000DWhile initially focused on harmful text detection, GRAID’s modular design makes it an inherently domain-agnostic framework adaptable to various applications beyond classification. In this talk, we will detail how GRAID distinguishes itself from existing solutions, discuss its building blocks, & share insights on its easy adaptation for diverse synthetic data generation needs.",,多伦多云; 温哥华云,多伦多云: AI Agent; 温哥华云: Reinforcement fine tuning (RFT),多伦多云: GRAID 的多智能体反射式增强与几何约束数据生成直接对应 AI Agent 的评估、覆盖与多步骤计划能力的挑战。该会话提出的两阶段方法（几何生成与自省增强）能够系统性地生成多样化、边界性攻击样本以测试和提升代理的鲁棒性、守则遵循与行为可控性，提供明确的评估覆盖率提升路径，符合该团队关于评估准确性、覆盖范围和自主学习持续优化的核心诉求。; 温哥华云: 该 session 聚焦自动化的红队化数据生成和守门机制提升，能为 RLHF/强化微调提供丰富的对抗性数据和边界样本，帮助缩短理论到生产的差距并提升样本效率与可扩展性。GRAID 的自省式多智能体流程能够为生产环境中的对齐和鲁棒性评估提供可操作的训练信号，支持该团队在实际云服务中的应用落地。
TUE 2 DEC,9:30 a.m.,Tutorial,Planning in the Era of Language Models,https://neurips.cc/virtual/2025/109596,,12:00 PM,"For over six decades, the field of automated planning has been at the heart of AI, empowering intelligent systems to reason, act, and achieve goals in complex, dynamic environments. From robotics and logistics to space exploration, planning research has fueled autonomous decision-making in real-world applications.Today, as large language models redefine what’s possible in AI, the principles and methodologies of planning are more vital than ever. The planning community brings decades of experience in designing, benchmarking, and interpreting intelligent behavior; expertise that can accelerate the development of powerful, trustworthy, and general-purpose LLM-based agents.Participants will gain a clear understanding of what planning truly entails, what has been learned (and sometimes forgotten) in the shift toward LLM-based approaches, and how foundational insights from the planning community can inform the creation of stronger, more reliable, and more scalable LLM-powered planners.","Overview: The webpage presents a tutorial titled 'Planning in the Era of Language Models' scheduled for December 2nd, 2025, at NeurIPS in San Diego, CA. The tutorial focuses on the integration of automated planning with large language models (LLMs). It highlights the importance of planning in AI, especially as LLMs redefine AI capabilities. The tutorial aims to provide participants with insights into planning methodologies and how these can enhance the development of LLM-based agents. | Research Interests: Automated planning, Large language models, AI reasoning and decision-making, Planning formalisms and languages, Evaluation of planning tools, Human-aware AI systems",多伦多云,多伦多云: AI Agent,多伦多云: 该session聚焦在将自动化规划与大语言模型结合，探讨规划形式、评估工具以及面向LLM驱动的智能代理的多步执行能力。Team 11 的关注点正是提升AI Agent的评估覆盖度与适用性，以及在具备规划能力的L3+代理中的持续优化。该Tutorial提供的规划方法论、规划形式以及面向规划的代理评估框架，可以直接用于改进其代理的规划集成、可靠性与多步执行能力的评估与优化路径。
TUE 2 DEC,9:30 a.m.,Tutorial,Foundations of Tensor/Low-Rank Computations for AI,https://neurips.cc/virtual/2025/109591,,12:00 PM,,,海思; 计算; 存储,海思: 大语言模型和扩散模型的加速; 计算: 高效模型架构; 存储: 模型训练,海思: 该 session 的“Foundations of Tensor/Low-Rank Computations for AI”主题直接对应大模型推理的加速需求。可能涉及的技术点包括对注意力的低秩近似、张量分解和稀疏/分块注意力等，能为海思在实现低延迟实时推理和多模态条件融合时提供直接的思路与工具。; 计算: 该 session 以张量/低秩计算的基础方法为核心，能帮助理解如何通过低秩张量表示、分解与近似来降低模型训练与推理的计算量与显存压力，进而影响未来模型架构的设计与计算系统需求的预判，尤其与低精度训练与推理的结合相关。; 存储: session 的低秩/张量方法可用于训练阶段的数据与激活/梯度的压缩、数据布局优化和内存带宽缓解，直接对应训练阶段的 IO/内存访问难题，并能为数据布局变更（如激活与权重的低秩分解、高效数据格式）提供具体的技术路径。
TUE 2 DEC,9:30 a.m.,Tutorial,"Human-AI Alignment: Foundations, Methods, Practice, and Challenges",https://neurips.cc/virtual/2025/109592,,12:00 PM,,,多伦多云; 温哥华云; 诺亚,多伦多云: AI Agent; 温哥华云: Reinforcement fine tuning (RFT); 诺亚: 大模型后训练Reasoning，RL,多伦多云: 该 session 聚焦人类-AI 对齐的基础、方法、实践与挑战，直接涵盖 AI 代理的对齐、评估与治理。对于团队提出的提升 AI Agent 评估的准确性与覆盖范围、提升代理行为的可靠性、对代理行为的管控与校验，以及上线后的自主学习与持续优化机制等问题，教程可提供评估框架、测试用例设计，以及人机交互与风险控制的实用方法，具有直接的技术关联性。; 温哥华云: 该教程覆盖将人类偏好对齐的基础与方法，其中包括基于强化学习的对齐方法及其在实际生产环境中的落地挑战。对你们提出的‘闭合理论到生产 gap’、‘样本效率’、‘性能与可扩展性’等问题，教程可以提供系统化的对齐框架、数据高效利用策略，以及从理论到生产的迁移经验与注意事项，具有直接的技术相关性。; 诺亚: 本次 session 的对齐主题涵盖强化学习在对齐中的应用、评估与鲁棒性等内容。对你们的研究方向——大模型后训练的推理能力、 RL、以及潜在推理（latent reasoning）与从经验学习等议题——教程能够提供关于后训练对齐的系统性思路、评估方法以及将 RL 方法用于后续推理优化的可操作框架，具有明确的技术关联性。
