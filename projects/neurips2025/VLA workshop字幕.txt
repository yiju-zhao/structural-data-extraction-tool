WEBVTT

002357.755 -- 002359.794
Number one, welcome

002359.855 -- 002401.905
to our workshop So

002401.905 -- 002404.165
please take a seat, and we will

002404.225 -- 002406.115
going to start our workshop

002406.595 -- 002407.664
this morning's session.

002411.255 -- 002412.725
So I will give, like,

002413.314 -- 002415.095
I will wait for, like, one minute.

002441.355 -- 002443.695
Okay. For organizing our workshop,

002443.985 -- 002446.304
we are gathering, at a really

002446.304 -- 002448.245
exciting moment of this field.

002448.845 -- 002450.545
War model have become a very

002450.925 -- 002452.685
popular concept these years, and,

002453.695 -- 002455.774
especially in embodied AI because

002455.775 -- 002457.874
it help agent to infer and

002457.875 -- 002459.655
predict the real world dynamics

002459.935 -- 002501.955
by modeling the external

002502.095 -- 002504.335
environment. More importantly, we are

002504.735 -- 002506.755
we have increasingly, powered

002506.814 -- 002509.165
progress in decision making and

002509.225 -- 002510.605
planning for interacting agents.

002511.985 -- 002514.144
The motivation of this workshop is to

002514.145 -- 002516.294
push that further. We want to

002516.294 -- 002518.635
bring together the researchers

002518.694 -- 002520.794
working across different domains

002520.794 -- 002522.335
like generative modeling,

002523.125 -- 002525.625
reinforcement learning, computer vision, and robotics.

002526.275 -- 002528.695
To explore, the next generation

002528.995 -- 002531.250
of embodied world models or

002531.250 -- 002533.174
general war models. And,

002533.415 -- 002535.835
we are going to discuss about

002536.194 -- 002538.274
how to enable agent not only

002538.274 -- 002540.455
to understand, but also

002541.444 -- 002543.684
how to predict the word and how to allows

002543.684 -- 002545.735
the agent to in, to

002545.735 -- 002547.755
modeling the interaction of the word.

002548.805 -- 002551.125
So the key theme throughout the day is moving

002551.125 -- 002552.954
beyond the world model as a

002553.434 -- 002555.914
passive predictor, and

002555.914 -- 002557.605
our work is emphasizing

002557.985 -- 002559.744
the following domains, like

002600.225 -- 002602.385
based reinforcement learning and long

002602.385 -- 002604.005
horizon planning.

002604.545 -- 002606.785
And also aligning simulation

002606.785 -- 002608.915
and the real world physics for

002608.915 -- 002611.025
robotics learning and the interactive

002612.515 -- 002614.694
scene generation video language action models.

002615.605 -- 002618.105
And for border, we are going to discuss

002618.165 -- 002620.184
about open war games and also

002620.814 -- 002622.355
autonomous driving as our application.

002623.375 -- 002625.304
And, for today's

002625.740 -- 002628.015
schedule, after opening remarks, we'll

002628.015 -- 002630.095
move into some outstanding lineup

002630.095 -- 002630.595
of

002632.465 -- 002634.545
invited talks, we are honored to host

002634.545 -- 002635.744
a keynote from

002636.625 -- 002638.645
Elias Berenbaum, Sanjay Fiddler,

002639.115 -- 002641.275
Nicholas Hanson, Chelsea Finn,

002641.275 -- 002642.655
Peter Stone, and

002643.865 -- 002645.985
first Birthdays in the morning

002645.985 -- 002648.145
session and after lunch and post session

002648.145 -- 002650.335
too. We are especially excited

002650.335 -- 002652.495
for the industry demo from our

002652.495 -- 002654.685
sponsor, Weave, and highlight

002654.825 -- 002656.925
how world models can empower evaluation

002657.145 -- 002659.375
and validation as a school.

002659.435 -- 002701.515
In the afternoon session, we will hear

002701.515 -- 002703.055
from, John Lanford,

002703.725 -- 002704.925
Yilun Du, and,

002705.835 -- 002707.755
Philip Ball, followed by some,

002707.915 -- 002710.135
panel discussion. And

002710.194 -- 002712.455
we also received strong submissions

002712.995 -- 002715.424
in both research and opinion

002715.564 -- 002717.595
formats, and all of the

002717.914 -- 002720.234
accepted papers will be presented in the poster

002720.235 -- 002722.264
session. So I hope you will

002722.264 -- 002724.125
enjoy spending time with discussion,

002724.664 -- 002727.035
with with the author

002727.035 -- 002729.045
of the posters and

002729.045 -- 002730.825
chat with the author according

002731.595 -- 002732.495
crossing the day.

002733.755 -- 002735.735
So before we begin, I'd like

002736.375 -- 002738.745
to offer a special thank to our sponsors,

002738.965 -- 002741.285
AGI boat, and, we've for

002741.285 -- 002743.225
their general support for our workshop.

002744.385 -- 002746.485
And the border embodied the world model's community.

002747.445 -- 002749.525
With that, let's get started. I hope you

002749.525 -- 002751.684
have a fantastic fantastic and inspiring

002751.684 -- 002754.184
day at Embody War Model for Decision

002754.325 -- 002754.825
Making.

002805.345 -- 002806.965
So our first speaker

002807.505 -- 002809.365
is Elias Paringpoint.

002810.184 -- 002812.485
He is, associate professor

002812.485 -- 002814.425
at the department of computer science

002814.645 -- 002816.934
and a director of Artificial

002817.075 -- 002819.085
Intelligence WAP at Columbia

002819.085 -- 002821.325
University. His

002821.325 -- 002822.625
talk today will be

002823.695 -- 002825.754
towards causal AI. Let's

002825.755 -- 002826.494
welcome him.

002844.815 -- 002847.215
Okay. Good morning, everyone.

002847.215 -- 002849.474
I'm Elias, happy to be here.

002850.395 -- 002852.475
I changed a little bit the title of my talk, but I think

002852.475 -- 002854.535
it's suitable as to art causal AI. I

002854.535 -- 002856.535
think it's very interesting kind of angle

002858.055 -- 002900.215
the point of view to have

002900.215 -- 002901.995
embodied bundles. And,

002902.555 -- 002904.345
and I'll try to elaborate on that.

002905.544 -- 002907.804
Most of my talk is in the my forthcoming

002908.025 -- 002910.395
book, that is, the

002910.395 -- 002912.095
draft is available in this website.

002913.435 -- 002915.315
Cause our air book .net.

002916.985 -- 002919.064
And I think we are all here today because we're

002919.064 -- 002921.335
excited about some kind of recent

002921.335 -- 002923.435
breakthroughs in the field of AI.

002925.705 -- 002927.014
Systems are able to

002928.135 -- 002930.555
perform extremely well in making predictions

002931.284 -- 002932.825
in high dimensional settings.

002933.515 -- 002935.695
In particular, there's,

002935.775 -- 002937.704
huge progress in fields such as

002938.024 -- 002940.284
NLP, computer vision, and enforcement

002940.345 -- 002942.595
learning. And building this

002942.595 -- 002944.935
capability of doing high dimensional predictions.

002945.915 -- 002948.115
Applications are everywhere. I don't feel I need to

002948.115 -- 002949.735
talk much here in the audience.

002950.565 -- 002952.815
With this audience. And this is kind of very

002953.055 -- 002955.445
exciting. I think we should be few

002955.445 -- 002957.715
or we are blessed to be

002957.955 -- 003000.114
living these times. A question

003000.115 -- 003001.634
that I'm asking I ask,

003002.825 -- 003005.115
myself and in the lab is is

003005.365 -- 003007.365
does this means that that we are done

003008.405 -- 003010.505
Get Duncan, experiment here.

003011.505 -- 003013.585
Thought experiment. Even if you assume that you have

003013.585 -- 003015.665
infinite amount of compute

003016.205 -- 003018.305
and data, I know that you don't,

003018.305 -- 003020.605
but doing yeah,

003020.605 -- 003022.664
this experiment I were

003022.664 -- 003024.605
done, in reality in terms of,

003025.055 -- 003026.905
the fundamental problems here, and

003027.065 -- 003029.145
all what we need is scale more. And,

003030.025 -- 003031.805
the more a little bit more of the same.

003032.045 -- 003034.125
In this neighborhood. And if not, what is

003034.125 -- 003036.575
missing So

003037.184 -- 003039.265
Now, the other side of the coin here in terms of

003039.265 -- 003041.715
the breakthrough, and what this technology

003041.715 -- 003043.895
led us, there are still

003044.034 -- 003045.820
very serious foundational

003046.350 -- 003048.605
issues. I have been saying that

003048.605 -- 003050.725
for maybe a few years now.

003050.725 -- 003052.804
I'm glad to see that it seems that there

003052.804 -- 003054.495
is a growing feeling

003055.035 -- 003057.135
from people even in the field that help

003057.515 -- 003059.615
us to reach here, acknowledging

003059.755 -- 003101.525
this point, For example,

003101.905 -- 003103.835
Ilya, said that just scaling

003104.075 -- 003106.155
just recently, this is last last week. Just

003106.155 -- 003108.535
scaling is not enough We

003108.535 -- 003110.305
need new ideas the next jump.

003111.345 -- 003113.585
So Le Cun Yang says scaling

003113.585 -- 003115.045
alone will not get us

003116.265 -- 003118.595
to systems that are that understand the world.

003119.554 -- 003121.715
With the keyword here is understand as well. What

003121.715 -- 003123.475
does this mean, and how this relates to

003123.855 -- 003124.235
Khosaleid

003126.005 -- 003128.505
Francois said that intelligence is not an emergent

003128.565 -- 003131.015
product of bigger models. So

003133.115 -- 003135.355
OSHA say robust and safe AI

003135.355 -- 003137.675
requires causal understanding.

003138.235 -- 003140.675
Without it, system will fail, fail unpredictably.

003142.304 -- 003143.695
Again, I think I'm

003144.335 -- 003146.405
super excited and that it

003146.405 -- 003148.585
seems that the

003148.585 -- 003149.945
thought leaders in the field,

003150.665 -- 003152.784
it seems they start to get

003152.784 -- 003154.845
this feeling that there is some some kind of

003154.845 -- 003156.705
important missing component to the puzzle.

003159.755 -- 003201.915
And I will elaborate a little bit more. We're

003201.915 -- 003204.064
not completely lost in terms of the

003204.165 -- 003206.265
what what is missing and maybe a solution if we

003206.265 -- 003208.479
don't have all of them. Breaking news here

003208.479 -- 003210.605
or or to to be upfront,

003210.755 -- 003213.015
I think you have ways of thinking about that. At

003213.015 -- 003215.160
least. And, and some kind of robust

003215.160 -- 003217.365
theory or language to to think

003217.365 -- 003219.054
about that. And here are a few kind of

003220.855 -- 003223.095
challenges for the current AI. They suffer

003223.095 -- 003224.385
from lack of,

003225.424 -- 003227.904
understanding the keyword there that was mentioned, and

003227.904 -- 003230.234
ability articulate the

003230.235 -- 003232.254
explanations. I I would defer making

003232.255 -- 003234.475
paper reviews here or a few reviews in terms

003234.475 -- 003235.694
of the literature of explainability,

003237.355 -- 003239.464
but I think we

003239.465 -- 003241.405
don't have this capability at the moment.

003246.705 -- 003248.804
Potentially, also don't need

003248.804 -- 003250.985
a maybe a PhD in in AI

003251.205 -- 003252.105
to see that,

003253.934 -- 003255.755
data in reality is a picture of the

003255.995 -- 003257.915
current reality that you are sampling from.

003259.005 -- 003300.625
Breaking news as a human,

003301.715 -- 003303.825
reality is kinda messed up. A

003303.825 -- 003305.125
way, they have many problems.

003305.895 -- 003308.295
And, and if you if you're sampling

003308.295 -- 003310.394
from this reality, these problems

003310.395 -- 003312.875
are in the data. This is queerness or the biases

003312.875 -- 003315.165
or your fingers is in the data. Then

003315.215 -- 003317.675
you are training a model, that is,

003318.375 -- 003320.552
there's a pattern recognition model

003320.552 -- 003322.605
in high dimensional high

003322.605 -- 003324.424
dimensional pattern recognition.

003324.665 -- 003326.715
What do you think that the model will be learning The

003326.715 -- 003328.834
the model is in some way trying to learn or

003328.835 -- 003330.775
replicate something that is in the data,

003331.804 -- 003333.575
that is just a mirror of what is in the world.

003336.985 -- 003339.304
Which you usually I'm teaching. If I'm teaching AI or

003339.305 -- 003341.675
ML, non causal

003341.975 -- 003344.054
course or I'd say that this is a punch in the

003344.054 -- 003346.115
face. Of the the paradigm

003346.115 -- 003348.315
I put this hat. Because

003348.555 -- 003350.705
there is a belief that, data

003350.705 -- 003352.915
is everything, give him more data,

003353.055 -- 003354.755
and eventually, things will be okay.

003355.845 -- 003358.085
And, and this observation here shows

003358.085 -- 003359.675
that this is not not the case.

003400.155 -- 003402.415
Maybe you're trying to move to a better world,

003402.794 -- 003404.955
counterfactual world that it doesn't even exist

003404.955 -- 003407.345
yet. And then you need some type of language

003407.664 -- 003409.805
to be able to articulate that. And I

003409.805 -- 003411.825
I could elaborate, we spend

003411.885 -- 003414.335
a good amount of time on that. There

003414.610 -- 003414.825
oh,

003419.205 -- 003421.445
They are very data inefficient. Right We need

003421.445 -- 003423.604
a kind of billion interactions to learn

003423.605 -- 003425.225
something that maybe a infant

003425.765 -- 003427.995
or something you you you you learn very

003427.995 -- 003430.365
quickly, Nothing

003430.665 -- 003432.825
against me excited about results that we

003432.825 -- 003434.845
have in terms of beam sample efficient,

003435.015 -- 003437.435
but they they are kinda neat and

003437.575 -- 003439.595
amazing in in their ways. Given they

003439.595 -- 003441.675
appreciate the mathematics. But the question is like, is

003441.675 -- 003443.785
it just a signal for us there

003443.785 -- 003445.944
is something wrong in the data structure, and what

003445.945 -- 003447.895
we are trying to fit something that doesn't really

003448.220 -- 003450.304
fit. We are kinda trying to hammer our way

003450.305 -- 003452.494
out. Of it. Then doesn't

003452.495 -- 003454.624
it doesn't it exist some kind of more natural

003454.865 -- 003457.104
parameterization that maybe is more aligned

003457.105 -- 003459.454
aligned in terms of English word here It's kinda

003459.454 -- 003501.504
more related to the real world that,

003501.734 -- 003504.175
that is simpler in a sense.

003505.095 -- 003507.305
That doesn't make us doesn't

003507.305 -- 003508.835
require us to dance so much.

003509.875 -- 003511.575
Or to try to use our hammer

003511.985 -- 003514.085
this way. They're very, nonrobust.

003515.125 -- 003517.275
Generous abilities are very poor. Right These systems

003517.275 -- 003519.435
break very easily. Think I don't need to tell

003519.435 -- 003521.454
the robotics or the people who use that.

003522.105 -- 003524.215
Yeah. I think this audience is

003524.855 -- 003527.024
acknowledges this one. The

003527.184 -- 003529.345
and they also lack controllability. You

003529.345 -- 003531.125
kinda train this billion dimension

003532.155 -- 003533.695
billions of orders of billions of dimension

003534.235 -- 003535.385
space, and then

003536.365 -- 003538.605
we can go there inside this space and then,

003538.845 -- 003540.745
try to change something, and then we end up

003540.985 -- 003543.225
doing kinda splash. Right It's very hard to go

003543.225 -- 003545.304
surgically in this space trying

003545.305 -- 003547.585
to change one thing to bring about the change that we want.

003547.585 -- 003549.885
We end up changing multiple things. Also

003549.885 -- 003552.045
try to give example. Those are

003552.045 -- 003553.265
kinda torny problems,

003554.075 -- 003556.365
long standing. It didn't appear

003556.365 -- 003558.665
here only because of these

003558.724 -- 003600.885
models, but We're carrying over that from decades.

003600.885 -- 003603.145
It just amplified it in a very

003603.445 -- 003604.774
very crazy scale

003605.815 -- 003608.094
given the scale of the models. And

003608.734 -- 003610.754
scary in their ways the sense of also

003610.754 -- 003613.075
the scale of penetration. Right And and

003613.075 -- 003614.855
scale of use of these systems

003615.155 -- 003617.165
by society. It's no longer been the

003617.165 -- 003619.325
rips when I started my PhD. As I don't

003619.325 -- 003620.705
know, much less people

003622.855 -- 003624.475
carrying or using

003625.085 -- 003627.105
what we are trying to do. Now,

003627.244 -- 003628.775
the the and that's good,

003630.715 -- 003633.115
We'll have we'll have a little bit more.

003633.675 -- 003635.855
To start move moving from there. The

003636.805 -- 003639.045
but question that also that I ask myself

003639.045 -- 003641.223
is like, do

003641.224 -- 003642.935
these problems have anything in common

003643.415 -- 003645.785
Or just have a bag of different types of problems there

003646.424 -- 003648.614
And, and now,

003648.615 -- 003650.775
he'd be here, I believe that

003650.775 -- 003653.014
they do have something in common. At the core

003653.015 -- 003655.115
of these challenges is

003655.115 -- 003657.475
the absence of a robust causal understanding.

003658.070 -- 003700.124
So They are never I haven't seen it.

003700.125 -- 003702.535
You can send me the paper. And

003703.155 -- 003705.175
any any paper,

003705.235 -- 003707.525
any work that, the systems

003707.525 -- 003709.985
are truly trying to optimize

003710.045 -- 003712.225
or trying to to go towards understanding

003712.285 -- 003713.995
the world a causal way.

003717.465 -- 003719.625
Now, given that we are a scientific

003719.625 -- 003721.865
conference as well, would

003721.865 -- 003723.945
like to to get this label science. Right We'd like to

003723.945 -- 003725.245
move towards a science

003726.285 -- 003728.395
of AI. And And here

003728.395 -- 003729.535
is an idea

003731.115 -- 003732.335
Why not try to model

003733.185 -- 003735.275
the agent and viral relationship truth

003735.895 -- 003736.304
causal language

003737.905 -- 003740.135
Then we have we have our

003740.135 -- 003742.265
cartoon here, that we have the

003742.325 -- 003743.475
world, and we have the agent

003744.725 -- 003745.830
being borrowed,

003747.785 -- 003749.865
And then, the agents see the world, and the agent

003749.865 -- 003752.025
acts in the world. And this is kind of

003752.025 -- 003754.134
following recipe. I like to trace it back

003754.135 -- 003755.845
more than seventy years ago,

003756.905 -- 003759.145
seventy five maybe, to

003759.145 -- 003801.165
to touring himself that he throw

003801.165 -- 003803.245
that, but we want to to build a is a

003803.245 -- 003804.865
machine that can learn from experience.

003805.545 -- 003807.725
Then I usually say, Turing

003807.725 -- 003808.890
was right

003809.775 -- 003811.795
as he almost always was

003812.095 -- 003813.815
then, like he's our hero here,

003814.474 -- 003816.495
the the idea of the

003816.495 -- 003817.955
experience is very underspecified.

003821.785 -- 003824.035
The the whole touring, the whole idea of the touring

003824.035 -- 003825.815
that is remarkable that has passed,

003826.705 -- 003829.095
was under first the avoidance

003829.095 -- 003830.875
of having to try to defy intelligence,

003831.945 -- 003834.104
and under the idea that if you can kinda

003834.105 -- 003835.744
be asked, if you can kinda mimic or

003836.305 -- 003838.524
pretend that you are this entity that you feel that

003838.525 -- 003840.795
is intelligence, things will will be

003840.795 -- 003842.175
able to kinda generate intelligence.

003843.495 -- 003845.735
We didn't expect that the idea that

003845.735 -- 003848.145
we can kinda disentangle someone

003848.145 -- 003850.275
that is able to be asked is

003850.275 -- 003852.514
different than someone that is intelligence or acquired

003852.515 -- 003853.595
his intelligence skill.

003854.525 -- 003856.685
And I see that about when I'm interacting with these

003856.685 -- 003858.965
systems, Everything that I don't understand about

003858.965 -- 003900.505
the world world that is

003901.125 -- 003903.465
very little about the world, very specific topics.

003903.605 -- 003905.615
All topics that I don't understand about,

003905.855 -- 003907.964
if I chat with the models, it's amazing.

003907.964 -- 003910.044
I think, wow, I'm learning. I almost want to

003910.045 -- 003911.955
love it. Right It's so coherent.

003912.365 -- 003914.505
And, so compelling, then I'm

003915.165 -- 003917.265
almost like you don't you hard the system in a way.

003917.535 -- 003919.635
Every topic, very few, but that I understand

003919.855 -- 003922.174
something about or relatively well, there

003922.174 -- 003924.494
there is always kind of things like, wow, man, no human

003924.494 -- 003926.975
will say something like that. We got completely

003927.035 -- 003929.315
off. Then I think

003930.115 -- 003932.145
must choose Mark maybe to realize that you

003932.145 -- 003934.225
could you could disentangle intelligence from BS

003934.225 -- 003936.385
ing. Right The idea of

003936.385 -- 003936.885
pretending

003938.805 -- 003940.884
many help that the the But the the

003940.885 -- 003942.315
but idea is good

003943.835 -- 003945.694
The idea of experience is good.

003946.075 -- 003948.155
And and they specified. Now we would

003948.155 -- 003950.264
like to kinda open up a little bit and

003950.265 -- 003951.975
refine what experience could mean.

003953.085 -- 003955.105
And here are different types of modalities

003955.705 -- 003957.784
about the how the agent could interact with the

003957.785 -- 003959.955
world. The first one is through senior or through

004000.435 -- 004002.595
passively observing the world unfold and kind of

004002.595 -- 004004.615
collecting data. The the second

004004.615 -- 004006.865
one is to doing, to intervene in the system,

004007.665 -- 004010.075
and the third one is to kinda

004010.295 -- 004012.495
hands off just kind of thinking about the different

004012.495 -- 004014.235
versions of reality that you could have

004016.155 -- 004018.495
They then, in some way, would like to unpack

004018.675 -- 004020.995
the idea of what is experience. There are different

004020.995 -- 004023.245
types of experiences. That the

004023.245 -- 004025.395
agent could face or

004025.395 -- 004027.695
experience. Now, it turns out that

004027.695 -- 004029.714
these different types of experience is tightly

004030.095 -- 004031.974
related to a structure, a mathematical

004032.115 -- 004034.305
structure that we have in causality, have been

004034.305 -- 004036.805
studied from some time. Now that is called the PCH,

004036.865 -- 004038.245
the pro causal hierarchy.

004038.875 -- 004040.955
Is a hierarchy of languages related

004040.955 -- 004043.355
to each of these types of experiences.

004044.045 -- 004046.205
Then here's the the the headline, I

004046.205 -- 004048.615
guess, that is like not all experiences created

004050.934 -- 004053.105
Then I would like to elaborate a little bit more

004053.265 -- 004054.625
about these different languages.

004055.505 -- 004057.525
Now, result here, number one in causality,

004057.924 -- 004100.164
and this is kind of popularized in this book, The Book

004100.164 -- 004102.305
of Why, I recommend. Second,

004102.655 -- 004104.145
recommendation of the day,

004106.575 -- 004108.765
The and the result here is about

004108.765 -- 004110.595
once we have a I had the word

004111.075 -- 004113.234
world here, but once we have a causal model that is a

004113.234 -- 004114.305
world model,

004115.625 -- 004117.945
this induces this hierarchy, the potential

004117.945 -- 004120.095
of the system that you

004120.095 -- 004122.254
can, have different types of observation of

004122.255 -- 004124.474
the We have one observational distribution.

004124.694 -- 004127.035
You have different types of ways of intervening,

004127.825 -- 004129.925
with the system, the kind of typical RL.

004130.224 -- 004131.765
And we have different, counterfactual

004132.305 -- 004132.805
distributions.

004134.365 -- 004136.305
I will fast here, spend a whole semester

004136.445 -- 004138.795
doing that, The the

004138.795 -- 004141.155
in class but the first

004141.295 -- 004143.335
layer of this hierarchy, this is why it's

004143.655 -- 004145.595
is a containment hierarchy, like p versus

004145.815 -- 004148.015
NP, or Chomsky or many

004148.015 -- 004150.194
that we have in computer science and mathematics,

004151.224 -- 004152.605
the the first one is associational,

004153.305 -- 004155.425
very related to the symbol p

004155.425 -- 004157.495
of y. Oh, where is my mouse here Oh, it's

004157.495 -- 004159.624
here. Yeah. Of

004159.625 -- 004201.385
y given x, highly related to

004201.705 -- 004203.775
observing the world in a way. It turns out

004203.775 -- 004205.535
that there are different ways of,

004206.445 -- 004208.715
different contact part of that the

004208.715 -- 004210.955
world of machine learning, is called the

004210.955 -- 004213.025
supervised and unsupervised learning or semi

004213.345 -- 004214.534
supervised learning as well.

004215.494 -- 004217.574
Depending on the decade that you live, some of you

004217.575 -- 004219.605
are just arriving here, but if you

004219.605 -- 004221.535
have been more time in the community,

004221.755 -- 004223.914
depending on the decade, we have a different

004223.915 -- 004225.915
formalism that is trying to evaluate

004225.975 -- 004228.375
expressions like that, p of y given x.

004228.375 -- 004230.765
But You can date this

004230.825 -- 004232.785
back even before before,

004233.025 -- 004235.125
machine learning existed. Have been doing that

004235.125 -- 004236.585
for two hundred, three hundred years.

004237.645 -- 004239.725
But now the different aspect here is

004239.725 -- 004242.105
that this variable x here can be a 1,000,000,000

004242.105 -- 004244.234
dimension. Kind of

004244.234 -- 004246.304
variable. So then we are kind

004246.305 -- 004248.325
of trying to predict, and the y is a big sequence.

004248.645 -- 004251.004
And this is why it's kind of very good for the

004251.004 -- 004253.165
computer scientists because kinda our business, we are trained

004253.165 -- 004254.905
with the idea of like how to scale up.

004256.144 -- 004258.164
And this is what we have been doing for decades.

004258.444 -- 004300.605
And transformers is the current or the the

004300.605 -- 004302.684
slash the fusion is the current one that we are able to

004302.684 -- 004305.145
do in inference inferences on top of this data structure,

004305.145 -- 004307.175
this object. This abstraction called p

004307.175 -- 004309.345
of y given x. But now there is the

004309.345 -- 004311.704
other one that is related different type of experience,

004312.605 -- 004314.625
that is related to interventions, symbolically

004315.235 -- 004317.315
we can't extend the language here, and

004317.315 -- 004319.415
we have p of y given do x.

004319.875 -- 004322.035
Comma c. And the do is that the agent with their

004322.035 -- 004323.735
own free will or their own volition

004324.135 -- 004326.454
or from outside the environment, it decides

004326.455 -- 004328.785
to intervene in this variable x. It's not

004329.004 -- 004331.325
observing other agent, and what will happen

004331.325 -- 004332.775
with why had the other agent sat

004333.895 -- 004336.124
have access to tracks, but the agent know

004336.125 -- 004338.445
is doing that because know the causes. Right It's driving

004338.445 -- 004340.535
kind of the process. Sees the context

004340.915 -- 004343.075
here related to doing, what if I do

004343.075 -- 004343.464
x

004345.145 -- 004347.225
There is also a counterpart for that in

004347.225 -- 004349.665
machine learning that's called reinforcement learning.

004351.285 -- 004353.444
Also, there are different formalisms that you can

004353.445 -- 004355.825
have to handle that different data structures,

004356.125 -- 004358.285
MDPs, POMDPs, and so on,

004358.285 -- 004400.295
or causal, version of that is a

004400.295 -- 004402.305
causal Bayesian network, and so on.

004403.825 -- 004405.855
The specific way I'll try to to show

004405.855 -- 004408.195
or or give a few examples about my

004408.195 -- 004410.455
own is great, but it's a very specific way of thinking

004411.155 -- 004413.575
about causality. It does allow us to get some causality,

004414.085 -- 004415.665
would like kinda to broaden that

004417.424 -- 004419.775
The the word is a little bit, yeah.

004419.934 -- 004422.015
Bigger. The the the third

004422.015 -- 004423.574
layer is the counterfactual layer.

004424.454 -- 004426.474
That is related to the idea of imagination,

004427.335 -- 004429.035
introspection, retrospection,

004431.004 -- 004433.084
The the why type of question was with this book of why

004433.085 -- 004435.215
that I recommended. Is very tied

004435.215 -- 004436.545
to the counterfactual layer,

004437.665 -- 004439.285
very related to the premise

004439.775 -- 004441.965
what if I hadn't acted differently I

004441.965 -- 004444.085
didn't. I did a particular thing, but

004444.085 -- 004445.170
what if

004446.215 -- 004448.715
dots dot dot Then they here, for example,

004448.905 -- 004450.954
Joe took the drug, that

004450.954 -- 004453.035
is x prime, and Joe is dad that is

004453.035 -- 004455.145
y prime. This happens in

004455.145 -- 004457.555
this world, as fact. And then you

004457.555 -- 004459.875
wonder, would he be alive That

004459.934 -- 004501.875
is why, the opposite of y prime,

004502.015 -- 004504.195
had he not taking the

004504.195 -- 004506.285
drug. That is the x, that is the opposite

004506.285 -- 004508.585
of x prime. And very hard type of questions,

004508.585 -- 004510.645
but we are kind of asking, I

004510.645 -- 004512.725
will, suggest you to try

004512.725 -- 004514.885
to introspect try to catch your own

004514.885 -- 004517.045
thought process, and see the variations. Maybe

004517.045 -- 004519.175
you're not using exactly the same language but how

004519.175 -- 004521.184
do you use, in your own lives,

004521.345 -- 004523.605
the idea of counterfactuals Very

004523.605 -- 004525.785
related to attribution, blame, responsibility,

004526.325 -- 004527.545
very important notion.

004529.635 -- 004531.975
For society and for humans in terms of understanding.

004532.035 -- 004533.835
K Keeping the word you are understanding.

004534.685 -- 004536.845
That I mentioned earlier. The there's no

004536.845 -- 004538.974
counterpart for that. In

004538.974 -- 004541.054
ML. Not do paper review here,

004541.055 -- 004543.155
but there's no no

004543.855 -- 004545.895
no counterpart, but there is a

004545.895 -- 004548.135
structural causal model that is a type of

004548.135 -- 004550.254
language or model that, gives

004550.254 -- 004552.275
semantics to quantities of this

004552.275 -- 004554.615
type, and then you can kinda unravel from there.

004557.015 -- 004558.795
Observation that the formalization

004559.095 -- 004601.115
of the PCH provides a way

004601.454 -- 004603.865
to measure the capabilities expressiveness

004604.405 -- 004606.745
with with different formalisms with

004606.805 -- 004608.825
respect to increasingly complex

004608.965 -- 004610.984
queries or questions or modes of

004610.984 -- 004613.484
reasoning that we like the agent to have

004614.775 -- 004616.935
or to able to articulate relative to some

004616.935 -- 004617.425
environment

004619.985 -- 004621.535
Now, what's the challenge here

004626.075 -- 004628.235
Elias, what's the challenge If it's true

004628.235 -- 004630.184
that a structure called a model

004630.505 -- 004632.865
a world model induces all

004632.865 -- 004635.025
these probability distributions, then why the problem

004635.025 -- 004636.895
is not trivial In a way

004637.135 -- 004639.635
And here is a a a cartoon here

004639.935 -- 004642.015
why why it is not trivial. Most of

004642.015 -- 004644.345
the data that we have in the world, Elias

004644.405 -- 004646.645
numbers, 99% of the data is

004646.645 -- 004649.075
coming from layer one. Is

004649.375 -- 004651.495
observational data. And most

004651.495 -- 004653.815
of the queries or the interesting queries or questions

004653.815 -- 004656.165
that you can ask about the world

004656.305 -- 004657.885
lives in layer two and layer three.

004658.375 -- 004700.805
I Then you have a fundamental mismatch

004701.105 -- 004703.335
here. That we are sampling from a distribution. Let's call

004703.335 -- 004705.355
layer one distribution p. And you're

004705.355 -- 004707.375
trying to make a statement about another

004707.435 -- 004709.855
probable distribution p prime. Attached

004709.855 -- 004712.075
to the other layers. How the world

004712.075 -- 004714.105
would be had we done dot dot

004714.105 -- 004716.395
dot. And we didn't collect data from this world.

004716.635 -- 004718.795
Then there is a intrinsic methodological

004718.795 -- 004720.934
here problem or struggle about how

004720.935 -- 004722.675
to connect these different worlds.

004723.795 -- 004725.955
Again, data is from layer one. Then

004725.955 -- 004728.375
is about how you observe the world, most of it.

004728.875 -- 004731.195
And then and the question is, are most of the inference

004731.195 -- 004733.204
are about cause or effects The effects

004733.204 -- 004735.464
of policies, treatments, decisions,

004736.265 -- 004738.197
so on. This is about layer two. Same can

004738.305 -- 004740.385
same picture for layer three. Now,

004740.525 -- 004742.335
question that kinda started the field

004743.295 -- 004745.455
maybe twenty five years ago plus, is like,

004745.455 -- 004747.905
how to use the data collected from observations

004747.905 -- 004749.965
from layer one to answer

004750.185 -- 004752.085
questions about interventions at this layer two.

004753.925 -- 004756.115
Now how to dance in the field that tries

004756.115 -- 004758.275
to answer that is called causal inference.

004758.434 -- 004800.595
Now I would like to put the kind of mother that addressing

004800.595 -- 004802.665
here to the same question, that is

004802.665 -- 004804.745
the idea everyone is using the word world

004804.745 -- 004806.835
models I'm not the most excited at

004806.835 -- 004808.993
the moment because it loses meaning when people use

004808.994 -- 004811.084
them so casual not so causal, but

004811.085 -- 004813.305
very casual way. Everything is

004813.305 -- 004815.384
world model as well. Now, they

004815.625 -- 004817.835
but challenge here from a kind

004817.835 -- 004819.915
of, hopefully, more mature causal perspective

004819.915 -- 004822.164
about what is world model. Don't like the name also

004822.165 -- 004824.275
cause a world model. Because a causal model

004824.275 -- 004826.065
is a world model. It's not a model

004826.845 -- 004828.994
model about what The and a world model

004828.995 -- 004831.075
does not cause us, which kind of, what are

004831.075 -- 004833.285
you saying in reality It doesn't doesn't parse for

004833.285 -- 004835.375
me, but I digress.

004836.395 -- 004838.475
But then you have some kind of unobserved world

004838.475 -- 004840.905
there that exists, There is this underlying

004841.045 -- 004842.625
collection of causal mechanisms

004843.265 -- 004844.715
and and and border conditions

004845.275 -- 004847.514
m star, that is called causal model m star,

004847.515 -- 004849.215
we don't observe that either.

004849.765 -- 004851.844
Just results from the previous lay slide

004851.845 -- 004854.245
that it does induce has the potential to induce

004854.245 -- 004856.565
these different probabilities or different

004856.565 -- 004858.625
kinds of experiences The first

004858.625 -- 004900.945
one is about observations, the agent just passively

004900.945 -- 004902.795
observing. That's red.

004903.175 -- 004904.995
Intervention, that is the yellow, and,

004905.635 -- 004907.715
and counterfactuals that are green. There's

004907.715 -- 004909.795
one observational, agent dependent. Each

004909.795 -- 004911.734
agent has his own kind of point of view,

004911.815 -- 004914.075
and then you have multiple intervention distributions

004914.295 -- 004916.645
and multiple counterfactual worlds that

004916.645 -- 004918.685
could exist. Now, it comes

004918.765 -- 004920.905
we don't observe these different

004920.905 -- 004922.985
realities. The yellow and green, we don't observe, but we

004922.985 -- 004925.085
observe the current world that is factual,

004925.925 -- 004928.345
observation of distribution. Now

004928.484 -- 004930.544
it comes humans here, us,

004930.704 -- 004932.724
and you are trying to learn a model in the right

004932.965 -- 004934.505
let's call a neural model, a transformer,

004935.045 -- 004935.595
m hat.

004937.115 -- 004939.335
I This m hat is a bunch of functions. You can

004939.335 -- 004941.544
also add probability distribution there. It's already there

004941.545 -- 004943.224
depending on the phone and transformers.

004943.705 -- 004945.785
Then there's something that could look like

004945.785 -- 004947.685
a collection of functions and probability

004948.165 -- 004950.105
distribution is great, which means that in principle,

004950.565 -- 004952.795
it has the potential of

004952.795 -- 004955.295
generating different types of distributions or

004955.435 -- 004957.465
experiences. If you wish. Now

004957.465 -- 004959.485
what we'll do What means training

005000.655 -- 005002.765
Training means that we'll get the data that we have that

005002.765 -- 005004.655
is from the left side that is red,

005005.935 -- 005008.435
and then we start kinda training. We go to the cluster,

005009.045 -- 005011.435
many GPUs, go there, you

005011.435 -- 005013.515
spend one week one month, sometimes two.

005013.515 -- 005015.295
You dance, you jump, you pray.

005015.635 -- 005018.115
And eventually, the guy in the right side

005018.405 -- 005019.915
is able to match

005020.555 -- 005022.635
or kind of, mimic the distribution on

005022.635 -- 005024.705
the left. That is the one from the data.

005025.345 -- 005027.395
Then that's great. And you say success.

005027.755 -- 005030.224
Great. Are great. Amazing.

005030.665 -- 005032.745
But now the questions about what does

005032.745 -- 005034.754
this the the this paradigm

005034.754 -- 005037.035
or this model m had That

005037.035 -- 005039.165
m hat that is trained through this idea, tell

005039.165 -- 005041.374
us about the yellow. That we haven't

005041.375 -- 005041.875
observed.

005044.965 -- 005046.435
I call this a fundamental problem

005047.545 -- 005049.485
and the question here just as likely

005049.625 -- 005051.955
more explicitly, under what conditions

005052.095 -- 005054.125
inferences in the m hat that

005054.125 -- 005056.605
is in the right side are valid.

005057.925 -- 005059.865
When do they recover the distributions

005100.004 -- 005102.130
induced by the true model in m

005102.130 -- 005103.765
star That is in the left.

005105.045 -- 005107.385
Of course, you can ask ask the same question

005107.444 -- 005108.754
related to layer three.

005109.635 -- 005111.415
And the same cartoon holds

005112.135 -- 005114.254
if you common question that I get, if

005114.254 -- 005116.335
you observe part of the yellow

005116.335 -- 005117.945
distribution. In the left.

005118.615 -- 005120.675
People come and say, Elias, I have a

005120.675 -- 005122.835
system It's enforcement learning. It's collecting

005122.895 -- 005124.955
some data. Which is experimental.

005125.125 -- 005127.285
In principle. Then I have a piece of

005127.285 -- 005128.735
the yellow distributions there.

005129.375 -- 005131.454
Then what you do is exactly the same cartoon.

005132.055 -- 005134.095
You you still have the chance about

005134.095 -- 005136.465
everyone else that you couldn't intervene in the world,

005136.785 -- 005138.805
that it is the rule, by the way.

005139.205 -- 005141.245
Even though in artificial environments,

005142.115 -- 005144.195
you can make the Mario Bros to jump

005144.195 -- 005146.565
through the cliff, half a dozen times,

005146.625 -- 005148.705
get a very low reward and say, oh, I

005148.705 -- 005150.845
will not jump again. Or I will shoot my

005150.845 -- 005152.935
friend Many times, seems a

005152.935 -- 005155.035
bad idea, get low score, I'll not do again.

005155.295 -- 005157.385
In the real world, we cannot do that. Right

005157.385 -- 005159.745
In other words, in the real world, the key assumption here

005200.305 -- 005202.565
that made most of our systems work

005204.065 -- 005205.605
and did it very beautiful results

005206.225 -- 005206.725
empirical,

005209.484 -- 005211.904
are that you have abundant amount of, interventions,

005212.045 -- 005214.455
which in the cartoon here means that you

005214.455 -- 005216.734
can collect essentially whatever you want in terms

005216.734 -- 005218.485
of the yellow in the left side.

005219.845 -- 005222.345
Now, in the real world, you get very

005222.565 -- 005224.655
little fragments of the yellow. We

005224.655 -- 005226.985
should leverage that 100%. But

005227.705 -- 005229.775
challenge remains the same. How do I make

005229.775 -- 005231.875
statements about the other parts of the yellow

005232.275 -- 005233.815
even about the green type of distributions

005235.695 -- 005237.445
So Then

005238.095 -- 005240.174
that's the the the the picture. Now I would

005240.175 -- 005241.625
like to say what

005242.405 -- 005244.465
is causal AI or what is our goal in causal AI Our our goal is

005244.465 -- 005246.625
to develop more general types of AI endowed

005246.785 -- 005248.744
endowed with the following capabilities.

005249.915 -- 005252.335
They have the capability of doing causal and counterfactual

005252.475 -- 005252.975
generation.

005255.025 -- 005257.045
They have some kind of causal understanding

005257.365 -- 005259.625
of the world, and they are able to articulate explanations.

005301.175 -- 005303.223
I they have

005303.224 -- 005305.085
more efficient and precise, more surgical

005305.305 -- 005306.910
type decision making.

005307.825 -- 005310.005
They are also more generalizable and robust.

005311.245 -- 005313.325
And they're able to act as a science.

005313.325 -- 005314.795
Right Cop take like a

005315.355 -- 005317.365
a a baby as a scientist do

005317.365 -- 005319.525
experiments in the world and eventually learn some things about

005319.525 -- 005320.555
the world and discover.

005322.855 -- 005325.015
Then each part here, there's a each part

005325.095 -- 005327.134
each of these kind of tasks or capability

005327.135 -- 005329.234
to expect is a different part of the book. There's

005329.234 -- 005331.294
a few 100 pages written about each one of

005331.295 -- 005333.075
them because I think they are important.

005333.395 -- 005335.175
Cannot make a claim here that they

005335.555 -- 005337.854
are, sufficient for the most

005337.855 -- 005339.935
type of intelligence that you like, but I'm sure

005339.935 -- 005341.974
that they are necessary. And,

005342.055 -- 005344.134
and I don't think that anyhow, I think you you

005344.135 -- 005346.324
should make some progress here. Now

005346.325 -- 005348.484
I will just get some examples based on the time that

005348.484 -- 005349.025
I have.

005351.415 -- 005352.855
About, some of these,

005353.994 -- 005356.015
problems. I'll start with the the counterfactual

005356.875 -- 005358.915
generation. Seems to be

005358.915 -- 005400.775
a topic of interest. Let's start with cartoon.

005401.275 -- 005403.514
A cartoon that we have some kind

005403.515 -- 005405.745
of amnest and then you have the digits

005406.475 -- 005408.545
that is deep and the color that is c,

005408.545 -- 005410.704
and there is some kind of correlation between that that is

005410.705 -- 005412.575
shown here. Like, the zeros

005412.895 -- 005414.925
distributed around the red. It's very the red.

005415.565 -- 005417.695
Redish. There is very very few,

005417.975 -- 005419.993
zeros that are blue. The

005419.994 -- 005422.325
the five is like cyan. Distribution

005422.465 -- 005424.615
around there, and so on. Now it is shown

005424.615 -- 005425.345
in the bottom.

005426.705 -- 005429.105
And then those are kind of example of digits

005429.105 -- 005431.325
that you have that are found found in

005431.325 -- 005433.265
nature in the wild. The agent can perceive

005433.645 -- 005435.454
that. If you want to do generation,

005435.655 -- 005437.734
you can say, I would like to sample from this.

005437.734 -- 005439.975
There is the c and the d that is something in the world,

005440.135 -- 005442.315
and then it'll take a picture, and you got

005442.315 -- 005444.384
this image I. 100 by

005444.385 -- 005446.035
100, let's say. And,

005447.460 -- 005449.254
and the task here then

005449.474 -- 005451.555
becomes how can you sample from the distribution p

005451.555 -- 005453.664
of I given d is equal to

005453.665 -- 005455.725
zero We're doing some kind of

005455.725 -- 005457.754
filtering we would like to get the natural in

005457.755 -- 005459.535
words here, it was the natural zeros.

005500.195 -- 005502.355
And things that looks like what I can see in the

005502.355 -- 005504.365
real world, in the current world. And

005504.365 -- 005506.385
we've got got things like that because zeros

005506.704 -- 005509.105
usually I didn't say something important. Sorry.

005509.105 -- 005510.894
The the correlation where is my mouse

005512.254 -- 005514.595
I cannot see it. Is it there

005518.185 -- 005519.645
Yeah. I don't know.

005521.725 -- 005523.660
The the the

005524.545 -- 005526.705
I don't know why they're masked, but there there there is

005526.705 -- 005528.875
a bidirectional dash arrow between

005528.875 -- 005530.884
d and c there. What this means here, the

005530.885 -- 005533.065
semantics about there is some kind of latent variable

005533.125 -- 005535.625
there. You that is both generating the digit

005535.925 -- 005538.315
and the c. That there is some kind of

005538.315 -- 005540.655
common cause that is generating the correlation

005540.975 -- 005543.145
between B and C. The digit and

005543.145 -- 005545.225
the color, which is the shown in the bar in the

005545.225 -- 005547.475
bottom. Concrete instantiation.

005548.095 -- 005550.175
Now if you ask your sample from distribution, this

005550.175 -- 005552.665
is kind of what we get that in reality,

005553.174 -- 005554.515
zeros tend to be red.

005555.424 -- 005557.204
But now if you ask for the layer

005557.504 -- 005559.384
two distribution, they do distribution as

005559.625 -- 005601.865
what I want to sample from the distribution p of y,

005601.865 -- 005603.405
given du,

005604.504 -- 005606.805
d is equal to zero. Now

005606.805 -- 005608.885
do it d equal to zero, it means I'm from out

005609.205 -- 005611.284
outside as the agent intervening in the digit

005611.285 -- 005613.435
d, and I'm kinda cutting. I

005613.835 -- 005615.915
the u variable that is hidden there that that generate the

005615.915 -- 005618.305
correlation is no longer active. Because

005618.305 -- 005620.385
we are I'm the agent, and I went there

005620.385 -- 005622.165
and set the digit to to zero.

005622.555 -- 005624.655
And I don't whatever is the viewers

005624.715 -- 005627.125
don't know. They it's hidden.

005627.455 -- 005629.735
Then you get kind of zeros from all colors.

005630.535 -- 005632.615
Right Because kinda averaging over the space of

005632.615 -- 005634.645
c. For any sea that is there.

005636.145 -- 005638.225
This is a layer two type of quantity. That doesn't

005638.225 -- 005640.385
happen to see from the rare distribution of

005640.385 -- 005642.565
zeros you cannot get that in principle.

005642.805 -- 005644.105
They're completely different distribution.

005646.674 -- 005648.795
What about the counterfact Example of a

005648.795 -- 005650.820
counterfactual here. The

005651.425 -- 005653.685
given that the digit was a a five,

005654.385 -- 005656.224
given that d is equal to five,

005656.465 -- 005657.685
we kinda wonder,

005659.375 -- 005701.725
what would be the image had the digit

005701.725 -- 005703.784
being a zero Had I

005703.785 -- 005705.175
made the digit be a zero,

005706.045 -- 005708.155
Naturally, the digit is a five, it means

005708.155 -- 005709.695
that I was originally cyan.

005710.975 -- 005713.005
Then because the u that

005713.005 -- 005715.244
generated five is the u that generates

005715.245 -- 005717.155
cyan. Right U that's there,

005717.345 -- 005719.435
got a value, generates c and generate d.

005719.435 -- 005721.515
D is equal to five, c is

005721.515 -- 005723.525
equal to cyan, more likely. But

005723.525 -- 005725.625
now you want and you go there and do the

005725.685 -- 005727.965
intervention and set the digit that will pop up

005728.165 -- 005729.575
as being, zero.

005730.175 -- 005732.297
Then you get in nature, you cannot

005732.297 -- 005734.635
see zeros or very rarely or cannot see

005734.635 -- 005736.695
zeros that are cyan, but now this

005736.695 -- 005738.775
is the distribution that you would like to be sampling for.

005738.775 -- 005740.155
Completely different distribution.

005741.535 -- 005743.545
I hope it makes sense. Cartoon

005743.545 -- 005745.608
here about the layers. Of the passage

005745.608 -- 005747.845
of the Khazadir Iraqi. So just

005747.845 -- 005749.925
to get intuition, move to some things a little bit

005749.925 -- 005752.085
more real, I guess. The what would a

005752.085 -- 005754.165
person look like counterfactual had they

005754.165 -- 005756.294
been dot dot dot, these

005756.295 -- 005758.455
papers that appear appear in the ICML and

005758.455 -- 005800.395
NERIPS this year. They

005801.295 -- 005803.455
maybe you ask AI to generate a human face, and

005803.455 -- 005805.605
then you get the first lady there

005805.605 -- 005807.764
and you ask what would happen had the person been,

005807.765 -- 005809.284
I don't know, ten years older

005810.125 -- 005812.284
then in reality, if you use a known causal

005812.285 -- 005814.365
method, the you get a guy on

005814.365 -- 005816.455
the other side. Which is totally

005816.455 -- 005818.585
off. Little bit older but is

005818.585 -- 005820.605
totally off. Right Because change there there's not

005820.605 -- 005822.765
even the same person. State

005822.765 -- 005824.874
of the art was generating that. Didn't

005824.875 -- 005826.414
try this week or in this is

005827.325 -- 005828.355
the original I sent out paper.

005829.795 -- 005831.874
You get the picture of the lady there that is

005831.875 -- 005834.055
blonde, AI generates, and you ask, what

005834.055 -- 005836.295
would have happened had the person been a different

005836.295 -- 005838.420
gender And then you get a guy

005838.420 -- 005840.515
there which seems okay,

005840.815 -- 005842.874
but very hard to understand the the

005842.875 -- 005844.735
the the consistency or the counterfactual

005845.195 -- 005847.395
here between these two people. They seem to be disparate.

005847.744 -- 005849.904
It doesn't feel that the guy is in another age

005849.905 -- 005852.235
group. In other words, it doesn't feel the the counterfactual

005852.855 -- 005854.925
payer. Here in a way. Let me skip the

005854.925 -- 005857.425
other one about the grey hair that is a sensitive

005857.725 -- 005859.795
issue here. They

005900.275 -- 005902.354
when you have causal, what we'd expect is

005902.355 -- 005904.494
about gasket the picture of someone and

005904.494 -- 005906.194
then you get the guy there with the glasses.

005906.875 -- 005909.034
How do we have been older This seems to be the guy

005909.035 -- 005911.054
that is the counterfactual pair,

005911.055 -- 005913.505
the older version of the guy on the left.

005913.825 -- 005915.205
And it kinda makes sense.

005916.205 -- 005918.455
You know, where he has more gray hair, less

005918.455 -- 005920.465
hair, and so on. Or you

005920.465 -- 005921.535
ask a picture,

005922.575 -- 005924.894
Where is it We got the picture, and then we got the lady

005924.895 -- 005926.925
first. And then we asked, what would happen

005926.925 -- 005929.055
had you been a male And then you

005929.055 -- 005931.335
got the I don't know what's the relationship.

005931.475 -- 005933.485
This is AI generated it

005933.485 -- 005935.905
seems kind of even almost like twin brothers.

005936.665 -- 005938.845
But it seems plausible that this is the counterfactual

005939.225 -- 005939.415
pair.

005941.415 -- 005943.435
I'll skip I'll skip the great heritage.

005944.395 -- 005946.504
The huge commotion here important,

005946.505 -- 005948.925
another one this is with language that was in the literature

005949.065 -- 005951.104
before. Have the pair there with two

005951.105 -- 005953.285
people and farmers and you and you ask what

005953.285 -- 005955.444
would happen if they had been flight

005955.444 -- 005957.725
attendants There's a whole drama here about this one,

005957.725 -- 005959.745
and we ended up changing the gender

005959.965 -- 010002.005
of the guy to become a a

010002.005 -- 010004.223
lady. Why Because strong

010004.224 -- 010006.494
correlation between gender,

010006.495 -- 010008.585
job type, in the real world, in the data,

010008.825 -- 010010.915
then AI is just kind of replicating that.

010011.075 -- 010013.575
Overwhelmed by the correlation in the latent space.

010013.895 -- 010016.055
Or you can ask them, make them

010016.055 -- 010018.174
look like doctors then you change the

010018.174 -- 010020.185
gender of the other one. To become

010020.185 -- 010022.445
a male. Again, there's a stronger correlation

010022.445 -- 010024.705
in the data between gender and job type.

010024.865 -- 010027.205
Totally off, of course. A lot of drama,

010027.795 -- 010030.135
from causal lenses not not right, of course,

010030.515 -- 010033.015
off. But not surprising. Correlations

010033.075 -- 010034.165
couldn't make it. It's like

010036.665 -- 010038.745
this is another one from this is from one or two

010038.745 -- 010040.795
weeks ago. It's not published, but we ask,

010041.194 -- 010043.235
generate the the carrot with the

010043.235 -- 010045.244
rabbit then we get the non causal

010045.245 -- 010047.665
and essentially we kind of replace the

010047.900 -- 010049.545
carrot with the wolf.

010050.345 -- 010052.765
But now, if you use a causal version of that,

010053.004 -- 010055.404
got a thing that is counterfactual plausible here,

010055.404 -- 010057.545
that the the rabbit is kind of

010057.545 -- 010059.885
running away from the wolf. It makes more sense.

010100.045 -- 010101.535
Than being a docile kind of

010102.174 -- 010104.355
cat rabbit as in the first. This is an unexpected

010104.575 -- 010106.759
one, that you should, and I'm not sure if you're familiar with

010106.759 -- 010108.845
the student that brought me this example, You

010108.845 -- 010111.015
asked Jared, what if is a wolf I

010111.015 -- 010113.055
thought it's cute because didn't expect to generate

010113.055 -- 010115.195
a baby wolf. Then it's okay that,

010115.355 -- 010117.035
it will be still playing ball.

010117.595 -- 010119.655
The the the rabbit with the the baby wolf.

010119.815 -- 010121.895
Just when you are too big that maybe

010121.895 -- 010124.115
it's not okay. The

010125.305 -- 010127.385
anyhow, this is an example of counterfactual human

010127.385 -- 010129.484
generation. We can do examples

010129.484 -- 010131.525
like that with text as well. I I

010131.525 -- 010133.865
would defer, but it's this exactly the same

010134.005 -- 010136.320
principle. Let

010136.320 -- 010138.365
me just Oh, my

010138.365 -- 010140.605
mouse is here. Let me skip this one, but it is about

010140.605 -- 010142.625
causal understanding. It's part two

010142.625 -- 010144.725
of the book. Let me move, just given that we are

010144.855 -- 010146.935
in the RL, let me move to the next one.

010146.935 -- 010149.265
Give me a second. Where am I Yeah.

010149.664 -- 010151.724
This is now yeah.

010151.724 -- 010153.804
I would like to do example about efficient and precise

010153.805 -- 010155.915
decision made. Even the URL dimension here.

010156.315 -- 010158.515
And we spend don't know, the last ten

010158.515 -- 010200.725
years thinking about causal RL. It's about

010200.725 -- 010202.885
what's the relationship between decision making

010202.885 -- 010205.035
and causal knowledge. And,

010205.234 -- 010207.254
if you want to know more, part

010207.255 -- 010210.255
three of my book or the website is crl.causalai.net.

010211.674 -- 010213.855
Now, this is the cartoon that we have before,

010214.175 -- 010216.254
which we have the environment and we have the agent

010216.255 -- 010218.395
on-site. Now the observation here

010218.395 -- 010220.474
here the here that we started from touring

010220.474 -- 010222.635
conversation is about two cube observations to

010222.635 -- 010224.575
move from the RL to the causal RL.

010225.005 -- 010227.165
First one is that the environment and the agent are

010227.165 -- 010229.394
are tied to a formal pair,

010229.395 -- 010231.634
SCMM collection of causal

010231.635 -- 010234.035
mechanisms, are the underlying kind of data generated

010234.035 -- 010236.254
model. And in the left side, we have some type

010236.255 -- 010238.365
of causal model g. A causal graph,

010238.665 -- 010240.714
for example. Second, observations, we

010240.714 -- 010242.734
will define different types of experiences

010243.035 -- 010245.115
or interactions of the system to avoid ambiguity

010245.115 -- 010247.434
we are using the PCH, seeing, doing,

010247.435 -- 010249.545
and imagining. Then those are the two

010249.545 -- 010251.675
things that can unravel as

010251.675 -- 010253.984
ten years old. Maybe my life and some

010253.985 -- 010254.735
of students.

010256.585 -- 010258.925
And we just released, before new reps,

010259.085 -- 010300.945
for new reps, there is some kind of GitHub

010301.165 -- 010302.545
with the colossal gymnasium.

010303.535 -- 010305.714
Is kind of our attempt to try to build the infrastructure

010305.775 -- 010307.805
to allow people to benchmark the

010307.865 -- 010309.525
causal capabilities of the agent.

010311.285 -- 010313.525
Very exciting looking for collaborators here.

010313.525 -- 010315.665
This is version 0.1. It's huge effort.

010315.935 -- 010317.875
Terms of coding and infrastructure building.

010318.015 -- 010319.315
Mainly, I think it's exciting.

010320.075 -- 010321.945
And, happy to get your feedback and

010322.515 -- 010324.605
any interest. But in any case,

010324.605 -- 010326.619
there are different once we take this idea

010326.620 -- 010328.715
of modeling experience in different

010328.715 -- 010330.975
ways, there are many new challenges and opportunities

010331.115 -- 010331.855
that appear.

010333.434 -- 010335.224
Our first task that we studied

010337.625 -- 010339.725
is causal offline to online learning. We call

010340.215 -- 010342.695
cool that is some kind of high generalized policy

010342.695 -- 010344.395
learning that we are trying to combine

010345.585 -- 010347.635
offline with online learning. Had

010347.635 -- 010350.135
been going on for a few years. Those are the references there.

010353.105 -- 010355.184
And here is an example of coup. This is the

010355.185 -- 010357.325
context that we we we have

010357.325 -- 010359.404
done the the experiments. I will not read

010359.405 -- 010401.125
the whole thing, but this is a cancer can't

010402.165 -- 010404.665
context, and we have the we are trying to do chemotherapy

010406.685 -- 010408.815
and other kind of treatment, The

010409.355 -- 010411.305
this is a randomized controlled trial.

010411.625 -- 010413.704
Then this is a two stage trial in

010413.704 -- 010415.810
the I have the x one and x two that is

010415.811 -- 010417.928
the the two shots. The patient get the the

010417.929 -- 010419.995
drug, the treatment, He comes back

010419.995 -- 010422.075
later, a month later, and this we decide what

010422.075 -- 010424.095
he'll do. There's a verbal ass that is

010424.095 -- 010425.945
the state of the patient, and

010426.345 -- 010428.504
different than the MDP or POMDP, there

010428.505 -- 010430.595
is this variable u there. Is

010430.595 -- 010432.755
affecting everyone in the system. It's a confounder a

010432.755 -- 010433.735
global confounder.

010437.805 -- 010440.045
And now this is the kind of results that we can

010440.045 -- 010442.295
get. If you're doing some kind of

010442.295 -- 010444.534
usual or Susan Murphy's, if you know some

010444.535 -- 010446.570
kind of dynamic treatment

010447.425 -- 010449.615
allocation, treatment regime, the

010449.615 -- 010451.954
plot here, x axis is the number of episodes

010452.305 -- 010453.694
y axis is the cumulative Higrat.

010454.605 -- 010456.685
And if you do the standard in the real literature

010456.685 -- 010458.545
that we are just doing RL slash randomization,

010459.405 -- 010501.565
you get the red curve. So

010501.805 -- 010503.963
On the other hand, if you use this causal knowledge,

010503.964 -- 010506.204
the structure that we have there, that is saying nothing more

010506.204 -- 010507.765
than what happening in the world,

010508.355 -- 010510.675
Right That, you got a treatment x one and this affects

010510.675 -- 010512.724
the state. And these states affects the

010512.725 -- 010514.805
outcome y at the end. That is the survival of the

010514.805 -- 010517.005
patient. After one year of taking the treatment

010517.005 -- 010519.015
and so on. They you get the the

010519.015 -- 010521.204
blue curve. This is, of

010521.204 -- 010522.644
course, much better. And,

010523.525 -- 010525.655
the the great is already a huge improvement related

010525.655 -- 010527.815
to the the the red just using basic

010527.815 -- 010529.965
causality and some type of some

010529.965 -- 010532.095
learning period not crazy. The interesting

010532.155 -- 010534.425
one here is you can prove results like that.

010535.225 -- 010537.425
Formally. Interesting one to me is the the

010537.425 -- 010539.685
green one. This is the one if you have observational

010539.745 -- 010541.845
data, now

010542.085 -- 010544.165
you can pretty quickly, the problem becomes

010544.165 -- 010546.365
super easy. Right Even

010546.365 -- 010548.625
though observation layer one is different than layer two,

010548.625 -- 010550.705
if you use observation naively, I'm not

010550.705 -- 010552.735
showing here, this will slow down.

010552.755 -- 010554.775
Or even, harm the convergence.

010555.875 -- 010557.930
Hinder the convergence. Or if you use

010557.930 -- 010600.335
the the observational data properly,

010600.335 -- 010602.345
the problem becomes easier. We don't

010602.345 -- 010604.425
even need to do experimentation. I don't even have the scale

010604.425 -- 010606.285
here. The Zoom is not enough. And,

010606.685 -- 010608.785
the speed up is huge. Then I think it's a good one.

010609.760 -- 010611.775
They they are not a task. When

010611.775 -- 010612.995
when and where to intervene

010614.065 -- 010616.085
Usually, if I put my RL hat,

010616.325 -- 010618.484
and I have all these degrees of freedom where I should

010618.484 -- 010620.575
intervene, if I one, x two,

010620.575 -- 010622.624
x 100, I'll try to intervene in all

010622.625 -- 010625.055
of them. Zero zero zero zero zero

010625.115 -- 010627.155
one, All the combinations to see

010627.155 -- 010629.225
what will be leading to the highest the

010629.270 -- 010630.795
the highest outcome.

010631.815 -- 010634.220
The the the

010634.775 -- 010636.854
but sometimes and this is the line of work that I'm

010636.855 -- 010639.234
not talking much, but sometimes I can just go

010639.535 -- 010642.035
to the variables x two and x 27.

010642.325 -- 010644.484
And leave the other 98 degrees of

010644.484 -- 010646.514
freedom there not touch, just let it

010646.515 -- 010648.594
naturally vary, and these will lead

010648.595 -- 010650.724
to the highest outcome. Then this

010650.725 -- 010653.145
was a kind of mind blowing for at least for me

010653.205 -- 010655.254
result that wasn't NewRePS 18.

010655.395 -- 010657.545
We kind of carry over. There's many ways of

010658.025 -- 010700.025
generalizing that with a question about when should we interfere

010700.275 -- 010701.914
when should we inter when

010702.295 -- 010704.665
intervene Yes or no Maybe no Never

010704.665 -- 010706.745
Or when Then if it's when, where should

010706.745 -- 010708.864
we touch the system Should Should do the

010708.865 -- 010711.105
intervention to bring about the changes Not all the

010711.105 -- 010713.235
variables as we previously

010713.375 -- 010713.875
believed.

010717.915 -- 010720.275
Another kind of task is related to layer three,

010720.275 -- 010722.475
we also learn It's called counterfactual

010722.475 -- 010723.225
decision making.

010724.665 -- 010726.825
That we are I I would just say that it's very related

010726.825 -- 010728.914
to the idea of free will And,

010729.135 -- 010730.575
yeah. I think I I and,

010732.115 -- 010734.355
and the different, different way of there's a different

010734.355 -- 010736.514
optimization function than

010736.515 -- 010738.565
the one that we usually use in

010738.565 -- 010740.884
RL. I will defer, given the time, but it's very

010740.885 -- 010742.775
interesting. Other's calls are

010743.315 -- 010743.665
imitation learning.

010745.725 -- 010747.775
Believe that, assumption there

010747.775 -- 010749.795
that we are doing some type of behavioral cloning

010750.665 -- 010753.165
IRL, and, the key and everything

010753.225 -- 010755.285
is good. If you give more data.

010755.505 -- 010757.364
Key assumption there that the two agents

010757.605 -- 010800.055
are kinda identical in their causal models.

010800.295 -- 010802.195
In their causal capabilities.

010802.355 -- 010804.704
Is almost never the case. No true agents

010804.705 -- 010806.945
are not the center. No true agents are created

010806.945 -- 010809.004
equal. Now we started this line of

010809.005 -- 010811.165
work saying, if one agent different than the other,

010811.165 -- 010813.004
just change one bit. From

010813.385 -- 010815.865
to another, can do situations that you have perfect

010815.865 -- 010818.015
cloning and the performance of the agent.

010818.015 -- 010820.494
The learning agent is already completely is a complete

010820.495 -- 010822.594
disaster. Then it's a kind

010822.595 -- 010824.204
of challenge of the key assumption

010824.765 -- 010826.925
underlying the known cause, I would say, or the the the the

010826.925 -- 010829.034
previous version of the imitation

010829.035 -- 010831.334
learning. We did a causal layer there And,

010831.775 -- 010833.044
towards going to

010833.805 -- 010835.905
should go towards the real world. In reality,

010835.905 -- 010837.384
to be real a way.

010839.195 -- 010841.435
There's curriculum learning as well that is kinda

010841.435 -- 010843.814
more, recent

010843.814 -- 010845.904
results. There is

010845.905 -- 010848.125
keyword shaping as well that we like to use with

010848.605 -- 010849.735
offline data.

010851.145 -- 010852.364
Me see how much time

010853.645 -- 010854.795
How much time do I have

010859.215 -- 010859.715
So

010901.365 -- 010903.435
you are shaping This is just an example.

010904.374 -- 010906.444
Cartoon here for class, but is in the

010906.444 -- 010908.764
simulator too that a robot is in a maze.

010908.765 -- 010910.854
Robot is in a maze and the the

010910.854 -- 010912.155
walls are made of lava.

010913.015 -- 010914.435
Which is highly little.

010915.555 -- 010917.795
Important here, the agents' movements

010918.015 -- 010920.115
are, affected by the wind.

010920.385 -- 010922.544
That is indicated by these symbols, the the

010922.544 -- 010924.664
circle and the arrows, but the

010924.665 -- 010925.705
agent doesn't see it.

010927.305 -- 010929.404
And, and the question is,

010929.405 -- 010931.564
how can the agent escape the maze without getting

010931.565 -- 010932.054
hurt

010934.055 -- 010936.235
And we are trying to generate this bread of crumbs.

010936.374 -- 010938.413
Right The the hero in a way that assuming

010938.413 -- 010939.765
that there is this

010940.805 -- 010942.984
systematic source of bias, is the

010942.985 -- 010945.304
wind that makes the the agent the the sensor related

010945.305 -- 010946.225
to wind, I guess,

010947.425 -- 010948.165
is damaged.

010950.064 -- 010952.225
And more broadly, how can you design a HE watch

010952.385 -- 010954.744
already said function that enables the agent to minimize

010954.745 -- 010956.834
online experimentation. In other words, you don't

010956.835 -- 010958.845
want the the agent in the real world to be

010958.845 -- 011001.225
experimenting. We like to leverage the data that you already

011001.225 -- 011003.105
have collected. It's coming from layer one.

011004.235 -- 011006.344
Where is the agent already moved

011006.345 -- 011007.244
here Yeah.

011008.465 -- 011008.845
Okay.

011011.035 -- 011013.195
And and and there is the baseline that is

011013.195 -- 011014.905
non causal and the causal one. The

011015.385 -- 011017.535
is in the bottom. And, the difference

011017.535 -- 011019.735
here is like of course, the agent that is,

011019.945 -- 011021.965
on the top is, like, very easily following in

011021.965 -- 011024.155
the lava because it's not taking into account the

011024.155 -- 011026.315
wind in a way, and the causal method in some

011026.315 -- 011028.555
way is able to protect against that. The

011028.555 -- 011030.794
first kind of column shows that. The second one, the agent is

011030.794 -- 011032.945
very temperature got to this tunnel, There is

011032.945 -- 011034.565
these coins that are very valuable.

011035.265 -- 011037.214
Of course, the causal agents avoids it.

011037.455 -- 011039.535
There is no way, based on this huge

011039.535 -- 011041.595
amount of partial observability, that

011041.595 -- 011043.825
he can go through the tunnel. And there's some type

011043.825 -- 011046.045
of conservative policy based

011046.045 -- 011048.365
on causal constraints that the agent is able

011048.365 -- 011050.453
to to to do well. Same here in

011050.453 -- 011052.484
the on in the last column. There is a little

011052.485 -- 011054.495
coin there that is very close to the lava,

011054.655 -- 011056.735
The known causal agent is trying to get this

011056.735 -- 011058.804
coin, causal agents already know that it's

011058.805 -- 011101.005
impossible. Because it's too dangerous. A

011101.005 -- 011103.265
way. Given that he doesn't know

011103.485 -- 011104.855
huge amount of the environment.

011105.815 -- 011108.235
Of course, the question is how to do that I'll defer

011109.040 -- 011111.115
to to read a paper or to talk

011111.175 -- 011111.895
with me offline.

011113.254 -- 011115.414
This is one task, another one that's kind

011115.415 -- 011117.425
of very exciting. That is about,

011117.885 -- 011120.085
everything that I'm saying here is single agent.

011120.085 -- 011122.245
Very recent, maybe two years ago or one year

011122.245 -- 011124.605
ago, two years maybe, we start seeing

011124.985 -- 011127.045
strategic settings in which we have,

011127.205 -- 011129.285
multiple players. And there's a kind of version

011129.285 -- 011131.545
of game theory is based on causality.

011133.205 -- 011135.545
Here is just an example. There are two prisoners

011135.605 -- 011137.615
that suspect for a crime, but there is no

011137.615 -- 011139.854
evidence convince convict them. PD. Right

011139.854 -- 011141.915
Prisoner's dilemma. Then

011142.065 -- 011144.164
they may either cooperate or the fact, the the

011144.165 -- 011146.405
c or d, and this is the variable

011146.405 -- 011148.565
x I. Also, they don't know what choice

011148.565 -- 011150.615
the other fellow is doing the other guy is

011150.615 -- 011152.775
doing. But now in addition to the

011152.775 -- 011154.814
setting, have a new new part here

011154.815 -- 011156.275
in the causality sense

011157.035 -- 011159.145
that the interrogation process by

011159.145 -- 011201.325
the police police officers

011201.325 -- 011203.104
may subconsciously affect

011203.645 -- 011205.725
their decision and contain formation about

011205.725 -- 011207.105
their faith. That is

011209.455 -- 011211.345
that is shown in this variable, u one.

011211.505 -- 011213.585
Then there is the police officer one that is

011213.585 -- 011215.745
affecting the decision x one and is affecting the

011215.745 -- 011218.205
outcome. The guy's kinda trying to bully.

011218.265 -- 011220.645
Maybe the the the the prisoner to the

011221.124 -- 011221.715
or to cooperate.

011223.155 -- 011225.414
Now the this can be described

011225.415 -- 011227.915
by other labs. Maybe you have a we we do the experiment.

011228.115 -- 011230.355
That you kinda wrap other lambs around each

011230.995 -- 011233.154
around each of the prisoners and the police officers and

011233.155 -- 011234.125
so on and the judge.

011235.965 -- 011238.014
It's kinda cute to see how the alarms are

011238.175 -- 011240.255
discussing between themselves. I'll skip for the time that I just

011240.255 -- 011242.285
have one minute, I believe. Or less.

011243.024 -- 011244.564
But now, interesting enough,

011246.105 -- 011248.145
the the usual game theory in the Nash,

011248.185 -- 011249.495
the the the

011250.615 -- 011252.915
the the x axis here is the probability

011253.455 -- 011255.625
that the prisoner can naturally read

011255.865 -- 011258.264
if the situation is favorable or not favorable.

011258.665 -- 011300.825
What is the intuition of the agent He doesn't know

011300.825 -- 011302.964
how to explain but he just got the gut feeling.

011303.955 -- 011306.115
The and the y axis here is the reward or

011306.115 -- 011307.694
expected utility in this case.

011308.175 -- 011310.265
Now the red line here is the game theory of the Nash

011310.265 -- 011312.345
equilibrium. The blue one is the

011312.345 -- 011314.445
one that if the agent is acting like layer one,

011314.685 -- 011316.315
like automata. Gut feeling.

011316.715 -- 011318.875
Then after sometimes, if the reading of the room

011318.875 -- 011321.004
is not very good, can see that if

011321.005 -- 011323.085
the engine the the blue line are below the

011323.085 -- 011325.205
red. The agent starts getting

011325.205 -- 011327.385
screwed. Then it may make sense

011327.385 -- 011329.465
to use the game theory in the settings. But then

011329.465 -- 011331.394
you have the green one here that is layer three.

011331.475 -- 011333.555
That is the counterfactual. It turns out to

011333.555 -- 011335.895
dominate the agent that is doing ASH.

011336.655 -- 011338.495
And dominate dominate the agent that is doing,

011339.294 -- 011340.274
following their gut.

011342.175 -- 011344.214
Are the grass just ruled the conclusions here for

011344.214 -- 011346.135
the time. I think it's quite exciting and re

011346.455 -- 011348.385
new stuff. Happy to talk about that.

011348.625 -- 011350.705
Let me skip this one. Give me a second. I'll go to the

011350.705 -- 011351.205
conclusions.

011355.650 -- 011357.715
The our general goal, the

011357.935 -- 011400.234
causal AI research program to

011400.235 -- 011401.855
develop more general and trustworthy

011402.365 -- 011404.705
AI systems and download the following capability.

011405.195 -- 011407.294
They have the capability of doing causal and counterfactual

011407.435 -- 011409.685
generation. Having

011409.685 -- 011411.725
some kind of causal of

011411.725 -- 011413.505
the system and the ability of articulating

011413.805 -- 011414.305
explanations,

011416.185 -- 011418.505
being more efficient and precise in terms of their decision

011418.505 -- 011420.605
making, being more generalized

011420.825 -- 011422.935
when robust, when you have changing

011422.935 -- 011424.794
condition and moving across environments,

011425.675 -- 011427.695
and they do some kind of model learning

011427.965 -- 011429.984
a world learning and and discovery.

011430.815 -- 011433.315
Again, this is in the book. Happy to get your feedback.

011434.395 -- 011436.714
Also check the LLM of salvatori.org.

011436.714 -- 011439.025
I didn't have time, but it's a different thread here.

011439.265 -- 011440.985
Related to benchmarking LLMs.

011441.705 -- 011443.864
There's a top of the dimension to most. We can kind of

011443.865 -- 011445.904
combine them, but all the problems here I still in

011445.905 -- 011447.485
the current system.

011448.045 -- 011449.825
Thank you. Also, the NSF, DARPA,

011450.285 -- 011452.695
and so that support in the lab.

011453.635 -- 011455.874
And the and the club the the students

011455.875 -- 011457.885
that are doing the work. I'm just

011457.885 -- 011459.565
presenting here and the the

011500.020 -- 011501.135
collaborators. Thank you.

011508.985 -- 011511.305
So thanks professor Birrah, for the

011511.305 -- 011513.385
wonderful talk. And for the sake of timing, we we

011513.385 -- 011515.395
might just skip the question for this, but feel

011515.395 -- 011517.445
free to reach out to the person. And our

011517.445 -- 011519.555
next speaker today will be professor Sanjay

011520.225 -- 011522.415
Fiddler from University of Toronto. Who's

011522.415 -- 011524.574
also affiliated with the professor faculty at the Vector

011524.575 -- 011526.735
Institute and also the VP of AI

011526.735 -- 011528.738
Research and In Media. And today her

011528.739 -- 011530.765
talk will be towards world

011530.765 -- 011532.864
models for autonomous driving. I'll give

011532.865 -- 011533.935
it to you, professor.

011541.024 -- 011543.215
Sorry. Too many cocktails during the week.

011544.745 -- 011545.565
Let me connect.

011720.605 -- 011722.795
Okay. Can you hear me now

011722.795 -- 011724.904
Okay. Great. Yeah.

011724.905 -- 011727.184
Thanks for inviting me. I'm Sonia

011727.185 -- 011728.945
and today I'm gonna talk about,

011729.755 -- 011731.835
world models for autonomous driving

011731.835 -- 011733.985
and I'm just a humble representative of the

011733.985 -- 011736.325
team, the spatial intelligence lab at in Birrahood.

011736.525 -- 011737.985
Who's doing all this great work.

011739.085 -- 011741.165
My talk is gonna be super different than the previous

011741.165 -- 011743.305
one, it's just gonna be lots of videos.

011743.305 -- 011745.545
You guys have had a lot of math during the week,

011745.545 -- 011747.715
so I'll just so spare you that.

011748.955 -- 011751.195
I'm running to the airport after this, so

011751.195 -- 011753.274
start waving at 10AM. If I'm still here,

011753.275 -- 011755.565
super chat. Okay Code red.

011757.965 -- 011759.145
I Okay.

011800.065 -- 011801.615
So, can you guys see anything

011802.385 -- 011804.725
Yeah. So a couple of years ago, right, everyone

011804.785 -- 011807.124
has seen kind of conquer the

011807.124 -- 011809.255
world. Right That was the era of of

011809.255 -- 011811.515
generative AI, and really unlocked

011811.515 -- 011813.745
so many different applications. And

011813.745 -- 011815.765
since then, we we have multimodal

011815.825 -- 011817.964
model, actual language models, and they're

011817.965 -- 011820.135
all kind of working synchrony

011820.135 -- 011822.615
together towards agentic AI to to

011822.615 -- 011823.795
solve complex problems.

011825.235 -- 011827.314
This was a super exciting area, but

011827.314 -- 011829.614
us work on computer vision or

011829.615 -- 011831.235
three d or robotics,

011832.155 -- 011834.215
were kind of sidelined. We weren't

011834.215 -- 011836.165
we weren't in a school anymore. But that

011836.415 -- 011838.885
that's changing. So the the next

011838.885 -- 011841.084
arrow that's coming is is physical

011841.084 -- 011843.274
AI. Robots are coming. And, of

011843.275 -- 011845.435
course, robots has been around for for

011845.435 -- 011847.310
many decades. Right So

011847.525 -- 011849.605
you might be wondering what what's different this

011849.605 -- 011851.805
time. And it's actually

011851.965 -- 011854.045
these big foundation models are going to have

011854.045 -- 011856.405
a huge impact in actually making this

011856.705 -- 011859.115
a reality. Right Because you're packing basically

011859.115 -- 011901.275
the entire human intelligence into these

011901.275 -- 011903.365
models, that we just need to get

011903.365 -- 011904.825
working on on the robots.

011905.544 -- 011907.705
Reasoning, the visual understanding, it's all

011907.705 -- 011909.759
kind of there. We just need to kind of connect it to

011909.760 -- 011911.995
the control and and make it work.

011914.115 -- 011916.224
Sounds easy, but maybe not. Alright. So this

011916.225 -- 011918.715
is kind of like the plot of how we're thinking while robots

011918.715 -- 011921.165
Right So on the bottom, we have environments,

011921.225 -- 011923.304
so kind of the increasing complexity of

011923.305 -- 011925.354
the environments. And on the

011925.354 -- 011927.774
y axis, we have skills that these robots are expected

011927.834 -- 011929.945
to perform. And, of course,

011929.945 -- 011932.365
like, the simplest environments we have, these robots

011932.425 -- 011934.745
are kind of limited in first of all, they're limiting

011934.745 -- 011936.825
the skills that can actually perform, and they're also

011936.825 -- 011938.975
not, like, super useful. Right So if you

011938.975 -- 011941.004
have manipulation robots, they're

011941.005 -- 011943.245
kind of, like, stuck on a particular table,

011943.245 -- 011945.395
and they they can do whatever you know,

011945.395 -- 011947.214
interact with the environment that's around them.

011947.855 -- 011949.935
And then we have autonomous driving. Right

011949.935 -- 011952.175
Which is really gonna becoming a a

011952.175 -- 011953.245
reality these days.

011954.345 -- 011956.744
The environment is entirely more complex.

011956.745 -- 011958.844
Right It's traffic. It's like so

011958.845 -- 012000.925
many things can happen on the road that I think

012000.925 -- 012003.164
is overtaking. Anything can kind of jump in front

012003.165 -- 012004.075
of the car.

012005.475 -- 012007.714
Super complex. So these robots actually need

012007.714 -- 012010.005
a really strong understanding

012010.225 -- 012012.354
of the world. Around them and also

012012.354 -- 012014.514
what's going to happen next so they can convert it

012014.515 -- 012016.765
into an action. And, obviously,

012016.765 -- 012019.005
kind of at a at a north star, there's

012019.005 -- 012021.115
a humanoid robot which you're represents

012021.115 -- 012023.184
kind of the general robotics. Right Like, I've done the

012023.185 -- 012025.595
most general robots the more generalized

012025.655 -- 012027.874
skills. Meaning that they can they need

012027.874 -- 012030.295
to do most complex

012030.435 -- 012032.595
environments. You know, work in a factory, drive

012032.595 -- 012034.135
you home, and then cook dinner.

012034.685 -- 012037.004
Right So, the expectation on the skill

012037.004 -- 012037.745
just grows.

012039.075 -- 012041.255
So today, I'm gonna mostly

012041.755 -- 012043.835
focus on autonomous driving, where kind of we

012043.835 -- 012046.034
have been focused on. And

012046.035 -- 012048.115
the reason why I chose this application, actually,

012048.115 -- 012049.885
when we joined NVIDIA is because

012050.285 -- 012052.325
it was an application with a lot of data.

012052.965 -- 012055.024
So you know, robotics is is coming,

012055.024 -- 012057.084
but a lot of the times, you're limited in the

012057.084 -- 012059.095
data you can collect with a robot. That's

012059.095 -- 012101.494
not the case with a car. Probably almost

012101.495 -- 012103.645
everyone in the room has a car. Right And

012103.645 -- 012105.925
we can actually collect data at scale. So

012106.085 -- 012108.165
feels like the first application with all this

012108.165 -- 012110.105
technology can actually come to fruition.

012112.845 -- 012115.185
Okay. So these days, right,

012115.565 -- 012117.805
a lot of the autonomy companies are

012117.805 -- 012119.825
are working on this kind of, like, architecture

012119.965 -- 012122.044
of the policy. Right So it's an end to end

012122.045 -- 012124.334
model. That takes some sensor

012124.335 -- 012126.515
reservation, maybe high high level action

012127.435 -- 012129.055
or high level kind of navigation,

012129.515 -- 012130.735
by the user user.

012131.605 -- 012133.845
And then it produces a set of action tokens.

012133.845 -- 012135.985
Right For example, a trajectory of where it should

012135.985 -- 012138.374
be moving next. And that kind of controls

012138.375 -- 012140.545
the car. Right And in

012140.545 -- 012142.715
the last maybe year or so, we've

012143.275 -- 012145.615
seen the transition towards including reasoning

012146.175 -- 012147.785
as part of this this

012148.425 -- 012150.585
action policy. Right For

012150.805 -- 012152.944
example, a lot of the if you if you

012152.945 -- 012155.445
wanna go to toward l four and l five,

012155.685 -- 012157.765
there is a lot of cases where maybe the car

012157.765 -- 012200.235
just gets stuck. It's very to kind of solve with traditional

012200.615 -- 012202.684
systems. It's even hard to solve

012202.685 -- 012204.765
with end to end models because end to end

012204.765 -- 012206.125
models typically train with,

012206.845 -- 012208.964
imitation learning. And maybe you just haven't

012208.965 -- 012210.905
seen some of those tail scenarios.

012212.135 -- 012213.615
This is really where kind of the

012214.175 -- 012216.435
premises or people think that reasoning models

012216.495 -- 012218.035
can actually just use

012218.635 -- 012220.975
know, human intelligence to solve those problems.

012221.315 -- 012223.395
I don't need to go to the cloud and query a

012223.395 -- 012224.935
human to get me unstuck

012225.795 -- 012228.115
from behind, you know, like a double parked car or what

012228.115 -- 012230.273
whatnot. I can just actually

012230.274 -- 012232.255
just ask an LLM what I should be doing.

012232.575 -- 012234.855
Okay So this is basically

012234.855 -- 012237.205
kind of where the community is trying to go.

012239.044 -- 012241.234
Now, obviously, this is like one

012241.235 -- 012243.395
model that in the end of the day is gonna run on

012243.395 -- 012245.085
the road. It's gonna run on the car.

012245.485 -- 012247.644
However, to make this work, actually,

012247.645 -- 012250.045
there is a gigantic infrastructure

012250.105 -- 012251.785
around making this model work.

012252.265 -- 012254.425
Just on its own, just collecting data and kind

012254.425 -- 012256.665
of, like, training this is obviously not gonna cut

012256.665 -- 012259.064
it. And one

012259.064 -- 012301.195
important part of that infrastructure

012301.655 -- 012302.955
is obviously simulation.

012303.775 -- 012305.855
Right So if you have end to end models that

012305.855 -- 012307.635
go from sensor data to action,

012308.125 -- 012310.625
and now I'm starting to tweak these different parameters,

012310.685 -- 012312.695
I need, like, some very quick signal of

012312.855 -- 012314.935
am I actually doing the right kind of choices

012314.935 -- 012316.555
How well is my model performing

012317.545 -- 012319.705
I just can't go to the real world to do that.

012319.705 -- 012322.075
That would be so inefficient Right Especially

012322.075 -- 012324.234
as we're starting to solve the long tail

012324.235 -- 012326.364
problems, just cannot go out and

012326.365 -- 012328.444
get all that, evaluated in the real world.

012328.925 -- 012331.085
Right So simulation is actually a key

012331.085 -- 012331.885
part of this,

012333.665 -- 012335.844
pipeline. Right So we

012335.845 -- 012338.065
want this policy to actually

012338.065 -- 012340.225
be driving in a completely simulated world,

012340.225 -- 012342.285
and this world should behave exactly

012342.345 -- 012344.645
like the real world. And if we can

012344.645 -- 012346.345
actually operationalize this,

012346.854 -- 012349.095
this can really be, like, so much faster

012349.095 -- 012351.105
development in the future. Know,

012351.105 -- 012353.345
kind of my dream is that maybe a ten ten years

012353.345 -- 012354.835
from my fifteen years from now,

012355.395 -- 012357.415
a grad student would be able to train an autonomous

012357.475 -- 012359.585
car. Right We just wanna

012359.585 -- 012401.945
kind of provide the core infrastructure and models.

012403.705 -- 012405.915
So So today, I'm going to talk a little bit about

012405.915 -- 012407.955
our journey on building this simulation. I

012407.955 -- 012410.035
pretty much started working on this as as

012410.035 -- 012411.175
as I joined NVIDIA.

012413.085 -- 012415.215
And in 2020, Nerf

012415.215 -- 012417.425
came out and and

012417.425 -- 012419.585
then Thomas Miller and Nvidia already started working

012419.585 -- 012421.755
on making that fast Right If you

012421.755 -- 012423.874
guys remember, Norforce actually rendering pretty slow,

012423.874 -- 012425.975
so it wasn't, like, ever useful technology at the

012425.975 -- 012428.435
time. But then we started seeing

012428.495 -- 012430.655
signs of actually this can go much, much faster

012430.655 -- 012432.745
and even real time. And

012432.885 -- 012434.964
we always the next day, we switched, and we said, okay.

012434.965 -- 012436.945
The killer app for that technology

012437.835 -- 012440.095
is going to be simulation for autonomous driving.

012440.765 -- 012442.845
Why Because it's actually a robot

012442.845 -- 012445.345
that doesn't need so much interaction with the environment.

012445.725 -- 012447.675
It's basically the physics that's on the ground.

012447.965 -- 012450.064
Whenever you hit something, you stop the simulation,

012450.525 -- 012452.325
obviously. Just bad, bad, bad car. Right

012453.475 -- 012455.635
And then everything else, you know, maybe

012455.635 -- 012458.115
maybe it's just visuals, basically. Right

012459.875 -- 012501.975
So we started working on this pretty

012501.975 -- 012503.514
much maybe 2021.

012504.265 -- 012506.425
And at the time, we were kind of an academic lab,

012506.425 -- 012508.375
if you will. And through this, we kind of

012508.775 -- 012510.785
learned a journey to become an industry research

012510.785 -- 012513.185
lab. And, a lot of this technology

012513.185 -- 012515.345
I'm gonna show you now is actually running in

012515.345 -- 012517.485
the production enabling the our

012517.485 -- 012519.324
autonomous vehicles at NVIDIA.

012521.624 -- 012523.666
Alright. So this is basic we're

012523.666 -- 012525.775
taking we're gonna take recorded data

012526.025 -- 012528.054
from the car and the goal is to turn it

012528.055 -- 012529.435
into simulation environment.

012530.525 -- 012532.615
Right Why Because what's

012532.615 -- 012534.854
better better than simulating the real world than

012534.854 -- 012537.265
actually collecting the real world Right

012537.265 -- 012539.345
We just wanna make those kind of like

012539.345 -- 012541.504
static collected videos into something

012541.504 -- 012543.354
I can modify. I can

012543.575 -- 012545.655
actually make the car change my action and get,

012545.655 -- 012547.735
like, a reaction from the simulator.

012548.785 -- 012550.864
Okay. So what we're building basically or

012550.865 -- 012553.255
what we have built is, like, kind of like a dynamic

012553.795 -- 012555.955
scene graph representation where you have the map, and all the

012555.955 -- 012558.044
cuboids are going to be controlled

012558.044 -- 012559.604
outside of this NERF wall.

012600.165 -- 012602.015
And then, and then

012602.315 -- 012604.655
we have the I guess, these days, Gaussian particle to

012604.655 -- 012606.334
represent the scene in each of the agents.

012608.855 -- 012610.955
Okay. So basically, on the top left, that's

012610.955 -- 012613.135
kind of the captured data. And then

012613.135 -- 012615.455
as you do reconstruction, one important capability

012615.455 -- 012617.695
is the three sixty sixty reconstruction.

012618.255 -- 012620.335
Why Because on the car, you actually have a lot of

012620.335 -- 012622.435
sensors, not just one front camera.

012622.435 -- 012624.935
You actually have maybe 10 or even more sensors.

012626.095 -- 012628.325
LIDAR included. And

012628.325 -- 012630.405
as you know, right, this this, Gaussian

012630.405 -- 012632.715
spots or whatever allow you to change viewpoint

012632.955 -- 012635.195
is super important way because, for example, NVIDIA

012635.195 -- 012636.815
is working with multiple OEMs.

012637.425 -- 012639.845
Each one is going to have slightly different sensor

012639.984 -- 012642.015
configuration. Both in terms of position, but

012642.015 -- 012644.225
also the know, the actual sensors.

012644.285 -- 012646.314
So you wanna be able to kind of take

012646.314 -- 012648.395
one capture and kind of resimulate it

012648.395 -- 012650.455
in different different maybe car

012650.455 -- 012652.335
models, but also in different camera set.

012654.385 -- 012656.464
So here is like maybe we can even

012656.465 -- 012658.435
make it like a van or something.

012659.475 -- 012701.935
So And then another important aspect, obviously,

012702.515 -- 012704.675
modifying all the other agents because

012704.675 -- 012706.745
the in the end of the day, if I'm

012706.745 -- 012708.524
making new action in my simulator,

012709.215 -- 012711.695
then every other agent also need to be reacting

012711.695 -- 012713.685
to me. So I need to be able to control them.

012714.405 -- 012716.725
And, obviously, when I record from a video,

012716.725 -- 012718.785
I only see, like, sides of the cars,

012718.925 -- 012721.165
right side of other agents. So I need to kind

012721.165 -- 012723.485
of hallucinate the other part such that later

012723.485 -- 012725.565
when I'm modifying these other agents, I

012725.565 -- 012727.635
can actually kind of recover and

012727.635 -- 012729.795
not show, like, the empty space of, of these

012729.795 -- 012730.295
agents.

012731.874 -- 012734.114
And here is just an example of how now we

012734.115 -- 012736.354
can drive multiple times just show it's

012736.355 -- 012738.445
not a video. It's for the same scene.

012738.445 -- 012740.665
I'm gonna drive two times taking

012740.665 -- 012741.485
different actions.

012744.625 -- 012746.805
So some challenges obviously

012746.865 -- 012747.125
include

012749.265 -- 012751.425
scale, Right A car is driving

012752.345 -- 012754.505
what I I like sports car. I actually will

012754.505 -- 012756.554
drive pretty fast. So don't know. I'm

012756.555 -- 012758.634
not gonna say how much, but you can go quite fast on

012758.635 -- 012801.045
the highway. So

012801.045 -- 012803.205
even in thirty seconds, you can actually drive

012803.205 -- 012804.715
maybe kilometers. Right

012805.915 -- 012808.155
Typically, nerve settings in papers that you see,

012808.155 -- 012810.455
it's like a phone. Right So maybe maybe

012810.455 -- 012812.615
this room you can scan, but here we're talking

012812.615 -- 012813.755
like really large environment.

012815.995 -- 012818.095
You know, at the scale of, like, a whole racetrack

012818.235 -- 012820.175
or a a few blocks

012820.314 -- 012822.785
of a city. So

012822.945 -- 012825.104
that's one of the challenges. We actually built

012825.104 -- 012827.145
a really cool spatial framework for you guys in

012827.145 -- 012828.864
case someone is interesting to use it.

012829.185 -- 012831.505
That uses as far as data structure called VDB

012831.505 -- 012833.445
that we borrowed from the VFX community.

012833.855 -- 012835.635
It's all released open source.

012836.025 -- 012838.265
And, allows you to actually scale these

012838.265 -- 012840.765
environments much larger than with typical frameworks.

012843.535 -- 012845.694
The other aspect is just like, oh my god. You know There's

012845.695 -- 012847.753
so much stuff that can happen on the road.

012847.754 -- 012849.994
Right And it's so visually challenging to

012849.995 -- 012852.084
actually reconstruct this. And you

012852.085 -- 012854.245
don't really understand how challenging it is

012854.245 -- 012856.415
until you actually try it. I know,

012856.415 -- 012858.575
you know, in papers, you show a couple of results

012858.575 -- 012900.615
or so on. But if you actually wanna

012900.615 -- 012902.814
deploy this in a production system, there

012902.814 -- 012904.934
are just so many, so many things you need

012904.935 -- 012907.295
to think about. Right For

012907.295 -- 012909.334
example, like, the the traffic

012909.334 -- 012911.455
light you actually need to get it perfectly

012911.455 -- 012913.235
when it switches from red to green.

012913.585 -- 012915.545
And if you're not paying attention,

012915.685 -- 012917.884
maybe you could just do, like, a blend of the red and green.

012917.885 -- 012920.195
Right Because maybe that's how training. Right

012921.395 -- 012923.475
Text on signs, super important, right, for

012923.475 -- 012925.535
the cars to make decisions. It cannot

012925.535 -- 012927.575
be blurry. Right You actually interconstructed

012927.795 -- 012929.095
at a very high resolution.

012929.825 -- 012931.984
So these are, like, small things that actually

012931.984 -- 012933.124
really matter in practice.

012935.044 -- 012937.285
The other thing that matters is actually going,

012938.405 -- 012940.645
kind of off rails. Right Like, typically,

012940.645 -- 012942.805
when you drive, you go on a you you

012942.805 -- 012944.814
move forward. But if the car

012944.814 -- 012946.894
is now making different decisions, maybe it can actually

012946.895 -- 012949.205
change a lane So you actually need to support

012949.205 -- 012951.285
novel view synthesis maybe five meters or

012951.285 -- 012952.954
even more potentially. Right

012953.675 -- 012955.915
And that's typically as we know, these methods kind

012955.915 -- 012958.065
of start performing worse.

012959.185 -- 013001.445
So we build this maybe, like, a very tiny

013001.585 -- 013003.714
introduction of generate AI for this

013003.714 -- 013005.975
that actually fixes novel views

013006.355 -- 013008.435
and it then we can actually go

013008.435 -- 013010.535
up to five meters, very easily.

013012.465 -- 013014.705
We also kind of complete the cars. So even

013014.705 -- 013016.865
if you see them from just the side, we're able

013016.865 -- 013019.025
to with something similar

013019.025 -- 013020.305
to Sam three d, I guess.

013022.235 -- 013024.265
And then the other thing

013024.265 -- 013026.345
is harmonizer. Again, a very tiny

013026.345 -- 013027.885
little generative AI network.

013028.765 -- 013031.245
Which allows us to be super stupid

013031.245 -- 013033.345
in how we're actually composing scenes.

013034.365 -- 013036.685
No light transport. No that. We're basically just

013036.685 -- 013038.565
reconstructing cars or other objects

013038.805 -- 013040.855
from different scenes. Compose them in

013040.855 -- 013042.905
the most naive way, It looks

013042.964 -- 013045.205
super bad. And then the generator is

013045.205 -- 013047.524
just gonna insert shadows and make it look like

013047.524 -- 013049.575
it blends in. So it's like a

013049.575 -- 013052.075
super nice hammer to actually

013052.485 -- 013054.675
make this all this modification to the scene.

013056.535 -- 013058.795
Here, I'm just gonna show you two examples

013059.015 -- 013101.315
of Probably, the the guys in the in the back

013101.315 -- 013102.935
won't be able to see it. But basically,

013103.485 -- 013105.945
the just to kind of show you how this, like, fixing.

013106.105 -- 013108.185
The generative AI that's factually fixing all

013108.185 -- 013110.045
the views is is operating.

013110.375 -- 013112.655
Originally, the car just kind of drove forward,

013112.815 -- 013115.135
And in this example, we're gonna do

013115.135 -- 013117.305
a u-turn. Which is obviously not what we're gonna

013117.305 -- 013119.725
do in real production when we're gonna be testing.

013120.084 -- 013121.845
But just to kind of see how

013122.325 -- 013124.485
the novel view just goes really off rails

013124.485 -- 013126.885
on the left, and then with some fixing

013127.635 -- 013129.065
it starts looking pretty good.

013131.755 -- 013134.005
So on the left side, we're gonna take a

013134.005 -- 013136.195
u-turn You're gonna see it's already

013136.495 -- 013138.725
like a a you already maybe see some

013138.725 -- 013140.345
artifacts on the rendering.

013141.124 -- 013143.425
On the right side, it's still pretty good.

013147.325 -- 013148.624
Now we're gonna return.

013150.084 -- 013152.145
But It's

013152.145 -- 013154.365
not perfect, but you almost don't don't

013154.365 -- 013156.535
have any observations here. Right You were

013156.535 -- 013158.764
just driving forward. So it's pretty

013158.765 -- 013200.944
cool that you can actually even do that maneuver.

013203.435 -- 013205.515
We also made some modifications to the

013205.515 -- 013207.375
actual formulation of Gaussian's plots. Right

013207.615 -- 013209.475
Which is based on the rasterization,

013209.855 -- 013212.075
and we kinda retrace it. Which allows

013212.075 -- 013214.235
us to do all these secondary effects. You get

013214.235 -- 013216.625
all the reflections right. And because

013216.685 -- 013218.845
we also reconstruct all the geometry and

013218.845 -- 013220.935
stuff, and it's all in three d, we can

013220.935 -- 013222.444
edit it. So you can get

013223.005 -- 013225.085
nice, like, edited scenes with

013225.245 -- 013227.405
whatever, physics effects we wanna

013227.405 -- 013227.775
add.

013230.245 -- 013232.485
This is pretty cool too. We can also relate

013232.485 -- 013234.555
to the scenes. Even though, you know, maybe I

013234.555 -- 013236.694
recorded the the capture in

013236.754 -- 013238.874
the early morning, I can actually simulate

013238.874 -- 013240.985
all days all, times

013240.985 -- 013243.094
of the day. Which is really useful

013243.095 -- 013244.635
for, like, structured testing.

013250.854 -- 013253.095
Okay. So how is this this actually used,

013253.334 -- 013255.515
when your test Right So you have the policy

013255.515 -- 013257.215
on the left. It's producing action

013257.665 -- 013259.825
our simulator is basically kind of composed of

013259.825 -- 013302.225
three things. There's a behavior

013302.285 -- 013304.765
model that's kind of moving all the cuboids around

013304.765 -- 013306.815
in the map. We run

013306.815 -- 013308.794
physics because we reconstruct the ground

013309.275 -- 013311.355
in geometry so you can have all the collisions. If I go

013311.355 -- 013313.434
over a speed bump, I can actually simulate

013313.435 -- 013315.595
that a effect. And then, which

013315.595 -- 013317.785
is kind of this stack, is then rendering

013317.785 -- 013320.104
the scene. Right Because you can recompute all where

013320.104 -- 013322.265
all the agents are, where you you are based

013322.265 -- 013324.485
on all the kind of the the ground dynamics.

013324.545 -- 013326.635
And then you're rendering out, and that basically

013326.635 -- 013328.874
gives you kind of the next state. Can compute

013328.875 -- 013330.954
the rewards or basically some evaluation.

013331.035 -- 013332.895
And then this process repeats.

013333.865 -- 013336.205
So just a couple of examples here is, again, the

013336.425 -- 013338.425
policy. So

013339.495 -- 013341.324
driving multiple times in this scene.

013347.025 -- 013349.185
I wanted to show this plot because this whole slide

013349.185 -- 013351.414
which, of course, is pretty cool. So so

013351.415 -- 013353.674
far, we had to construct the 75,000

013353.975 -- 013355.705
scenes We're going after a million.

013357.025 -- 013359.125
And around 2,000 are constructed

013359.185 -- 013401.665
every day. Automatically,

013401.725 -- 013403.585
and then there's some queuing on top.

013403.855 -- 013406.175
And there is about 1,000,000 simulation

013406.175 -- 013408.545
rollouts per day that all the developers are doing

013408.545 -- 013410.714
in the AV. And

013410.715 -- 013412.795
the bottom plots are basically showing kind of

013412.795 -- 013414.919
our KPIs that we were he'll climbing when

013414.919 -- 013417.015
we were developing our tech. So for

013417.015 -- 013418.955
example, the plot on the right is showing

013419.075 -- 013421.235
kind of where we started on the left,

013421.235 -- 013423.455
which is you don't really see, but

013424.584 -- 013426.814
reproducibility of what happened

013426.815 -- 013428.975
in the real world. So, for example, imagine I draw

013428.975 -- 013431.004
somewhere and I had a ghost break, Now

013431.005 -- 013433.165
I do my reconstruction, and I resimulate the

013433.165 -- 013434.745
car the say exact same soft

013435.305 -- 013437.314
drives, exactly the same in the same

013437.314 -- 013439.255
scenario. And we wanna

013439.395 -- 013441.564
then see a ghost break. Right If it's

013441.565 -- 013443.645
not reproducible, then obviously something is

013443.645 -- 013445.795
wrong in our simulation. So when

013445.795 -- 013448.035
we started, the reproducibility wasn't super

013448.035 -- 013450.165
high. Not just because

013450.464 -- 013452.584
of the visual effects, some was also our homework we

013452.584 -- 013454.744
had to do and actually improve our our

013454.745 -- 013457.015
reconstruction, like traffic lights was definitely

013457.015 -- 013458.735
one of the problems and the tax.

013459.695 -- 013501.174
But also all these other

013502.135 -- 013504.395
effects, like just simulating the latency

013504.455 -- 013506.525
of different cameras when the policy

013506.665 -- 013508.765
interacting with the wall. The physics,

013509.165 -- 013511.405
kind of the whole hardware in the loop needs to kind

013511.405 -- 013513.575
of be like the real world. And that makes

013513.575 -- 013515.635
it more and more realistic. But

013515.635 -- 013517.574
the cool part is that every week, we was like,

013518.295 -- 013520.575
clear progression. Of how how this was, like,

013520.575 -- 013522.465
closer and closer to actually being

013522.945 -- 013524.885
like, a really good measurement of reality.

013526.315 -- 013527.935
I'm just gonna show you this video.

013529.755 -- 013530.555
Physical AI.

013531.834 -- 013533.855
Transforming the way we move and

013533.855 -- 013536.355
simulation is driving the field's latest advances

013537.695 -- 013539.934
To test autonomous vehicles, real

013539.935 -- 013541.395
world data is transferred

013542.674 -- 013544.484
based three d scenes.

013545.125 -- 013547.225
Using Omniverse neural reconstruction

013547.605 -- 013549.874
or NURAC. Converting video

013549.874 -- 013551.360
into photo realistic

013551.925 -- 013552.985
digital environments.

013553.995 -- 013556.395
The reconstructed scenes are then simulated

013556.695 -- 013558.745
with variations. To assess the

013558.745 -- 013600.765
AI driver's ability to generate safe

013600.765 -- 013603.075
trajector Physical

013603.294 -- 013605.314
AI isn't just driving on roads,

013605.565 -- 013607.065
it's operating in kitchens,

013607.625 -- 013608.855
offices, and warehouses.

013611.325 -- 013613.405
Each of these robots needs to be simulated in

013613.405 -- 013615.274
virtual worlds reconstructed

013615.575 -- 013616.794
from multiple sensors.

013618.115 -- 013620.275
With NURAC, can rapidly render these

013620.635 -- 013622.665
simulations in real time with high

013622.665 -- 013622.915
fidelity.

013625.395 -- 013627.555
New capabilities like physics

013627.555 -- 013629.635
bring realistic interaction to three

013629.635 -- 013631.774
d Gaussian scenes. Letting

013631.775 -- 013633.934
robots engage naturally with virtual

013633.935 -- 013636.005
environments. Generative

013636.064 -- 013637.914
AI uses the visual properties

013638.234 -- 013640.325
these simulations to add diversity,

013640.805 -- 013642.285
scaling a single scene into many.

013643.805 -- 013646.064
Neural reconstruction is already advancing

013646.124 -- 013646.785
the world's

013648.125 -- 013650.625
AI. And now, with just a text NewReck

013650.685 -- 013652.735
and Cosmos can generate three

013652.735 -- 013654.205
d simulation environments

013655.185 -- 013656.885
that refine testing and validation

013657.725 -- 013659.585
for the next generation of robotics.

013704.755 -- 013706.475
Okay. Cool. When

013706.874 -- 013708.954
really speed up to the airport, but I'm gonna give you a

013708.955 -- 013710.995
few more minutes. I just wanted to show what

013710.995 -- 013713.425
we're working on towards now. So

013714.095 -- 013716.215
basically, we're kind of kind of finished in the research in terms

013716.215 -- 013718.444
of you know, that that's simulator is already

013718.445 -- 013720.455
in action. So we're thinking, like, what's next

013720.695 -- 013722.774
Right Like, what can our research team actually help

013722.774 -- 013724.825
and build next And we were

013724.825 -- 013726.905
kind of looking at this this plot. Right

013726.905 -- 013728.535
So in the bottom, you'll have a

013729.015 -- 013731.095
scenario as a commonality of a scenario. And on the

013731.095 -- 013733.254
kind of, like, the the y axis is

013733.254 -- 013735.034
the volume at which you recorded when

013735.354 -- 013737.065
you're driving around with the development fleet.

013738.025 -- 013740.184
And, obviously, just the data collection

013740.185 -- 013742.475
fleet is kind of on this left side. Right

013742.475 -- 013744.495
You're you're typically collecting common

013745.115 -- 013747.465
maybe higher volume of common scenarios.

013748.075 -- 013750.314
Maybe you have a very targeted data collection

013750.314 -- 013752.334
when you go out and you want a particular

013752.825 -- 013754.365
scenario. Is it kind of hunting for that

013755.985 -- 013758.225
Maybe you even get some, you know, customer free

013758.225 -- 013800.005
data or acquire some data.

013800.925 -- 013803.004
NURAC allows you to do some variation of

013803.005 -- 013804.655
that, so it adds a little bit more

013805.215 -- 013807.495
to whatever you recorded. But, obviously,

013807.555 -- 013809.655
we wanna go here. Right Especially

013809.715 -- 013811.755
for l four or l five, and

013811.755 -- 013813.795
Drago is sitting here, so he definitely knows

013813.795 -- 013815.715
this. You need to simulate

013816.255 -- 013818.364
rare cases. Right It's just cases where

013818.365 -- 013820.235
you're almost never gonna record.

013821.435 -- 013823.825
And ideally high volume of that. So

013824.495 -- 013826.405
question is obviously, the premises

013826.624 -- 013828.245
go after generative wall models.

013829.395 -- 013831.315
So for that purpose, we have been building,

013831.635 -- 013833.865
cosmos. We announced it

013833.865 -- 013835.944
at CES, this past year. That was

013835.944 -- 013838.115
in January. And it's a platform

013838.255 -- 013840.115
basically meant for physical AI developers.

013840.495 -- 013842.815
So we've been building, several foundation

013842.955 -- 013845.235
models that are hopefully enabling everyone

013845.235 -- 013847.285
to build you know, robotic

013847.285 -- 013849.525
systems. From the reasoning model, visual

013849.525 -- 013851.753
reasoning models to basically

013851.754 -- 013852.814
world simulation models.

013855.535 -- 013857.745
And, obviously, there's just, like, so

013857.805 -- 013859.965
much work, so much data, so much compute

013859.965 -- 013901.825
going into making this a reality.

013903.155 -- 013905.395
Pretraining, basically train on twenty million

013905.395 -- 013906.915
hours of kind of real world

013907.475 -- 013909.735
physical, videos. And

013909.735 -- 013910.795
then there's post training

013911.845 -- 013913.925
that, you know, you kind of use the data

013913.925 -- 013916.094
for a particular robot, and then you post train

013916.095 -- 013917.820
the model to make it really,

013918.425 -- 013920.584
successful for that domain. And

013920.585 -- 013922.585
for that, we're basically throwing the entire

013922.665 -- 013924.665
or a subset

013924.725 -- 013926.915
selected from the entire NVIDIA collected fleet

013927.425 -- 013929.745
and we're just contributing that, in the models

013929.745 -- 013932.015
we have been releasing. That's twenty

013932.015 -- 013934.175
thousand hours, plus we also acquired

013934.175 -- 013935.314
a couple of datasets.

013937.215 -- 013938.834
Maybe I'll skip this part.

013941.065 -- 013943.085
So Cosmos Drive was, I think, released

013943.225 -- 013945.314
maybe more April. That was

013945.315 -- 013947.335
our kind of first post trained Cosmos

013947.635 -- 013949.995
models specifically for for driving.

013950.135 -- 013952.264
And kind of what we did here, we took Cosmos

013952.265 -- 013954.075
that was kind of pretrained

013954.895 -- 013957.295
and we built a foundation model for driving,

013957.295 -- 013959.475
which was then fine tuned on this twenty

013959.475 -- 014001.955
thousand hours of data plus some of these acquired

014001.955 -- 014004.055
data sets. And

014004.055 -- 014006.345
then we, of further post train it

014006.485 -- 014008.685
with with further

014008.685 -- 014010.005
conditions, which are very used

014013.785 -- 014015.865
which is basically cuboids maps

014015.865 -- 014018.355
and even lighter. So these are all optional inputs.

014019.205 -- 014021.035
And then the video or multiple

014021.675 -- 014024.095
camera videos are coming out of these models.

014025.375 -- 014027.455
And I should say here, at least until

014027.455 -- 014029.315
these models maybe mature more,

014029.485 -- 014031.565
this kind of conditioning signal is really

014031.565 -- 014033.865
a boost to quality. So without

014033.865 -- 014035.885
that, it's very hard. There's a lot of hallucinations

014036.105 -- 014036.705
going on.

014038.715 -- 014040.785
Here are just a couple of

014040.785 -- 014042.635
examples. Of kind of

014043.274 -- 014045.514
conditioning on a particular scenario and creating

014045.515 -- 014046.015
variations

014048.314 -- 014050.655
and simulating that with multiple cameras.

014054.374 -- 014056.535
Since we've been working

014056.535 -- 014058.715
on making it, much longer,

014059.355 -- 014101.344
The first models were five seconds.

014101.585 -- 014103.845
Now we can go minute a minute

014103.845 -- 014105.785
or longer. I'm just

014105.865 -- 014108.025
we'll just do the crazy prompt here just

014108.025 -- 014109.565
so that you can see it's generated.

014111.035 -- 014113.275
This video is one minute. I'm not gonna play one

014113.275 -- 014115.185
minute. But it can actually also go longer.

014115.565 -- 014117.675
The key trick actually in making it longer,

014117.675 -- 014119.794
we made it work in almost a week. Is

014119.795 -- 014122.225
basically just a longer foundation model

014122.805 -- 014124.884
So cosmos initially did five second

014124.885 -- 014126.425
generation at a time. Right

014127.075 -- 014129.314
And we basically just took that and post train

014129.314 -- 014130.714
it to be more than that.

014131.355 -- 014133.595
I think ten ten seconds. And with

014133.595 -- 014135.725
that that model, like, you can actually roll out

014135.725 -- 014137.854
much, much longer So it's really

014137.854 -- 014139.934
key to just kind of post train that model to

014139.935 -- 014141.834
do, larger batches of

014142.075 -- 014142.924
frames at one time.

014145.755 -- 014147.995
And we also can do multiple cameras

014147.995 -- 014149.455
now in the same duration.

014150.205 -- 014151.584
So it can go much longer.

014152.785 -- 014154.605
Not five seconds. Again, this is five

014154.845 -- 014157.064
twenty seconds here, but it can actually do much longer.

014158.765 -- 014201.005
I wanted to show a limitation slide. This was

014201.005 -- 014203.334
from March and a lot of this has been

014203.334 -- 014205.255
way improved. But I

014205.495 -- 014207.504
think the the the challenges are still

014207.745 -- 014209.905
kind of valid. These are the things that we actually

014209.905 -- 014212.134
need to work on. To to kind

014212.135 -- 014214.315
of bring this to production to real value.

014215.235 -- 014217.735
So one of the first problem that everyone in navy

014217.875 -- 014220.075
complained about and they really didn't believe that this

014220.075 -- 014222.175
model is ready yet is, on

014222.175 -- 014224.285
the top left, The, the model

014224.285 -- 014226.365
symptom simulates both red and green

014226.365 -- 014228.135
lights to be on at the same time.

014228.745 -- 014230.904
Obviously, that's super bad. You know How how will

014230.905 -- 014232.965
the policy even interpret that You know, such

014232.965 -- 014235.165
a small just a few pixels wrong

014235.165 -- 014237.345
like that can actually make this technology

014237.345 -- 014239.805
super useless. So that's definitely something

014239.944 -- 014242.324
we it's already much, much better in the latest

014242.325 -- 014244.484
models, but it's something that, you know, you need to pay

014244.484 -- 014246.935
attention. With.

014247.115 -- 014249.164
Hallucinations, you're gonna see this car just

014249.405 -- 014251.185
kind of goes into thin air.

014251.505 -- 014253.585
Obviously, with that, we cannot have

014253.585 -- 014255.805
that happen. With conditioning, that's

014255.805 -- 014258.205
a little bit less of a problem, but it still happens

014258.205 -- 014259.105
to some extent.

014300.775 -- 014302.855
The top right is an interesting one. So I'll tell

014302.855 -- 014304.835
you in advance what to pay attention to.

014305.955 -- 014308.385
It's basically wrong logic.

014308.955 -- 014310.895
So basically, here we weren't conditioning

014311.035 -- 014312.734
on the on the traffic lights.

014313.365 -- 014315.525
State. We just condition on kind of the

014315.525 -- 014317.315
map and where the traffic lights are.

014317.825 -- 014320.064
And then the video comes out. And then

014320.065 -- 014322.385
the VLM is obviously generating a prompt.

014322.385 -- 014324.655
Right And that goes into the model that generates

014324.655 -- 014326.665
the actual video. So what you're gonna

014326.665 -- 014328.445
see here is that there

014328.765 -- 014330.845
the the traffic light is still showing

014330.845 -- 014332.864
red, but the cars are already being

014332.865 -- 014335.115
navigated to drive. Which means

014335.115 -- 014337.195
that the logic there something went wrong in the

014337.195 -- 014339.405
logic of of actually producing

014339.405 -- 014341.455
this video. So the cars are driving,

014341.455 -- 014343.665
but actually, the the light was red. Which

014343.665 -- 014345.865
wouldn't happen in the real world. Again,

014345.865 -- 014347.965
something that we need to solve for a better reasoning

014348.585 -- 014350.755
inside, the video generation models.

014351.725 -- 014353.745
Text remains kind of a problem, especially

014353.805 -- 014356.135
for faraway text. Right And that's super important.

014356.375 -- 014358.695
All the traffic, like, you know, traffic text actually

014358.695 -- 014359.675
needs to be read.

014401.345 -- 014403.424
And pedestrians, I think, have improved a

014403.425 -- 014405.445
lot, but the especially with scenes with a lot

014405.445 -- 014407.685
of pedestrians, it's, it's still kind

014407.685 -- 014409.695
of like a a jungle. Sometimes you

014409.695 -- 014412.195
see someone with multiple legs or people disappearing.

014414.895 -- 014416.515
This is a funny one.

014417.455 -- 014419.614
I talked in the beginning that we need

014419.615 -- 014421.745
to simulate kind

014421.745 -- 014423.905
of these OD scenarios, right, like, very

014423.905 -- 014426.084
rare cases. But those are actually

014426.565 -- 014429.065
hard to simulate. So here you just

014429.685 -- 014432.025
prompting with you know, here in a highway scenario,

014432.405 -- 014434.431
there was a tire that breaks So let

014434.565 -- 014436.515
let's see what the model generates.

014444.624 -- 014445.765
Super weird.

014446.775 -- 014449.175
The other one is a traffic officer

014449.175 -- 014450.794
signal. It's a turn around.

014452.165 -- 014454.485
At a closed street. So the street is closed,

014454.725 -- 014456.585
and the video model is gonna paint

014456.965 -- 014459.185
that correctly. But then the logic just kinda

014459.185 -- 014500.845
breaks. So

014507.185 -- 014509.245
Goodbye. It just goes

014509.245 -- 014511.324
through. This one is gonna be hard to

014511.325 -- 014512.945
see, but there is a basically

014513.595 -- 014515.375
a cyclist that doesn't move.

014515.995 -- 014518.004
He sitting on a cycling pedaling,

014518.004 -- 014520.015
but he's just stationary. Which

014520.015 -- 014522.094
is kinda funny. And in

014522.095 -- 014524.035
the bottom right is a

014524.295 -- 014526.535
the the prompt was ego car crashes into

014526.535 -- 014528.545
another car. And then, you know,

014528.545 -- 014529.925
this kind of stuff comes out.

014540.815 -- 014542.655
Yeah. Not not quite there yet.

014543.135 -- 014544.835
But, actually, we are making progress.

014545.325 -- 014547.415
And so why does this happen Is

014547.415 -- 014549.495
this kind of failure cases The

014549.495 -- 014551.425
issue is that in our in our the

014551.805 -- 014553.975
collected data that we have from the fleet,

014554.135 -- 014556.395
actually, you'd never see these edge cases.

014556.455 -- 014558.935
Right You we actually never recorded

014559.235 -- 014601.495
a tire breaking off or or

014601.915 -- 014604.045
or crashing into someone. Maybe for

014604.045 -- 014606.205
Joao, that's gonna be different. But for Nvidia, we

014606.205 -- 014607.745
don't actually have those examples.

014608.975 -- 014611.055
So it's kind of up to whatever the model

014611.055 -- 014613.425
still has from that world world

014613.425 -- 014615.585
knowledge. So it needs to kind of extract whatever it

014615.585 -- 014617.925
has learned from the Internet with very

014617.925 -- 014619.945
nicely behaved AV videos we collected.

014620.165 -- 014622.518
And it's just not kind of there yet.

014622.972 -- 014625.185
So the way that we basically solved

014625.185 -- 014627.285
it not solve it, but at least

014627.345 -- 014629.435
improved it it's actually

014629.435 -- 014631.785
super simple. We just acquired some crashing data.

014632.425 -- 014634.645
And now the model's actually successfully

014635.040 -- 014637.374
crash. And simulate

014637.374 -- 014639.075
this much much harder cases.

014639.785 -- 014641.865
You can just go to a Dashcam provider that's

014641.865 -- 014643.325
collecting this for a living.

014644.045 -- 014645.965
And throw a little bit of that data inside

014646.125 -- 014648.365
And then the remaining challenge of how we're training

014648.365 -- 014650.434
this model is is really dashcam

014650.435 -- 014652.674
data looks a bit different than what we collected from

014652.675 -- 014654.795
the fleet. You need to kind of bridge that

014654.795 -- 014657.035
visual gap, but you can actually make it work pretty

014657.035 -- 014659.255
well. Is another one where you can just,

014659.255 -- 014701.184
like, drive. Crazy.

014701.345 -- 014703.414
And the physics also looks pretty correct.

014705.655 -- 014708.075
Let's see if I have something more to show you.

014709.075 -- 014711.235
Yeah. So I think, like, the first use case we are

014711.235 -- 014713.284
thinking right now just because some of

014713.285 -- 014714.885
these limitations remain is,

014716.035 -- 014718.435
just generating these videos and actually

014718.435 -- 014720.684
turn it into just reconstruct

014720.685 -- 014722.925
them because from there on, we know how to simulate

014722.925 -- 014725.104
closed loop and everything. So we

014725.105 -- 014727.184
can actually just that video that we generated

014727.185 -- 014729.245
before we can now

014729.245 -- 014731.035
just lift it to three d and

014731.515 -- 014733.755
plug it into, our simulator. So that

014733.755 -- 014735.455
basically unlocks our team

014735.905 -- 014737.924
that has never collected a particular

014737.984 -- 014740.064
scenario to just kind of hallucinate it and then

014740.064 -- 014742.295
put it in d and have perfect control of it.

014744.985 -- 014747.295
And the other one is editing. Editing

014747.295 -- 014749.374
is super, super useful feature because that

014749.374 -- 014750.915
means that I have maybe collected

014751.485 -- 014753.575
an example, which is perfectly, like,

014753.575 -- 014755.603
normal, I can type some prompts

014755.604 -- 014757.814
and can start editing and making that scenario look

014757.975 -- 014759.355
like much more challenging.

014800.405 -- 014802.565
So that's Corona edit. We also released it a couple

014802.565 -- 014804.814
of weeks ago. Just wanted

014804.815 -- 014806.895
to show you just and then I really need to

014806.895 -- 014809.035
go. Our latest research

014809.035 -- 014811.353
is just probably we're gonna release this sometimes

014811.354 -- 014813.455
next year, but it's very

014813.835 -- 014816.075
early work. We're we're trying to make this,

014816.635 -- 014818.575
so all the stuff that I was showing before,

014818.995 -- 014821.074
was actually open loops. So you write some text, and

014821.075 -- 014823.015
it generates, like, a minute long video.

014823.165 -- 014825.334
And the car is kind of

014825.334 -- 014827.574
driving in whatever the prompt was there, right,

014827.575 -- 014830.055
which is not really what we want. We want the actual

014831.255 -- 014833.715
policy to drive in this role model.

014833.715 -- 014835.875
Right You guys have all seen Genie. I don't need

014835.875 -- 014837.915
to preach here. We all kinda wanna

014837.915 -- 014840.074
make it here work for for the car. So

014840.075 -- 014841.375
make this model interactive.

014842.235 -- 014844.234
Which basically means I don't

014844.395 -- 014846.544
not as human, I'm gonna be operating

014846.545 -- 014848.564
this with a keyboard control, I want

014848.564 -- 014850.604
the policy to be produced in the actions,

014850.604 -- 014852.610
and then this model to be

014852.610 -- 014854.644
reactive. Then maybe you know, generate

014854.645 -- 014856.905
one second of video, then policy takes over,

014857.285 -- 014859.465
predict and then I kind of go like

014859.465 -- 014901.705
that. So we're working towards the system.

014901.705 -- 014903.455
So we're still gonna make it condition the

014904.015 -- 014906.334
same as we have it in in Yurag with the boxes and

014906.334 -- 014908.405
and map. And and text

014908.405 -- 014910.805
prompt. You're gonna generate now a very short

014910.805 -- 014912.975
snippet of video, maybe just like half

014912.975 -- 014915.145
a second. This is thrown

014915.145 -- 014917.544
to the policy model that's producing an action.

014917.785 -- 014919.645
And then you you obviously iterate.

014920.555 -- 014922.495
And we also wanna add a

014922.635 -- 014924.885
memory cache so the model

014924.885 -- 014926.665
actually has, like, long history.

014927.725 -- 014929.885
Example, especially, you have dynamic scenes where

014929.885 -- 014931.924
some cars come, slow down, come

014931.925 -- 014934.085
again, like, you don't want them to completely lose

014934.085 -- 014936.305
identity. And especially for uterus,

014936.305 -- 014938.385
it's super important to have, like, some sort of a

014938.385 -- 014940.464
three d spatial cache. And

014940.465 -- 014942.485
the other super important thing

014942.785 -- 014944.795
is a reasoning in the loop.

014945.275 -- 014947.675
Right So before I talked about, hey. The policy

014947.675 -- 014949.825
needs to reason about the how

014949.825 -- 014951.985
the world is kinda behaving and what I should be

014951.985 -- 014954.044
doing next. Here is a little different.

014954.044 -- 014956.195
Here is, like, okay. The policy has

014956.195 -- 014958.354
decided to make this action. What

014958.355 -- 014959.965
is everyone else going to do

015000.685 -- 015002.924
And that goes back and into the prompting and

015002.925 -- 015003.825
generating extremes.

015005.095 -- 015007.175
Alright. So I'm just gonna show you, like, a

015007.175 -- 015009.194
couple of very early results. In the

015009.195 -- 015011.595
bottom is, bottom left is our conditioning signal.

015011.595 -- 015014.055
So that's basically the input the model, press

015014.274 -- 015016.515
some text prompt. On the bottom right is kind of the policy

015016.515 -- 015018.055
that's gonna be making prediction.

015018.735 -- 015020.595
And at the top left, we're gonna run

015021.104 -- 015023.185
So the the model's condition on the first frame.

015023.185 -- 015025.295
We're giving the first frame, and the and

015025.295 -- 015027.475
the kind of the HD map and the and the boxes.

015028.215 -- 015030.375
The top left is going to be the world model. So that's this

015030.375 -- 015032.810
generating model is gonna hallucinating

015032.875 -- 015034.905
from there on, from the first frame on.

015035.105 -- 015037.345
And on the right side, we actually took that full

015037.345 -- 015039.503
scenario where we construct it

015039.504 -- 015041.665
with those Gaussian plots. So and then we're

015041.665 -- 015044.025
gonna start comparing how the two star deviated.

015044.495 -- 015046.645
Does it make sense And the policy is

015046.645 -- 015048.675
basically reacting to whatever the world model

015048.675 -- 015051.124
is painting, and then we're rendering in both worlds.

015051.715 -- 015054.195
And as you go forward, they're gonna start deviating

015054.195 -- 015056.255
more and more. But the difference here is

015056.255 -- 015057.955
that the policy is actually driving.

015058.295 -- 015100.375
It's actually driving, and the video model is

015100.375 -- 015101.915
reacting, painting, and extremes.

015108.125 -- 015109.905
So here we are already kind of deviating

015110.825 -- 015112.925
but it's still pretty good in how it's

015113.155 -- 015114.135
what is creating.

015117.035 -- 015119.141
The text is not great. As you

015119.142 -- 015120.334
can see on the road.

015122.835 -- 015125.255
Here it's easier to see. It's bigger. I'm gonna

015125.255 -- 015125.915
just be swiping.

015137.795 -- 015139.875
And I would like to close, actually, because

015139.875 -- 015141.905
I I need to finish. With

015142.205 -- 015144.345
a failure case. This is just gonna show you

015144.345 -- 015146.365
how important is to actually have VLMs

015146.425 -- 015148.785
in the loop So

015149.085 -- 015151.174
here, maybe I'm just gonna start

015151.175 -- 015153.195
because the mistakes are just all over

015153.255 -- 015155.484
here. Okay So here in the top,

015155.485 -- 015157.765
there's gonna be a car that's gonna come

015157.765 -- 015159.225
here just very unrealistically.

015202.005 -- 015202.745
Just disappear.

015204.265 -- 015206.424
And then the text prompt is mentioning a

015206.425 -- 015208.544
grandma on a bike. And because we're

015208.544 -- 015210.775
not changing the prompt as we go,

015211.095 -- 015213.535
this grandma just gonna appear

015213.675 -- 015216.015
in every every few second

015216.075 -- 015218.085
chunk, which is kinda funny.

015218.245 -- 015220.405
And the other failure cases, here,

015220.405 -- 015222.494
we we haven't used the crash data yet.

015223.705 -- 015225.745
We're gonna try to crash the car

015226.785 -- 015229.124
And on the right side in Europe, you just go

015229.185 -- 015231.345
through this car and the video model just

015231.345 -- 015233.514
doesn't want to crash. So it just starts

015233.515 -- 015234.704
painting something different.

015235.665 -- 015237.744
Okay. I'm gonna stop here. Thank you

015237.745 -- 015238.434
very much.

015248.345 -- 015250.665
Yeah. Thanks, Those are wonderful demos

015250.665 -- 015252.684
and amazing talk. And, so for

015252.685 -- 015254.945
time being, we might just keep it a q and

015254.945 -- 015257.134
a. And next, our speaker today

015257.135 -- 015259.374
will be Nicholas Hanson from UC San

015259.374 -- 015300.825
Diego. And,

015303.975 -- 015306.305
so Nicholas received, like, the NIMVIA

015306.305 -- 015308.484
graduate research fellowship and done many amazing

015308.485 -- 015310.514
works in in robotics. And

015310.515 -- 015312.855
their topic today will be massively multitask

015313.675 -- 015316.095
word models for continuous control. Welcome.

015358.845 -- 015401.105
Alright. Good morning everyone.

015402.125 -- 015404.205
I wanna talk about some of the work that I've been

015404.205 -- 015406.265
doing during my PhD on on world

015406.265 -- 015408.535
models for continuous control. I wanna

015408.535 -- 015410.595
talk about some of the older work that we've

015410.595 -- 015412.135
been doing, and then also some

015412.915 -- 015414.345
new work that we just released last week.

015416.564 -- 015418.645
But first, I want to zoom out a little bit and just

015418.645 -- 015420.945
talk about AI more broadly. We've

015420.945 -- 015423.025
seen a lot of progress in digital creative

015423.025 -- 015425.085
tasks we have models now that

015425.085 -- 015426.934
are very capable. You can,

015427.175 -- 015429.185
snap a photo with your phone. You can ask

015429.185 -- 015431.254
it to give some tips for

015431.255 -- 015433.714
decorating your room, and it will give you pretty

015433.714 -- 015435.764
sensible ideas. Also have

015435.765 -- 015438.205
models that are able to generate high fidelity

015438.205 -- 015440.185
on video that matches your text

015440.484 -- 015442.295
prompts. And

015442.645 -- 015444.965
the commonality between all of these, models

015444.965 -- 015446.995
that we now have is that or the progress

015446.995 -- 015449.155
that we now have is that it's usually just

015449.155 -- 015451.275
one algorithm or one model that can be a

015451.515 -- 015453.615
to lots of different problems. Without

015453.615 -- 015455.855
the user necessarily having, expertise in

015455.855 -- 015457.924
those specific domains. So this is

015457.925 -- 015459.385
really what's driving the progress.

015500.465 -- 015502.655
So There's roughly three things that go

015502.655 -- 015504.874
into such a generalist model. We need

015505.435 -- 015507.475
lots of diverse data, We need

015507.475 -- 015509.635
scalable architectures that can consume all of

015509.635 -- 015511.705
this data. And then we need some,

015511.945 -- 015514.104
general objective that is general enough that we

015514.104 -- 015516.165
can repurpose the model for

015516.165 -- 015518.565
new problems without necessarily updating

015518.565 -- 015519.545
the model parameters.

015521.645 -- 015523.744
So So a question that I get a lot is like,

015523.745 -- 015526.105
what might this actually look like for Embodied AI

015527.125 -- 015529.445
I would argue that predicting the future is perhaps

015529.445 -- 015531.524
the most general objective that we can think of.

015532.405 -- 015534.805
So And building world models

015535.105 -- 015537.185
specifically allows us to do reasoning about

015537.185 -- 015538.244
the physical world.

015539.305 -- 015541.485
We seeing a lot of progress in

015541.485 -- 015543.745
this already in industry. There's one x technologies

015543.885 -- 015545.895
have built a a world model for their

015545.895 -- 015548.055
specific robot. It's able to generate pretty

015548.055 -- 015550.285
short horizon videos of their robot

015550.285 -- 015552.395
interacting with the world. So

015552.794 -- 015554.955
WAVE has designed, similar world models

015554.955 -- 015556.735
for autonomous driving that is able to

015557.135 -- 015558.835
simulate here, short term videos.

015559.334 -- 015601.545
And same as what we just saw with with NVIDIA.

015602.505 -- 015604.665
And Google D Mind released the Genie

015604.665 -- 015606.765
model, multiple

015606.765 -- 015609.004
iterations of it where you see here the the world model

015609.004 -- 015610.145
simulating digital,

015611.575 -- 015612.714
video game like environments.

015614.345 -- 015616.554
The commonality here is that all of these

015617.035 -- 015619.435
works are very, very impressive, but none of them are actually

015619.435 -- 015621.505
open source. So that's what I want to focus

015621.505 -- 015621.905
on.

015623.865 -- 015626.104
Overall goal of my research and also the talk

015626.105 -- 015628.125
today is advancing the

015628.185 -- 015630.260
research in open world models and RNN false

015630.260 -- 015632.485
learning. I wanna talk about

015632.485 -- 015634.885
some of the work that we've been doing in developing data

015634.885 -- 015636.794
sets and benchmarks for researchers to use.

015637.495 -- 015639.515
Some of the algorithms that we have been proposing

015639.575 -- 015641.564
that have some kind of scaling,

015641.805 -- 015643.974
capabilities now. And I also

015643.975 -- 015646.134
wanna talk about some of the work that we've been doing and just

015646.135 -- 015648.195
demarcate the tools to

015648.195 -- 015650.435
make it, more accessible and easier to use

015650.435 -- 015652.455
for for researchers more

015652.455 -- 015654.915
broadly. And so this includes, for example,

015655.214 -- 015657.415
sharing, research papers, but

015657.415 -- 015659.575
also, open sourcing all of the checkpoints

015659.575 -- 015701.605
and everything that we have used to produce

015701.985 -- 015702.155
our results.

015704.075 -- 015704.575
So

015706.665 -- 015708.744
my working definition of a world model for for

015708.745 -- 015710.774
control specifically is that it's

015710.774 -- 015712.791
a model of the transition function. So it's

015712.791 -- 015715.005
a model, that takes a state and action

015715.005 -- 015717.104
as input and it produces the the future

015717.485 -- 015719.505
state. We want to do,

015719.745 -- 015721.985
reinforcement learning, we also have usually a reward

015721.985 -- 015724.095
signal, might look

015724.095 -- 015726.175
in many different ways, but we always have,

015726.175 -- 015728.365
like, one objective that we're trying to maximize.

015729.245 -- 015731.645
The way that we do that with a world model is that we can do planning.

015731.645 -- 015733.625
So we can sample lots of different actions

015734.025 -- 015736.185
We can simulate what they would look like with

015736.185 -- 015738.275
our world model, then we can maximize

015738.575 -- 015740.894
the cumulative reward, so the reward over

015740.895 -- 015742.885
over a longer horizon using,

015743.045 -- 015744.154
this planning algorithm.

015746.870 -- 015748.965
So The first work I wanna

015748.965 -- 015751.205
share here is, TDMZ two. This is

015751.205 -- 015753.365
a work we released a while ago.

015753.525 -- 015755.385
It's the second iteration of our algorithm.

015756.265 -- 015758.504
And really the focus here in this work is to build

015758.505 -- 015800.045
a model based RL algorithm

015800.745 -- 015802.745
that is scalable. So more

015802.805 -- 015805.085
data and more parameters

015805.225 -- 015807.305
gives us better performance, like we see in in

015807.305 -- 015808.635
language models for example.

015809.515 -- 015811.595
And we also want it to be robust, meaning that

015811.595 -- 015813.655
we have a single algorithm that can be applied to

015813.655 -- 015815.675
lots of different problems without necessarily

015815.735 -- 015817.890
the user having to do any high perimeter

015817.890 -- 015818.085
tuning.

015826.835 -- 015829.325
I want to show first here how the planning with TDMC

015829.325 -- 015831.335
two works. We

015831.765 -- 015833.465
receive observations from the environment.

015833.925 -- 015836.105
There must be this dock here, for example.

015836.595 -- 015838.805
We encode that into a latent representation.

015839.814 -- 015842.075
And then condition on actions that we are sampling,

015843.015 -- 015845.174
we can simulate in the latent space what would

015845.175 -- 015847.334
happen, in the future by taking those

015847.335 -- 015849.424
actions. Then we also have

015849.425 -- 015851.925
some kind of reward and value, model

015852.385 -- 015854.484
here that is conditioned on the latent, state.

015855.585 -- 015858.085
Predicts the rewards and values over the planning

015858.145 -- 015900.235
horizon. And so here, we use,

015900.635 -- 015902.745
short term rewards when we roll out the model

015902.745 -- 015904.685
at each step, and then we use a value

015906.125 -- 015907.505
to bootstrap our returns

015908.584 -- 015910.275
for the more long horizon, estimates.

015910.995 -- 015912.385
So that's how the planning works.

015913.115 -- 015915.395
During training, we do something very similar. We

015915.395 -- 015917.604
have data sequences of of state

015917.604 -- 015919.814
and actions that we have collected from the

015919.814 -- 015921.834
environment. We trained a model

015921.834 -- 015923.865
by first taking the first state

015923.865 -- 015925.905
in a sequence, and code that into

015925.905 -- 015927.104
a latent representation.

015928.105 -- 015930.185
Predict quantities like, the optimal

015930.185 -- 015932.205
action, reward that we will receive,

015932.575 -- 015934.755
at that state, and also a value function.

015935.825 -- 015938.035
So And we

015938.035 -- 015940.275
conditioned the model on the actions that we actually

015940.275 -- 015942.345
took when we collected the data, and we

015942.345 -- 015944.425
used that to recurrently roll out the model and predict

015944.425 -- 015945.084
these quantities.

015946.484 -- 015948.645
So The TDMC

015948.705 -- 015950.785
two world model does not have a decoder. So it's not a

015950.785 -- 015952.884
reconstructive world model. Way

015952.885 -- 015954.865
that we ground this model in the dynamics

015954.925 -- 015957.030
of the real world is

015957.085 -- 015959.185
that we minimize the difference in the latent

015959.725 -- 020001.515
representations. So it's a self predictive

020001.895 -- 020004.045
world model. That works, for example, is

020004.045 -- 020006.124
that at the time step three here

020006.125 -- 020008.295
in the right, we minimize the difference

020008.295 -- 020010.315
in latent we would get from encoding

020010.455 -- 020012.575
the first observation and rolling that out

020012.575 -- 020014.734
into the latent space versus just taking

020014.734 -- 020016.774
the ground truth of observation at that time

020016.774 -- 020018.715
step and encoding that with the same encoder.

020019.005 -- 020021.025
Ideally, we would want these latencies to be

020021.025 -- 020021.165
the same.

020024.124 -- 020026.424
So So we can use this world model

020026.425 -- 020027.964
for, online RL.

020028.715 -- 020030.965
This is an example of a single task

020031.370 -- 020033.624
just doc from DM control.

020034.294 -- 020036.773
We want it to run forward as fast as possible.

020036.774 -- 020038.825
And initially, we don't have any

020038.825 -- 020041.084
data, and our model is just random parameters.

020042.005 -- 020044.085
But as the world model interacts with environment

020044.085 -- 020046.185
and collects more and more data, it keeps improving

020046.485 -- 020048.854
its policy, with the

020049.075 -- 020051.355
planning algorithm. And eventually, after 10,000,000

020051.415 -- 020053.345
environment steps, we have a pretty,

020054.155 -- 020056.455
proficient policy. So And notably,

020056.595 -- 020058.915
this is just about twelve hours on one GPU

020058.915 -- 020100.975
without any parallelization. If you do

020100.975 -- 020103.135
parallelize the environment, you can, get even

020103.135 -- 020103.794
more speedups.

020106.955 -- 020109.185
So So we repeated this process on

020109.185 -- 020111.635
hundreds of different tasks. Here's examples

020111.635 -- 020113.765
of some of the tasks that we have been working with.

020114.005 -- 020116.084
Many of them are DM Control, MetaWorld,

020116.564 -- 020118.704
MySQL, Maniscal, and and

020118.705 -- 020120.805
some other ones that we have, applied the algorithm

020120.865 -- 020123.285
to. And we want to train

020123.765 -- 020125.945
world models on all of these tasks, separately.

020126.345 -- 020128.365
A way that doesn't require high parameter

020128.505 -- 020128.735
tuning.

020132.064 -- 020134.484
We have some numbers here. This is

020135.005 -- 020137.065
all the tasks that I just showed. Break

020137.125 -- 020139.065
in, we break them down by domain.

020139.245 -- 020141.405
And we can measure here the average performance

020141.405 -- 020143.445
across these different domains. So

020143.445 -- 020145.524
the y axis here is is score, and that's where we

020145.524 -- 020147.555
want to maximize. Have

020147.555 -- 020149.535
a few different baselines like, self-service

020149.595 -- 020151.675
critic, Dreamer v three. The first

020151.675 -- 020153.225
iteration of TDMC.

020154.535 -- 020156.774
This first version here, the blue bar, it does

020156.774 -- 020158.005
do high parameter tuning.

020159.115 -- 020201.515
TDM two world model here we released does not require

020201.515 -- 020203.575
any parameter tuning, so it's the same high parameters

020203.575 -- 020205.425
across all the different environments.

020205.665 -- 020207.744
And it seems to be, comparable or

020207.745 -- 020209.905
better across the the task that we tried.

020214.015 -- 020216.175
So something that I'm increasingly interested in

020216.175 -- 020218.145
is trying to take the TDMC2

020220.195 -- 020222.535
model based RL framework and not just do

020223.295 -- 020225.464
multiple tasks, with training individual models

020225.465 -- 020227.325
there, but also trying to train multitask

020227.545 -- 020229.601
policies. The way that

020229.601 -- 020231.975
we do that is we collect a big offline RL dataset

020231.975 -- 020234.025
for 80 different tasks, Disbands both DM control and

020234.025 -- 020236.084
meta world. So many different

020236.084 -- 020237.805
embodiments. Then we,

020238.124 -- 020240.444
just train increasingly bigger models on this fixed

020240.445 -- 020242.775
dataset. So And we see

020242.775 -- 020245.135
something interesting now here that as we increase

020245.135 -- 020247.315
the number of parameters in the model, we get better and

020247.315 -- 020249.615
better performance. This was at the time,

020249.865 -- 020250.365
fairly

020252.885 -- 020254.494
unheard of in model based RL.

020256.505 -- 020258.745
And I also wanna emphasize here that even

020258.745 -- 020300.935
these models even that they're bigger, they're

020300.935 -- 020302.794
actually not too expensive to train.

020302.955 -- 020305.314
Mostly because we don't have the reconstruction objective.

020305.795 -- 020307.875
You can train this 48,000,000 parameter model

020307.875 -- 020309.655
in just three days on one GPU.

020310.335 -- 020312.255
Now this experiment is state based.

020312.575 -- 020314.475
We we also can extend it to vision,

020314.715 -- 020316.955
and it adds a little bit more compute, and the numbers

020316.955 -- 020319.005
are a little bit lower, but it it kind of works.

020323.495 -- 020325.734
So if you're interested in anything like

020325.734 -- 020327.765
this, we have open

020327.765 -- 020330.005
stalls pretty much everything about this project.

020330.005 -- 020332.095
We have released every single checkpoint for every

020332.095 -- 020334.285
single experiment that we ran. We

020334.285 -- 020336.525
have all of the data available, so you can train your own

020336.525 -- 020337.825
offline RL, multitask,

020338.865 -- 020340.864
agents. Also have code and, and

020341.105 -- 020343.124
training and emulation of your own TDMC

020343.185 -- 020345.354
two checkpoints. So you can all find

020345.354 -- 020346.824
it at at tdmc2.com.

020349.624 -- 020351.634
So I wanna zoom out

020351.635 -- 020353.725
a little bit and just think

020353.725 -- 020355.805
about more broadly what are the trends that we're

020355.805 -- 020357.811
seeing in language models and how does

020357.812 -- 020400.104
that look different from the work that we've been doing in model

020400.104 -- 020400.615
based around

020402.505 -- 020404.715
If we look at the most

020404.715 -- 020406.895
common LML recipe that we have nowadays,

020407.735 -- 020409.814
very significant part of this is self

020409.814 -- 020411.935
supervised pretraining on lots and lots of data.

020412.734 -- 020414.774
We have some amount of supervised fine

020414.775 -- 020417.014
tuning, which is more instruction based,

020417.015 -- 020418.714
it's it's curated dataset.

020419.445 -- 020421.525
And then our we also still do online RL,

020421.525 -- 020423.365
but it's a much it's a much smaller

020423.584 -- 020425.735
part of the overall pipeline. What I've been

020425.735 -- 020427.625
doing in the previous, experiments.

020430.105 -- 020432.274
And one thing that is very

020432.275 -- 020434.645
important when you do the online RL is that

020434.805 -- 020437.044
with all of the pre training that we are doing in language,

020437.044 -- 020438.665
we get really, really good strong

020439.205 -- 020441.285
priors for doing RLs. So the exploration

020441.285 -- 020442.905
problem becomes much more feasible.

020444.845 -- 020446.945
And also another difference is

020447.005 -- 020448.874
that throughout all of these three different,

020449.805 -- 020451.885
training segments, we have always thousands,

020451.885 -- 020453.895
if not million, of different tasks that

020453.895 -- 020456.155
we're training our models on. And this allows

020456.215 -- 020458.375
them both to get really, really good priors, but also

020458.375 -- 020500.145
to be able to generalize the new tasks.

020500.875 -- 020503.035
This is not really something that we're doing in model based

020503.035 -- 020503.855
RL at the moment.

020506.254 -- 020507.764
The RL environment lends

020508.645 -- 020510.755
in Embodied AI looks a

020510.755 -- 020512.695
little bit more like this. We have a lot of different

020513.295 -- 020515.454
environments like DM control and meta world

020515.455 -- 020517.874
and Atari. They're served for a lot of purposes.

020519.095 -- 020521.375
Most of the purpose of these algorithms

020521.515 -- 020523.595
or the benchmarks is that we are training single

020523.595 -- 020525.775
task policies. We train

020525.775 -- 020527.955
maybe, our RL policy

020528.255 -- 020530.325
algorithm here on lots of

020530.325 -- 020532.485
different tasks from the same benchmark, and we have

020532.485 -- 020534.584
some aggregate numbers of how well our algorithm

020534.585 -- 020535.555
is doing in general.

020537.795 -- 020540.115
But this is very different from what we're doing in in

020540.115 -- 020541.005
language models.

020542.605 -- 020544.545
So I had a discussion with my advisers

020544.685 -- 020546.774
and just had a little bit of a crazy

020546.775 -- 020549.095
idea. What if we just combine all of the RL benchmarks

020549.095 -- 020551.335
that we have access to right now and just try

020551.335 -- 020553.575
to train policies on all of the different tasks at

020553.575 -- 020554.075
once

020559.035 -- 020601.274
So in order to study this, we developed

020601.275 -- 020603.444
what we call Bench. It's a

020603.445 -- 020605.624
benchmark for massively multitask RL.

020606.455 -- 020608.705
It's essentially a combination of all the existing

020608.705 -- 020610.944
task domains that we could find. So there's 10

020610.944 -- 020613.264
different task domains and about 200 training

020613.265 -- 020615.445
tasks in total. We collect

020615.445 -- 020617.685
single task, checkpoints for all of these

020617.685 -- 020619.805
different tasks. So you wanna work

020619.805 -- 020621.964
on this benchmark, you have expert policies, and

020621.964 -- 020623.705
we also have, a demonstration

020624.005 -- 020626.025
data set that you can use, to to bootstrap

020626.405 -- 020628.035
your your online RL learning.

020629.874 -- 020631.925
Here's some examples of what the different tasks

020632.405 -- 020633.975
in the benchmark look like.

020634.455 -- 020636.584
So many of them are, probably

020636.585 -- 020638.825
familiar to you. We also have some new environments like

020638.825 -- 020641.135
the the Mini Arcade and DM control

020641.135 -- 020643.145
extended are just a little

020643.145 -- 020645.274
bit of a new set of tasks that that

020645.275 -- 020646.104
we've been working on.

020648.585 -- 020650.665
Here's some, examples of some of

020650.665 -- 020652.705
the new tasks. We have, new

020652.705 -- 020654.864
embodiments for a DM control environment, and we also

020654.865 -- 020657.245
have a lot brand new tasks, that you can use.

020702.275 -- 020704.285
So let's move on to

020704.285 -- 020706.495
training world models on this benchmark.

020707.325 -- 020709.564
Propose what we call a newt, it's a massively

020709.565 -- 020711.725
multitask world model that is trained with online URL

020711.725 -- 020713.975
on all of these different tasks. At the same time.

020714.795 -- 020716.895
Way that we do that in practice is that we instantiate

020717.115 -- 020719.181
one parallel environment per task So we

020719.182 -- 020720.375
have 200 environments.

020721.395 -- 020723.005
We have a unifying API that

020723.805 -- 020726.044
that maps all of these different tasks to a common

020726.045 -- 020727.494
observation and action space.

020728.485 -- 020730.725
We provide, state observations that are

020730.725 -- 020732.875
zero padded to match, the

020732.995 -- 020735.135
largest observation space in the

020735.135 -- 020737.235
dataset. Then we also optionally

020737.235 -- 020739.555
have RGB observations. So if you wanna

020739.555 -- 020741.890
work on visual RL, you can can do that with it. This

020741.890 -- 020744.115
benchmark. You can also disable it if you wanna

020744.115 -- 020746.455
be more, compute

020746.455 -- 020748.544
efficient. So

020748.785 -- 020750.944
We condition our new world model here on

020750.945 -- 020753.405
these two inputs. As well as a language

020753.465 -- 020755.625
description that describes the embodiment that we're

020755.625 -- 020757.674
working with, as well as the task.

020757.895 -- 020759.955
So for example, here in this instruction, it's a

020759.955 -- 020801.985
quadruped and we want it to

020801.985 -- 020804.425
run as fast as possible. We encode

020804.425 -- 020806.865
these tasks, in descriptions

020807.085 -- 020809.575
with a pre trained clip and to produce

020809.635 -- 020811.295
embeddings. So

020811.695 -- 020814.035
And then the world model here, as I showed previously,

020814.095 -- 020816.285
we do planning so we're predicting actions

020816.285 -- 020818.385
here over multiple time stops, and we also

020818.604 -- 020820.774
model the rewards of all of these tasks

020820.775 -- 020822.935
as as supervision. So

020823.415 -- 020825.435
And the backbone here is again the TDMC

020825.575 -- 020827.775
two world model that I that I showed

020827.775 -- 020828.275
previously.

020833.895 -- 020835.995
So how well does this actually work

020836.915 -- 020839.335
We have some experiments here where we train

020839.555 -- 020841.615
a single newt world model

020841.615 -- 020843.695
on 200 different tasks at the same

020843.695 -- 020845.845
time. The x axis here is the

020845.845 -- 020847.925
environment steps, and then the y axis here

020847.925 -- 020848.665
is the performance.

020851.405 -- 020853.484
But first, we train some B policies

020853.485 -- 020855.604
on the demonstrations that we collected. And

020855.604 -- 020857.785
we see something interesting. We see that the multitask

020858.004 -- 020900.375
BC policy here, which is the gray dash line,

020900.805 -- 020903.225
actually underperforming compared to 200

020903.655 -- 020905.235
single task BC policies.

020905.775 -- 020907.855
We suspect that this is because a lot of the task

020907.855 -- 020910.225
domains are very disjoint, So if you just measure

020910.225 -- 020912.385
in domain performance, you will probably see that

020912.385 -- 020914.205
a multitask policy is a little bit worse.

020916.925 -- 020919.124
We also tried PPO in this environment.

020919.435 -- 020921.515
We do see that it is struggling with the exploration

020921.515 -- 020923.874
and it's not actually exceeding the performance

020923.875 -- 020925.295
of the initial BC policies.

020927.255 -- 020929.495
Tried a newer algorithm, Fast TD3. This

020929.495 -- 020931.515
was released about six months ago.

020932.044 -- 020934.124
It's actually able to exceed the performance of

020934.124 -- 020936.495
both the multitask and the single task BC

020936.495 -- 020938.785
policy. We also

020938.785 -- 020941.104
tried the new, world model here without access

020941.104 -- 020943.285
to demonstrations. You'll see that it learns

020943.285 -- 020945.445
faster and it also seems to converge a little bit

020945.445 -- 020947.535
higher than model free baselines.

020950.095 -- 020952.445
What's most interesting is that when we add

020952.445 -- 020954.685
demonstrations and do pre training of our world model

020954.685 -- 020956.065
before doing online RL,

020956.905 -- 020959.355
we see that we actually start higher So doing world

020959.485 -- 021001.805
model based pretraining outperforms

021001.865 -- 021003.905
the BC baseline. And we also see

021003.905 -- 021006.385
that it both learns faster and converges to a higher

021006.899 -- 021009.014
performance. This would indicate that

021009.015 -- 021011.334
probably there's something useful in that pretraining

021011.334 -- 021013.485
that allows us to to do more efficient exploration

021014.915 -- 021015.895
when we do online RL.

021018.135 -- 021020.285
Again, this is pretty cheap to train.

021020.525 -- 021022.305
It's about four or five days

021022.555 -- 021024.415
on just two consumer grade GPUs.

021024.755 -- 021026.135
So it should be pretty accessible.

021030.295 -- 021032.695
We look at the effectiveness of doing online

021032.695 -- 021034.475
RL also on a per domain basis.

021034.735 -- 021036.975
This is still the same model trained on 200 different

021036.975 -- 021038.915
tasks. Now we just break it down by domain.

021039.754 -- 021041.914
And we'll see that for some of domains, newt

021041.915 -- 021043.774
here is doing a lot better than the baselines.

021045.045 -- 021047.305
For things like meta world, it's a little bit more

021047.365 -- 021049.705
mixed. And for many of the other environments,

021049.705 -- 021051.735
it also kind of mixed. We

021051.735 -- 021054.135
see, for example, for the box two d environment

021054.135 -- 021056.165
in the bottom left corner, we see all

021056.165 -- 021057.315
the algorithms that kind of

021058.195 -- 021100.055
performing on par with the BC baseline.

021100.445 -- 021102.603
So there's definite definitely still room for

021102.604 -- 021104.995
improvement here. This is also true for Atari,

021104.995 -- 021107.005
for example. Where again a

021107.005 -- 021108.545
lot of the tasks are very disjoint.

021109.975 -- 021110.475
So

021112.545 -- 021114.485
We do some ablations in the paper

021115.355 -- 021117.435
just studying different abilities and

021117.435 -- 021119.657
and scaling. Of

021119.657 -- 021121.795
the new agents. And we generally

021121.795 -- 021124.035
see that there's a threshold for a fixed number

021124.035 -- 021125.995
of tasks in which performance

021126.055 -- 021127.965
or the size of the model is good enough.

021128.525 -- 021130.685
For example, here when we scale the model size, we

021130.685 -- 021132.695
see that for 200 tasks, 20,000,000

021132.695 -- 021134.775
is actually sufficient, and

021134.775 -- 021136.935
you can still scale more, but it doesn't really add

021136.935 -- 021139.155
much to the performance. We

021139.155 -- 021141.345
do see that is not shown here, that if

021141.345 -- 021143.455
you decrease the number of tasks, the

021143.455 -- 021145.155
threshold here looks different.

021146.945 -- 021149.025
The same for batch size. We see that bigger batch

021149.025 -- 021151.185
sizes generally help when we have more tasks.

021151.345 -- 021153.465
But again, there's there's a point here

021153.465 -- 021155.395
in which the batch size is big enough.

021155.904 -- 021158.035
So We see that also including

021158.095 -- 021200.354
demonstrations is important for performance.

021200.545 -- 021202.705
And we tried a few different ways of doing that,

021202.705 -- 021204.715
but it seems that it doesn't matter that much.

021204.955 -- 021207.194
Just the fact that you do have demonstrations and you do pre training

021207.195 -- 021208.175
is the most important.

021211.235 -- 021213.245
We also see that including

021213.245 -- 021215.324
the language instructions is actually really helpful

021215.325 -- 021216.865
even for in domain performance.

021218.195 -- 021220.274
See here that when you provide the clip embeddings, it's

021220.274 -- 021222.285
able to more distinguish the different

021222.285 -- 021224.625
tasks than if you just provide the observations alone.

021226.865 -- 021229.054
So What I'm most excited

021229.055 -- 021231.294
about is that we can use these world models to do open

021231.294 -- 021233.475
loop control. And we actually

021233.475 -- 021235.794
trained the world model on just three time steps into

021235.794 -- 021237.825
the future. But at test time, with this

021237.825 -- 021240.225
world model, we can actually do six teams

021240.225 -- 021242.725
times longer, planning than than than that.

021243.775 -- 021246.265
Here's an example of the DM control working

021246.745 -- 021248.905
agent. You'll see here that it's able to walk

021249.564 -- 021251.645
fairly well. And then the dynamics

021251.645 -- 021253.615
kind of diverge over,

021253.855 -- 021255.935
as we do longer and longer planning. And you'll see

021255.935 -- 021257.475
here that it folds over eventually.

021258.085 -- 021259.445
But it's doing something useful.

021300.385 -- 021302.545
Able to do simple manipulation tasks.

021304.175 -- 021306.624
Also open loop. Are

021306.624 -- 021308.705
able to do two d navigation here, also open

021308.705 -- 021311.035
loop. And we also have this little

021311.095 -- 021313.174
lunar lander environment where, the agent

021313.174 -- 021315.305
is supposed to hover at this target

021315.305 -- 021317.405
and it's able to reach the target but it overshoots

021317.544 -- 021319.335
a little bit because the dynamics are

021319.545 -- 021321.735
just slightly off. This is

021321.735 -- 021323.275
all generated by one model.

021326.635 -- 021329.005
So We still just have 200

021329.065 -- 021331.325
tasks, so we can't expect too much generalization

021331.515 -- 021333.485
to new tasks and embodiments.

021334.045 -- 021336.365
We do have some early signs. We're able to do serial

021336.365 -- 021338.525
shot manipulation, where we change

021338.525 -- 021340.637
either the task or the object to

021340.638 -- 021341.294
something unseen.

021342.755 -- 021345.075
And we're also able to, again, do the walking task

021345.075 -- 021347.185
here on an incline. It does kind

021347.185 -- 021349.344
of fold over eventually. So

021349.345 -- 021351.825
there's definitely some

021351.825 -- 021353.874
limitations here. If we do online RL

021353.874 -- 021355.979
for just 50 trials, we now have policy

021355.979 -- 021357.905
that that runs on this incline.

021359.185 -- 021401.505
We repeat this fine tuning experiment

021401.505 -- 021403.375
over about 30 different tasks.

021403.535 -- 021405.245
And this is what the results look like.

021406.075 -- 021408.155
See that if you train the world model from scratch

021408.155 -- 021410.145
on just that task, you

021410.445 -- 021412.564
do, worse on average than if you take the

021412.564 -- 021414.575
pretrained multitask world

021414.575 -- 021416.485
model and you fine tune that on that new task.

021417.525 -- 021419.855
So We expect that if you increase

021419.995 -- 021422.154
the number of parameters and you increase

021422.155 -- 021424.314
the dataset, then eventually we will

021424.314 -- 021426.724
probably get better and better future fine tuning performance

021426.725 -- 021427.505
as well.

021429.215 -- 021431.374
So if any of this is interesting to you, it's

021431.374 -- 021433.455
also available. We have released

021433.455 -- 021435.895
all the checkpoints, the data, everything should be there.

021436.854 -- 021437.595
On this link here.

021445.274 -- 021447.164
I also wanna just end on an

021447.645 -- 021450.145
positive note, on getting involved.

021450.205 -- 021452.415
So I

021452.415 -- 021454.554
believe there's not been a better time to

021454.555 -- 021456.475
get involved in world model research than now.

021456.794 -- 021459.055
There's so much open SOLUS work going on.

021459.455 -- 021501.535
And you can actually, with pretty limited resources,

021501.535 -- 021503.785
get involved. And contribute

021504.245 -- 021505.115
to the to the research.

021506.265 -- 021508.424
If you don't know where to start, we do have

021508.425 -- 021510.365
in our paper we have about two pages

021510.615 -- 021512.773
of just open problems that would be super interesting

021512.774 -- 021514.794
to work on. I don't have time to work

021514.794 -- 021516.575
on all of them myself, so

021517.085 -- 021519.025
please give it a read. There's lots of ideas

021519.385 -- 021521.624
for how to, improve the visual RL,

021521.624 -- 021523.165
how to improve the language understanding.

021523.715 -- 021526.035
To change the architecture, stuff like that.

021526.115 -- 021528.125
So definitely check it out if you're interested

021528.125 -- 021529.395
in in working on this.

021531.025 -- 021533.185
That, I wanna thank my advisers also for all of

021533.185 -- 021534.885
their support in in these papers.

021535.455 -- 021536.925
Yeah. Happy to take questions.

021545.745 -- 021547.895
Nicholas. Maybe we can't afford one question.

021548.045 -- 021549.255
Any question from the audience

021551.095 -- 021551.545
Yeah.

021553.505 -- 021555.345
So I was wondering if you thought about the,

021555.745 -- 021557.835
partial setting. Seems like you're you're kinda

021557.835 -- 021559.835
set up for a markup setting right now.

021601.165 -- 021603.585
We don't have a lot of capability for partial observability.

021604.255 -- 021606.335
We've done some experiments with frame stacking, and it

021606.335 -- 021608.405
kinda works in the short term. So you can

021608.405 -- 021610.425
estimate things like velocity and acceleration.

021611.095 -- 021613.314
But more like long term, memory for a task

021613.315 -- 021615.655
like navigation, which is not really something we have

021615.715 -- 021615.725
touched.

021618.625 -- 021620.645
Sure. Thank you.

021625.665 -- 021627.904
Yeah. Our next speaker today will

021627.905 -- 021630.345
be, Chelsea Fing from Stanford University.

021630.405 -- 021632.425
She's also cofounder of physical intelligence.

021632.795 -- 021634.975
And her topic today will be developing

021635.195 -- 021637.505
long term autonomy. Thank

021637.505 -- 021637.524
you.

021655.445 -- 021656.684
Awesome. So

021657.485 -- 021659.565
hi everyone. I'm going to be talking a

021659.565 -- 021701.645
bit about autonomy and,

021702.485 -- 021704.744
kind of set the stage for this, I think that

021706.125 -- 021708.231
imitation learning has played like a

021708.231 -- 021710.255
really major role in a lot of

021710.255 -- 021712.323
advances in robotics and

021712.324 -- 021714.565
embodied intelligence recently. I think

021714.565 -- 021716.534
we've seen really, like, admittedly,

021716.755 -- 021719.234
like really really cool stuff that robots can do

021719.555 -- 021721.874
through just like vanilla imitation learning

021721.874 -- 021723.895
including a humanoid tying shoelaces,

021724.335 -- 021726.734
a surgical robot tying a knot, sauteing

021726.735 -- 021729.015
a piece of shrimp, tearing off a piece of tape,

021729.015 -- 021729.805
and so forth.

021731.245 -- 021733.325
And I'd like to talk about today is

021733.646 -- 021735.425
something that imitation learning struggles with,

021735.726 -- 021738.165
which is developing long

021738.165 -- 021740.225
term autonomy. In, embodied

021740.226 -- 021742.384
agents and specifically in real world

021742.385 -- 021744.455
robots. And

021744.696 -- 021747.135
as some examples, I wanna talk about tasks that

021747.135 -- 021749.234
are pretty complicated and

021749.235 -- 021750.775
much longer, horizon

021751.315 -- 021753.355
than the typical tasks that we see

021753.355 -- 021755.435
robots, do with kind of an adaptation learning

021755.435 -- 021757.573
including dusting shelves and replacing

021757.574 -- 021759.435
them, packing items into a plastic

021759.655 -- 021801.954
bag, performing a procedure on a gallbladder,

021802.095 -- 021804.236
and tidying a novel bedroom. And these are

021804.236 -- 021806.474
things where if you just kind of apply vanilla imitation

021806.475 -- 021808.505
learning out of the box, you're going to get

021808.505 -- 021810.665
pretty poor performance because of how long the

021810.665 -- 021812.495
task is. So

021813.305 -- 021815.324
should mention also that I think this is a really important

021815.385 -- 021817.605
problem because that oftentimes

021818.095 -- 021818.595
the

021820.925 -- 021823.085
a lot of the AI systems we interact with today,

021823.085 -- 021825.163
it's okay if they make mistakes if they're

021825.164 -- 021827.625
not operating fully autonomously. For

021827.765 -- 021829.975
robots to actually be useful in

021829.975 -- 021832.055
many real world contexts, they need to be

021832.055 -- 021834.065
operating autonomously long periods

021834.065 -- 021836.163
of time. Now, why is

021836.164 -- 021838.294
this hard There's

021838.294 -- 021840.453
a couple reasons why this is hard and I'll focus on

021840.454 -- 021842.495
two of them in this talk. The first

021842.495 -- 021844.595
is that to operate over long periods

021844.595 -- 021846.695
of time, robots may need

021846.696 -- 021848.385
memory over long periods of time.

021849.025 -- 021851.105
And that sort of long term memory introduces a number

021851.105 -- 021853.085
of challenges, and the second is that

021853.325 -- 021855.825
you're operating for longer periods of time, there's more opportunities

021855.885 -- 021858.026
to make mistakes, and more opportunities to get

021858.026 -- 021900.154
stuck that prevent you from even

021900.155 -- 021901.655
moving on to the next part of the task.

021902.855 -- 021905.015
Cool. So let's talk about memory a little bit.

021905.015 -- 021907.175
So, we've seen some

021907.175 -- 021909.275
really cool robot foundation

021909.335 -- 021911.835
models and all of the robot foundation

021911.975 -- 021913.736
models shown here lack,

021914.885 -- 021917.344
basically, lack any notion of memory.

021918.225 -- 021920.645
And maybe this is okay, like a lot of fundamental

021920.705 -- 021922.864
motor skills don't require memory.

021923.565 -- 021925.644
So maybe we can kind of develop

021925.645 -- 021927.695
a lot of kind of physical intelligence

021927.915 -- 021929.825
without that sort of memory. But

021930.066 -- 021932.464
memory is also really useful for a lot of different

021932.465 -- 021934.630
things, whether it be finding objects,

021934.946 -- 021937.285
operating when there's occlusion, counting,

021938.526 -- 021940.245
how many objects you've kind of

021940.646 -- 021942.884
done or kind of how many times you've done something,

021942.885 -- 021944.976
keeping track of steps, Also

021945.035 -- 021946.896
tasks where there isn't a visual impact,

021947.515 -- 021949.705
of of what you did, like if you're sanitizing

021949.765 -- 021952.175
something, for example. And

021952.236 -- 021954.375
so in this first part of the talk, I'd like to develop

021954.375 -- 021956.455
I'd like to talk about whether we can develop memory

021956.455 -- 021958.776
in a way that is compatible with robot foundation

021958.776 -- 021959.276
models.

022001.025 -- 022003.075
And there are a couple

022003.476 -- 022005.875
challenges with this. The first key challenge

022005.875 -- 022008.125
is that as you introduce

022008.125 -- 022010.284
long term memory, you're

022010.284 -- 022012.445
increasing spurious correlations between

022012.445 -- 022014.545
past observations and future actions.

022015.885 -- 022018.185
And the we already have issues with distribution

022018.185 -- 022020.465
shift with imitation learning and basically as you

022020.625 -- 022022.785
add history, you're basically

022022.785 -- 022024.915
exacerbating all of those challenges that come up.

022025.155 -- 022027.255
With imitation learning. And in

022027.255 -- 022029.494
fact, that pre really cool previous work has shown

022029.494 -- 022031.794
that, if you add history,

022032.174 -- 022034.334
your perplexity gets better. Your

022034.334 -- 022036.744
your on the validation set, you're getting better generalization.

022037.635 -- 022039.795
When then you actually look at performance when you're rolling

022039.795 -- 022041.655
it out, performance is a lot worse.

022041.816 -- 022044.055
For example, you have, like, almost double the number of

022044.055 -- 022044.555
collisions.

022046.555 -- 022048.715
And the predominant hypothesis for why this is

022048.715 -- 022051.085
the case is what I talked about. It's these kind of spurious

022051.085 -- 022053.106
correlations between history

022053.165 -- 022055.125
and next actions.

022056.725 -- 022058.425
And the second challenge that

022058.776 -- 022100.856
I also don't wanna understate is that as you

022100.856 -- 022102.864
introduce memory, it actually

022102.864 -- 022104.645
introduces substantial

022105.236 -- 022107.474
compute challenges, both in terms

022107.475 -- 022109.345
of the amount of memory,

022109.744 -- 022112.065
during training, as well

022112.065 -- 022114.105
as just the kind of how long

022114.105 -- 022116.185
it takes to to train these models and how long it

022116.185 -- 022117.486
takes to run them at inference

022118.615 -- 022120.905
we Cool. So let's ground

022120.905 -- 022122.935
ourselves in a couple example tasks. So,

022123.015 -- 022124.875
say that we want the robot to,

022125.175 -- 022126.954
kind of put six scoops of ingredients

022127.794 -- 022129.695
into the green and and blue bowls.

022131.015 -- 022133.175
And we'd like it to remove the items from

022133.175 -- 022135.465
the shelf, dust the shelves, and place the items

022135.465 -- 022137.075
back. Where they were.

022137.556 -- 022139.655
These are both tasks that require significant

022139.715 -- 022141.736
memory, actually extending to

022141.895 -- 022143.435
the very beginning of the episode.

022145.435 -- 022147.545
So So there's two core

022147.545 -- 022149.725
ideas that we're going to introduce to try to

022149.965 -- 022151.665
tackle the challenges I talked about.

022152.625 -- 022154.515
The first idea is that

022155.555 -- 022157.715
a lot of the relevant information the robot

022157.715 -- 022159.865
needs is only present in

022159.865 -- 022202.185
a small number of previous frames. And so,

022202.185 -- 022204.364
you don't actually need to have complete memory

022204.905 -- 022207.195
of the past. And the

022207.195 -- 022209.385
second is that a lot of the decisions that a robot

022209.385 -- 022211.686
is making really only

022211.686 -- 022213.295
require memory at a higher level of

022213.776 -- 022215.995
abstraction, not the lowest level of action prediction.

022217.035 -- 022219.130
And so what we're gonna do with these two key

022219.130 -- 022221.145
ideas first, we're going to retrieve only a few

022221.145 -- 022223.535
key frames and use those key frames

022223.675 -- 022225.785
in memory rather than using the full

022226.345 -- 022228.405
video as memory. The

022228.405 -- 022230.564
second is that we're gonna when we retrieve these key frames,

022230.565 -- 022232.574
we're gonna give this memory to the

022232.574 -- 022234.525
high level policy and not to

022235.005 -- 022235.605
low level policy.

022237.544 -- 022239.625
And then the low level policy will give either no

022239.625 -- 022241.005
context or or a short context.

022243.205 -- 022245.055
And potentially, this could address both

022245.215 -- 022247.555
challenges, both the challenge of of reducing spurious correlations

022247.614 -- 022249.775
because we're only gonna be giving memory to

022249.775 -- 022251.795
a high level policy that's predicting kind of

022252.035 -- 022254.135
at a high level what the robot should do next.

022254.196 -- 022256.276
And second, it can help address some of the compute

022256.276 -- 022258.465
challenges because we're only gonna be using

022258.625 -- 022300.465
a few key key frames of history,

022300.865 -- 022302.940
at test time. So

022303.255 -- 022305.335
So, specifically, what this looks like is we're gonna

022305.335 -- 022307.435
have a high level policy and a low level policy.

022307.736 -- 022309.976
Where the interface between them is the language primitive

022309.976 -- 022311.605
that the robot should complete next.

022312.110 -- 022314.135
And And, this kind of

022314.135 -- 022316.295
hierarchical architecture has been done in a number of

022316.295 -- 022318.325
prior works. The

022318.325 -- 022320.665
high level policy is going to have

022320.925 -- 022323.085
essentially, have full memory, but it's going to be

022323.085 -- 022324.705
kind of using its memory

022325.165 -- 022327.505
selectively to kind of select out which keyframes

022327.565 -- 022329.645
it wants to use

022329.646 -- 022331.685
readily it's making a prediction.

022332.585 -- 022334.575
And so these keyframes, it's going to be

022334.655 -- 022336.896
based on the past pre preframes, it's gonna be selecting

022336.896 -- 022338.975
which which of those keyframes it wants to remember into

022338.976 -- 022341.324
the future. And we'll also have an

022341.464 -- 022343.565
aggregation step to remove redundancy in

022343.565 -- 022344.465
keyframe predictions.

022345.595 -- 022347.755
And then that high level policy is going to use its

022347.755 -- 022349.375
kind of memory of those keyframes

022350.244 -- 022352.625
to actually predict what the robot should do next.

022353.185 -- 022355.125
And then, the low level policy will,

022356.165 -- 022358.245
get actually in this case no memory, just the

022358.245 -- 022400.145
current frame and the joint positions

022400.365 -- 022402.385
and output target joint

022403.015 -- 022405.025
positions, for the next, like, half second.

022405.425 -- 022406.864
Using the language primitive.

022408.385 -- 022410.515
So This is trained using demonstrations that

022410.515 -- 022412.445
are segmented and annotated with

022412.765 -- 022415.004
language instructions. This is also following previous

022415.004 -- 022417.105
works that use this kind of hierarchical architecture.

022417.415 -- 022419.495
That you can say, okay, this part, I'm gonna

022419.495 -- 022421.875
pick up the scoop. This next part, I'm gonna scoop

022422.356 -- 022424.435
some M and M's. This next part, I'm gonna put those M

022424.435 -- 022426.585
and M's in the green bowl, and so forth.

022427.935 -- 022430.165
Now And then the last key question that I haven't

022430.165 -- 022431.545
talked about is how we actually

022432.266 -- 022434.125
decide which frames to put,

022434.505 -- 022436.125
which key frames to store.

022437.945 -- 022440.105
There's a number of different ways you could do this.

022440.905 -- 022443.034
We actually opted for in some

022443.034 -- 022445.324
ways, like, something that's not particularly

022445.385 -- 022447.465
sophisticated. In principle, you could

022447.465 -- 022449.614
kind of annotate this, you could

022449.614 -- 022451.605
try to derive some method that

022452.005 -- 022454.164
figures out which key frames are gonna contain the

022454.164 -- 022456.265
minimal information for predicting the action.

022457.035 -- 022459.276
We actually just found that using the segment boundaries

022459.276 -- 022501.375
in the dataset seemed to work well

022501.375 -- 022503.455
for all the tasks that we studied, and so we

022503.455 -- 022505.525
used that for simplicity. Think that future

022505.525 -- 022507.685
work could explore other ways of deciding which key

022507.685 -- 022508.565
frames to use.

022511.875 -- 022513.885
So that's the gist of the method. In the

022513.885 -- 022516.065
experiments, we're using only 50 demonstrations

022516.285 -- 022518.375
per task. A pretty small

022518.375 -- 022520.155
number of demonstrations.

022520.696 -- 022522.855
And the reason why we can do this is we're gonna be

022522.855 -- 022524.914
using pretty powerful pretrained models for both

022524.914 -- 022526.825
the high level policy and the low level policy.

022527.066 -- 022528.925
So the high level policy is a pretrained,

022529.545 -- 022531.685
QEN model, this has video data

022531.685 -- 022533.844
in pre training and so it has kind of relevant

022533.845 -- 022535.824
information we could run this asynchronously

022536.044 -- 022538.125
during inference is another

022538.125 -- 022540.214
benefit of using memory for the high level is

022540.214 -- 022542.135
you don't have to run it quite as frequently.

022542.695 -- 022544.935
And, as immediately as the

022544.935 -- 022547.015
low level. And then for the low

022547.015 -- 022549.175
level policy, we're going to be taking the PIO five

022549.335 -- 022551.446
the open source PIO five droid model and

022551.446 -- 022553.405
fine tuning it on the 50 demonstrations.

022554.025 -- 022556.185
This model, out of the box, can already do something

022556.185 -- 022558.285
pretty good, and that means that we don't need

022558.285 -- 022559.754
as many demonstrations for fine tuning.

022601.275 -- 022603.295
So We then compared the approach that I talked about

022603.295 -- 022605.545
with these kind of keyframe prediction and and

022605.785 -- 022608.165
and so forth to different high level policy designs.

022608.405 -- 022610.455
The first is just using no memory,

022610.535 -- 022612.875
to test that we actually need memory for these tasks.

022613.196 -- 022615.215
The second is just kind of vanilla

022615.356 -- 022617.645
long context memory. As much

022617.645 -- 022619.776
memory as we can fit into

022619.776 -- 022621.635
memory for reasonable inference time.

022622.065 -- 022624.225
We can also just do vanilla short context

022624.225 -- 022626.465
history that basically just kind of a blading away

022626.705 -- 022628.895
using those key frames. And lastly,

022628.895 -- 022631.135
we'll use a human high level policy as an upper

022631.135 -- 022632.175
bound of performance.

022633.885 -- 022636.125
Oh, and then lastly, we'll also consider GPT

022636.125 -- 022638.145
five and Gemini out of the box

022638.145 -- 022640.324
as high level policies as well to test

022640.526 -- 022642.695
where are kind of these frontier models are at.

022644.614 -- 022646.775
Now we'd like to actually first make sure that we

022646.775 -- 022648.835
are doing tasks that require long term memory, and

022648.835 -- 022650.935
so this is a policy that only has short

022650.935 -- 022653.095
context memory. It needs to pick the items

022653.095 -- 022655.155
off the shelf, dust the shelves, and replace

022655.155 -- 022657.415
them back. And we can see the high level predictions

022657.685 -- 022659.955
basically the predictions of the high level policy in the top left.

022700.515 -- 022702.675
And what we see is the the policy successfully kind of

022702.675 -- 022704.896
takes the items down, dust the shelves, and and

022704.896 -- 022706.975
then it kind of doesn't have memory, and so it will keep on

022706.975 -- 022708.135
dusting the shelves again.

022710.665 -- 022712.485
And kind of repeat that process.

022712.965 -- 022715.345
So it gets quite confused due to the lack of memory.

022715.960 -- 022718.016
So in contrast, when we

022718.016 -- 022720.095
incorporate memory using the approach

022720.095 -- 022722.135
that I talked about, it's

022722.195 -- 022724.365
able to not only remember that

022724.365 -- 022726.215
it has dust the shelves at this point,

022726.375 -- 022728.455
but also remember, which shelves the

022728.455 -- 022729.805
objects were located on.

022730.845 -- 022732.925
So Second, we can look

022732.925 -- 022735.345
at the scooping task that I talked about and

022735.915 -- 022737.995
here, we can see that the policy, is

022737.995 -- 022738.735
able to

022740.155 -- 022742.395
able to perform a long horizon task that requires, like,

022742.396 -- 022744.705
knowing that it has put three scoops in, count that.

022744.705 -- 022746.646
It also recover from mistakes

022746.865 -- 022748.935
and know that it actually hasn't put a scoop

022748.935 -- 022751.015
in even if it kind of made a mistake during the

022751.015 -- 022753.415
scooping process. So it's quite robust

022753.415 -- 022754.674
to those sorts of things.

022756.435 -- 022758.515
Quantitatively, we see that first, memory

022758.515 -- 022759.895
is really important for performance.

022800.835 -- 022803.236
Second, vanilla history is helpful,

022803.375 -- 022804.596
but skills poorly

022805.655 -- 022807.834
to the the kinds of tasks that we're looking at.

022808.236 -- 022810.556
And with our approach, we're actually able to get performance

022810.556 -- 022812.505
that's very similar using

022812.645 -- 022813.955
a human as a high level policy.

022815.475 -- 022817.964
So Now, we're using

022818.025 -- 022820.105
visual history here. In principle, there are some scenarios

022820.105 -- 022822.155
where you might only need text history.

022822.635 -- 022824.645
For the task that we looked at, long term visual

022824.645 -- 022826.185
memory is actually helpful.

022826.864 -- 022829.364
Compared to using text history of the commands

022829.736 -- 022831.815
that you have seen before. We see that in

022831.816 -- 022832.476
this comparison.

022834.175 -- 022836.335
Then lastly, the comparison I showed before

022836.335 -- 022838.275
didn't include GPT five and Gemini.

022839.614 -- 022841.795
And the reason for this is that it was in practically

022842.075 -- 022844.155
slow to run on the robot. The latency was around

022844.155 -- 022846.244
ten to fifteen seconds. Which is kind of

022846.244 -- 022848.275
far too slow to kind of be

022848.275 -- 022850.355
running repeatedly as an out of the box

022850.355 -- 022852.505
high level policy. So instead,

022852.505 -- 022854.925
we did an offline evaluation to test the accuracy

022854.985 -- 022857.015
of these methods, kind of separate from

022857.015 -- 022859.074
the latency concerns. And

022859.074 -- 022901.155
we find that, we're kind of at a point

022901.155 -- 022902.615
where these models are not yet

022903.905 -- 022906.224
accurate for this kind of high level policy

022906.225 -- 022908.325
prediction task, and significantly

022908.325 -- 022910.375
less accurate than, than

022910.375 -- 022912.385
this MIMR approach. Now, in some

022912.385 -- 022914.545
ways, this is an unfair comparison because these models

022914.545 -- 022916.685
are being applied out of the box, and so they don't get

022916.685 -- 022918.806
access to the fine tuning data, on

022918.806 -- 022921.066
the flip side, these models are also far larger

022921.125 -- 022922.935
than, the kind of open source based

022923.175 -- 022925.415
model that we're using, and so it's basically just a test

022925.415 -- 022927.425
of, where are these kinds of out

022927.425 -- 022929.584
of the box, kind of API based models

022929.584 -- 022929.965
stand.

022931.776 -- 022933.856
Cool. So in summary, we see that

022933.856 -- 022935.905
feeding history is the level policy avoids

022935.905 -- 022937.285
issues with spurious correlations,

022937.965 -- 022940.045
It also reduces with frame

022940.045 -- 022942.355
retrieval, we can reduce compute, and allow

022942.355 -- 022944.435
high level policies to focus on important bits

022944.435 -- 022945.095
of information.

022946.495 -- 022948.736
And, kind of from a practical standpoint, it allows

022948.736 -- 022951.145
robots to remember object locations, count,

022951.705 -- 022953.865
keep track of task steps, and so forth, which I think

022953.865 -- 022955.805
is gonna be really important as we

022956.045 -- 022958.065
hope to develop, intelligence

022958.205 -- 022959.745
in physical robots.

023001.374 -- 023003.535
So Cool. So that's kind of the first bit

023003.535 -- 023005.661
about long horizon a autonomy, which

023005.661 -- 023007.605
is needing memory over long periods of time.

023008.195 -- 023010.034
The second aspect of,

023010.355 -- 023012.505
long horizon autonomy is that when

023012.505 -- 023014.664
you're operating over long periods of time, the robot

023014.664 -- 023017.075
has more opportunities to make mistakes and get stuck.

023017.236 -- 023019.345
And that kind of can really tank performance

023019.526 -- 023021.205
you're looking at longer horizon tasks.

023022.805 -- 023024.885
For this part of the talk, I'm actually gonna be be

023024.885 -- 023026.905
talking about a paper that's fairly,

023026.986 -- 023029.095
I guess by machine learning standards is ancient.

023029.415 -- 023031.275
I think it's like a year or two old.

023032.465 -- 023034.784
And, the reason why I wanna talk about it is actually

023034.784 -- 023037.035
something that we've been using technique that we've

023037.035 -- 023039.095
been using in in, like,

023039.095 -- 023041.475
actually, like, all of our recent

023041.475 -- 023043.415
works that are doing long horizon tasks.

023044.530 -- 023046.084
So Cool. So,

023046.625 -- 023048.705
the motivation for this is that,

023048.865 -- 023051.266
if you train a policy with imitation learning, you see mistakes

023051.266 -- 023053.415
like this, where the robot kind of repeatedly

023053.896 -- 023055.975
is unable to pick up the Sharpie. When it

023055.975 -- 023057.915
does, it kind of drops it and so forth.

023058.455 -- 023100.535
We can also see, mistakes here

023100.535 -- 023102.595
where it's able to pick up the Sharpie, but it

023102.755 -- 023104.905
doesn't put it into the bag successfully.

023105.335 -- 023107.355
These are the kinds of mistakes that really tank

023107.415 -- 023109.115
performance over long time horizons.

023110.155 -- 023112.315
And, we'd like to enable the robot to

023112.316 -- 023114.485
improve and recover in those

023114.485 -- 023116.405
situations. I And

023116.500 -- 023118.614
so, the setup that we have

023118.614 -- 023120.755
in mind is one where we're also gonna

023120.755 -- 023122.835
have a hierarchy, kind of a high level policy and a

023122.835 -- 023123.725
low level policy.

023124.925 -- 023127.185
And instead of kind of providing interventions

023127.324 -- 023129.565
at kind of a low level to help the

023129.565 -- 023131.566
robot recover, we're gonna actually be providing

023131.566 -- 023133.664
interventions at a high level. So we'd

023133.664 -- 023135.744
like to be able to tell the robot if it's in this

023135.744 -- 023137.784
state, and is trying to put the sponge into the

023137.784 -- 023139.655
bag, tell it instead,

023140.815 -- 023142.844
use the sponge to open the bag wider. That

023142.844 -- 023144.835
might be a way to recover

023145.155 -- 023147.065
from the state that it found itself in.

023147.465 -- 023149.624
And that could then allow the robot

023149.624 -- 023151.005
to complete the task successfully.

023152.205 -- 023154.285
Now, this kind of intervention, kind

023154.285 -- 023156.215
of requires a human in the loop. So,

023156.375 -- 023158.005
it's not kind of a final solution,

023158.425 -- 023200.825
we'd like to do is we'd like to take this language correction

023200.825 -- 023202.785
data, pull it into our data set and

023202.946 -- 023205.026
and use it to improve our high level

023205.026 -- 023207.106
policy so that we get higher and higher

023207.106 -- 023209.145
performance when the robot encounters these

023209.146 -- 023210.405
kinds of states and gets stuck.

023211.924 -- 023213.944
So So essentially we can leverage this we wanna be able to

023213.945 -- 023215.975
leverage this high level supervision both on

023216.295 -- 023218.635
the fly and for kind of iterative

023218.795 -- 023220.896
improvement our policy for long term

023220.896 -- 023223.085
tasks. So how do we

023223.085 -- 023225.114
do this Need to

023225.114 -- 023227.235
connect our robot behavior with language and so,

023227.315 -- 023229.475
like in the previous work, we'll be using a high level

023229.475 -- 023231.815
policy that predicts language

023231.815 -- 023234.235
instructions, and then that language instruction along with

023234.385 -- 023236.645
the observation be passed to the low level policy

023236.645 -- 023238.545
for it to predict target

023238.845 -- 023240.945
joint positions. And the

023240.945 -- 023243.105
key insight in this work is that if you

023243.105 -- 023245.566
have a good low level policy,

023245.625 -- 023247.765
you can actually get improvement just

023247.765 -- 023249.125
with a better high level policy.

023250.245 -- 023252.324
And the high level policy can be updated just with

023252.324 -- 023253.145
language supervision.

023254.535 -- 023256.635
And so as long as the level of

023256.635 -- 023258.915
a policy follows a wide range of different

023258.915 -- 023300.235
instructions, including instructions

023302.565 -- 023304.585
that it it won't get stuck with, then

023304.585 -- 023306.905
you can improve this full system by only

023306.905 -- 023308.505
updating the high level policy.

023309.946 -- 023312.165
So you if you're familiar with something Dagger,

023312.165 -- 023314.405
this is essentially doing Dagger on

023314.405 -- 023316.435
the high level policy rather than in

023316.435 -- 023318.815
the action space, and kind

023318.815 -- 023321.135
of a form of language diagram because we can provide these sorts of corrections

023321.135 -- 023322.196
in language space.

023323.975 -- 023326.025
So So specifically, what this looks

023326.025 -- 023327.965
like is we'll freeze the low level policy,

023328.285 -- 023330.446
a person will intervene and say what the robot should

023330.446 -- 023332.405
be doing in a particular state.

023332.565 -- 023334.725
And then this could override the high level

023334.725 -- 023337.025
policy prediction, which as we're running the robot.

023337.105 -- 023339.344
Can also do this in an offline procedure as

023339.345 -- 023341.414
well, which we've done in some of our our future works where

023341.414 -- 023343.655
you kind of take a rollout and then say instead

023343.655 -- 023345.696
of this high level policy prediction, the

023345.696 -- 023347.345
high level policy should have said this.

023348.864 -- 023351.104
And then we fine tune the high level policy on this

023351.105 -- 023352.575
language correction data.

023353.715 -- 023355.615
With kind of standard supervised learning.

023357.696 -- 023359.705
So, Cool. And so

023359.705 -- 023401.646
in our experiments, we

023402.225 -- 023404.305
after fine tuning on these language corrections, the

023404.305 -- 023406.405
robot can learn how to self correct.

023406.704 -- 023408.755
So is an example where it made the

023408.755 -- 023410.905
same mistake that we saw before, and now

023410.905 -- 023412.976
it's gonna say move towards me, go

023412.976 -- 023415.215
higher. So it's kind of self correcting itself with these

023415.215 -- 023417.265
kinds of corrections then putting the Sharpie into

023417.265 -- 023418.164
the bag successfully.

023419.425 -- 023421.585
We can also kind of correct for things

023421.585 -- 023422.725
like, grasping

023423.855 -- 023425.895
so that the it

023425.895 -- 023428.204
can kind of move to the right, instead

023428.204 -- 023430.364
of making a mistake. And then lastly, this example

023430.364 -- 023431.835
of putting the sponge into the bag

023432.555 -- 023434.485
the robot at this point is trying to shove the

023434.805 -- 023437.305
the sponge in, it's not successfully doing that.

023437.895 -- 023440.055
And so, instead, it tries to release

023440.055 -- 023442.135
it and instead tries to poke it into the bag and

023442.135 -- 023442.625
is more

023445.065 -- 023447.145
and really the kind of key point that

023447.145 -- 023449.385
I'd like to make here is that these kinds of language

023449.385 -- 023451.335
corrections allow for

023451.476 -- 023453.775
better long horizon performance. They allow the robot

023453.775 -- 023455.935
to recover from the kinds of mistakes and the

023455.935 -- 023457.575
kinds of times it's getting stuck

023458.135 -- 023459.976
so that it can complete tasks that,

023500.215 -- 023502.405
are over a

023502.405 -- 023503.355
minute length.

023505.675 -- 023507.946
So the only thing that's kind of nice about

023507.946 -- 023509.646
this is it also provides an interface

023510.295 -- 023512.254
for humans to be working with robots.

023512.534 -- 023514.565
And so a human especially in

023514.565 -- 023516.985
safety critical domains where you don't wanna make a mistake,

023517.276 -- 023519.436
see right here the robot's, like, about to spill a

023519.436 -- 023521.516
ton of, a ton of peanuts onto the the

023521.516 -- 023523.605
table. So, you can intervene and say,

023523.605 -- 023525.615
okay, stop, move move

023525.615 -- 023527.775
the left arm to the left, go higher, move the

023527.775 -- 023529.885
scoop into the bag, and then prevent

023529.945 -- 023531.045
that sort of mistake.

023532.645 -- 023534.765
Okay. And then after fine tuning on that sort

023534.765 -- 023536.985
of data, you see kind of the robot ability to

023537.465 -- 023539.624
autonomously correct for that, where it's about to make the

023539.624 -- 023541.004
same mistake here and instead

023541.816 -- 023543.085
it learns how to correct for that.

023545.736 -- 023547.975
Cool. So on these long horizon tasks, we

023547.975 -- 023549.495
see a 20% gain in

023549.976 -- 023552.135
quantitative performance just from using the verbal

023552.135 -- 023554.196
corrections. And it closes a lot

023554.196 -- 023556.245
of the gap to actually

023556.245 -- 023558.665
using these human corrections or using kind of an oracle

023558.755 -- 023600.065
human high level policy.

023601.584 -- 023603.824
And in this particular work, the the low level policies

023603.824 -- 023605.835
have a lot of for improvement. But if you

023605.835 -- 023607.995
apply this sort of technique to settings where you have

023607.995 -- 023610.175
better low level policies, you can actually get

023610.175 -- 023612.655
really high performance for long horizon

023612.655 -- 023614.744
tasks. So the kind

023614.744 -- 023616.824
of the takeaway from a technical standpoint is

023616.824 -- 023619.305
that with this sort of language feedback, robots

023619.446 -- 023621.225
can, improve significantly

023622.294 -- 023624.555
by fine tuning the high level instruction policy.

023625.135 -- 023627.215
This can be a lot more data efficient than trying

023627.215 -- 023629.265
to provide corrections at the low level.

023630.824 -- 023632.905
But this relies on a a performant instruction

023632.905 -- 023633.705
following policy.

023635.066 -- 023637.365
So Now, I mentioned that we've

023637.365 -- 023639.505
used this work quite a bit or this

023639.505 -- 023641.605
technique quite a bit since. And

023641.664 -- 023643.685
so let's see how we can then translate these

023643.685 -- 023645.705
capabilities to more complex tasks.

023646.695 -- 023649.015
And so in the first case, we'd like to apply this to

023649.015 -- 023651.114
surgical robots. This is collaboration with Johns

023651.255 -- 023653.275
Hopkins University, and in particular,

023653.574 -- 023655.845
there's a procedure of removing

023655.985 -- 023658.145
a gallbladder, and the most complex part of the procedure

023658.145 -- 023659.965
is where you need to kind of apply these ducts

023700.765 -- 023702.705
and ultimately kind of cut

023703.135 -- 023705.164
cut the ducts so that the you could

023705.164 -- 023706.305
then remove the gallbladder.

023708.145 -- 023710.225
And here's a video of a

023710.225 -- 023712.245
held out gallbladder bladder, as you might

023712.245 -- 023714.095
imagine, that's fully

023714.315 -- 023714.805
autonomous.

023716.565 -- 023719.065
And, this is You can see the high level policy predictions

023720.455 -- 023722.535
on the top left. Is a completely different

023722.535 -- 023723.915
domain for making trail mix.

023724.975 -- 023727.035
And, we see that the robot is

023727.276 -- 023729.575
able to kind of fully, autonomously complete

023729.575 -- 023730.925
this part of the procedure.

023732.365 -- 023734.155
I'll speed it up a little bit.

023736.135 -- 023738.315
Applying the different ducts. Here's kind of an irreversible

023738.534 -- 023740.505
step of cutting the ducts.

023741.625 -- 023743.725
Continuing, for the second one.

023743.965 -- 023744.784
And repeating.

023746.705 -- 023748.865
So Cool. So the same sort of procedure is

023748.865 -- 023750.905
useful for surgical, it's kind of

023750.905 -- 023752.925
relevant to surgical robots. And the other thing that's nice

023752.925 -- 023755.035
here is that because we have this

023755.035 -- 023757.355
interface between the high level policy and the low level

023757.355 -- 023758.855
policy, this could be useful,

023759.715 -- 023801.465
with, kind of

023802.105 -- 023804.285
surgeons and and people with medical expertise

023804.425 -- 023806.865
that may not have the kind of dexterity

023806.925 -- 023809.245
to operate these kinds of tools, to intervene

023809.245 -- 023810.925
and kind of prevent bad things from happening.

023813.215 -- 023815.475
And then second, we also use this technique

023815.946 -- 023818.106
in, the PIO five model. So

023818.106 -- 023820.255
we wanted, in this case, to be looking at

023820.255 -- 023822.295
mobile manipulation problems. So we're not

023822.295 -- 023824.515
just controlling two arms, but we're also controlling

023824.736 -- 023827.105
the wheels of a mobile base. To complete

023827.105 -- 023829.045
long horizon tasks like tidying a kitchen

023830.114 -- 023831.845
And and

023832.165 -- 023834.405
like before, we'll use this technique to post train

023834.405 -- 023836.625
the high level policy on verbal

023836.625 -- 023838.175
instruction data, like

023838.785 -- 023840.925
telling it to, telling it to kinda

023840.925 -- 023843.085
put the the cup in the sink, place the pillow on the bed,

023843.085 -- 023845.055
and so forth. So

023845.740 -- 023848.105
And here's kind of videos of the final

023848.105 -- 023850.185
policy. So, on the left, the task

023850.185 -- 023851.325
is to clean the bedroom.

023852.435 -- 023854.454
A pretty long horizon task. I can't remember

023854.696 -- 023856.776
On the order of minutes, I think it's Yeah. On the order of

023856.776 -- 023858.795
like four to five minutes. And the

023858.795 -- 023900.945
policy is after using this kind

023900.945 -- 023903.364
of high level instructional data, is able

023903.584 -- 023905.375
to better recover from mistakes and

023906.135 -- 023908.175
better complete this long

023908.175 -- 023910.195
horizon task. So, it first puts

023910.195 -- 023912.355
the laundry into the hamper, then it makes the

023912.355 -- 023914.503
bed, or or tidies the bed, and then

023914.503 -- 023916.649
it, kind of throws away a couple pieces

023916.649 -- 023918.885
of trash. And

023918.885 -- 023921.305
on the right, the robot is

023921.615 -- 023923.856
completing different tasks in tidying a

023923.856 -- 023925.685
kitchen like closing a cabinet,

023926.285 -- 023928.605
here you can see kind of Lucy is providing language

023928.605 -- 023930.705
commands, high level language commands to the robot.

023930.975 -- 023933.055
That's kind of showing examples of the kind of,

023933.055 -- 023935.125
like, verbal interface

023935.125 -- 023936.985
you could have these robot policies.

023938.824 -- 023941.045
So Cool. And the robot

023941.045 -- 023943.135
eventually cleans things

023943.135 -- 023944.490
up. So

023946.465 -- 023948.776
Cool. I'd like to highlight, the folks who

023948.776 -- 023950.936
who led the work I talked about. Specifically, Lucy

023950.936 -- 023953.185
led, the the Yay robot project, Brian

023953.185 -- 023955.446
led the surgical project, and Jenny and Ajay

023955.795 -- 023958.215
led the memory project that I talked about at the beginning.

023958.275 -- 024000.295
And the whole physical intelligence team played

024000.635 -- 024002.715
a big role in, developing

024002.715 -- 024004.095
the pie o five model.

024005.395 -- 024007.555
I'd be happy to take questions and while I do

024007.555 -- 024009.675
that, I'll also mention that same sort of technique

024009.675 -- 024011.755
was also used in kind of the

024011.755 -- 024014.115
PIO six work, that we recently released

024014.596 -- 024016.835
specifically for this coffee making

024016.835 -- 024018.975
task. And so, while we take questions, we

024018.975 -- 024021.215
can watch the robot make espresso. Thanks.

024031.475 -- 024033.664
Yeah. Any questions from the audience Yeah,

024033.664 -- 024035.665
please. Hi, Charles.

024035.825 -- 024037.915
There's a microphone. Yeah. There's a microphone. Over

024038.050 -- 024040.124
there. Chelsea.

024040.124 -- 024042.054
Quick questions. In MER,

024042.695 -- 024044.776
you think the key frame memory is

024044.776 -- 024046.845
more like brief state in

024047.225 -- 024049.305
Palm DP Or is more like a high

024049.526 -- 024051.695
level retrieval prompt for planning

024054.526 -- 024056.605
Interesting. I think that so

024056.685 -- 024058.925
for it to be a belief state, you would need to actually it wouldn't

024058.925 -- 024101.144
just be each key frame, but it would be kind of the

024101.145 -- 024103.305
aggregation of of the information. And

024103.465 -- 024105.545
It's certainly less compact than what you would typically

024105.545 -- 024107.325
think of as a a belief state, I think.

024108.625 -- 024111.045
And it's hard to guarantee that we're getting Markovian

024111.106 -- 024113.205
ness even we do incorporate this

024113.205 -- 024115.335
memory. So I think

024115.335 -- 024117.494
of it, I guess, maybe more closer to what I

024117.494 -- 024119.896
was presenting in terms of just like information

024119.955 -- 024122.276
that that might be useful. But perhaps some ideas

024122.276 -- 024124.395
from that formalism could be for selecting keyframes.

024124.995 -- 024125.635
Thank you.

024128.755 -- 024131.236
Hi. Yeah. Thanks for the talk. I think I have

024131.236 -- 024133.345
a related question only. So

024133.425 -- 024135.665
I can look at it, you know, in memory could be like

024135.665 -- 024137.765
infinite memory, I may need it, ideally. Right

024137.925 -- 024140.185
So how do you look at it, to store

024140.245 -- 024142.375
it in some of our concepts Like, if

024142.375 -- 024144.615
I like to have an infinite kind of memory,

024145.175 -- 024147.335
in robots, so how the memory will be organized

024147.335 -- 024149.565
Like, will you look look at, like, concepts

024149.584 -- 024151.645
being stored and use it in a high

024152.146 -- 024154.305
the high level policy or what's your thought on

024154.305 -- 024155.615
that Yeah.

024156.505 -- 024158.745
I think well, it's definitely one thing that we haven't

024158.745 -- 024200.945
explored that you need to do to scale this up is

024201.265 -- 024203.425
also deciding if you want to, like, remove some items

024203.425 -- 024205.574
from memory as well, because right now,

024205.574 -- 024207.595
the memory will just grow and grow and grow.

024207.755 -- 024208.975
I also think that

024210.255 -- 024212.275
if it does grow, like, kind of significantly

024212.335 -- 024214.454
larger, then ideas from

024214.454 -- 024216.074
retrieval might be useful.

024216.766 -- 024218.925
Where you don't just use all of

024218.925 -- 024221.015
the key frames in your memory, but you actually

024221.015 -- 024223.276
retrieve specific things that are useful for a downstream

024223.415 -- 024225.825
task, like if you're in the kitchen remembering

024226.045 -- 024228.225
where the the, the blender is, for example.

024228.455 -- 024230.615
From your long term memory and you don't need, like, all

024230.615 -- 024232.664
sorts of memory from the bedroom, or from

024232.664 -- 024235.084
the dining room, when making that sort of prediction.

024235.295 -- 024237.455
But, yeah, it's definitely an an open research

024237.455 -- 024239.745
problem, to, yeah, figure out how to scale this completely.

024239.905 -- 024242.065
And just one more related question. If the robot is doing

024242.066 -- 024244.125
multiple tasks, it may need different kind of

024244.125 -- 024246.145
memory for each of the task. Right Yeah.

024246.145 -- 024248.225
So will you be able to extend that as well for

024248.225 -- 024250.035
the multitask robots or

024250.335 -- 024252.415
Yeah. I should actually mention that in the MIMR work in

024252.415 -- 024254.735
those results, we did actually fine tune on all three tasks

024254.975 -- 024257.215
at the same time. And so it was actually a policy

024257.455 -- 024259.255
memory module and a policy that was

024259.655 -- 024301.895
applied to all three of them at the same time, and it was able

024301.895 -- 024303.856
to figure out for the high level prompt,

024304.015 -- 024306.195
which which frames to to keep.

024306.195 -- 024307.065
Thank you.

024309.155 -- 024311.495
Hello, Cholcee. That was a great talk. I think

024311.736 -- 024313.365
my question was, do you think that, like,

024314.325 -- 024316.405
we're lacking, like, reasoning traces on

024316.405 -- 024318.544
using memory, like long horizon reasoning

024318.544 -- 024320.725
traces, like for example, if do something wrong,

024320.725 -- 024322.805
we will remember, like, our mistakes and probably

024322.805 -- 024324.205
just try something else instead of

024324.946 -- 024326.475
trying the same thing or, like,

024327.455 -- 024329.565
just remember where things are, just remember

024329.565 -- 024331.685
to, like, try to remember like, where we put stuff

024331.685 -- 024333.785
very like, just for that key frame, like, just

024333.785 -- 024335.863
like, kinda like marking it. You think that

024335.863 -- 024337.625
we're just lacking, like, really long horizon

024337.865 -- 024339.955
supervision on, like, kind of recent

024339.955 -- 024342.026
choices And that's why we don't have, like, long horizon

024343.055 -- 024344.754
abilities or like maybe exploration

024348.555 -- 024350.575
I do think that kind of thinking carefully about

024350.655 -- 024352.896
the kind of supervision we provide to these systems

024352.896 -- 024355.055
is important, especially for long horizon tasks.

024355.720 -- 024357.825
And the in actually, in kind of

024357.825 -- 024400.145
the PIO six work when we were doing reinforcement learning, you kinda

024400.145 -- 024402.215
need to think about, how do you provide that supervision,

024402.215 -- 024404.295
that reward signal And I think that Yeah. Especially for

024404.295 -- 024406.615
long horizon tasks, don't wanna just provide

024406.615 -- 024408.856
like a sparse reward at the very end, for example.

024408.856 -- 024410.936
So yeah. I think that there's a

024410.936 -- 024412.635
a lot of work to figure out in terms of like

024413.035 -- 024415.115
and and a lot of things to explore also in

024415.115 -- 024417.275
terms of different forms of supervision for long horizon

024417.275 -- 024419.675
tasks. Don't know

024419.675 -- 024421.895
if that's like think it kinda depends on what problem domain

024421.895 -- 024423.975
you're talking about in terms of like whether that's the

024423.975 -- 024426.204
bottleneck or not. But,

024426.204 -- 024428.284
yeah, I think it yeah. Definitely lots of things to explore

024428.284 -- 024430.195
there. Yes.

024431.205 -- 024432.985
Time. Maybe just one last question.

024433.446 -- 024435.515
Okay. Hi. Hi, Chelsea.

024435.515 -- 024437.635
Thanks the great talk and also sharing the

024437.635 -- 024439.806
vision of long horizon task. So,

024439.965 -- 024442.015
I have a question. So, want you

024442.015 -- 024443.785
think one step ahead. So

024444.825 -- 024446.895
when let's say, when we have Dacher's hand

024446.895 -- 024449.074
with a tactile sense of feedback,

024449.405 -- 024451.544
in the future, Do you envision

024451.879 -- 024453.625
that that change would

024454.265 -- 024456.365
incur another major paradigm

024456.471 -- 024458.505
shift in this long horizon

024458.505 -- 024500.645
task that you shared or you

024500.646 -- 024502.665
think that it's more or less

024502.725 -- 024504.915
likely that will be same

024504.915 -- 024506.935
as what you are sharing right now And also,

024507.465 -- 024509.805
Professor Reid Sutton recently shared

024509.864 -- 024510.635
that these are

024512.034 -- 024514.015
these are these are new vision of

024514.355 -- 024516.446
continual learning. So could

024516.446 -- 024518.035
you share a bit more, like, also

024518.550 -- 024520.525
thinking the context of physical

024520.744 -- 024522.425
AI Yeah. Thank you. Yeah. So

024523.199 -- 024525.204
architecture. Yeah. Yeah. I think that the on the tactile

024525.204 -- 024527.405
front, I think that a lot of these same ideas

024527.405 -- 024529.566
are very applicable to to tactile sensors.

024529.566 -- 024531.725
And so I don't imagine there being particular

024531.784 -- 024533.985
paradigm shift. I think the you can

024533.985 -- 024536.065
also implicitly get tactile feedback in these kinds

024536.065 -- 024538.145
of systems because the the camera on the

024538.145 -- 024540.315
wrist can see how much the the the fingers

024540.315 -- 024542.375
are and so they can see kind of the

024542.375 -- 024544.615
deformation in the finger to get information about

024544.615 -- 024546.655
how how it's gripping the porta filter. For

024546.655 -- 024548.776
example, and then on the

024548.776 -- 024550.835
second front in terms of continual learning, I think

024551.075 -- 024553.236
yeah, some of the ideas that we explored in p I o

024553.236 -- 024555.175
six where we're trying to iteratively improve

024555.295 -- 024556.845
I think is really important, especially

024557.635 -- 024559.074
as we might want robots to,

024559.705 -- 024601.955
be reliable in the real world.

024602.035 -- 024604.075
We've also have some work on trying

024604.075 -- 024606.275
to enable that sort of iterative improvement

024606.275 -- 024608.345
and continual improvement in language models,

024608.505 -- 024610.164
not just in weight space, but in text space.

024610.646 -- 024612.725
As well, that I'm quite excited about. So, yeah,

024612.725 -- 024615.205
I think it's a really interesting solve.

024615.625 -- 024617.865
I think is really critical, especially as

024617.865 -- 024619.905
we, try to put these systems in the real world.

024619.905 -- 024622.265
Thank you. Thanks.

024625.485 -- 024627.645
Yeah. Our next speaker will be professor

024627.645 -- 024629.435
Peter Stone. From UT Austin.

024629.915 -- 024631.935
Peter is like the founding, director

024632.074 -- 024634.375
of Texas Robotics and also the chief scientist

024634.375 -- 024636.465
at Sony AI. And today, his

024636.465 -- 024638.545
talk title will be beyond scene to real,

024638.785 -- 024641.026
leveraging simulation support of embodied world

024641.026 -- 024642.845
models for real world robot learning.

024643.565 -- 024645.555
I'll give it to you here. Thank you.

024657.335 -- 024657.725
Nothing

024703.365 -- 024704.665
Plugged in. It says mirror.

024730.685 -- 024731.824
Looks like that, it accepts

024733.675 -- 024733.715
it.

024737.004 -- 024737.645
And this is,

024744.066 -- 024744.725
Always what's gonna

024803.155 -- 024804.170
coming up.

024807.145 -- 024807.585
Try to

024817.034 -- 024818.494
Maybe we can try another

024819.065 -- 024819.995
excellent chapter.

024840.505 -- 024841.195
To mix

024849.095 -- 024851.335
You know, everybody crowding around here does not help.

024945.236 -- 024947.385
I mean, there's clear that the computer

024947.445 -- 024947.495
is

024953.635 -- 024955.254
That's a HER question.

024956.345 -- 024958.475
Somebody else connects a computer and makes a Zoom, then I

024958.475 -- 025000.725
can I mean, we

025000.725 -- 025001.175
can

025024.744 -- 025026.905
My computer's my computer knows that it's sending

025026.905 -- 025029.034
something out It's just not getting to the display. Okay.

025029.355 -- 025031.566
You have Linux or Yeah. It's a

025031.566 -- 025033.705
Linux. Maybe there any resolutions

025033.705 -- 025035.885
I think or things The resolutions

025035.885 -- 025037.285
are right here, so we can do any

025049.396 -- 025051.525
Resolution, do you want So

025051.525 -- 025052.655
this is within the night

025053.675 -- 025054.895
and then I can apply

025056.335 -- 025057.705
my computer thinks it's sending.

025103.265 -- 025105.505
Yeah. I can connect it. If you give me a Zoom link, I can connect.

025203.946 -- 025205.085
A computer, so

025211.225 -- 025212.155
Okay. Thanks,

025213.495 -- 025215.685
thanks for your patience, and, thanks

025216.436 -- 025218.596
to the organizers for putting together

025218.596 -- 025221.055
a fantastic workshop. This is a great topic

025221.125 -- 025223.095
for a workshop. Embodied World Models.

025223.754 -- 025225.914
And as as I was trying to figure out what to

025225.914 -- 025228.095
to talk about today, there's a lot of different

025228.095 -- 025230.175
things, that that I I could have discussed and that

025230.175 -- 025232.024
you've had some already some talks

025232.824 -- 025234.975
up to this point, that talk about causality.

025235.035 -- 025237.115
Elias's talk, which is really important for for

025237.115 -- 025237.935
world models

025239.695 -- 025241.775
and long horizon tasks and and, and language

025241.775 -- 025242.985
conditioning and a ton

025244.226 -- 025246.565
all of those are sort of things that that

025246.645 -- 025248.824
that I that I could talk about to some degree.

025249.505 -- 025251.305
But what I decided to focus on

025251.625 -- 025253.485
as as suggested by this title

025254.185 -- 025256.624
is work that is using a simulator

025256.845 -- 025258.925
or a world model in some in some

025258.925 -- 025301.155
ways but is not neither

025301.555 -- 025303.794
pure sim to real nor pure imitation

025303.794 -- 025305.885
learning. But is sort of using a world model

025305.885 -- 025307.986
in a more more creative way.

025308.476 -- 025310.395
And, this talk is gonna be in two parts.

025310.635 -- 025312.795
At the in the first part, I'm gonna tell

025312.795 -- 025315.035
you almost nothing

025315.035 -- 025317.425
about a whole bunch of things. Sort of introducing

025317.425 -- 025318.965
where I'm coming from and

025319.446 -- 025321.495
and sort of the kinds of things we do in my lab,

025321.655 -- 025323.816
This will be the a great part of the talk for people

025323.816 -- 025325.896
with ADHD. It'll be like flashing right in

025325.896 -- 025327.975
front of you, lots different things, but and you'll you'll have

025327.975 -- 025329.835
to go to the papers to get more detail.

025330.535 -- 025332.845
And then at the end, I for the second half, I'm

025332.845 -- 025335.085
gonna dive a little more, in a little more

025335.085 -- 025337.345
detail on some recent work that appeared in coral

025337.345 -- 025339.405
just last month conference on robot

025339.405 -- 025341.346
learning, that's exactly on the theme

025341.646 -- 025342.946
of this, of this workshop.

025343.735 -- 025345.815
Okay. So here we go. Starting with that

025345.815 -- 025347.885
really, fast part, first of

025347.885 -- 025350.045
all, I'm coming from UT Austin. A lot of exciting

025350.045 -- 025352.135
things happening we happening. We actually have

025352.195 -- 025354.355
two AI institutes there now and NSF funded.

025354.355 -- 025356.535
One that was one of the first ones headed by Adam

025356.535 -- 025358.955
Clivens, the Institute of Foundations of Machine Learning.

025400.405 -- 025401.385
We have a great,

025402.485 -- 025404.665
ethical AI initiative called Good Systems.

025405.026 -- 025407.346
As Bo mentioned, I was the founding director of Texas

025407.346 -- 025409.365
Robotics and now the chair of computer science.

025410.135 -- 025412.215
And a lot of innovative AI education going

025412.215 -- 025414.585
on including a public avail publicly

025414.725 -- 025417.145
available AI literacy course and an online

025417.285 -- 025419.955
masters in artificial intelligence for $10,000.

025422.495 -- 025424.575
The things that all the things going to my research,

025424.575 -- 025426.105
the the research question that connects

025427.945 -- 025430.055
and is this question, to

025430.055 -- 025432.295
what degree can autonomous intelligent agents

025432.455 -- 025434.696
learn in the presence of teammates and their adversaries

025434.696 -- 025436.815
in real time dynamic domains Now not

025436.816 -- 025438.965
all the research answering this

025438.965 -- 025441.145
question is embodied. Not all uses robots.

025441.606 -- 025443.625
And not all of it uses world models.

025443.785 -- 025445.566
But a good chunk of it does. And,

025447.026 -- 025449.315
and so, we we publish

025449.315 -- 025451.725
in a bunch of different sub areas of AI as shown here.

025453.016 -- 025455.335
And some of the sort of use inspired

025455.335 -- 025457.615
research that's that's inspired a lot of the research

025457.615 -- 025459.685
in my lab is a lot

025459.685 -- 025501.755
about world models

025501.755 -- 025504.015
and in many cases, multi agent world

025504.075 -- 025506.255
models. So robot soccer requires

025506.255 -- 025508.035
a model of teammates and their adversaries,

025508.415 -- 025510.715
I'm not gonna talk about that today, except

025510.715 -- 025512.875
to show you a clip from, Robocop this

025512.875 -- 025514.935
past this past summer in

025514.935 -- 025517.015
the human versus robots game. So I have the

025517.015 -- 025519.075
ball here, you're gonna see something that

025520.115 -- 025521.735
oops. Why did that not work

025522.915 -- 025525.155
Try that one more time. Something

025525.155 -- 025527.215
that couldn't have happened even like four or five months

025527.215 -- 025529.295
ago. I left the ball for the robot. I was on the team

025529.295 -- 025530.614
of the robot. And,

025531.356 -- 025533.495
we've saw, a goal that

025533.816 -- 025535.436
we're seeing the robots having

025536.005 -- 025538.345
some sort of model to be able to

025538.776 -- 025540.935
find the goal, find the ball, and and kick it into

025540.935 -- 025543.015
the goal. We're getting to a point where not quite yet at

025543.015 -- 025545.155
the where there's competitive games

025545.155 -- 025546.455
between people and robots.

025547.195 -- 025549.260
But it's, you know, there's been a lot of progress over the

025549.260 -- 025550.875
past twenty five years or so.

025551.755 -- 025553.855
I also do work also a lot on on

025554.414 -- 025556.645
general purpose service robots. So,

025556.725 -- 025558.895
Robocup isn't just about soccer. There's also

025559.875 -- 025601.495
Robocup at home, which involves

025602.155 -- 025603.775
robots that have to do things like,

025605.265 -- 025606.955
like, acting as a party host.

025608.295 -- 025610.614
Or putting away groceries oh, this is the video

025610.614 -- 025612.615
I meant to show. This is a much

025612.615 -- 025614.075
shorter one. So

025615.115 -- 025617.515
introducing people to each other, putting

025617.515 -- 025619.566
away groceries, setting the

025619.566 -- 025621.645
table. Here, this is it putting away groceries.

025622.125 -- 025624.270
There's a for those of you interested in in

025624.270 -- 025626.436
embodied embodied,

025626.816 -- 025628.865
competitions, this Robocop at Home is fantastic.

025630.315 -- 025632.395
I also did have a car in the DARPA urban challenge,

025632.395 -- 025634.760
and have been inspired a lot by autonomous driving,

025635.085 -- 025637.405
And in my, in my role at at

025637.405 -- 025638.605
Sony AI, there's been,

025639.455 -- 025641.405
some really interesting developments

025641.864 -- 025644.115
there. In fact,

025644.115 -- 025646.215
we've had it was a really exciting

025646.276 -- 025648.634
week for us at Sony this

025648.635 -- 025650.774
this week. Some many of

025650.775 -- 025652.935
you know about our work from a couple years ago,

025652.935 -- 025654.865
GT Sophie, that was featured on the

025655.026 -- 025656.405
the cover of Nature where we

025657.655 -- 025659.736
had the first AI agent that could beat the

025659.736 -- 025701.369
best human

025702.315 -- 025704.565
humans at a real time control task in a

025704.805 -- 025706.575
task that people in a physically realistic

025706.976 -- 025709.135
task that people really care, you know, care about

025709.135 -- 025710.516
enough to practice a lot,

025712.095 -- 025714.495
We, for those of you who aren't, familiar

025714.495 -- 025716.485
with it, this is a sort of a demonstration

025716.544 -- 025718.885
of how realistic the simulator

025719.185 -- 025721.425
is. This is a an expert

025721.425 -- 025723.574
driver, Ken Chan, driving in the real world

025723.574 -- 025725.765
on the bottom and in the simulator with the same

025725.765 -- 025728.034
track the same car. The

025728.034 -- 025730.125
simulator, And finishes the

025730.125 -- 025732.245
lap in about a tenth of a second different time in the real

025732.245 -- 025734.445
world versus the simulator because it's modeling all of the physics

025734.445 -- 025736.464
in in very high fidelity. And then

025736.464 -- 025738.385
we did beat some of the best

025738.606 -- 025740.305
drivers with an end to end reinforcement

025740.686 -- 025742.775
learning agent, and

025743.205 -- 025745.325
and learning things

025745.325 -- 025747.646
just from, again, no

025748.205 -- 025750.284
and and oh, I should say in this video, the colored car

025750.284 -- 025751.645
is GT Sophie. The fork

025752.445 -- 025754.525
human ones are driving the white cars.

025754.525 -- 025756.385
It's learned through the skills of defensive

025757.385 -- 025759.445
driving where it's know, trying to get into

025759.445 -- 025801.425
a position where the other cars can't pass it.

025801.585 -- 025803.825
But this was all model free, or at least with

025803.825 -- 025805.854
no explicit model. This was end to

025805.855 -- 025807.794
end deep reinforcement learning

025808.015 -- 025809.785
starting from from random behavior.

025810.665 -- 025812.905
Meanwhile, at, at Sony AI,

025812.905 -- 025814.985
just just today, or I guess,

025815.065 -- 025817.165
this week, two days ago, we had

025817.165 -- 025819.165
another paper featured on the cover of Nature,

025820.235 -- 025822.315
so it's the current issue. This has nothing to

025822.315 -- 025824.470
do with embodied world models, but it's so recent that I

025824.470 -- 025826.625
can't I can't help sharing it with all of

025826.625 -- 025828.355
you. Because, it's

025828.835 -- 025831.335
called Phoebe, a fair human centric image benchmark,

025831.395 -- 025833.395
which is the first globally diverse,

025833.455 -- 025835.555
consensually collected fairness evaluation

025836.015 -- 025838.294
data set for human centric computer

025838.294 -- 025839.825
vision tasks. So what it is is

025840.595 -- 025842.606
basically a evaluation

025842.745 -- 025845.135
set for vision models where all of

025845.595 -- 025847.726
the subjects that appear given their consent, have been

025847.726 -- 025849.745
compensated, have, themselves

025849.885 -- 025851.874
contributed the labels for where,

025852.275 -- 025854.595
you know, for all of the features about them, and there's

025854.595 -- 025856.885
subjects from 81 different

025856.885 -- 025858.925
countries. It's it's, diverse in

025858.925 -- 025901.015
many different ways. This is not

025901.015 -- 025902.795
at all the the subject of the talk, but

025903.915 -- 025906.155
there's, you know, it's it's not enough to train

025906.155 -- 025908.175
models, but it's it's, we we show in the

025908.175 -- 025910.114
paper that it's very useful for evaluating

025911.495 -- 025913.574
models. Okay. So

025913.574 -- 025915.675
that's the sort of the whirlwind introduction

025916.045 -- 025918.125
to the kinds of things that that are going on in

025918.125 -- 025920.545
my lab, most many of which I say are are

025921.155 -- 025923.255
embodied and and have world models

025923.555 -- 025925.715
involved. Some other things I'm I'm one

025925.715 -- 025927.875
other thing I'm not gonna talk about, which is very

025927.875 -- 025930.005
subject in central to this topic,

025930.145 -- 025932.315
is social navigation. So we have work

025932.650 -- 025934.765
on robots trying to navigate

025934.824 -- 025937.074
through crowds, and that requires

025937.135 -- 025939.074
an embodied world model of people,

025939.874 -- 025942.114
and so, you know, that that was something that that didn't quite

025942.114 -- 025944.115
make the cut for for this talk, but I had I had

025944.115 -- 025946.265
a talk at ICRA not too long ago, and

025946.265 -- 025948.605
that's online that you could you could see if you're interested.

025949.575 -- 025951.445
What I am gonna focus on in

025952.084 -- 025954.105
in the balance of this talk is three

025954.735 -- 025956.995
three works. Three papers.

025957.776 -- 025959.936
Again, two of them I'm gonna tell you almost

025959.936 -- 030002.095
nothing about very quickly, and then the third

030002.095 -- 030003.946
one, I'm gonna dive into in,

030004.105 -- 030006.185
in much more significant detail. So that's where I'll

030006.185 -- 030008.335
give you sort of all of the all of the

030008.335 -- 030009.155
technical details.

030010.395 -- 030012.655
So first, from a couple years ago,

030014.025 -- 030016.204
we have, have some work on self supervised

030017.215 -- 030019.475
environment synthesis. And

030019.475 -- 030021.795
the this is, first author here is Yifan

030021.795 -- 030023.665
Xu, who's a Ph. D. Student in my lab.

030024.066 -- 030026.175
And the the idea here is that

030026.175 -- 030028.445
we want to do training

030028.445 -- 030029.265
and simulation,

030030.615 -- 030031.915
and of navigation policies.

030032.725 -- 030035.045
And, but in general, the real world

030035.045 -- 030037.085
is not the same as the simulator.

030037.085 -- 030039.085
We can and and I think it's, you know, too much to

030040.285 -- 030042.465
simulation. And so

030042.465 -- 030044.615
there's gonna be completely different environments

030045.096 -- 030047.516
and we want them to, you know, nonetheless, generalize.

030048.635 -- 030051.135
Many people have done sort of pre training in simulation

030051.276 -- 030053.295
and then fine tuning in the real world,

030053.295 -- 030055.115
the idea of this this paper

030055.675 -- 030057.755
and I think this was very clever of Zafan, was to do

030057.755 -- 030059.776
some pre training and simulations, sure,

030100.454 -- 030102.614
but then do a small deployment in the real

030102.614 -- 030105.045
world and find the environments

030105.105 -- 030107.204
where the navi where the behavior fails,

030107.645 -- 030109.964
and then try to generate more simulated

030109.964 -- 030111.705
environments that

030112.026 -- 030114.365
that fall that are similar to those failure environments.

030115.235 -- 030117.394
To then be able to, again, fine tune in

030117.395 -- 030119.584
simulation, but in environments that are more reflect

030122.986 -- 030125.304
And so, you know, sort of the the the overview

030125.305 -- 030127.165
is you have your small real world deployment,

030127.395 -- 030129.445
you find your, your

030129.765 -- 030131.875
database of challenging environments,

030132.515 -- 030134.856
then use a GAN to sort of synthesize those

030135.135 -- 030137.315
create a whole bunch of new environments for

030137.375 -- 030139.405
your for your simulation training. And the best way to get a sense of

030139.405 -- 030141.155
this just to watch some watch this,

030141.475 -- 030142.614
on a video.

030146.175 -- 030148.075
Okay. This happened again. Let's see.

030153.395 -- 030155.795
I think this was because of the setup at the beginning.

030155.795 -- 030158.005
Let me see if this if I can quickly get to the

030159.276 -- 030201.226
to the video. Good way to appreciate it.

030202.065 -- 030203.364
Let me fast forward.

030204.124 -- 030206.624
I don't wanna show you the whole video yet. So this is where

030207.084 -- 030209.165
it's. So the

030209.165 -- 030211.244
idea is that, you know, you you deploy your

030211.244 -- 030212.665
robot in a bunch of environments,

030213.465 -- 030215.625
and then you try to find the ones where it

030215.625 -- 030217.635
failed and generate a whole bunch of

030217.635 -- 030219.655
new ones using this this scan,

030219.655 -- 030221.446
and then, you know, synthesize

030221.965 -- 030224.045
the environments where the the agent is having

030224.045 -- 030226.245
trouble. And then after training, using

030226.245 -- 030228.324
our method, it's able to come go through those the

030228.324 -- 030230.365
the sort of distribution of environments, that

030230.365 -- 030232.145
you care about, much more

030232.615 -- 030234.775
much more efficiently. So in some sense, we're

030234.775 -- 030236.945
we're finding a way to give an embodied

030236.945 -- 030239.106
world model to the robot, that's reflective

030239.325 -- 030241.405
of of the task that it really, that you

030241.405 -- 030243.435
really care about, and, and

030243.435 -- 030245.755
then, you know, this is deployed. Everything I'm gonna show you

030245.755 -- 030247.865
today is is meant is shown to work on real

030247.865 -- 030250.056
robots. So this is, this was

030250.056 -- 030252.115
for navigation through constrained spaces, where

030252.115 -- 030254.175
the robot, you know, has to go through spaces that

030254.175 -- 030256.275
are barely bigger than the robot. And so

030256.275 -- 030258.285
that's where especially this method is is

030258.285 -- 030258.785
shining.

030300.345 -- 030302.585
Okay. So that's the that's the very brief

030302.585 -- 030304.615
that can that you can find all the details in

030304.615 -- 030306.785
the paper. Like many,

030306.785 -- 030308.365
we're also very interested in

030310.205 -- 030310.705
we're

030312.705 -- 030314.865
for partially observable task and motion

030314.865 -- 030316.245
planning is the

030317.365 -- 030319.365
is the domain that we're looking at. And so

030319.785 -- 030321.925
this is first the first author of this is is

030321.925 -- 030324.085
Yoon Woo Kim. And the idea here is that

030324.085 -- 030326.245
we're gonna try to have a robot doing

030326.245 -- 030328.325
these sort of long autonomy tasks,

030328.325 -- 030330.565
the kinds of things that, that Chelsea was talking

030330.565 -- 030332.755
about as well, the robot has to go search

030332.755 -- 030334.965
for things in an in an environment, and

030336.305 -- 030338.465
the using the the foundation model

030338.465 -- 030340.555
not to do the actual

030340.696 -- 030342.776
low level task and motion planning, but

030342.776 -- 030344.855
rather to provide some of the

030344.855 -- 030346.935
common sense of where should we be searching for the for

030346.935 -- 030349.265
the objects. So, and there's a

030349.265 -- 030351.065
couple of different inductive

030351.445 -- 030353.555
biases that we that we inject. One is that you

030353.555 -- 030355.635
know, as you might imagine, we can we can

030355.635 -- 030358.055
have knowledge of where objects tend to appear,

030358.766 -- 030400.806
what's in the living room, what's in

030400.806 -- 030402.385
the refrigerator, and what's in the cabinet. And also,

030403.146 -- 030405.304
objects tend to be collocated. If you see a spoon

030405.305 -- 030407.325
somewhere even outside of the kitchen,

030407.655 -- 030409.896
there's more likely to be a fork nearby that spoon

030409.896 -- 030412.345
because ob there are certain objects that tend to appear

030412.665 -- 030414.825
together. And so by by leveraging

030414.825 -- 030416.846
that, we're able to have the

030416.846 -- 030418.255
robots much more effectively

030418.895 -- 030421.075
search through an environment for the kinds of objects

030421.535 -- 030423.615
that it's looking for. And so this is again a world

030423.615 -- 030425.675
model, coming from a foundation model

030425.675 -- 030427.795
that's that's helping a robot plan, and

030427.795 -- 030429.905
we have some, again, real world experiments. I'm not gonna

030429.905 -- 030431.975
show you the videos here, but, again, all

030431.975 -- 030433.585
the the details are in the paper.

030434.865 -- 030436.845
Okay. That's the end of the sort of

030437.085 -- 030439.165
whirlwind segment of the of the talk.

030439.165 -- 030439.495
So

030441.265 -- 030443.425
now I'm gonna dive a much much more deeply

030443.425 -- 030445.435
into this paper that that, appeared at

030445.435 -- 030447.715
Coral just a month ago. That

030447.715 -- 030450.025
is, I think, perfectly on theme for this

030450.345 -- 030452.505
for this workshop called Simulation Pretain

030452.584 -- 030454.185
Trained Latent Action Space

030454.744 -- 030456.265
for real world policy learning.

030456.824 -- 030459.065
And this is the PhD thesis work of

030459.065 -- 030500.925
of Jiaheng Hu, and it's also collaborative

030501.145 -- 030503.315
with Roberto Martin. Martin, we're

030503.315 -- 030505.574
both co advisors of, of Jahang.

030507.635 -- 030509.945
And so the, the goal

030509.945 -- 030512.024
of of Slack of this of the system that

030512.025 -- 030514.075
we that we published is to learn visual

030514.075 -- 030516.455
motor contact rich, whole body,

030516.975 -- 030519.035
mobile manipulation. So that means, you know,

030519.436 -- 030521.455
robots that are moving around and manipulating

030521.675 -- 030522.175
things.

030523.954 -- 030526.454
With real world RL, without any demonstration.

030527.675 -- 030530.124
So neither using sim to reel

030530.635 -- 030532.874
nor using imitation learning, but doing

030532.874 -- 030533.564
the reinforcement

030535.244 -- 030537.395
starting from all in the real world.

030538.835 -- 030540.805
And so here's, you know, here's the task

030541.285 -- 030543.605
after training. So the kind of task we have, where the robot

030543.605 -- 030545.995
has to you know, navigate around an obstacle

030545.995 -- 030548.205
in the environment, and be able to wipe

030548.205 -- 030549.255
a wipe a whiteboard.

030550.775 -- 030552.935
And so that's just one of the tests. I'll show you a couple

030552.935 -- 030554.945
others that we, we look at as

030554.945 -- 030555.095
well.

030557.645 -- 030559.805
Okay. So so why do we wanna use, you know,

030559.805 -- 030601.736
RL in the real world Learning

030601.875 -- 030604.055
this whole whole mod body manipulation is difficult,

030604.345 -- 030606.585
and zero shots seem to real. It's first it's costly

030606.585 -- 030608.806
to build high fidelity simulators. And

030608.806 -- 030610.885
I think often, you know, you can't you

030610.885 -- 030612.095
can't get them just right.

030613.385 -- 030615.545
And imitation learning, it can be very costly

030615.545 -- 030617.685
to to collect the high quality demonstrations

030617.685 -- 030619.695
and also difficult. And so

030619.695 -- 030621.735
the idea is we wanna directly learn

030621.975 -- 030623.985
via real world interactions without

030623.985 -- 030625.925
high fidelity simulation or demonstrations.

030626.775 -- 030628.935
And so most methods, if you just give them their

030628.935 -- 030631.055
action space in the real world and tell them to try to

030631.055 -- 030633.215
learn, they're gonna do what I what you saw in this video,

030633.215 -- 030635.155
just sort of motor babbling,

030635.294 -- 030636.994
and never really make any meaningful

030637.534 -- 030639.675
progress towards the the task.

030639.675 -- 030641.755
You're not you know, in the long horizon task, you're not

030641.755 -- 030643.765
even gonna get the reward signal once,

030644.935 -- 030647.095
through random behavior, and so there's no way to

030647.095 -- 030649.185
to create a gradient and to try to

030649.744 -- 030652.145
try to learn. And so the idea is that the challenge

030652.145 -- 030654.223
here is to overcome the the

030654.223 -- 030656.459
efficient the sample efficiency needs and also the

030656.459 -- 030658.696
safety concerns. You don't want the robot if it's

030658.697 -- 030700.855
learning real world, to to, you know, drive off

030700.855 -- 030702.885
a cliff, when we're

030702.885 -- 030705.125
working in in complex with complex robot

030705.125 -- 030707.025
embodiments. And so,

030708.040 -- 030710.045
so the the insight

030710.045 -- 030712.245
from Slack is that we what we

030712.245 -- 030714.265
really need is a good action space.

030714.885 -- 030717.385
An action space that facilitates both

030717.885 -- 030719.995
sample efficiency and safety. So rather than

030719.995 -- 030721.915
this random motor babbling in the

030722.155 -- 030724.315
low level joint space, we wanna start

030724.315 -- 030725.455
by giving the robot

030726.585 -- 030728.545
some some high level knowledge

030729.085 -- 030731.165
about what the or or information, or have

030731.165 -- 030733.186
the robot itself discover are the

030733.346 -- 030735.606
what's a good action space. And so for this,

030735.815 -- 030737.895
we use a low fidelity simulation, which is

030737.895 -- 030740.265
a lot easier to generate than a than a high fidelity

030740.425 -- 030742.585
simulation. It doesn't have the requirements of of being

030742.585 -- 030743.885
really faithful to the robot.

030744.735 -- 030747.235
And do unsupervised latent action learning,

030747.445 -- 030749.524
to try to, to learn an

030749.525 -- 030751.635
efficient action space for the for the robot.

030751.875 -- 030753.955
What do I mean by that So here's here's what the you know, a

030753.955 -- 030756.304
low fidelity simulator of that robot

030756.305 -- 030758.705
that I showed you. And unsupervised

030759.085 -- 030801.306
reinforcement learning, what that means is is having

030801.306 -- 030802.526
the robot explore

030803.385 -- 030805.545
the environment without having any reward signal at all.

030805.545 -- 030807.685
It's not trying to do a task. It's just

030807.955 -- 030810.065
sort of the analogy of a baby in a

030810.066 -- 030812.186
crib. You give them give the baby a bunch of toys.

030812.186 -- 030814.415
It, you know, sort of gravitates to the things that

030814.415 -- 030816.494
are surprising or interesting or gives it

030816.895 -- 030818.435
empowers it to do things.

030818.896 -- 030820.976
But it doesn't really have a task yet, and

030820.976 -- 030823.056
yet the skills that it learns there are gonna help

030823.056 -- 030825.075
it do tasks in the future. So

030825.075 -- 030827.145
that's sort of the analogy here. So

030827.145 -- 030829.224
we're gonna learn task agnostic behaviors

030829.225 -- 030831.356
in that low fidelity simulator, How

030831.356 -- 030833.595
do we do that We learn a latent action space, and this is

030833.595 -- 030836.095
based on work from

030837.425 -- 030839.585
the the diversity is all you need paper from Sergey

030839.585 -- 030841.745
Levin's lab, of gives this the

030841.745 -- 030842.495
sort of first

030844.574 -- 030846.665
I think, idea

030846.665 -- 030848.935
in this direction was learning a latent

030849.155 -- 030851.255
action space where you're just trying to have the

030851.255 -- 030853.365
robot figure out how different actions lead

030853.365 -- 030855.845
to different behaviors. Right

030855.845 -- 030858.155
So a a a low dimensional latent action space,

030859.275 -- 030901.514
that allow for temporary temporally extended

030901.515 -- 030902.494
whole body motions.

030904.125 -- 030906.205
But we we wanna go a step further than that.

030906.205 -- 030908.215
We want to not just learn a low

030908.215 -- 030910.294
dimensional action space, but we wanna learn one that's

030910.294 -- 030912.245
controllable. And so we we,

030912.405 -- 030913.925
we introduced a method to do

030914.565 -- 030916.645
disentangled unsupervised skill discovery. So

030916.645 -- 030918.495
what that means is in this load action

030918.766 -- 030920.515
space, want each

030920.835 -- 030922.995
dimension to only control one dimension

030922.995 -- 030925.294
of robot action. So maybe one dimension controls

030925.294 -- 030927.315
the motion of the base, another dimension

030927.374 -- 030929.375
how the camera moves, another how the

030929.375 -- 030931.856
arm moves. And

030931.995 -- 030934.075
so, and so we have we add sort of an

030934.075 -- 030934.285
extra

030936.125 -- 030938.115
diverse and temporally extended behaviors.

030939.236 -- 030941.395
That are structured, which makes it easier to learn

030941.396 -- 030943.465
the downstream tasks, and also we

030943.465 -- 030945.725
can we can lead learn a safety reward,

030946.485 -- 030948.664
that that penalizes moving in this

030948.664 -- 030950.845
low fidelity simulator into, like, self collision

030950.905 -- 030952.765
state or any anything else you define.

030953.316 -- 030955.655
As being unsafe. And so,

030956.776 -- 030959.085
and so if we have a dimension for the camera,

030959.085 -- 031001.135
shown in red, for the for the

031001.135 -- 031003.335
arm shown in blue, and the base shown in green, we

031003.495 -- 031005.675
wanna try to, as I said, learn different dimensions

031005.816 -- 031007.975
that that that control those

031007.975 -- 031010.055
those different action components. And

031010.055 -- 031012.095
so we have a this what we call a skill

031012.095 -- 031014.255
empowerment reward, and then also the safety

031014.255 -- 031016.696
reward that penalizes self collisions and anything

031016.915 -- 031019.115
else. And that allows us to learn this

031019.115 -- 031020.816
low dimensional action space

031021.245 -- 031022.545
in this low fidelity

031023.465 -- 031024.605
low fidelity simulator.

031025.685 -- 031027.945
Then in the next step, we can take those

031028.735 -- 031030.871
that abstract action space, and

031030.871 -- 031032.805
use it for downstream task learning

031033.110 -- 031035.415
with all of it working, all of the learning happening

031035.415 -- 031037.485
in the real world, and

031037.885 -- 031039.905
so we we, develop a factorized

031040.045 -- 031042.175
version of soft factor critic, that

031042.395 -- 031044.555
leverages this structure. And a key

031044.555 -- 031046.405
thing here is that we assume there's a

031046.645 -- 031048.805
factorized reward function. So there's different

031048.805 -- 031050.755
terms, you know, that that,

031050.835 -- 031052.995
are dependent on where the base is, where the arm is,

031052.995 -- 031055.265
where the camera is. And

031055.585 -- 031058.005
and using a mutual information

031058.385 -- 031100.565
reward signal, we can sort of separate

031100.565 -- 031102.486
which action features,

031102.625 -- 031104.995
the z's, impact which

031105.475 -- 031107.494
reward signals. So we can come up with sort of a

031107.494 -- 031109.255
much more sparse representation

031109.495 -- 031111.445
which allows for a more stable

031112.084 -- 031114.476
value function estimate and more efficient

031114.476 -- 031116.976
reinforcement learning trading in the real world,

031117.085 -- 031119.245
We infer these dependencies, this is a different

031119.245 -- 031121.425
paper from RSS on causal

031122.330 -- 031124.364
causal policy gradient, so this connects

031124.364 -- 031126.375
to Elias's first the first talk this

031126.375 -- 031128.465
morning. But it's basically underlying

031128.525 -- 031130.485
it is a conditional

031130.625 -- 031132.175
mutual information reward signal,

031133.025 -- 031135.066
that's that's, having the robot learn for

031135.066 -- 031137.005
itself which action which of these latent

031137.146 -- 031139.026
action dimensions impact which

031139.516 -- 031141.596
component of the reward signal and then coming up

031141.596 -- 031143.885
with a masking matrix that allows, when you're trying

031143.885 -- 031145.965
to maximize one component, to mask

031145.965 -- 031147.265
out the other action components.

031149.005 -- 031151.084
And then we can use that data to initialize the

031151.084 -- 031153.096
replay buffer for for efficient

031153.236 -- 031155.455
learning. So so the kinds

031155.455 -- 031157.615
of tasks we look at, I'm now gonna move to show you

031157.615 -- 031159.695
the show you results, and all the details are are there

031159.695 -- 031201.865
in the paper. But in the,

031202.005 -- 031203.805
you know, we're we're gonna just use onboard

031204.025 -- 031206.109
observations from the robot's

031206.110 -- 031207.475
camera and proprioception,

031208.435 -- 031210.755
We then are gonna learn a task policy

031210.755 -- 031212.765
with our frozen latent action space

031212.765 -- 031214.925
that was learned before the task was

031214.925 -- 031215.425
given.

031217.215 -- 031219.305
And do this, to to

031219.364 -- 031221.545
try to and and then we'd give a a factored reward

031221.605 -- 031224.074
signal that that, is

031224.195 -- 031226.315
dependent on the the camera,

031226.375 -- 031228.696
the arm, and the and the and the base

031228.696 -- 031229.835
for each of the tasks.

031232.055 -- 031234.455
And so and that's, you know, leveraging that causal

031234.455 -- 031236.696
matrix from the previous slide, and then up we can update

031236.696 -- 031238.715
the policy. So here's we can

031238.715 -- 031240.954
now at the beginning of training, it looks sort of

031240.954 -- 031243.175
like other other robots, except now

031243.175 -- 031245.415
it's the actions aren't quite as motor babbling.

031245.415 -- 031247.185
It's doing some more reasonable things.

031248.215 -- 031250.295
But still not getting any task reward, of course,

031250.295 -- 031252.365
at the beginning. But

031252.365 -- 031254.526
then, if we let the robot go for,

031254.845 -- 031256.914
about forty minutes, of real

031256.914 -- 031259.110
world training, here's the sensor in the top left, and

031259.110 -- 031301.485
it's you know, given that the reward

031301.545 -- 031303.565
here for wiping the the

031303.565 -- 031305.895
board, and it's figure it has fig

031305.975 -- 031308.005
oops. Didn't show you the rest of that video. So

031308.005 -- 031309.664
here's that whole video. The eighteen second video, but this

031310.465 -- 031312.324
remember, this is forty minutes into training.

031312.765 -- 031314.745
Starting from the random behavior. And

031314.805 -- 031316.965
it's able to successfully wipe the board. It's getting

031316.965 -- 031318.800
all all of the letters. And so

031319.128 -- 031321.095
and re recall that it's,

031321.423 -- 031322.735
why is it stopping

031327.696 -- 031329.936
Well, you might have to take my word for it that it does finish

031329.936 -- 031332.035
the board. I'll show you the other tasks as well.

031335.856 -- 031337.054
Here's here's the robot,

031337.885 -- 031339.985
pushing garbage into a into a tray.

031341.425 -- 031343.185
Okay. What is never

031343.645 -- 031345.275
had these videos stop partway through.

031351.295 -- 031352.155
One more time.

031353.834 -- 031355.775
See if this is deterministic or nondeterministic.

031356.205 -- 031358.285
I'm doing some real world reinforcement learning here

031358.285 -- 031358.785
too.

031402.795 -- 031404.955
It's the in Yeah. The Internet's flaky, so it's gonna stop

031404.955 -- 031407.195
right around here. But there there did get through that task.

031407.275 -- 031408.735
Let's see if this one is able to

031409.515 -- 031411.755
to get through. So here's pushing garbage into

031411.755 -- 031413.685
the tray, and again, this is all happening with

031413.765 -- 031415.645
all of the learning happening in the real world.

031415.885 -- 031418.055
And then here's sweeping garbage into a bag.

031418.295 -- 031420.455
And so it's using that latent action space with

031420.455 -- 031422.075
these reward components to

031422.946 -- 031425.105
and this is gonna have to load again.

031426.475 -- 031428.555
Okay, you'll take my word for it. It does fit it does

031428.555 -- 031430.685
sweep it into the into the bag.

031430.685 -- 031432.736
And the key here is that, with

031432.736 -- 031434.815
all of these tasks, it's less than an hour of real

031434.815 -- 031437.055
world training, it's doing better

031437.055 -- 031439.145
than if you're using just the raw

031439.145 -- 031441.535
action space for sure. It

031441.535 -- 031443.875
has a higher success rate and fewer safety violations

031444.016 -- 031446.145
if you're using just plain straight sim to

031446.145 -- 031448.235
reel. With all of this the, you know,

031448.235 -- 031450.255
sort of zero shot sim to reel these

031450.255 -- 031452.494
are sort of the the other state of the art

031452.494 -- 031454.815
methods that you would, compare against. And again,

031454.815 -- 031456.425
the key here is that all of the policies

031456.905 -- 031459.344
trained in less than one hour because it has this embodied

031459.345 -- 031501.364
world model that it's learned in the low

031501.424 -- 031503.795
fidelity simulator. So you can find details

031503.795 -- 031505.945
on the paper here. Again, this is the work

031505.945 -- 031507.174
of of Jiaheng Hu.

031508.065 -- 031510.145
And, so just to remind you,

031510.625 -- 031512.696
the theme of this of this talk was

031512.696 -- 031514.856
to tell you about some of the work that we've been doing

031514.856 -- 031516.175
on going beyond

031516.935 -- 031519.035
pure sim to real, still no imitation,

031519.685 -- 031521.845
using simulators in creative ways, either

031521.845 -- 031523.945
to do environmental synthesis

031524.004 -- 031526.414
to be able to to get a a richer simulation

031526.414 -- 031528.755
that's more reflective of your real world environments,

031528.765 -- 031530.925
to be able to do common sense reasoning

031530.925 -- 031533.105
about where to to search in long horizon tasks,

031533.105 -- 031535.298
and then finally, to learn this latent

031535.298 -- 031537.495
action space real world policy learning.

031537.495 -- 031539.655
So with that, thank

031539.655 -- 031541.785
you for thanks for your attention. Sorry about some of

031541.785 -- 031542.285
the technical

031550.914 -- 031553.414
Yeah. Let's see if we have any questions from the audience.

031554.015 -- 031555.355
The microphone is over there.

031600.744 -- 031601.485
Go ahead.

031603.494 -- 031605.585
Great talk. So I had one

031605.986 -- 031608.325
question about the synthesized environment.

031608.566 -- 031610.585
Approach you mentioned. Yes. So when you're injecting

031610.646 -- 031612.735
those learned stochastic process for fine

031612.735 -- 031614.745
tuning, right, are you doing that

031614.745 -- 031617.095
online during training or pre generating

031617.095 -- 031619.415
No. That's pre generated. So it's using it's using

031619.415 -- 031621.745
a GAN. So it's basically we're we're identifying

031621.885 -- 031623.824
the environments where the robot

031624.284 -- 031626.365
fails. Getting some features

031626.365 -- 031628.436
of those environments, and then

031628.495 -- 031630.915
generating features that that's, you know, that are indistinguishable

031630.975 -- 031631.475
from

031633.085 -- 031635.196
generating environments that are indistinguishable

031635.196 -- 031637.375
from those environments where the robot had

031637.535 -- 031639.615
trouble. So it's not just the environments where

031639.615 -- 031641.505
it had trouble, but it's sort of learning a distribution

031645.025 -- 031647.225
and then we can

031647.225 -- 031649.625
do retraining in the simulator still offline

031649.625 -- 031651.455
before putting it onto the real robot.

031651.856 -- 031653.936
Just one follow-up. So if if you wanna do that

031653.936 -- 031655.975
online is like, is there is

031655.975 -- 031658.185
it very hard or how do you counter

031658.405 -- 031700.665
the computational challenges Yeah. I mean, I think that's

031700.805 -- 031702.955
the key would be like know, how because

031703.276 -- 031705.675
in this case, we're still doing the learning in simulations.

031705.675 -- 031707.745
So if we're doing it online with real robots,

031708.066 -- 031710.085
it's gonna be hard to generate enough data

031710.226 -- 031712.319
to learn to improve the policy. So it's it's more of

031712.319 -- 031714.585
a mostly a sample efficiency challenge

031714.585 -- 031716.855
than Thank

031716.855 -- 031717.685
you. Yeah.

031720.565 -- 031722.335
Hi, Peter. Thank you for the great talk.

031722.655 -- 031724.784
Have ADHD myself, so the one already

031724.784 -- 031726.685
hooked me in. Thank you so much for

031727.085 -- 031729.364
it. I have one high level

031729.364 -- 031731.225
question maybe. So I noticed that nowadays

031731.445 -- 031733.325
when people are talking about like,

031733.486 -- 031735.646
generating data or simulation for

031735.646 -- 031737.945
training agents, Most of the people are focusing

031737.945 -- 031740.074
on single agents. So I was wondering,

031740.074 -- 031742.235
I have this idea in my mind times ago, so I want

031742.475 -- 031744.755
was wondering, like, what's your opinion if we can

031745.295 -- 031747.535
gather some data for, like, multi agent

031747.535 -- 031749.965
interaction Right Yeah. That's a that's a fantastic

031750.025 -- 031752.195
question. And and actually, I didn't talk about it. My

031752.195 -- 031754.454
a lot of my research in my lab is about multi agent

031754.515 -- 031756.614
systems and about multi agent data.

031757.775 -- 031800.175
And I often show this video of, like, you know, what traffic

031800.175 -- 031802.445
signal symbol traffic intersections

031802.445 -- 031804.765
should look like in the future when we have autonomous

031804.765 -- 031806.905
cars. It's gonna be, you know, you you

031806.905 -- 031808.985
can have there's gonna be a lot of interaction. And

031808.985 -- 031811.214
to do this, you either need a a

031811.214 -- 031813.225
centralized control control algorithm,

031813.285 -- 031815.605
or you need a predictive model of what all the other

031815.605 -- 031817.621
agents are gonna do. And, you

031817.621 -- 031819.196
know, in traffic, we can do it with,

031819.935 -- 031822.175
you know, with with some kind of a controlling signal

031822.175 -- 031824.355
like we do in that video, But if you're doing

031824.355 -- 031826.595
social navigation, which there's like I say, there's a talk

031826.595 -- 031828.785
on my website from ICRA in the

031828.785 -- 031830.924
spring, which was all about our work

031830.924 -- 031833.164
on social navigation, and that's a a robot

031833.164 -- 031835.195
moving an environment that has to do a lot

031835.195 -- 031837.544
of prediction of, like, where are the people gonna move,

031838.026 -- 031840.265
What are other you know, what are and it's not just where are they gonna

031840.265 -- 031842.574
move, It's also how is what you do gonna

031842.574 -- 031844.805
influence where they're gonna move. And so

031844.805 -- 031847.045
that's that's very much a multi agent model, and

031847.045 -- 031849.337
there's a whole stream in my lab on

031849.337 -- 031851.383
ad hoc teamwork, which is very much

031851.383 -- 031853.565
about teammate modeling, opponent modeling, didn't feature it

031853.565 -- 031855.725
in this talk because most of it is not embodied. I tried

031855.725 -- 031857.970
to talk mostly about robotics things in this this

031857.970 -- 031900.025
talk. But think, you know, ultimately,

031900.025 -- 031902.255
we need to put those together and and have, you know,

031902.415 -- 031904.395
multi multi agent models

031904.635 -- 031906.635
that are embodied in the real world. And,

031906.795 -- 031908.824
so, yeah, I appreciate the question. Thank you

031908.824 -- 031909.355
so much.

031911.755 -- 031913.760
Yeah. Hi. A quick question. What is

031913.760 -- 031915.625
really the benefit of learning

031915.765 -- 031917.760
a compact latent action representation

031918.125 -- 031920.225
Instead of just designing it myself, like, say, the end

031920.225 -- 031922.305
effector poses for instance. So we're just trying

031922.305 -- 031924.665
to bake in the experience inductive

031924.665 -- 031926.755
bias. To the loss functions here Instead,

031926.815 -- 031929.135
why not just design it Yeah. So I mean, you know, if you

031929.135 -- 031930.805
have it's a question of

031931.696 -- 031933.856
you know, how much human intuition is

031933.856 -- 031936.145
gonna be correct. Mhmm. Right So you could

031936.545 -- 031938.784
you know, just say, well, there's gonna be a component for the base

031938.784 -- 031940.864
movement, one the camera, one for the arm. We've actually

031940.864 -- 031942.945
found in the in the initial work on this where we

031942.945 -- 031945.305
learned the latent action space,

031945.305 -- 031947.436
Dosti, that it's not always it's

031947.436 -- 031949.485
not you know, those intuitive dimensions are not

031949.485 -- 031951.625
what come out from this method. So so we

031951.625 -- 031953.405
want controllable, we want latent,

031953.975 -- 031956.055
but we want them to be reflective of

031956.055 -- 031957.905
the environment Okay. And,

031958.395 -- 032000.635
you know, human intuition is for is not

032000.635 -- 032002.714
always reflective of what's actually gonna

032002.714 -- 032004.765
be the low dimensional action space for the robot.

032005.165 -- 032007.555
So you could, but I think it's gonna be more effective

032007.555 -- 032009.895
to allow the robot to in sort of the, you know, the bitter

032009.895 -- 032012.055
lesson concept as well. The more you know, we can try to put in

032012.055 -- 032014.085
human you know, human biases,

032014.146 -- 032016.244
but the more we can let the robot figure

032016.244 -- 032018.324
out what's there for itself, the more, I think, the more

032018.324 -- 032020.350
accurate it's gonna Right. So the intuition was easy to

032020.350 -- 032022.385
bake in in the loss function of the objective of

032022.385 -- 032024.125
the unsupervised learning rather than going and and testing

032024.446 -- 032026.205
Yeah. Okay. Thank you. Yeah.

032027.295 -- 032029.375
Sorry for the sake of time that we are

032029.375 -- 032031.435
running out of time now, but you you probably can

032031.435 -- 032033.525
reach out Peter offline. You have more questions.

032033.525 -- 032035.365
Thank you. Thank you. Yeah.

032035.526 -- 032036.325
Thanks, Peter.

032038.625 -- 032040.525
And, yeah.

032040.845 -- 032043.005
And now let's welcome our today's last

032043.005 -- 032045.115
speaker, from Mila

032045.115 -- 032047.135
University, the Mont Trello. And, his

032047.135 -- 032049.315
topic today will be using foundation models

032049.816 -- 032051.605
for embodied control. Thank you.

032052.526 -- 032052.955
Problem.

032055.975 -- 032057.935
Let's see if this goes smoother than it did.

032058.175 -- 032059.275
For some other

032108.875 -- 032111.155
Alright. Great. How's everybody doing

032112.455 -- 032114.535
Fantastic. Everyone loves robotics. Good to

032114.535 -- 032116.606
be in this room. So I'm gonna

032116.606 -- 032118.696
cover a couple of, like, really recent pieces

032118.696 -- 032120.855
of research out of the lab in this area.

032121.255 -- 032123.494
And I guess, basically cover a lot why I'm

032123.494 -- 032125.635
always still and will be a big of

032125.695 -- 032127.935
using deep reinforcement learning to do all these things

032127.935 -- 032130.175
unless try to make that easier and easier

032130.735 -- 032132.805
with every project. And

032132.805 -- 032135.205
this is really just a large belief of mine because

032135.205 -- 032137.285
robots can still be much better tools

032137.365 -- 032139.465
I want them to be able to help people

032139.725 -- 032141.185
do a lot of different things.

032141.805 -- 032143.965
Make a lot of, like, better agency for the things

032143.965 -- 032146.225
we can have these days, especially as we get older.

032146.825 -- 032149.066
One that really gets me these days is we're still

032149.066 -- 032151.276
not fantastic at being able switch our

032151.276 -- 032153.324
means of production. Another, like,

032153.324 -- 032155.485
electric car company comes up, it takes a lot of

032155.485 -- 032157.185
time to automate that entire process.

032157.925 -- 032200.165
And in particular areas of things like being able to grow

032200.165 -- 032202.305
food, avoid a lot of waste,

032202.305 -- 032204.065
and things like that, and recycle much better.

032204.545 -- 032206.704
So then how can we basically make

032206.704 -- 032209.035
some good automated systems that can do these

032209.595 -- 032211.635
easier faster, less direct

032211.635 -- 032213.105
human involvement at every step.

032214.225 -- 032216.305
So this is where I'm gonna talk

032216.305 -- 032218.175
today about figuring out how to

032218.495 -- 032220.575
understand how to integrate skills a little

032220.575 -- 032222.664
bit better, and then figure out

032222.664 -- 032225.014
how with each iteration, we can get these agents

032225.015 -- 032227.075
to learn faster and faster. I mean, we're getting better

032227.075 -- 032228.785
at making foundational models

032229.465 -- 032231.235
making world models, things like that.

032231.475 -- 032233.555
So these are good ways to be able to, like,

032233.555 -- 032235.455
bootstrap basically the agents that we can

032235.615 -- 032237.856
learn. So they should require a little less data every

032237.856 -- 032240.015
time we've collected some data. We

032240.015 -- 032242.285
should really be using RL to go collect stuff

032242.446 -- 032244.686
for that we don't know the solutions yet to,

032244.686 -- 032246.835
and we can use a lot more of the foundational

032246.835 -- 032248.914
models for where we have data already to build

032248.914 -- 032251.325
on. So this is basically

032251.325 -- 032253.225
a version of this. So the world is

032253.465 -- 032255.805
fantastic and really diverse and complicated.

032256.196 -- 032258.276
Bunch of examples like multi agent environments

032258.276 -- 032259.815
here on the right and sports.

032300.454 -- 032302.614
And basically go through this process, and what

032302.614 -- 032304.905
I'm gonna talk a bit about today is

032304.905 -- 032307.065
also incorporating in a little bit of a step

032307.065 -- 032309.205
for like, almost internal thinking and planning

032309.205 -- 032311.525
that's built into the agent. That helps

032311.525 -- 032313.605
it maybe deliberate a little bit on

032313.605 -- 032315.736
each step. If it works a little bit on

032315.736 -- 032318.034
this thinking and planning process, can help

032318.034 -- 032320.275
save us a little bit of data at that time. And

032320.275 -- 032322.425
then maybe we just save that data for later

032323.045 -- 032325.125
things become more reactive and less thinking

032325.125 -- 032327.285
and planning on newer tasks that Aiden has

032327.285 -- 032329.486
to solve. Okay.

032329.816 -- 032331.895
So then my favorite tool here, just in case

032331.896 -- 032333.725
people aren't totally familiar with this,

032334.284 -- 032336.445
is deep reinforcement learning. It's great because

032336.445 -- 032338.505
we can dump whatever sensor information we

032338.505 -- 032340.665
have from the robot into the deep network.

032341.225 -- 032343.465
Figure out how to estimate what is a better action,

032343.465 -- 032345.585
you know, using advanced or something, or some

032345.585 -- 032347.736
human comes along and pushes a button and I

032347.736 -- 032349.816
prefer what the robot did there instead of what it did

032349.816 -- 032352.205
last time. And in this case, basically,

032352.265 -- 032354.324
propagate these gradients back such that that

032354.324 -- 032356.405
thing has a higher probability the next

032356.405 -- 032358.425
time the agent visits that same state.

032358.525 -- 032359.385
Or observation.

032400.585 -- 032402.825
The, the challenging point though

032402.825 -- 032404.975
is this still struggles to

032404.975 -- 032407.455
generalize basically outside of this distribution.

032407.455 -- 032409.605
And some ways, and it requires an

032409.605 -- 032412.055
enormous amount of data. So

032412.646 -- 032414.726
what might be your first guess at things

032414.726 -- 032416.835
that you might try on this Since we're in this

032416.835 -- 032418.315
workshop on embodied stuff

032420.155 -- 032422.105
Let's maybe try some sort of

032422.425 -- 032424.715
giant video model. And still

032424.715 -- 032426.915
trying to kinda like did a little poll in the lab.

032426.915 -- 032429.195
How many people think that we should just do this for

032429.254 -- 032430.714
everything and it'll solve all of our solutions

032431.475 -- 032433.635
Maybe a quick show of hands. How many people think the

032433.635 -- 032435.885
videos I'm about to show are gonna be perfect

032435.885 -- 032438.045
and there's gonna be no issues for any of the

032438.045 -- 032439.345
robots in any of these videos

032440.785 -- 032442.865
Nobody. Okay. So maybe people still need

032442.865 -- 032445.125
more coffee. Whoops.

032446.765 -- 032448.845
Okay. That still works for you even though it just went upside

032448.845 -- 032450.945
down on my screen right now. So this was just

032450.945 -- 032453.025
a fun example of trying to figure out, okay,

032453.025 -- 032455.045
can the robot screw in some particular

032455.045 -- 032455.545
examples

032458.035 -- 032500.205
This is it, like, just pick up one of the tools in

032500.205 -- 032502.285
the lab and start using the screwdriver. And

032502.285 -- 032502.945
this was

032504.696 -- 032506.935
was kind of prevalent across many different video

032506.935 -- 032508.984
models we train, but these are just the ones my students were

032508.985 -- 032510.205
generating last week.

032511.005 -- 032513.486
This one was a bit better. We're getting a little bit better

032513.486 -- 032515.825
at prompting these models in the last few weeks.

032515.825 -- 032517.815
Although, the use of screws

032517.875 -- 032520.035
was a little confusing. You might also get a

032520.276 -- 032522.124
idea of this problems I'm interested

032523.065 -- 032525.505
in in the current future. And this was just try to pick

032525.505 -- 032527.665
up objects basic inside of the background. And

032527.665 -- 032529.845
there's a lot of things that transform objects

032530.285 -- 032532.025
and basically all of these videos

032532.905 -- 032534.985
so it's basically this might be

032534.985 -- 032536.935
a way to go in the not too distant future.

032537.095 -- 032539.494
I think here there's still a bit of a struggle

032539.494 -- 032541.515
because there's actually not a lot of, like,

032541.755 -- 032543.794
first person robotics data. Tons of

032543.794 -- 032545.975
data. Robots have really only been

032545.975 -- 032548.245
around not too long There's actually many,

032548.245 -- 032550.395
many of them these days, but there's not really enough

032550.395 -- 032551.864
at least to make well, me happy,

032552.745 -- 032554.985
and the people in the lab right now that are working on these

032554.985 -- 032557.345
problems. So then,

032557.646 -- 032559.784
what's maybe a slightly better solution where we can

032559.784 -- 032601.864
extract maybe representations out of these

032601.864 -- 032604.015
models and then use them for a little bit better, like,

032604.175 -- 032606.335
post training in order to get them to be more

032606.335 -- 032608.195
skilled. Okay.

032608.365 -- 032610.385
Hold on. You get to

032610.385 -- 032612.405
see that one again in case you missed it. The

032612.405 -- 032612.645
first time.

032616.925 -- 032619.005
Alright. That'll work a bit better.

032619.146 -- 032621.225
So then I'm gonna talk a bit about this work, what

032621.225 -- 032623.315
I'm gonna call it's called seg dac is how

032623.315 -- 032625.395
it's pronounced, but it's basically how to overcome

032625.395 -- 032627.335
visual reinforcement. Learning.

032628.295 -- 032630.355
With a, basically, a clever way to figure out how to

032630.914 -- 032633.155
extract segments from basically pre trained

032633.155 -- 032635.534
good segmentation model. And use those

032635.755 -- 032637.855
such that we're basically doing something really

032637.994 -- 032640.415
related to kind of like object centric reinforcement learning.

032641.285 -- 032643.526
Using some of the recent pre trained models. So this is a

032643.526 -- 032645.486
picture of Alexander's

032645.705 -- 032647.765
bedroom. Joking. I'm

032647.765 -- 032649.705
pretty sure he generated this one as well.

032650.026 -- 032652.425
But we have really good segmentation models to basically

032652.425 -- 032654.365
break this up into the component pieces.

032654.925 -- 032657.085
Kind of like the last question that was just asked for

032657.085 -- 032659.345
Peter's talk too, like why build in some structure

032659.865 -- 032702.344
I don't think all of you like, perceive

032702.345 -- 032704.765
things on a pixel level, certainly not an atom

032704.824 -- 032707.025
atom by atom level when you go to actually do

032707.025 -- 032709.105
operations. There's some built in structure

032709.105 -- 032711.275
there that is going to pretty much always be

032711.275 -- 032713.325
reused. So basically in this project,

032713.325 -- 032715.485
we're looking at this type of structure and how

032715.485 -- 032717.825
can we make a more object centric representation out

032717.825 -- 032720.065
of this So

032720.385 -- 032722.465
the challenge in doing this though is trying to

032722.465 -- 032724.606
make this fast. So segmented revenue

032724.606 -- 032726.815
is great. It does segment things

032726.815 -- 032728.276
pretty well. But we basically

032729.055 -- 032731.184
developed a way to be able to

032731.185 -- 032733.265
extract the object representation for each

032733.265 -- 032735.585
image really fast so we can still do online

032735.585 -- 032737.645
reinforcement learning. So here we

032737.645 -- 032739.855
really combine, like, the YOLO world model

032739.935 -- 032742.414
So this will give us a nice bounding box, basically,

032742.414 -- 032744.355
for every object inside the scene.

032744.815 -- 032747.135
We don't really want the bounding box. We want the segments

032747.135 -- 032749.255
for how everything actually look in the image.

032749.255 -- 032751.276
So from that, we then give this to Sam.

032751.665 -- 032753.745
To then segment out the object inside of

032753.745 -- 032755.925
that. And then we kinda

032755.925 -- 032758.105
put this two step process together.

032758.645 -- 032800.845
To just get a segment here.

032800.845 -- 032802.925
We've got here on the left. Basically, an

032802.925 -- 032805.015
embedding per object where it's

032805.155 -- 032807.325
located inside the scene relative to where the

032807.965 -- 032810.055
agent actually is in the scene. So

032810.055 -- 032812.135
here's that basically process a little bit more

032812.135 -- 032813.675
in two steps. So first,

032814.355 -- 032816.435
you can basically use segment anything

032816.435 -- 032818.515
to actually like, this last chunk

032818.515 -- 032820.674
here is gonna remove and extract

032820.674 -- 032822.675
out Right now, it's just, like, class

032822.675 -- 032824.614
label or not, basically, for the segmentation

032824.994 -- 032825.445
concept.

032827.055 -- 032829.295
And you're gonna get a patch embedding for whether

032829.295 -- 032831.534
or not that of embedding is going to be matching

032831.534 -- 032833.924
for that object. From that, because

032833.924 -- 032835.984
we've got the bounding box, we can chop

032835.985 -- 032838.405
out from the whole image, which should definitely be corresponding

032838.545 -- 032840.715
to that object. Then

032841.095 -- 032843.395
from there, basically do this average pooling step,

032843.395 -- 032845.555
which hopefully everyone can see since it's actually

032845.555 -- 032847.725
pretty small. In the back of the

032847.725 -- 032849.896
room. This so that we get one

032850.196 -- 032852.255
segment embedding per object. By

032852.255 -- 032854.555
being able to kinda overlap these processes together.

032855.035 -- 032857.285
So this was some fantastic

032857.285 -- 032859.526
work that Alexander was doing. He's like a magician,

032859.526 -- 032901.624
basically. Being able to get some of the stuff to work

032901.624 -- 032904.095
well together. And to get it to work fast.

032904.405 -- 032906.315
Then, basically, now we're gonna really

032906.476 -- 032908.575
the next step is just we've got this embedding.

032908.575 -- 032910.655
Let's just use this in our RL

032910.655 -- 032912.696
policy. So there's

032912.696 -- 032914.785
a few more details. So this ends up

032915.026 -- 032917.505
just as an example here. You know, this is like the Frank

032917.505 -- 032919.365
arm and the many skill

032920.155 -- 032922.236
I think simulator for being able to move objects

032922.236 -- 032924.294
around. And here, these are basically

032924.294 -- 032926.374
the three or six segments that come out. You've

032926.374 -- 032928.645
got one for the arm, target, the

032929.125 -- 032931.225
some background, the teeny little

032931.225 -- 032933.095
blue object that actually needs to move.

032933.255 -- 032935.435
I was kind of impressed that these models were still detecting

032935.495 -- 032937.445
such tall op small objects really well.

032937.925 -- 032939.615
And this is just background.

032940.016 -- 032942.175
Basically. Stuff that isn't really an object and can

032942.175 -- 032942.664
be ignored.

032944.345 -- 032946.505
Alright. So then there

032946.584 -- 032948.905
at least there was one step about being able to design

032948.905 -- 032950.995
the policy here. Because there's a

032951.236 -- 032953.635
possibility to have a different number of these segments,

032953.635 -- 032955.755
like a different number of objects basically

032955.755 -- 032958.235
at any point in the scene. So although

032958.375 -- 033000.555
this is like a policy design, it's

033001.195 -- 033003.275
not too complex here in terms of the other

033003.275 -- 033005.574
model. But there is at least a transformer

033005.574 -- 033007.815
in the first layer because it has to process a

033008.135 -- 033010.295
variable number of segments that could come

033010.295 -- 033011.745
in for each possible task.

033012.705 -- 033014.785
So the most of these policies though

033014.785 -- 033017.055
are pretty much exactly the same you

033017.055 -- 033019.064
know, just the the critic here on the right has

033019.065 -- 033021.465
the extra input for the actions. But

033021.465 -- 033023.865
besides that, the input for the each of these models

033023.865 -- 033025.885
now is just these list of

033025.885 -- 033027.945
segmentation embeddings then

033027.945 -- 033029.964
the actual action at least for

033029.964 -- 033032.204
the critic, and some of the proprioceptive state

033032.204 -- 033033.380
to also helpful.

033034.345 -- 033036.574
Okay. So that was really

033036.574 -- 033038.784
good. And then we created so for

033038.784 -- 033040.825
some of the experiments for this, so

033040.825 -- 033042.985
we used a lot of the environments that are

033042.985 -- 033045.074
already in many scale for able to move some

033045.074 -- 033047.155
objects around. Even got some of

033047.155 -- 033049.235
the ones over here for the g one robot being

033049.235 -- 033051.245
able to manipulate some of the

033051.825 -- 033053.525
simulated fruit around in the room.

033054.065 -- 033056.345
But then Alexander was still

033056.405 -- 033058.565
not happy enough with how difficult the tasks were, so

033058.565 -- 033100.605
he decided to create a bunch

033100.605 -- 033102.765
of really hypnotic stuff from the seventies

033102.765 -- 033104.225
and basically change all the text

033106.084 -- 033108.164
to see, okay, how good are these

033108.164 -- 033110.255
models at generalizing and being

033110.255 -- 033112.355
able to be robust to these types of changes

033112.414 -- 033114.614
So I mean, I would probably

033114.614 -- 033116.855
have a bit of trouble with the version on the right,

033116.855 -- 033119.080
especially if it was like this morning when I really

033119.080 -- 033121.196
tired. But this adds just

033121.196 -- 033123.165
another layer of actually looking at the type of

033123.405 -- 033125.714
robustness we can start to get from

033125.714 -- 033127.835
maybe being using these different types of models.

033128.955 -- 033131.035
Okay. So after training a policy like on

033131.035 -- 033133.334
this task, we can actually start to look at,

033133.334 -- 033135.555
like, the queue values for attention embeddings

033135.555 -- 033136.935
that we're getting for each object.

033137.556 -- 033139.795
So they're really small here, but maybe I'll

033139.795 -- 033141.865
zoom into this one and these two steps. So

033141.865 -- 033143.735
like this is the Frank arm here.

033144.294 -- 033146.415
On the left and here it is on the right. And

033146.415 -- 033148.784
you can see that, like, the the Q value

033148.845 -- 033151.005
for that actual segment for that part

033151.005 -- 033153.035
of the model is going up. The

033153.035 -- 033155.375
arm is pushing the very small

033155.375 -- 033157.615
little block that you can hardly see because it's

033157.615 -- 033159.946
teeny in the screen. As these things start

033159.946 -- 033201.964
to come together. So it's also giving us a

033201.964 -- 033204.065
little bit of an intuition, okay, is this model

033204.624 -- 033206.645
learning something useful and related

033206.645 -- 033208.784
to the Q value It is. At least

033208.784 -- 033211.265
here, looking at the attention values and how they're related

033211.265 -- 033213.288
to the Q value for what's being seen for the

033213.288 -- 033215.265
model. Alright.

033215.705 -- 033217.805
And then from that okay. A whole

033218.066 -- 033220.145
ton of experiments because we're very

033220.146 -- 033222.015
thankful for the detailed thought

033226.196 -- 033228.515
algorithm they could think of under the sun that we'd run experiments

033228.515 -- 033230.235
on. But

033230.794 -- 033232.874
this is so this analysis is

033232.874 -- 033235.005
in two steps. So there's a little bit of a

033235.005 -- 033237.165
lighter bar here So this is the

033237.165 -- 033239.455
original many scale environments

033239.595 -- 033241.615
such that there wasn't any extra, like, perturbations

033241.755 -- 033244.045
or changes of color or anything like

033244.045 -- 033246.365
that. And then the solid bar performance here

033246.365 -- 033248.755
is like so these are kinda like the original

033248.755 -- 033250.835
ones plus these visual perturbations of

033250.835 -- 033252.845
a bunch of different types of changes in

033252.845 -- 033254.864
the environment that we've described from before.

033255.615 -- 033258.115
You can see there's some similarity, especially

033258.335 -- 033300.204
for these easier environments

033300.345 -- 033302.115
with only small amounts of changes.

033302.516 -- 033304.596
But then as we start to get down

033304.596 -- 033306.955
to the much harder environments, where

033306.955 -- 033309.135
these are much more drastic changes, much

033309.135 -- 033311.295
more like that kind of hypnotic pattern

033311.295 -- 033313.445
that was on the past screen rather than just, like,

033313.685 -- 033315.395
switching something from green to blue.

033316.585 -- 033318.895
Here, we can see that on this

033318.895 -- 033321.105
bottom row, our use of being

033321.105 -- 033323.295
able to use CEGDAC is much more robust

033323.295 -- 033325.635
to any of these types of visual perturbations

033325.776 -- 033327.864
that people can supply. And that we

033327.864 -- 033330.325
created because we're also basically

033330.465 -- 033332.565
the challenge of being able to do, like,

033332.946 -- 033335.106
DM control suites and changing the color in

033335.106 -- 033337.346
the background wasn't really that challenging. We wanted

033337.346 -- 033339.606
to add a lot more variation to every object

033339.875 -- 033341.345
every part of the robot.

033342.015 -- 033344.095
Because I guess a lot of what we're also interested in is having this

033344.255 -- 033346.335
generalized to other robots as some part

033346.335 -- 033348.005
too and other objects really well.

033349.365 -- 033351.865
So this was really good visual robustness,

033352.725 -- 033354.106
especially for these harder tasks.

033355.055 -- 033357.395
And this is a comparison to pretty much any

033358.516 -- 033400.675
kind of recent visual algorithm that

033400.675 -- 033402.075
we could find in the literature

033403.205 -- 033405.385
Okay. And then we

033405.385 -- 033407.624
did also a little bit of a comparison for sample

033407.624 -- 033409.755
efficiency. So some might be wondering,

033409.895 -- 033411.975
alright. Great. Glenn, this sounds awesome. Does this

033411.975 -- 033414.475
take three weeks to train So it doesn't.

033414.535 -- 033416.615
Thank goodness. Because of a

033416.615 -- 033418.784
lot of the coding and the use use of those specific

033418.784 -- 033420.864
models, it only takes about a day to train

033420.864 -- 033423.164
one of these. At least in simulation.

033424.084 -- 033426.035
And as well for these cases here,

033426.835 -- 033429.236
the sample efficiency for this model is about

033429.236 -- 033431.545
on par basically, with any of the other kind

033431.545 -- 033433.566
of visual based RL methods.

033433.755 -- 033436.175
Most of them are doing types of image augmentation

033436.395 -- 033438.115
and some of the other ones are using

033438.436 -- 033440.856
some pretrained models, but require a lot more

033440.915 -- 033443.155
assumptions in the way that data needs to be labeled.

033444.275 -- 033446.335
Really, basically, this

033446.335 -- 033448.115
SeGDAC model is fairly flexible.

033448.555 -- 033450.635
Such that we can also use it right now in

033450.635 -- 033452.585
other visual r l problems. So we decided

033452.905 -- 033454.986
just to load up Vizdoom. We created

033454.986 -- 033456.895
what I'll at least call right now for

033457.215 -- 033459.454
seg d q n, because it's just the same segmentation

033459.454 -- 033501.395
model. You can go plug it into whatever

033502.026 -- 033503.965
RL algorithm you feel like after this.

033504.995 -- 033507.235
And then here, this is just on Viz

033507.235 -- 033509.565
Doom right now trying to collect health kits as fast

033509.869 -- 033511.955
possible. And this ends up

033512.175 -- 033514.435
working out pretty well. Here without

033514.435 -- 033516.525
any really additional details for it to learn.

033516.925 -- 033519.025
And you can also see here I'm visualizing

033519.164 -- 033521.635
the Q values for, like, the attention,

033522.415 -- 033524.845
objects that are actually in the scene. So

033524.905 -- 033527.145
you can maybe read that that says, like, closest health kit

033527.145 -- 033529.125
is the one that's got the highest queue value, and you

033529.365 -- 033531.765
can see the other queue values for other objects

033531.765 -- 033533.545
in the scene as the agent is navigating.

033535.355 -- 033537.335
Okay. So that was the first

033537.655 -- 033539.895
project where we're basically figuring out, okay,

033539.895 -- 033542.356
how do how to adapt basically

033542.816 -- 033545.134
some good pretrained models directly into our

033545.215 -- 033547.375
It's been a struggle really to do that

033547.375 -- 033549.686
really smoothly, especially for being able to do do it

033549.686 -- 033551.735
for online RL recently. Yeah. If

033551.735 -- 033553.815
you wanna learn more about this paper, you can see

033553.815 -- 033555.785
the updated version here off of the

033556.026 -- 033558.106
link for SeggDek online. It's been out

033558.106 -- 033600.045
for maybe a month or so right now.

033600.550 -- 033601.925
So Okay.

033602.875 -- 033604.925
Right. This is gonna happen again, it

033604.925 -- 033607.015
seems. Okay.

033607.015 -- 033609.175
So then this gets into kind of the second chunk

033609.175 -- 033611.186
of the work. That I'm gonna talk about a

033611.186 -- 033611.925
bit briefly.

033613.526 -- 033615.545
I'll click on that. Where there's a lot of

033615.545 -- 033617.125
recent work instead

033617.606 -- 033619.865
Alright. So the first part was being able to

033620.165 -- 033622.185
basically plug in some good pre trained model and

033622.185 -- 033624.455
do RL right on top of it. But we're getting

033624.455 -- 033626.696
a lot of great progress in what we're seeing in the research

033626.696 -- 033628.485
lately for adapting vision language

033628.805 -- 033631.065
models in some way to use them for imitation

033631.124 -- 033633.245
and for robotics. And

033633.245 -- 033635.405
this is just really, you know, people, you

033635.405 -- 033637.725
grab some vision language model, you chop

033637.784 -- 033639.945
off the end of that model, and you start training that for

033639.945 -- 033642.186
some robotics task. Could be,

033642.186 -- 033644.305
like, a diffusion model that's doing torque,

033644.305 -- 033646.475
something like that at the end. And then

033646.475 -- 033648.635
this next piece of work trying to figure out,

033648.635 -- 033650.795
okay, what's good way to really make use of

033650.795 -- 033653.295
these models and some model based

033653.925 -- 033655.345
embodied planning as well.

033656.465 -- 033658.545
So here in this next chunk of work, where we're

033658.545 -- 033700.614
looking at improving pre trained vision

033700.614 -- 033702.935
language models, with model based search.

033703.215 -- 033705.294
So I'm really we're and this work really

033705.294 -- 033707.635
trying to get a really good trade off between what we get

033707.946 -- 033710.026
from these pretrained models and what you can

033710.026 -- 033712.075
also get from search, which help

033712.075 -- 033714.395
for some other things downstream, including like safety

033714.395 -- 033716.005
and adjustments. Things like that.

033716.805 -- 033718.855
But basically, one of the

033718.855 -- 033721.275
many things that's nice about a good vision language,

033721.574 -- 033723.844
action policy is it's a

033723.845 -- 033726.015
great prior. Over like, good

033726.015 -- 033728.515
possibilities for actions that are likely

033728.655 -- 033730.885
to be successful for this particular task.

033731.525 -- 033733.625
It's not perfect in all cases.

033733.835 -- 033735.895
But if we had a model can basically

033735.895 -- 033737.785
use this in some way to figure out

033737.946 -- 033740.045
now it can search over, like take that prior

033740.505 -- 033742.685
rather than searching over what could be a really large

033743.065 -- 033745.185
search space, can basically seed, you

033745.185 -- 033747.505
know, that training or basically the action

033747.505 -- 033749.715
to take at that particular step. With the output

033749.715 -- 033751.505
from a good vision language action model.

033752.065 -- 033754.144
And this is what I'm gonna talk about here for the next

033754.145 -- 033756.225
steps. So this is what's called

033756.225 -- 033758.284
a VLabs here. So it's going

033758.284 -- 033800.565
to take in, just for some background on the input,

033800.805 -- 033802.965
still using a vision language action model, so it takes

033802.965 -- 033805.055
in text. Be able to do a large number

033805.055 -- 033807.095
of tasks. It's gonna produce

033807.095 -- 033808.835
some action sequence to specify

033811.446 -- 033813.845
And I'm gonna talk about this chunk here about

033813.845 -- 033815.885
how to get the best use of combining these

033815.885 -- 033817.755
two things together with some

033817.915 -- 033819.995
planning where we want the plan to eventually be

033819.995 -- 033822.125
successful, The better

033822.125 -- 033824.545
the VLA that we have, the better we can basically

033824.885 -- 033827.045
sample down this trajectory on the left and get

033827.045 -- 033828.755
to a successful state fast.

033829.574 -- 033831.655
And we'll show an interesting example about that

033831.655 -- 033833.175
in a minute. So

033833.794 -- 033835.945
first part, planning and continuous space.

033836.664 -- 033838.985
Really, really painful and kind of infinitely

033838.985 -- 033841.415
complex if you wanna get down to the details.

033841.896 -- 033844.215
But as I was saying, so we can use a vision language

033844.215 -- 033845.875
model here to basically now let's

033846.435 -- 033848.686
sample what's gonna be a likely good,

033848.946 -- 033851.026
actions from that model to start this planning at

033851.026 -- 033852.075
least at this stage.

033853.276 -- 033855.405
So it's a good place to start. One of

033855.405 -- 033857.835
the challenging aspects for at least doing this is that

033857.835 -- 033859.965
the VLA is still really, really

033859.965 -- 033902.064
slow. So that means you really don't

033902.065 -- 033904.345
wanna do a ton of samples through the Vision Language

033904.345 -- 033906.425
action model if you wanna be able to expand a lot

033906.425 -- 033908.475
of nodes. So what we actually ended up doing

033908.475 -- 033910.715
is we would sample a number of actions and then

033910.715 -- 033912.805
we would actually build a of, like,

033912.805 -- 033915.164
local quick distributions around those actions,

033915.225 -- 033917.545
things like little Gaussians around all those action

033917.545 -- 033919.976
sequences. And sample around those

033920.276 -- 033922.475
as well. In order to be able to

033922.475 -- 033924.645
basically expand the tree. Here in

033924.645 -- 033926.795
a well informed way. So this is

033926.795 -- 033928.875
just basically take the v VLA, the

033928.875 -- 033931.255
image of the text, do a bunch of sampling

033931.255 -- 033933.385
according to this distribution, and that

033933.385 -- 033935.545
you get a bunch of whatever you wanna call

033935.545 -- 033937.165
them these days, macro actions,

033937.865 -- 033939.935
action chunks, a whole bunch of names for

033939.935 -- 033942.144
these, but basically a number of those. That

033942.145 -- 033944.325
can then be used for sampling

033944.865 -- 033946.875
for the model based process.

033947.435 -- 033949.145
So this is just an example of

033949.566 -- 033951.725
so here's a bunch of these action chunks that have

033951.725 -- 033953.776
been sampled from that first state. So

033953.776 -- 033956.034
it's just first, basically, root node

033956.034 -- 033956.925
expanded here.

033958.285 -- 034000.315
Okay. So From this step,

034000.315 -- 034002.635
we need to figure out if everyone's

034002.635 -- 034004.715
really, you know, remember everything from

034004.715 -- 034007.106
your early AI class, to do the expansion

034007.165 -- 034008.946
and selection step for model,

034009.325 -- 034011.385
like Monte Carlo Tree Search, We need

034011.385 -- 034013.465
some way to actually be able to select better

034013.465 -- 034015.475
actions. So we did

034015.475 -- 034017.635
a kinda clever trick in this case, because we

034017.635 -- 034019.695
basically need something to compute the value.

034020.095 -- 034022.254
Here, we actually used to use the VLA

034022.254 -- 034024.305
again. So after sampling these actions and you get

034024.305 -- 034026.204
into the next state, how likely

034026.425 -- 034028.505
were those actions under the VLA

034028.505 -- 034030.705
given the ones that were recently generated

034030.865 -- 034032.725
Pretty quick actually to compute that likelihood.

034033.186 -- 034035.446
So, basically, we use that as

034035.446 -- 034037.395
our next selection mechanism here.

034037.875 -- 034040.045
So this is just, you know, whether or not you've

034040.045 -- 034042.225
visited that node already many, many times,

034042.845 -- 034044.925
kinda multiplied by how likely is that

034044.925 -- 034046.725
action sequence given by the VL

034046.965 -- 034048.895
that was generated, and then you can just run

034049.295 -- 034051.555
And this part on the bottom is much easier to parallelize

034051.614 -- 034053.526
because you're keeping the same, like,

034053.815 -- 034055.895
cache and everything for, like, the image and

034055.895 -- 034057.975
the text, and you're just sticking in the actions at

034057.975 -- 034059.981
the end. So it's much faster. To be able

034059.981 -- 034102.385
to do that kind of evaluation step

034103.795 -- 034105.755
Okay. And then from this,

034106.095 -- 034108.255
combining this, we've already if you just take

034108.255 -- 034110.305
some VLA that exists already, add

034110.305 -- 034112.385
this model based planning step, you already start

034112.385 -- 034114.324
to get boosts in performance for

034114.915 -- 034116.995
the liberal environment, which does couple

034116.995 -- 034119.335
of data sets for spatial and object oriented.

034120.025 -- 034121.615
So you can see here

034122.335 -- 034124.495
Okay. So this is just the baseline trying to be

034124.495 -- 034126.525
able to grab one of these bowls and be able to pick

034126.525 -- 034128.585
it up But really what we started to

034128.585 -- 034130.605
discover is like where VLA's kinda

034130.606 -- 034132.965
break down is they kind of know most of the behavior

034132.965 -- 034135.215
to grab the object, but they might make kinda

034135.615 -- 034137.785
silly little mistakes just like missing

034137.785 -- 034139.864
by quarter of an inch on stuff, and it's not

034139.864 -- 034141.245
good at recovering from those.

034142.215 -- 034144.375
Instead, this is like really one of the nice parts that

034144.375 -- 034146.405
the model based search helps

034146.465 -- 034148.555
us from. Here. So in this

034148.555 -- 034150.795
case, now this, I believe, is now using

034150.795 -- 034152.825
our method because now

034152.825 -- 034155.145
it can do a little bit of planning just a few steps

034155.145 -- 034157.265
ahead to know, okay, I was gonna make a mistake there.

034157.265 -- 034159.425
I can easily adjust that action a little

034159.425 -- 034201.415
bit, and that's really the kinda easy part

034201.495 -- 034203.715
provides the extra performance that's the game here.

034205.155 -- 034207.295
Okay. Did a kind of fun

034207.295 -- 034209.655
analysis on this too. To figure

034209.715 -- 034211.934
out this. Because there's basically a trade off here. Because we're

034211.935 -- 034213.954
doing something kinda like test time compute,

034214.245 -- 034216.375
basically combining these two together. But

034216.375 -- 034218.235
if you start with some VLA policy,

034218.446 -- 034220.485
and you can start doing some additional,

034220.545 -- 034222.785
basically, post training on the data set that

034222.785 -- 034224.885
you have, then this

034224.885 -- 034227.105
starts to basically the VLA policy

034227.105 -- 034229.125
on the o alone at the beginning is terrible.

034229.446 -- 034231.625
Even, like, a somewhat okay

034231.686 -- 034233.805
VLA policy gives really large gains in

034233.805 -- 034235.545
performance. Because it's already helped

034236.105 -- 034238.265
like, basically provide a good prior for the search space

034238.265 -- 034240.185
really fast. But another

034240.486 -- 034241.705
interesting trade off is

034242.825 -- 034244.895
here, basically, as you get

034244.895 -- 034246.975
a better VLA, performance also starts to go

034246.975 -- 034249.125
down. So depending on how much data

034249.125 -- 034251.172
you have and how much time you have do some

034251.172 -- 034253.345
of this planning, you can basically play around with

034253.715 -- 034255.995
using this in the space or maybe you have no data

034256.236 -- 034258.316
collecting enough to start to get started on being

034258.316 -- 034300.655
able to train your model, But, sure, more data

034300.655 -- 034302.914
always helps, but it also improves the

034304.115 -- 034306.215
performance either for, like, this planning VLapse

034306.276 -- 034308.351
model or it basically helps reduce the

034308.351 -- 034310.395
time it takes to do the search. As

034310.395 -- 034312.475
you can train these models better and get better

034312.475 -- 034312.975
performance.

034315.026 -- 034317.075
Okay. Those were the two, like, main

034317.075 -- 034319.155
projects I was gonna cover about for a bit, and then

034319.155 -- 034320.855
kind of some extensions of these,

034321.235 -- 034323.105
because here, this kind of ends on

034323.265 -- 034325.505
we can do this really well. It also depends on how well

034325.505 -- 034327.505
we can fine tune vision language models.

034327.905 -- 034329.986
So I've got a recent piece of work that was

034329.986 -- 034332.056
at the reinforcement learning conference on, like,

034332.056 -- 034334.135
what is the best way to fine tune basically

034334.135 -- 034336.155
your vision language model you can use

034336.705 -- 034338.865
LoRa. You can use input adapters. You can

034338.865 -- 034341.215
use prefix tuning. You can use

034341.215 -- 034343.276
direct adaptation. And

034343.276 -- 034345.356
we're trying to figure out, okay, what's the best way to

034345.356 -- 034347.345
go about given how much data that you have

034348.465 -- 034350.625
The, basically, short story out of this

034350.625 -- 034352.715
is it really depends on how much data you

034352.715 -- 034354.765
have. So if you don't have a lot of data, there's

034354.765 -- 034356.745
not a big difference between doing

034357.125 -- 034359.295
LoRa and some prefix tuning And as your

034359.295 -- 034401.455
new data that you're trying to integrate in your model

034401.455 -- 034403.645
is gonna start to grow, you definitely

034403.645 -- 034405.725
can start to use things like LoRa, increase your

034405.725 -- 034408.045
prem, count. If you think there's a lot more to be learned

034408.045 -- 034410.255
from your model, and

034410.255 -- 034412.414
there's a bit of a mix of some other options

034412.414 -- 034414.545
here that can be more data efficient

034414.545 -- 034416.144
if you go ahead and use them as well.

034417.185 -- 034419.215
Okay. And then this kinda

034419.215 -- 034421.405
gets a little bit towards so fine tuning works well.

034421.405 -- 034423.475
There's always still some challenges. Maybe

034423.476 -- 034425.875
some of you have also read like John Shulman's recent

034425.875 -- 034427.885
blog post about how to do good, like,

034427.885 -- 034429.965
post training with RL, and you should

034429.965 -- 034432.254
use Laura in pretty much most cases,

034432.395 -- 034434.555
at least for language models. It's a bit true for

034434.555 -- 034436.614
robotics as well. Long as

034436.614 -- 034438.735
you have a lot of data. But this

034438.735 -- 034440.994
also led me to a couple of questions on

034442.145 -- 034444.565
basically how good is deep reinforcement

034444.784 -- 034446.815
learning at learning from its own data

034447.695 -- 034449.645
Because I if you take basically

034451.095 -- 034453.255
basically in this case, the plot on the right is I trained a

034453.255 -- 034454.975
policy in like a

034455.355 -- 034457.475
deterministic environment This is basically

034457.475 -- 034459.515
this line is the best stuff

034459.515 -- 034501.755
that the policy was able to generate. It's

034501.755 -- 034504.065
really great. High value stuff. A replayed

034504.065 -- 034505.775
in the environment again so we know it

034506.255 -- 034507.835
as it can absolutely be learned and used.

034508.565 -- 034510.645
But usually, the policy, what it actually

034510.645 -- 034512.885
learns, there's quite a large gap between how, like,

034512.885 -- 034515.236
the best stuff it actually finds in the environment

034515.375 -- 034517.795
and the average policy it gets from learning.

034518.115 -- 034520.535
And this is a bit concerning if we're gonna end

034520.595 -- 034522.766
up, you know, trying to get the most out of our data we're gonna

034522.766 -- 034525.165
hope our policies are gonna learn from this stuff. We're gonna

034525.165 -- 034527.345
finally deploy in robot in the real world and

034527.345 -- 034529.385
use all of our time for. So

034529.385 -- 034531.625
this has led to at least a couple of recent pieces

034531.625 -- 034533.725
of research I'm looking at better optimization

034533.945 -- 034536.026
algorithms for re reinforcement learning

034536.245 -- 034538.445
in order to basically close the distance on

034538.445 -- 034540.544
this gap. Basically, I want the policy to

034540.544 -- 034542.655
always be able to learn the maximum and,

034542.655 -- 034544.755
like, best stuff it generates during learning

034544.896 -- 034547.025
and not some kind of weird mixture

034547.025 -- 034549.275
of it because deep learning is quite

034549.275 -- 034551.375
painful. It does not like non stationary distributions

034551.675 -- 034553.704
for the most part. This

034553.704 -- 034555.816
led to a recent

034555.816 -- 034557.976
work that I presented yesterday with some of

034557.976 -- 034559.914
my students on being able to

034559.995 -- 034602.075
do this, but also scale up deep learning

034602.075 -- 034604.135
models, things like ResNet, and things like

034604.135 -- 034605.855
that for more complicated tasks.

034606.815 -- 034609.015
And this is very difficult because there's a lot of

034609.015 -- 034611.185
non stationarity in the network. With

034611.186 -- 034613.465
respect to how the deep learning policy model sees

034613.465 -- 034615.044
it. Every time the policy changes,

034615.805 -- 034617.805
distribution and your replay buffer changes,

034617.965 -- 034620.045
It's not the same distribution anymore and it's

034620.045 -- 034622.175
kind of like a successive new

034622.235 -- 034624.314
task distribution problem in terms of just

034624.315 -- 034625.795
how the deep learning model sees it.

034626.835 -- 034628.855
So this basically gets worse whenever your

034628.855 -- 034631.015
network gets larger. So the two

034631.015 -- 034633.085
things that was found for that piece of work that

034633.325 -- 034635.405
seemed to work really well. So one was being able to do

034635.405 -- 034637.884
multi skip connect So

034637.885 -- 034640.165
this is basically kind of like a ResNet,

034640.165 -- 034642.324
but instead of it just being in blocks of three,

034642.324 -- 034644.445
you can skip over pretty much from the

034644.445 -- 034646.704
encoding model from your computer

034647.555 -- 034649.785
your convolutional network. Many

034649.785 -- 034651.975
steps forward So you can kind of skip large chunks

034651.975 -- 034654.054
of the model in case they're not being too useful.

034654.535 -- 034656.855
But the other thing we're also looking into and what worked

034656.855 -- 034659.124
really well is looking at second order

034659.505 -- 034701.715
optimizers. Actually really helped

034701.715 -- 034703.875
policies be able to recover and scale better for

034703.875 -- 034706.055
much larger models. So

034706.055 -- 034708.215
here, we ended up using Kron as an example

034708.215 -- 034710.325
for this. And then we can basically

034710.325 -- 034712.425
apply this and get good,

034712.425 -- 034714.585
like, monotonic improvement performance from

034714.585 -- 034717.055
these deep learning models. Policies and models

034717.055 -- 034719.345
for much deeper networks. And this is

034719.345 -- 034721.595
better than some of the prior work that was

034721.595 -- 034723.754
doing things like resetting networks, because

034723.754 -- 034725.954
it's difficult to use a resetting network

034725.954 -- 034727.995
task to be able to get I mean, you don't,

034727.995 -- 034730.075
especially if you've already spent a lot of work on your

034730.075 -- 034732.165
foundation to model training. And you're gonna tell someone

034732.165 -- 034733.695
you're gonna reset a big chunk of it.

034734.655 -- 034736.905
But also that resetting only really worked

034737.145 -- 034739.544
for off policy based methods that had a replay

034739.544 -- 034741.595
buffer that could then learn from that good stuff

034741.595 -- 034742.795
in replay buffer again.

034743.615 -- 034745.776
This now works on any RL algorithm. So

034745.776 -- 034748.276
we ran it for PQN, for PPO, DQN,

034749.205 -- 034751.305
I think SAC owned Rainbow as well.

034751.736 -- 034753.976
And it's nice. I like these solutions because they work

034753.976 -- 034756.034
in the kinda deep learning model space.

034756.675 -- 034758.925
And not specifically RL. So then it can be applied

034758.925 -- 034801.015
to wherever your model is using deep learning.

034801.015 -- 034803.305
For the future. Okay.

034803.525 -- 034805.605
And this is just basically just kinda wrapping

034805.605 -- 034807.662
things up. This overall recipe that we're going with

034807.662 -- 034809.575
is basically train your large foundational

034809.715 -- 034811.855
model, You've got a lot of data somewhere.

034812.015 -- 034814.215
Should help with generalization. It's kind

034814.215 -- 034816.425
of the definition of doing these foundational models.

034816.905 -- 034819.005
Then basically integrate this with reinforcement

034819.065 -- 034821.085
learning in some way because it'll save

034821.085 -- 034823.155
you a lot more data to up to actually

034823.155 -- 034824.855
collect in the environment for your agent.

034825.986 -- 034828.225
Maybe these two parts where you'll bang your head

034828.226 -- 034830.355
against the wall that why it's not your agent isn't

034830.355 -- 034832.775
learning from the data it's generating very well.

034832.905 -- 034835.066
So look at some of the better optimization methods

034835.066 -- 034836.765
that maybe I've been working on lately.

034837.405 -- 034839.725
Discover some new high value data. Go dump

034839.725 -- 034841.735
that new high value data into your origin

034842.374 -- 034844.454
so you can train your foundation model again and

034844.454 -- 034846.705
kinda iterate in this process in

034847.665 -- 034849.905
engulfed all the data in the universe and it catches

034849.905 -- 034852.305
up to how fast we generate data. Maybe.

034852.945 -- 034855.104
Okay. So then just a bit of acknowledgments for

034855.105 -- 034857.375
the team that's helped cram all the stuff together and

034857.375 -- 034859.455
do a lot of this research. And I'm happy to

034859.455 -- 034900.575
take any questions now.

034910.646 -- 034912.925
Thank you for the great talk. I had one

034912.925 -- 034915.155
question on how do you handle temporal

034915.776 -- 034918.065
credit assignment problem across

034918.285 -- 034920.325
sensor latency Have you done anything on

034920.325 -- 034922.606
that For sensor latency,

034922.606 -- 034924.695
is that what you said Or and latency

034924.695 -- 034926.925
Sensing latency, like there's a lag between

034926.925 -- 034928.845
the sensors and the actuators. Right So have you

034929.165 -- 034931.465
like, one of one of the limitations of DRL techniques

034931.785 -- 034933.846
Right So I just wanted to know if your

034933.846 -- 034935.664
your take on that. Yeah. Well,

034935.885 -- 034938.125
the I mean, so, Chelsea

034938.125 -- 034940.066
was talking a little bit early about incorporating

034940.205 -- 034942.245
some form of, like, more flexible memory.

034942.535 -- 034944.265
Which can also be a good solution here.

034944.745 -- 034946.824
Usually the first thing that people try though is some

034946.824 -- 034949.105
amount of kinda stacking of the recent

034949.164 -- 034951.316
history So keep your

034951.316 -- 034953.394
model doesn't take in the most recent sensor reading.

034953.635 -- 034955.725
Given, like, the last five times steps of re

034955.725 -- 034958.105
sync, history for that sensor

034958.165 -- 035000.225
information. And just let them give the model a lot of

035000.225 -- 035002.175
data, and it'll start to figure out how to

035002.495 -- 035004.735
approximate what that sensor reading really

035004.735 -- 035006.435
means in order to do good control.

035007.105 -- 035009.085
Yeah. Makes sense Thank you. Yeah. Sure.

035011.784 -- 035013.285
Hello. Thank you for the great talk.

035014.975 -- 035017.015
I have a few questions about the

035017.015 -- 035018.905
first project on the SAM

035019.975 -- 035022.055
Sure. Two questions. First question is,

035023.545 -- 035025.515
I really liked your motivation on

035026.155 -- 035028.315
automating certain tasks and we know that the success rates we need

035028.315 -- 035030.545
are extremely high for those for those tasks in

035030.545 -- 035031.895
production. Mhmm.

035032.675 -- 035034.755
What are your thoughts on accumulation of errors

035034.755 -- 035037.135
for foundation models For example,

035037.135 -- 035039.295
Simon, we know that you know, if looks very impressive,

035039.295 -- 035041.316
but in many cases it's still quite brittle.

035042.225 -- 035044.215
The other question I have about the project is,

035044.455 -- 035046.815
there's this kind of trade off between abstracting away

035046.975 -- 035049.105
sort of non important information like light,

035049.345 -- 035051.375
lighting conditions, shadows, but in the case of Sam,

035051.375 -- 035053.316
we might be missing park level information.

035054.185 -- 035056.235
Can you think of ways to strike

035056.235 -- 035058.314
a better balance between those two two

035058.315 -- 035100.475
aspects Yes. Although I think

035100.475 -- 035102.815
I'm gonna give the same answer for both of these.

035103.415 -- 035105.495
I mean, again, the the nice part

035105.495 -- 035107.574
is is that, like, the same model gives

035107.574 -- 035109.915
a most likely a much better representation

035109.915 -- 035111.255
than just starting from nothing.

035111.975 -- 035114.135
That's already a really good start. So even if Sam

035114.135 -- 035116.415
isn't perfect here and there's some challenges

035116.415 -- 035118.115
in the way it can learn good embeddings,

035119.026 -- 035120.815
you're using RL on top of this,

035121.374 -- 035123.454
you end up still being able to learn from that

035123.454 -- 035125.464
information from the embeddings. Like we

035125.464 -- 035127.704
didn't end up having any trouble in this project even

035127.704 -- 035130.045
when Sam might have some issues.

035130.555 -- 035133.034
Because in like when we were running this, the number of embeddings

035133.034 -- 035135.165
would change because Sam would have a couple of errors

035135.165 -- 035137.324
and it wouldn't quite capture, objects really

035137.324 -- 035139.445
well. But because of the way we were training

035139.445 -- 035141.505
the policy that was kinda taking a

035141.505 -- 035143.585
variable length number of embeddings, it learned to

035143.585 -- 035145.635
maybe deal with the fact that of these

035145.635 -- 035147.965
were either, like, occurring or disappearing or coming back.

035148.856 -- 035151.335
This maybe goes a little bit towards your second question

035151.335 -- 035153.465
where obviously,

035153.465 -- 035155.675
maybe I'm an RL and kinda control person,

035156.155 -- 035158.315
and with a little bit more compute in the future,

035158.315 -- 035200.495
I would also train basically the segmentation

035200.615 -- 035202.856
model, and that would all be trained together and it can improve

035202.856 -- 035205.016
on the task. Overall, this

035205.016 -- 035207.105
should save a lot of data

035207.405 -- 035209.435
needed to learn on these visual RL tasks.

035210.235 -- 035211.455
Thank you. Sure.

035213.585 -- 035215.905
Thank you. Nice talk. I had a question about the VLA

035215.905 -- 035217.475
stuff. You could go to that slide, please.

035218.355 -- 035218.725
Yes.

035220.665 -- 035222.824
So it seems like, what you're proposing

035222.824 -- 035225.004
is to sample a set of

035225.004 -- 035227.026
action chunks and then score them

035227.026 -- 035229.185
by the likelihood under VLA, And

035229.185 -- 035231.265
I was a little surprised by that. I I would have thought you'd

035231.265 -- 035233.505
use a value function for scoring. And

035233.505 -- 035235.665
then secondly, if you're just scoring by the likelihood,

035235.665 -- 035237.695
you're basically trying to find

035237.695 -- 035240.066
like the top k most likely sequences.

035240.066 -- 035242.114
Right And there might be other algorithms for that

035242.114 -- 035244.595
beyond just generating test Mhmm.

035244.995 -- 035247.195
Yes. So the first question so it's tricky

035247.195 -- 035249.295
to be able to train a value

035249.355 -- 035251.655
function. Here. So in this case,

035252.665 -- 035254.905
at least it's it seems to have

035254.905 -- 035256.925
been tricky the community to train a value function on

035256.925 -- 035259.085
top of some foundational model, and people

035259.085 -- 035301.185
haven't at least gone that route very

035301.185 -- 035303.365
much. It seems to be easier to train a policy

035303.645 -- 035306.065
So it doesn't have to incorporate in as much information.

035306.355 -- 035308.595
And there's also been a couple of good, like deep

035308.595 -- 035310.445
reinforcement learning papers lately

035310.605 -- 035312.765
that have been scaling to better performance than what

035312.765 -- 035314.955
they've been doing is adding like hundreds of

035314.955 -- 035317.074
layers to the value function, not to

035317.074 -- 035318.865
the policy. There's probably

035319.106 -- 035321.266
more challenging to understand how to do a good, to

035321.266 -- 035323.295
like predict a good value function for those.

035323.776 -- 035325.785
So it's a it is an interesting trade off

035325.946 -- 035328.026
There was a bit of a difference because we generate a

035328.026 -- 035329.995
couple of action chunks then

035330.135 -- 035332.065
we actually make a Gaussian distribution

035332.526 -- 035334.606
along that, and then we generate some more from

035334.606 -- 035336.765
those, and then we evaluate those from the actual

035336.765 -- 035338.775
policy just to see, I don't know, and

035338.775 -- 035340.365
the generation process was just

035340.925 -- 035343.085
noise was added in the wrong place along

035343.085 -- 035345.135
that little action chunk that's gonna cause the

035345.135 -- 035347.605
error, that way, if you go and ask for the likelihood

035347.605 -- 035349.545
of the VLA again, it'll say, okay. No.

035349.705 -- 035351.715
The one that's more towards the

035351.715 -- 035354.125
mean is actually a little bit better. So

035354.185 -- 035356.385
we also even though we did some experiments where

035356.385 -- 035358.525
we turned down the temperature for the VLA,

035358.525 -- 035400.395
that didn't quite solve the problem either.

035400.795 -- 035402.815
So the stochasticity and resampling

035402.875 -- 035404.946
does seem to help. So you find that

035404.946 -- 035406.965
likelihood of the VA is a good indicator of

035407.285 -- 035409.435
success It actually was really well.

035409.435 -- 035411.555
I mean, yes. As long as

035411.555 -- 035413.595
your VLA has been trained over some data,

035413.675 -- 035415.545
And I think that's sort of what this

035416.515 -- 035418.664
plot here is indicating. So

035418.664 -- 035420.445
if if your VLA is

035421.106 -- 035423.265
terrible, then no, it's basically not

035423.265 -- 035425.305
even a any

035425.305 -- 035427.325
kind of approximation for how well that's doing.

035427.486 -- 035429.646
But after a little bit of training on what should

035429.646 -- 035431.765
be expert data, so if it's expert

035431.765 -- 035433.855
data and you train something to generate that, this

035434.255 -- 035436.335
sounds like it should also be essentially a

035436.335 -- 035438.574
value function as long as you can get it to train pretty

035438.574 -- 035440.784
well. We all assume it is expert

035440.784 -- 035443.095
data, even though it's it's not really most of the time.

035443.335 -- 035445.465
Then it becomes at least a fairly good

035445.571 -- 035447.865
approximation But we're looking instead

035448.665 -- 035450.925
we're doing some methods to be able to improve

035450.986 -- 035453.135
the evaluation here. We're also looking at

035453.135 -- 035454.914
being able to use a much better kinda

035455.425 -- 035457.505
model for our model based search into the future as well.

035458.225 -- 035459.525
Yeah. Thank you. Sure.

035504.185 -- 035506.685
Let's, thank Professor Keng again

035507.205 -- 035509.284
for the wonderful talk. And,

035509.445 -- 035511.755
thank you everyone for attending the morning session.

035511.755 -- 035513.935
We have forty five minutes up before the lunch break.

035514.095 -- 035515.395
And there's the poster

035516.665 -- 035518.725
like, at the side. And please

035518.725 -- 035521.044
come come back before 1PM,

035521.045 -- 035522.844
and we'll have five more keynote

035523.325 -- 035525.645
talks this afternoon and a wonderful, like, panel

035525.645 -- 035527.045
discussion. Thank you.

035527.685 -- 035529.675
Thank you. Sure. No problem.

043355.429 -- 043356.348
Later, And,

043359.689 -- 043401.389
will let another organizer

043402.514 -- 043404.531
to introduce our sponsor,

043404.912 -- 043407.412
WAVE, and also their demonstration

043408.227 -- 043408.652
part.

043419.522 -- 043421.602
Okay. So now we are very pleased to

043421.602 -- 043423.611
welcome the speakers from Wave

043423.611 -- 043425.392
to represent their industry

043425.931 -- 043428.152
demo and topic is scaling world models

043428.452 -- 043430.392
to power evaluation and validation.

043430.692 -- 043433.152
So So let's welcome Gianluca

043433.372 -- 043434.272
and Lorenzo.

043447.232 -- 043449.172
All right. Well,

043449.312 -- 043451.392
thank you everyone for coming here just

043451.392 -- 043453.442
right after lunch. So

043453.442 -- 043455.672
we'll wave. And today, we wanna just

043456.231 -- 043456.731
demonstrate

043458.242 -- 043500.562
how do we think of, scaling world models

043500.562 -- 043502.672
a way for the purpose of

043502.672 -- 043504.532
powering our evaluation and validation.

043505.162 -- 043507.422
But let me start with a brief introduction,

043507.482 -- 043509.651
maybe don't

043509.652 -- 043511.892
know who we are. So Wave is a British startup.

043511.892 -- 043514.052
We build self

043514.592 -- 043516.691
driving intelligence And what we're building is this

043516.692 -- 043518.432
product called the Wave AI Driver.

043518.992 -- 043520.491
Which is a product built

043521.132 -- 043523.532
to be integrated into any vehicle because

043523.532 -- 043526.012
it works off, pretty lean

043526.072 -- 043528.412
and flexible AV stack. It's sensor agnostic.

043528.962 -- 043531.042
And it can it's basically ready to integrate

043531.042 -- 043533.222
into any vehicle that has cameras and compute.

043534.262 -- 043536.422
Doesn't rely on any on any HD maps, so it's

043536.422 -- 043538.512
quite scalable. It's

043538.512 -- 043540.742
adapting to any new place that

043540.962 -- 043543.341
it goes extremely well. And it's built

043543.642 -- 043545.802
for the automotive systems today. So it's

043545.802 -- 043547.882
versatile, It can cover

043547.882 -- 043550.302
all applications from ADAS to full IV.

043551.412 -- 043553.462
And because of

043553.462 -- 043555.082
the way we've been building this product,

043555.481 -- 043557.642
this year we really started stress testing

043557.642 -- 043559.682
it in this thing that we call the

043559.682 -- 043601.851
AI 500 series. Where

043601.852 -- 043604.012
the way by a driver is currently driving around

043604.012 -- 043606.112
the world, through 500 different

043606.112 -- 043608.211
cities. And, you know, if I'm not mistaken, we might have

043608.212 -- 043610.452
a couple of cars right now being at the Arctic

043610.452 -- 043612.482
Circle trying to drive there. And, you know,

043612.482 -- 043614.862
we keep seeing this great generalization

043615.162 -- 043617.342
coming from these models. And

043618.011 -- 043620.112
this is just one small example

043620.172 -- 043622.181
of places where we tested almost

043622.181 -- 043624.001
zero shots, so we went to Paris and

043624.562 -- 043627.062
we tried to drive to one of them most

043627.202 -- 043629.402
complex roundabout we could find

043629.542 -- 043631.482
in Europe. This is the Arc De Triomphe.

043631.822 -- 043633.942
As you can see, know, this model

043635.142 -- 043637.072
is built to work in a variant structure

043637.552 -- 043639.531
environment. Like, this is a huge roundabout. There are

043640.011 -- 043642.442
no lane markings, There are many exits.

043642.821 -- 043645.082
Cars are coming from all the sides.

043646.072 -- 043648.162
And the way that I drive it, it just capable

043648.222 -- 043650.242
to generalize, understand how to negotiate

043650.302 -- 043652.562
this kind of behaviors. And

043652.562 -- 043654.771
just safely go around such

043654.772 -- 043655.272
roundabouts.

043658.602 -- 043700.682
The big question is, we kind of

043700.682 -- 043702.911
seen huge

043702.912 -- 043704.482
acceleration in the way we can

043705.042 -- 043707.382
build and improve our driving systems,

043707.762 -- 043709.732
which poses like a new problem

043709.892 -- 043712.382
which is do you evaluate these things at scale

043713.102 -- 043715.202
And, you know, real world testing is essential.

043715.581 -- 043717.661
But it's also very expensive. And so here I

043717.662 -- 043719.842
put some examples of things we encounter

043719.842 -- 043721.732
on the road almost

043721.872 -- 043722.871
on a daily basis.

043724.271 -- 043726.351
But that's still not enough. Like you want to

043726.352 -- 043728.672
make sure that when you validate such

043728.892 -- 043730.951
products, you're very thorough covering all

043730.951 -- 043732.452
the space of possibilities.

043733.222 -- 043735.462
And, also, there is this other property that

043735.462 -- 043737.562
has your model improves, on

043737.562 -- 043739.262
road testing starts to become more and

043739.642 -- 043741.671
more uneventful because you fail a

043741.672 -- 043744.172
lot less. And

043745.102 -- 043747.261
for this reason today, we wanna

043747.262 -- 043749.602
talk about GAIA three. So

043749.602 -- 043752.102
Gaia three is the latest version of our Gaia model

043752.242 -- 043754.092
series that is starting few years ago.

043754.252 -- 043756.451
And this is how we think of

043756.511 -- 043758.571
approaching this problem, and Lorenzo will show you

043759.052 -- 043800.562
a little bit more about GAIA three.

043805.731 -- 043807.912
Hi, everyone. So let

043807.912 -- 043809.961
me deep dive on Gaia three

043809.962 -- 043812.002
and its applications. First

043812.002 -- 043814.312
of all, what is it Gaia three is

043814.551 -- 043816.632
our latest, latent diffusion

043816.632 -- 043819.472
transformer. It's 15,000,000,000

043819.472 -- 043821.542
parameters model. So it's twice the

043821.542 -- 043824.042
size of the previous iteration, Gaia two.

043824.492 -- 043826.652
And we train it with 10 times the amount of

043826.652 -- 043828.902
unique data of our previous

043828.902 -- 043831.141
version. And when we look at the

043831.141 -- 043833.251
training data, we have a proper data

043833.252 -- 043835.331
set of many cars driving around

043835.331 -- 043837.752
the three continents and covering

043838.292 -- 043840.072
different embodiments, different type of vehicles.

043840.442 -- 043842.584
With a focus specifically on

043842.584 -- 043844.622
safety critical scenes vulnerable

043844.922 -- 043847.342
users, and traffic con traffic control signs.

043849.342 -- 043851.550
So let's take a look at how Gaia three

043851.551 -- 043853.571
compares with with Gaia two first.

043853.972 -- 043856.051
So this is this video is a video of

043856.051 -- 043858.032
Gaia two, and even before I start

043858.192 -- 043900.271
start playing, you can take a look at the

043900.271 -- 043902.212
billboard in the front image.

043902.672 -- 043904.832
And you can see that the text is quite blurry.

043904.832 -- 043907.262
We know these are big problems for world models,

043907.742 -- 043909.982
things like test or science are quite crucial

043909.982 -- 043912.002
for driving. So

043912.142 -- 043914.381
now while playing the video, focus on the train

043914.382 -- 043916.622
just behind the billboard and on the cyclist

043916.682 -- 043918.832
in front As

043918.832 -- 043920.452
you can see, the train is disappearing,

043921.052 -- 043922.812
so the cycling partially.

043923.452 -- 043924.752
I'll play it one more time.

043926.892 -- 043928.972
So you can take a look. And

043928.972 -- 043931.052
now, this is the new iteration with Gaia

043931.052 -- 043933.141
three. You can already see again before I play

043933.622 -- 043935.092
the video, that this the

043935.891 -- 043937.972
design, the billboard, is much

043937.972 -- 043940.001
more clear and it's written the same as

043940.002 -- 043942.322
the actual white billboard on the on

043942.322 -- 043944.801
the left part of the

043944.942 -- 043947.062
front image. Now that I play the video, you can see how

043947.062 -- 043949.312
the train is much more consistent than before,

043949.551 -- 043950.771
and so is the cyclist.

043952.922 -- 043953.842
One more time.

043957.301 -- 043959.622
Another interesting, element

043959.622 -- 044001.861
is vulnerable or user. In particular,

044001.862 -- 044003.902
in the presence occlusions. So

044003.902 -- 044006.141
in this case, we'll see a pedestrian passing behind

044006.141 -- 044008.342
the car and coming from the

044008.342 -- 044010.521
other side. We can see the model

044010.581 -- 044012.472
can also deal with short occlusions,

044013.622 -- 044016.042
in generating the pedestrians with the same appearance.

044016.352 -- 044018.022
As before. The occlusion.

044021.582 -- 044023.662
And then, let's take a look at snapshot

044023.662 -- 044025.442
of, again, very small details.

044026.292 -- 044028.521
Crucial for driving. So can see

044028.521 -- 044030.862
here, vulnerable users, in particular, motorcyclist,

044031.921 -- 044033.921
can see a police car rendered in the in the

044034.002 -- 044036.391
but in the bottom part, we see

044036.391 -- 044038.691
Gaia three, and in in the top part, Gaia two.

044038.852 -- 044041.091
You can see the details, for example, the police cars

044041.092 -- 044043.002
are much more crisp, much more clear.

044043.162 -- 044045.271
And so you can even read the

044045.271 -- 044047.397
driving plate in the in the second

044047.397 -- 044048.882
example in the bottom wall.

044050.322 -- 044052.132
Why you can't for the for the top one.

044052.852 -- 044055.172
And and as I mentioned, one important element

044055.172 -- 044057.502
for driving is signs traffic control

044057.882 -- 044100.201
signs, traffic lights, and you can see how

044100.362 -- 044102.462
here we render correctly the

044102.602 -- 044104.862
arrows for going straight, turning left,

044105.072 -- 044107.132
is they are quite small. These are

044107.132 -- 044109.372
basically, screenshots of a small

044109.372 -- 044111.072
part of the generated video.

044114.422 -- 044116.582
And now let's talk a little bit

044116.582 -- 044118.852
about as Gianluca started to mention

044118.852 -- 044121.061
about the applications and how we use

044121.462 -- 044123.002
GAIA three for evaluation and validation.

044123.662 -- 044125.821
We have all sort of applications. Let

044125.822 -- 044128.012
me start with the the

044128.227 -- 044130.382
capability to reuse the data

044130.382 -- 044132.432
that we collected on a specific

044132.432 -- 044134.512
embodiment, on a specific vehicles and

044134.512 -- 044136.571
recreate scene from another sensor

044136.572 -- 044138.992
configuration. This is extremely useful

044139.042 -- 044141.242
because you may change the vehicle

044141.242 -- 044143.562
type, and you want still want to reuse the same amount

044143.562 -- 044145.702
of data. So as you see on the left

044145.702 -- 044147.751
part of the of the video, you have the

044147.752 -- 044149.822
original embodiment with different

044150.382 -- 044152.542
camera field of view, with key different camera

044152.542 -- 044154.572
heights and positions. Then you want to

044154.572 -- 044157.031
recreate the scene from a different embodiment.

044157.512 -- 044159.572
So if you look at the again, on

044159.572 -- 044201.992
the front camera specifically, there is no,

044202.461 -- 044204.541
there is no car, for example, part of the

044204.542 -- 044206.641
car, there's no boner, there's no internal part.

044206.642 -- 044208.582
And this is due to the camera configuration.

044208.962 -- 044210.502
This is a scene in Japan.

044212.462 -- 044214.522
And let's take a look

044214.522 -- 044215.502
also at the foliage,

044216.672 -- 044218.602
quite high high frequency details.

044219.021 -- 044221.262
When we look at the bottom video, you even before

044221.262 -- 044223.352
I play, you can start already see that there is a

044223.511 -- 044225.831
a little part of the car as this scene is rendered

044225.831 -- 044228.262
from a different point of view. Similarly,

044228.482 -- 044230.912
all the other cameras are actually rendered

044231.072 -- 044233.252
from a different point of view. Let

044233.312 -- 044235.552
me play both of them another time.

044242.292 -- 044244.532
Another application is targeted

044244.532 -- 044246.862
testing. Want to be able

044246.862 -- 044248.921
to take a scene and perturb it

044248.922 -- 044250.972
in a controlled way, so that you can

044250.972 -- 044253.282
test the driving model into the scene.

044253.762 -- 044255.942
So in these examples, we will see how

044256.162 -- 044257.981
we want to generate a scene

044258.701 -- 044300.941
and and see how driving model respond to

044300.942 -- 044302.221
this per turbine.

044303.242 -- 044305.462
Particular, the green trajectory is the

044305.942 -- 044308.022
conditioning we give to Gaia, our

044308.022 -- 044310.122
model, to generate a new scene.

044310.612 -- 044312.772
And the ping trajectory is the

044312.772 -- 044314.392
response of the driving model

044315.221 -- 044317.381
to this perturb scene make the the model drive

044317.382 -- 044319.531
on. So we see, for example, on the on

044319.531 -- 044321.581
the left one, will generate a scene where

044321.582 -- 044323.982
we try to drift left and see the response

044323.982 -- 044325.272
of the driving model with it.

044327.592 -- 044330.092
So here, the scene now starts to diverge from the original

044330.401 -- 044332.562
as we are drifting left, and we see that the driving

044332.562 -- 044334.102
model correctly predict

044334.891 -- 044337.131
to push on the on the right side to come back

044337.132 -- 044338.332
to the original lane.

044339.212 -- 044341.302
And if you look playing it again on

044341.302 -- 044343.701
the right example, is the opposite happening.

044343.702 -- 044345.781
We are ask we're generating a

044345.782 -- 044347.922
imperturbacine drifting right,

044347.922 -- 044350.232
and we see the driving models trying to push.

044350.562 -- 044352.251
To come back to the original lane.

044354.732 -- 044356.752
In this case, we're actually going off

044356.752 -- 044359.022
road in the generated scene.

044359.902 -- 044402.222
And now, I mean, these are just two examples. Right We can

044402.222 -- 044404.412
start to generate, an entire suite

044404.412 -- 044406.272
with many, many more examples.

044406.632 -- 044409.032
Going too fast, going too slow, and many other variation

044409.032 -- 044410.092
in different scenarios.

044419.902 -- 044422.012
Now a third application now

044422.012 -- 044424.031
linked to the previous one is robustness

044424.091 -- 044426.102
testing. May generate scenes

044426.102 -- 044428.182
where you're drifting left and drifting right, but you

044428.182 -- 044430.112
never know whether the driving model will be

044430.512 -- 044432.652
robust to variations of the scene. We

044432.732 -- 044434.942
it be robust to lightning Change

044434.942 -- 044437.101
of lighting, change of semantics, or change of,

044437.102 -- 044439.502
for example, time of the day, or weather

044439.502 -- 044441.661
as in this case. So here, we're taking the

044441.661 -- 044443.802
same scene make sure

044444.021 -- 044446.042
the route configuration in the agent's

044446.102 -- 044448.182
behavior the same way, but we are changing in

044448.182 -- 044450.392
this case time of the day, and weather.

044454.772 -- 044456.932
So, again, here, we can compare model

044456.932 -- 044459.042
performances on different

044459.042 -- 044501.352
conditions, see whether the the performance

044501.411 -- 044503.531
is robust. To these variations.

044514.982 -- 044517.271
So then finally, one of the most interesting application

044517.271 -- 044519.592
in my opinion is safety critical testing.

044519.912 -- 044522.122
So, how do you test on rare

044522.122 -- 044524.522
events that are unpredictable and also quite dangerous

044524.522 -- 044526.862
It's like near misses or collisions.

044527.322 -- 044528.222
It's obviously

044530.052 -- 044531.881
in the real world. So

044532.202 -- 044534.462
one approach one common approach of the industry

044534.762 -- 044536.852
is to use, test tracks, for

044539.252 -- 044541.471
for example, One example

044541.471 -- 044543.892
of them are end cap scenarios where the specific

044543.952 -- 044546.192
target seems are reconstructing

044546.252 -- 044547.462
and regenerating in the

044548.582 -- 044550.301
to test the the driving model.

044550.662 -- 044552.802
Now so we start with that. This

044552.802 -- 044554.812
is a scene generated that

044555.447 -- 044557.517
reproduce the end cap scenarios

044557.517 -- 044559.602
of you know, moving into the oncoming

044559.602 -- 044601.392
vehicle and crashing on it.

044605.572 -- 044607.691
But the beauty of the the fact that

044607.692 -- 044609.862
this is a simulation is generated by a world model

044609.862 -- 044611.902
is not real, is that we can

044611.902 -- 044614.171
actually create way

044614.172 -- 044616.362
more realistic, scenes that are actually

044616.362 -- 044618.522
more representative of the complexity of the

044618.522 -- 044620.681
real world. So here, you'll see the same

044620.682 -- 044623.002
exact scene in terms of, behavior

044623.221 -- 044625.432
of agents, but in the

044625.432 -- 044627.452
real world. And not in

044627.452 -- 044629.612
a test track, which is something obviously very hard

044629.612 -- 044631.732
to do in real. Instead of in

044631.732 -- 044633.812
a simulation. And I I think

044633.812 -- 044635.892
one element to highlight here is that the other

044635.892 -- 044637.781
agents is not reacting to it. This is

044638.182 -- 044639.882
World Rail, which is a specific

044640.642 -- 044642.242
element. You want to test for.

044643.961 -- 044645.341
And and again, we can

044646.052 -- 044648.092
recreate differences,

044648.522 -- 044650.612
different variations of it, I found

044650.612 -- 044652.772
this to be particularly interested because we have a

044652.772 -- 044654.812
construction track in front

044654.812 -- 044656.972
of us another construction truck

044657.132 -- 044659.292
on the other on the other lane, and we can

044659.292 -- 044701.232
still recreate the same type of scenarios.

044704.992 -- 044706.852
So here, we are colliding with

044707.091 -- 044709.121
the the incoming the incoming truck.

044709.911 -- 044711.991
This gave gave you a little bit of overview of the type

044711.992 -- 044714.432
of applications we are we are

044714.672 -- 044716.892
are dealing with. When using IAA three.

044717.422 -- 044719.202
And, I want to conclude

044719.682 -- 044721.972
with a video of the of a coastal

044721.972 -- 044724.051
path that could look like, probably similar

044724.052 -- 044726.121
to the type of coastline

044726.122 -- 044728.011
we are in here in San Diego.

044729.052 -- 044731.212
We released a blog post just a few days

044731.212 -- 044733.441
ago, the link is below. So

044733.442 -- 044734.581
thank you for your attention.

044759.062 -- 044801.221
Thank you Luca and Lorenzo for

044801.222 -- 044801.882
their representation.

044804.202 -- 044806.322
So now we are now proceed to our

044806.322 -- 044808.492
next keynote session, and our next speaker

044808.492 -- 044810.622
is John Longfer. He's

044810.622 -- 044812.332
a director of

044812.972 -- 044815.092
learning and Microsoft research and

044815.332 -- 044817.572
he leads cutting edge work in machine

044817.572 -- 044819.752
learning and large scale predictive learning.

044820.862 -- 044823.312
And he's talk today,

044823.372 -- 044825.602
it's titled Next

044825.602 -- 044827.462
Latent Prediction Transformers

044828.162 -- 044830.512
Learn compact world models. So let's

044830.512 -- 044831.412
welcome John.

044931.052 -- 044932.322
Alright. Very good.

044936.482 -- 044938.722
Okay. So this is, something we've been working

044938.722 -- 044939.972
on over the summer.

044940.982 -- 044943.052
And it's related to

044943.212 -- 044945.191
several of the talks that have been going on

044947.752 -- 044949.911
One of the things that's been bothering me for a long

044949.911 -- 044951.931
time is in in reinforcement learning,

044952.622 -- 044955.112
this notion of harshly observed market decision

044955.332 -- 044957.582
processes And then, of course, you have a market

044957.582 -- 044959.281
decision process. Where

045000.322 -- 045002.441
decision process, because it's easy.

045004.031 -- 045006.181
But the real world

045006.502 -- 045008.902
is always a partially observed Markov decision

045008.902 -- 045010.982
process where you don't have all the

045010.982 -- 045012.591
information you need right up front.

045013.791 -- 045015.952
And it turns out that

045015.952 -- 045018.111
this is a trick which allows you

045018.112 -- 045020.342
to convert from a partially

045020.342 -- 045022.422
micro observed market position

045022.522 -- 045024.727
process to marker position

045024.727 -- 045026.097
process. In practice.

045026.882 -- 045028.952
This is kind of a key thing related very

045028.952 -- 045030.972
much to compact world models.

045032.191 -- 045034.272
That's what this is really about in in assessments.

045034.272 -- 045036.351
But I wanna kinda start with, what

045036.352 -- 045038.561
is a world model. Because I think there's

045038.561 -- 045040.311
there's two definitions running around.

045041.641 -- 045042.141
And,

045043.712 -- 045045.042
and and they're very different.

045046.142 -- 045048.462
And they both have, points, and they kinda connect

045048.462 -- 045049.971
together. In import. Ways.

045050.612 -- 045052.822
So for many of the

045052.822 -- 045055.202
talks that we've been looking at today,

045055.842 -- 045057.921
the notion of what is the world model is,

045059.492 -- 045101.602
simulation of video.

045102.561 -- 045103.061
Essentially.

045105.212 -- 045107.302
So for for I just

045107.462 -- 045109.622
I'm just kinda curious what where people are on this.

045109.622 -- 045111.342
How do people think of world models as

045111.742 -- 045114.102
ability to simulate video controlled simulation

045114.561 -- 045116.312
of videos Yeah.

045117.602 -- 045119.802
Okay. There's

045120.022 -- 045121.611
another notion of world model.

045122.332 -- 045124.622
Which is which which I'll get to. But if

045125.082 -- 045127.242
you think about this let me give

045127.242 -- 045129.672
let me give you a little bit of pause. Which

045129.732 -- 045129.811
is,

045132.732 -- 045134.811
yeah. So realistic video generation, which

045134.811 -- 045137.002
is if somebody's blind all their life, there's

045137.002 -- 045138.861
no way they're actually simulating a video.

045139.502 -- 045140.801
But it seems a little

045141.642 -- 045143.311
strange to say that they don't have

045143.712 -- 045146.061
some sort of world model internally

045146.722 -- 045148.802
So there's there's another definition of world model which

045148.802 -- 045150.701
is very natural. Which is something about

045151.021 -- 045152.561
kind of, having a simulation

045153.262 -- 045155.342
an internal simulation of what's going on in the

045155.342 -- 045157.252
world. Right

045157.882 -- 045158.762
And so that's,

045201.242 -- 045203.482
this is not like a it's not a gotcha in the sense

045203.482 -- 045205.531
of, oh, you discriminated against a blind person. This

045205.531 -- 045207.771
is a we should really think about what the definition means

045207.772 -- 045209.832
here. And I think that this

045209.832 -- 045211.852
definition actually gets at a really key

045211.852 -- 045213.712
element of of the limitations

045215.311 -- 045217.572
of current, like, large language

045217.791 -- 045219.821
models. Current large language models

045221.872 -- 045223.702
are, you know, wonderful in many ways, but

045223.941 -- 045225.942
they learn very slowly. They require

045225.942 -- 045228.062
a lot of data. A lot more than a human.

045228.862 -- 045231.051
And and they're also kind of brittle in sort

045231.051 -- 045233.262
of funny ways that are little bit hard to

045233.262 -- 045233.602
codify.

045237.792 -- 045239.872
Thinking about what is this, a different kind

045239.872 -- 045242.147
of world model. That blind people

045242.572 -- 045244.272
can have. Think, helps address this.

045245.792 -- 045247.722
Okay. So let's go back to why

045248.122 -- 045250.282
you want a world model. And

045250.282 -- 045252.512
I think there's there's

045253.022 -- 045254.342
several possibilities people think about.

045256.242 -- 045258.022
One of them is you wanna do better generalization.

045258.931 -- 045300.982
And better generalization is a very generic thing to

045300.982 -- 045303.282
say like, oh, I wanna generalize better. That's great.

045304.242 -- 045306.642
I think we there's a there's a very specific kind

045306.642 -- 045308.822
of better generalization, which is possible if you have a good

045308.822 -- 045311.172
world model. Which is you can extrapolate better.

045311.572 -- 045313.782
Right So you can you can,

045314.012 -- 045316.032
drop a rock on

045316.032 -- 045318.122
the earth, and you can extrapolate to

045318.122 -- 045320.122
estimate what would happen if you dropped a rock on the

045320.441 -- 045322.601
moon. That that that that that that's an extrapolation which

045322.602 -- 045324.992
is possible because we have some kind of world model

045325.392 -- 045327.542
that we built as

045327.542 -- 045329.552
a civilization. So the

045329.552 -- 045331.632
the the extrapolation ability is kind

045331.632 -- 045333.902
of a key element of what it means to have

045333.902 -- 045335.872
a a good internal world model.

045336.992 -- 045339.072
There's another, which is used by several

045339.072 -- 045341.341
of the posters over here, which is essentially

045341.342 -- 045343.352
model predictive control. If you

045343.352 -- 045345.042
have a good moral world model,

045345.682 -- 045347.462
then you can roll out the future

045347.802 -- 045350.062
You can look at the consequences of actions

045350.062 -- 045352.192
that you take. And then you can you can

045352.192 -- 045354.352
try that several different ways, and you can choose to take

045354.352 -- 045356.452
one action or or another based upon the

045356.452 -- 045358.222
consequences that you compute.

045400.061 -- 045402.172
Okay. So this is kind of a distinct one.

045402.652 -- 045404.081
And then there's a third

045405.322 -- 045407.742
notion of why you want a world model. Which,

045408.762 -- 045410.771
is kind of informed exploration. And

045410.772 -- 045412.852
I think this comes up in in two sort of

045412.852 -- 045413.512
distinct approaches.

045415.142 -- 045416.021
One of them is,

045418.302 -- 045420.411
if you are moving

045420.412 -- 045422.481
around in a world it's good to

045422.482 -- 045424.532
know when you came back to where you were. This could

045424.532 -- 045426.852
be in simulation. It could be in in in in the actual

045426.852 -- 045428.991
moving in in in in life. Because when you

045428.992 -- 045431.272
come back to where you were, often things are actually

045431.272 -- 045433.332
a little bit different. The signs have changed. The the the

045433.332 -- 045434.972
the trees blown over, whatever.

045435.642 -- 045437.722
Things are different, and yet still it's the same you you

045437.722 -- 045439.861
are where you were before. And so that ability to

045439.862 -- 045441.902
kinda test data quality is

045441.902 -- 045443.982
kind of a critical aspect of having a

045443.982 -- 045446.121
good world model. If you don't have

045446.122 -- 045448.622
a good world model, it's it's it would be very hard for

045448.762 -- 045450.142
you to to realize that, oh,

045451.762 -- 045453.841
this place that I see right now is the same as the

045453.842 -- 045455.892
place that I see at night, unless you

045456.132 -- 045458.202
specifically train for that kind of thing or something like that.

045459.801 -- 045502.172
And the other part is if you're if

045502.197 -- 045504.431
you if you actually just don't know things

045504.431 -- 045505.652
and you wanna gather information.

045506.492 -- 045508.732
Then having a good world model is very helpful

045508.732 -- 045510.792
here. So, like, you walk into a house, you've never been

045510.792 -- 045512.952
in the house before, You want

045513.171 -- 045515.332
to, know where the bathroom is so that

045515.332 -- 045517.342
later you can go to the bathroom. Or maybe you can go to the

045517.342 -- 045519.551
bathroom now, can

045519.551 -- 045521.791
systematically explore the house because you have

045521.791 -- 045523.772
a world model based upon what you've seen so far.

045524.972 -- 045527.042
So these kind of these these are

045527.842 -- 045529.922
kind of three different categories. And I guess I'd be

045529.922 -- 045532.422
very curious Hands up, anyone.

045532.952 -- 045534.872
Are there other reasons to have a world model

045538.262 -- 045540.051
There other things that people want from world model

045541.162 -- 045543.181
These are the ones that I, see

045544.562 -- 045546.762
fairly often. Nobody

045547.612 -- 045549.281
If you come up with one, I'm very interested.

045550.772 -- 045552.531
A hold of me. Oh. Have one.

045554.252 -- 045555.312
Causal manipulation.

045556.262 -- 045558.342
I think that the causal manipulation has a lot to do

045558.342 -- 045559.552
with a model predictive control.

045600.352 -- 045602.371
You are, you you you you

045602.932 -- 045605.352
checking to see what happens if you do something differently.

045607.062 -- 045609.072
And observing

045609.072 -- 045611.482
the consequences of that. Yeah

045616.952 -- 045619.112
Yeah Active

045619.112 -- 045621.431
learning. Active learning has a lot to do with that informed

045621.432 -- 045623.682
exploration. The second version of

045623.682 -- 045625.782
it. You have you understand some part of the

045625.782 -- 045627.912
world and you want to,

045628.322 -- 045630.402
come to understand another part

045631.092 -- 045633.462
of the world where you you don't have a full

045633.462 -- 045635.352
understanding yet. Yes.

045639.422 -- 045640.112
Say again

045641.772 -- 045642.272
Evaluation.

045644.942 -- 045647.052
Interesting. I don't

045647.052 -- 045649.312
quite understand that one. Let's

045653.002 -- 045655.021
Using role models to evaluate a policy.

045656.782 -- 045658.862
Yeah. So I guess I would think of this as as

045658.862 -- 045701.172
kind of the model predictive control.

045701.172 -- 045703.572
Type angle. Mostly Because

045703.712 -- 045706.052
you're you're using you're you're rolling

045706.052 -- 045708.151
things out in your synthetic world

045708.792 -- 045710.913
to try to understand what

045710.914 -- 045712.132
your policy is going to do.

045714.932 -- 045715.992
Another. Yes.

045717.692 -- 045720.092
Say again Surprise. Surprise

045725.602 -- 045727.302
So being able to measure surprise.

045731.392 -- 045733.572
Yeah, that seems interesting. That

045733.572 -- 045734.382
might be

045735.822 -- 045737.902
new in some sense. It it it

045738.462 -- 045740.541
it's related to the informed exploration,

045741.072 -- 045743.031
it it's it's it's a little bit different because

045743.832 -- 045745.862
it's at least a new line under the

045745.862 -- 045748.202
informed exploration, I think. Because what you're saying

045748.342 -- 045750.542
is you could be going through the world, and you could just

045750.542 -- 045752.702
you could be surprised, and and you could have an informed

045752.702 -- 045754.742
notion of surprise. And that

045754.742 -- 045755.842
could, you know,

045756.962 -- 045759.142
trigger something about your data gathering policy,

045759.522 -- 045801.702
for example. That you could then use,

045802.912 -- 045803.652
later on.

045805.332 -- 045807.652
Interesting. Good. Yeah.

045808.192 -- 045808.852
What about

045813.682 -- 045815.462
Understanding concepts like gravity.

045816.102 -- 045817.782
I guess you would hope that

045818.902 -- 045821.242
the better generalization, the ability to extrapolate

045821.781 -- 045824.252
is, one of the key aspects

045824.347 -- 045825.942
of like,

045827.542 -- 045829.752
you if you have a good world model, then maybe

045829.752 -- 045831.811
you understand gravity. And because

045831.812 -- 045833.832
you understand gravity, you can even

045834.072 -- 045836.122
about what happens if you have less gravity or more gravity.

045836.522 -- 045837.602
On other planets.

045839.041 -- 045841.440
So the the the gravity is is kind of

045841.441 -- 045843.042
a key element of extrapolation.

045843.492 -- 045845.422
I think, as far as

045845.582 -- 045846.972
way you would use it in a world model.

045851.662 -- 045852.902
Alright. This is very good.

045854.342 -- 045856.842
Okay. So we would like this inner world model.

045856.842 -- 045858.882
Let's think about just at

045858.882 -- 045901.152
a semantic level, what we'd like from a world model.

045904.102 -- 045906.202
So what you would like to have is better extrapolation.

045907.072 -- 045909.222
And this is this is an example of

045909.222 -- 045911.373
of a data set where you can actually measure

045911.373 -- 045912.572
extrapolation relatively easily.

045913.691 -- 045915.852
This is a simple data set where you

045915.852 -- 045917.462
have taxi rides in Manhattan.

045918.102 -- 045920.132
It's kind of here's a start, here's

045920.132 -- 045921.382
a goal, here's a path.

045923.021 -- 045924.882
And now you want to learn to predict

045925.302 -- 045926.771
a good pass given the start and the goal.

045927.572 -- 045929.872
Right And if you use a

045929.872 -- 045930.932
next token transformer,

045932.592 -- 045934.732
then, everything in red a

045934.732 -- 045935.132
hallucination.

045937.262 -- 045939.442
We're gonna do something slightly off distribution,

045939.502 -- 045941.752
just to be clear. So if

045941.752 -- 045943.832
you if you measure on the distribution that you trained with, you

045943.832 -- 045946.242
you have no error. But we're gonna be slightly

045946.392 -- 045948.652
off distribution. We're just gonna take random start

045948.652 -- 045950.661
goal pairs. And ask for a good path

045950.661 -- 045951.652
between the start and the goal.

045953.592 -- 045955.622
And then, all

045955.622 -- 045957.862
the red is solutionations. It's like, oh, you

045957.862 -- 050000.061
go from this intersection to that intersection, and there's

050000.062 -- 050001.522
no real road there. Doesn't work.

050002.402 -- 050004.531
Right And so you can see there's a

050004.532 -- 050006.612
a lot of errors. So it's not in

050006.612 -- 050008.772
a huge amount, but you there's still a substantial amount of

050008.772 -- 050011.132
errors where it just kind of of teleports

050011.191 -- 050013.222
from one intersection to another. Doesn't make any

050013.222 -- 050015.371
sense. And yet there's

050015.372 -- 050017.702
a way to train slightly

050017.842 -- 050019.472
differently which has

050020.382 -- 050022.161
about one third as many errors.

050024.902 -- 050026.482
And they tend to be more local.

050028.561 -- 050030.721
K. So this is a small change to the way

050030.722 -- 050032.842
that we train. Which

050033.142 -- 050035.152
causes underlying transformer

050035.792 -- 050037.842
to develop a better world

050037.842 -- 050040.012
model. Which is what we're talking

050040.012 -- 050040.522
about here.

050046.772 -- 050047.572
Alright. So,

050048.842 -- 050050.702
what it was a world model semantically

050051.042 -- 050052.962
What does it really mean to be a world model

050053.522 -- 050055.172
for a blind person Right

050055.812 -- 050057.852
And here's what I think it means. I

050057.852 -- 050100.001
think it means that it's you you you're

050100.002 -- 050102.122
developing a belief state, developing

050102.342 -- 050104.532
a transition model. Alright Those are

050104.532 -- 050106.611
those are the key fundamental things. And so what what

050106.612 -- 050108.661
I mean by belief state, what I mean by belief

050108.661 -- 050110.722
state is you have all the

050110.722 -- 050112.782
information from the past, that,

050113.022 -- 050115.151
is relevant for the future.

050115.712 -- 050117.912
Alright. So you can try to predict actions in the future

050117.912 -- 050119.091
and you would like to

050120.752 -- 050122.982
you know, like to be able to do that effectively. You

050122.982 -- 050125.032
need all the information from the past that's gonna help you predict things in

050125.032 -- 050126.352
the future. Future.

050128.612 -- 050130.772
And then if you're going to think about a transition

050130.772 -- 050132.921
model, well a transition model is in which takes a belief state,

050133.722 -- 050135.702
and an action, and it gives you the next belief state.

050138.102 -- 050140.322
State. K. So I I'm building into

050140.322 -- 050142.771
the definition here since you're converting

050142.772 -- 050144.992
my POMDP into an

050144.992 -- 050147.001
MDP. And

050147.002 -- 050149.142
then the assertion is going to be that we can do this

050149.622 -- 050150.122
systematically.

050153.552 -- 050154.372
Alright. So

050155.822 -- 050158.122
to learn a world model, you

050158.122 -- 050200.202
need to learn a belief state, and a transition

050200.202 -- 050201.892
model. And these need to be compatible with each

050203.252 -- 050203.752
other.

050212.392 -- 050213.192
Okay. So,

050214.522 -- 050216.772
now now a question. Is a transformer a world

050216.772 -- 050218.941
model Right So just, think

050218.942 -- 050221.022
of a next token transformer is

050221.022 -- 050222.042
is this a world model

050223.322 -- 050225.722
And I think a lot of people here are gonna say, no.

050225.896 -- 050227.851
Because they wanna do something new.

050228.156 -- 050230.221
Right Who who says

050230.222 -- 050230.622
no

050233.022 -- 050234.561
Okay. Who says yes

050235.662 -- 050237.802
Yeah. So I'm

050237.802 -- 050239.112
gonna go with yes here.

050241.422 -- 050243.922
Because you they they are, in some sense,

050244.702 -- 050246.882
a world model. You have

050246.882 -- 050248.892
all the keys and values. Say that

050248.892 -- 050250.931
it's your belief state. And you

050250.931 -- 050252.962
have your current observation. It

050252.962 -- 050255.017
gives you all the information from the past that you

050255.017 -- 050257.012
need as

050257.152 -- 050259.172
long as you optimize well. And

050259.172 -- 050301.092
in the in the action is your next token.

050302.602 -- 050304.682
And and you can even do this in a pixel centric

050304.682 -- 050306.852
way. There's this world in human

050307.072 -- 050309.222
action model stuff that some folks in,

050309.382 -- 050311.052
Microsoft Cambridge published in Nature.

050314.152 -- 050316.652
But, you know, I think there's a

050316.652 -- 050318.992
a is is a but.

050319.212 -- 050319.712
Right

050321.332 -- 050323.502
And the but is that if we

050323.642 -- 050325.342
go back to earlier, we see that

050325.742 -- 050327.281
just a simple transformer

050328.342 -- 050330.392
tends to produce a lot

050330.392 -- 050332.472
of these hallucinations that it's sort of

050332.472 -- 050334.672
incoherent. In terms of what his world

050334.672 -- 050336.752
model is internally. So

050336.752 -- 050338.822
for the definition that work a world

050338.822 -- 050340.492
model that works for a blind person,

050342.012 -- 050343.242
it's it's a it's a little iffy.

050345.792 -- 050347.892
Somehow, doesn't

050347.892 -- 050350.051
extrapolate very well. It doesn't generalize as well as you

050350.051 -- 050350.602
would hope to.

050353.032 -- 050355.172
So so what do we wanna do to change things

050355.502 -- 050355.892
Well,

050358.052 -- 050400.132
what goes wrong with a transformer as a

050400.132 -- 050402.212
world model And what I would

050402.212 -- 050404.171
say goes wrong with transformer as a world model

050404.652 -- 050406.432
is that it's extremely loose.

050406.752 -- 050409.122
You have all these keys and values,

050409.922 -- 050411.981
and they're and every time you get

050411.982 -- 050414.222
a new token, you get a new set of keys and

050414.222 -- 050414.722
values.

050416.892 -- 050418.972
So that it's the the the notion

050418.972 -- 050420.991
of what is the model

050420.992 -- 050423.132
of the world is is extremely

050424.092 -- 050426.382
expansive. There's a lot of extra capacity running around.

050428.302 -- 050430.362
And that's kind of

050430.602 -- 050432.781
that's that's the issue. What you would really

050432.781 -- 050434.672
like is a compact world model.

050436.912 -- 050439.152
And so, what what is a compact

050439.152 -- 050441.352
world model A compact world

050441.352 -- 050443.672
model is a world model with a minimal sized

050443.672 -- 050445.802
belief state. So we'd like to have

050445.802 -- 050447.381
a belief state which is not too large.

050448.072 -- 050450.132
And the reason why we want a belief state

050450.132 -- 050452.452
which is not too large is because

050453.112 -- 050455.122
you'll end up helping

050455.122 -- 050457.161
us extrapolation. And

050457.161 -- 050459.491
of course, it's easier to compute and easier to work with

050459.652 -- 050500.552
and and all of these things.

050503.842 -- 050506.222
Alright. So then, minimal can a belief

050506.372 -- 050508.737
state be Well, there's something which is essential

050508.737 -- 050510.757
that information that must be in a belief

050510.757 -- 050513.242
state. Right So the information

050513.301 -- 050515.402
that must be in a belief state is

050515.402 -- 050517.742
all the information that you're

050518.282 -- 050520.292
going to use to take actions in

050520.292 -- 050522.332
the future. So

050522.332 -- 050524.412
whatever whatever basis upon

050524.412 -- 050526.562
which a policy is going to make to take action in the future,

050526.802 -- 050528.781
you need to have that information in your belief state.

050531.022 -- 050533.072
State. And syntactically, you know, may

050533.232 -- 050535.332
maybe instead of having all the key values,

050536.272 -- 050538.282
and and and the current observation, you could

050538.282 -- 050540.372
have a few of your latent

050540.372 -- 050542.492
activations. Or one of your latent

050542.492 -- 050543.882
activations be your belief state.

050545.162 -- 050547.322
I I think it's really the the semantics is really the

050547.322 -- 050548.671
incorrect important thing. We would

050549.402 -- 050551.722
we have to have all the information necessary to take action

050551.722 -- 050553.992
in the future. Now,

050554.612 -- 050557.102
it was kind of interesting watching the the wave folks,

050557.742 -- 050600.062
because there there there are stimulating

050600.062 -- 050601.252
things at the pixel level.

050602.292 -- 050604.542
And that's, it's very high precision.

050604.542 -- 050606.342
There there's is a is a

050607.912 -- 050610.012
definition of progress there, which is you get higher and

050610.232 -- 050612.272
higher fidelity, and eventually you have to kind of cover

050612.272 -- 050614.441
everything. But

050614.772 -- 050616.651
you may not actually need to know

050617.212 -- 050619.611
exactly what the signage is on various

050619.612 -- 050621.722
billboards. Because it doesn't actually

050621.722 -- 050624.172
affect what a safe driving policy

050624.172 -- 050624.622
is.

050626.622 -- 050628.712
And so there's this differences

050628.712 -- 050630.742
in how much information you need to simulate

050631.357 -- 050633.591
for the policies that you

050633.592 -- 050634.991
care about. Versus

050636.442 -- 050638.692
versus the the general program of just simulating everything

050638.692 -- 050639.172
perfect.

050643.802 -- 050645.932
Okay. So we're gonna

050645.932 -- 050648.432
make a small change to the way you train a transformer.

050649.211 -- 050650.912
And I'm gonna claim that this

050651.232 -- 050653.242
creates learn to believe states.

050653.882 -- 050655.241
And transition models.

050656.602 -- 050657.902
K So the the changes

050659.212 -- 050701.452
so you have a transformer. It has multiple layers.

050701.452 -- 050703.402
Top layer you have

050703.462 -- 050705.541
a a head which decodes, a token.

050705.862 -- 050707.222
It's gonna predict the next token.

050708.342 -- 050710.641
We're also gonna have another head which

050710.642 -- 050711.571
predicts the next late.

050713.892 -- 050715.972
Okay. And then implicitly, the

050715.972 -- 050717.882
next latent is gonna predict the

050718.132 -- 050720.282
next next token. And the next next

050720.282 -- 050722.362
latent and and so forth.

050723.241 -- 050725.401
And then there's only one little trick here which is a

050725.402 -- 050727.852
little important to understand, which is we're going

050728.152 -- 050730.162
to predict

050730.162 -- 050732.302
the next latent using the

050732.552 -- 050734.601
true decoded token. So

050734.602 -- 050735.661
this is a teacher forced

050738.242 -- 050739.512
a prediction of the next late

050740.711 -- 050742.572
Okay. So why do we do that Well,

050743.132 -- 050743.832
turns out that

050745.802 -- 050747.892
well, turns out that h three ends

050747.892 -- 050750.052
up being a a belief state, first of all. And second

050750.052 -- 050752.291
of all, turns out that the head which

050752.292 -- 050753.591
predicts the next latent

050754.552 -- 050755.712
is a transition mode.

050756.832 -- 050758.941
Right Because it takes a belief state as input,

050759.422 -- 050801.622
and an action which is the teacher

050801.622 -- 050803.422
force token, and it predicts the next belief

050805.662 -- 050806.162
state.

050811.941 -- 050814.082
Alright. So, we can prove a theorem.

050814.642 -- 050816.362
This may be the only theorem in this workshop.

050817.162 -- 050819.182
If you do next latent, and you kind

050819.182 -- 050821.471
of assume infinite

050821.471 -- 050822.992
data and compute as necessary,

050823.472 -- 050825.332
then you're going to learn a compact world model.

050825.731 -- 050828.072
Model. The proof is by induction.

050828.972 -- 050831.192
Our base case is gonna we're gonna be at the end of a sequence.

050831.432 -- 050832.812
And the next latent is trivial.

050836.092 -- 050838.392
And so inductively, we're gonna say,

050839.442 -- 050839.632
oh,

050841.971 -- 050843.991
I'm going to predict the next token

050846.002 -- 050846.502
So,

050848.852 -- 050849.592
my latent

050851.012 -- 050853.422
must be a belief state for the next

050853.422 -- 050855.161
action. T plus one action,

050856.602 -- 050858.592
And I'm also gonna predict the next latent

050900.352 -- 050901.892
which is a belief state for

050903.192 -- 050905.052
the following actions. So this is the inductive

050905.842 -- 050906.622
hypothesis here.

050908.622 -- 050910.682
And so I might getting a belief

050910.682 -- 050912.442
state for actions t plus two,

050913.072 -- 050915.201
two plus three, and so forth. From

050915.201 -- 050916.732
predicting the next latent.

050917.452 -- 050919.122
And I'm getting a a

050919.501 -- 050921.442
belief state for predicting the next action,

050921.691 -- 050922.971
off the next token prediction.

050924.412 -- 050926.572
And those are the that that prediction is coming

050926.572 -- 050927.912
from the same latent and so

050929.202 -- 050931.211
the latent is now a belief state for

050931.772 -- 050933.032
all the actions going forward.

050935.352 -- 050936.572
Simple claim.

050937.402 -- 050939.562
As long as you have support over this and

050939.562 -- 050941.622
you have a a good

050941.622 -- 050943.782
ability to approximate, which transformers are wonderful

050943.782 -- 050945.802
at, you can converge

050945.862 -- 050946.922
to having a belief state.

050949.962 -- 050951.502
Okay. So that's that's kinda cool.

050952.542 -- 050954.622
Transformers traditionally have had a bit of an

050954.622 -- 050956.532
issue compared to RNNs

050956.912 -- 050958.981
because RNNs

050958.981 -- 051001.141
kind of naturally have a belief state because it's it's whatever

051001.142 -- 051003.251
bottleneck you have in in a recursive

051003.252 -- 051005.432
neural network. Transformers don't have that.

051005.432 -- 051007.341
This is a way to make transformers do that.

051007.582 -- 051009.482
So we can make transformers dance in a new way.

051012.532 -- 051014.852
Okay. So now what what goes wrong with this Let's

051014.852 -- 051017.021
start with, the downside. Downside is you're gonna

051017.022 -- 051018.132
do a little bit more compute.

051020.112 -- 051021.501
So I'm going to compare

051022.262 -- 051024.411
several different approaches. For

051024.411 -- 051024.911
computing

051027.282 -- 051029.542
the next token belief states

051029.842 -- 051031.511
and belief state like things.

051032.312 -- 051034.491
So GPT is standard next token

051034.552 -- 051036.622
prediction. We're gonna be able to train at,

051036.941 -- 051039.092
3.72 iterations per second.

051039.092 -- 051041.232
This is on, some little data set.

051041.632 -- 051042.681
That's tiny stores.

051044.862 -- 051047.202
And then, if you do next latent,

051047.731 -- 051049.952
you just go one ahead, which is

051049.952 -- 051052.102
sufficient you

051052.102 -- 051053.582
get 3.26.

051054.702 -- 051056.802
So this is about a 12% slowdown.

051057.132 -- 051059.262
And you're training I expect that as

051059.262 -- 051101.522
you scale up, this becomes more and more negligible

051101.662 -- 051103.962
because the head complexity becomes

051103.962 -- 051105.532
a smaller part of the overall compute.

051106.092 -- 051107.552
As you, scale up a transformer.

051109.892 -- 051111.972
There's several other possibilities here. One of them is this

051111.972 -- 051114.052
belief state transformer. This is our first attempt

051114.052 -- 051116.082
to create belief state with transformers.

051116.562 -- 051118.642
This is a more complex approach. I don't wanna

051118.642 -- 051120.582
get into the details of it, but, oops.

051122.661 -- 051124.602
It it's it's a lot slower. To compute.

051126.431 -- 051128.702
It involves quite a bit more computation.

051128.702 -- 051130.882
It works, but it's heavy.

051131.922 -- 051133.982
This is multi token prediction. This is,

051134.382 -- 051136.421
the Facebook style multi token prediction where

051136.422 -- 051138.622
you put you have you have to kind of predict each

051138.622 -- 051140.732
of the following tokens. Each each of

051140.732 -- 051143.152
the tokens. So you get the the marginals

051143.212 -- 051144.842
of the next tokens.

051146.322 -- 051148.132
And it's it's a bit

051148.772 -- 051150.232
heavier, compute wise.

051151.012 -- 051153.352
You also see later that, next latent

051153.422 -- 051154.782
with one

051156.182 -- 051158.282
ahead is about as good as multi

051158.282 -- 051200.132
token prediction eight ahead.

051200.612 -- 051202.812
In terms of how much information they can suck in

051203.247 -- 051203.682
effective.

051205.842 -- 051207.702
In token prediction is a is a variant,

051208.642 -- 051210.872
where instead of just predicting the

051211.432 -- 051213.772
marginals of the future tokens, you predict

051213.981 -- 051216.022
the joint of the future tokens using a

051216.022 -- 051217.772
teacher forced version of things.

051218.731 -- 051220.972
It's, also I

051220.972 -- 051223.251
mean, it's a little bit faster computationally, but

051223.312 -- 051225.441
you'll see that the next latent is

051226.482 -- 051228.792
indeed more effective.

051229.222 -- 051231.452
In practice. So we're

051231.452 -- 051233.532
paying 12% at most, probably less when

051233.532 -- 051235.522
we scale up. This is the downside.

051235.722 -- 051236.802
Of of doing. This.

051241.741 -- 051243.802
Alright. So now what goes right So we

051243.802 -- 051245.552
talked about extrapolation error reduction.

051246.592 -- 051249.062
Why do we get extrapolation

051249.062 -- 051250.891
error reduction if we do this

051253.132 -- 051255.292
I've kinda hinted at this. But

051255.292 -- 051256.512
I I think about this

051257.382 -- 051258.382
by an analogy.

051259.502 -- 051301.482
It's sort of miraculous. How how do you

051301.722 -- 051303.802
minimize errors on things that you don't

051303.802 -- 051305.852
even optimize for Right I

051305.852 -- 051307.101
mean, it's sort of weird.

051309.512 -- 051310.991
It used to be the case

051311.632 -- 051313.702
for many centuries over a

051313.702 -- 051315.902
millennium The people predict

051315.902 -- 051317.302
where the planets would be in the sky.

051318.181 -- 051319.942
And they had a system for doing this.

051320.182 -- 051322.391
Right That system involved this kind

051322.391 -- 051323.132
of epicycles,

051325.102 -- 051327.442
and it could predict where plants would be in the sky.

051327.582 -- 051329.832
That worked. So we had a way to explain

051329.832 -- 051331.132
our observations effectively.

051332.752 -- 051333.892
Later on,

051335.122 -- 051337.142
Copernicus came up with the Heliocentric

051337.502 -- 051339.502
theory. Right

051340.222 -- 051342.312
So earth was no longer the

051342.312 -- 051344.462
center of the universe, instead it was the sun

051344.702 -- 051346.812
and you had orbits, and you would calculate things

051346.812 -- 051347.882
based on the orbit.

051352.562 -- 051354.602
It turns out that Copernicus

051355.062 -- 051357.102
theory works on Mars.

051359.582 -- 051401.802
The epicycle system does not. Or

051401.802 -- 051403.882
you you can design a new version, but you have to,

051403.882 -- 051405.632
like, have all new constants. Things like that.

051407.882 -- 051409.961
We switched, of course, to the heli centric approach

051409.961 -- 051412.291
before we ever observed anything from Mars.

051412.772 -- 051414.402
And the reason why is because it's a simpler.

051416.162 -- 051417.852
So there's this this

051418.812 -- 051421.292
understanding in science that you want a simple correct

051421.292 -- 051423.511
theory. And I think that's what we're

051423.512 -- 051425.672
driving for here. We're trying to get a simple correct theory

051425.672 -- 051426.332
of observation.

051427.812 -- 051429.852
That's kind of the key driver of

051430.172 -- 051431.932
getting a good compact world model.

051435.312 -- 051437.491
Alright. So there are other things that go right as well.

051438.692 -- 051440.632
We discussed Monte Carlo

051441.592 -- 051443.657
search. We haven't actually done that. But we've done

051443.657 -- 051445.682
something close to that. Which is

051445.922 -- 051447.172
implicit look ahead planning.

051449.702 -- 051451.862
There's a there's a problem there's several problems which

051451.862 -- 051453.882
are kind of combinatorial problems that

051453.882 -- 051455.342
are known to be hard for

051456.092 -- 051457.282
next token prediction.

051458.161 -- 051500.221
This is the countdown problem. Kinda

051500.282 -- 051502.322
famous because I think on GPT four

051502.642 -- 051504.752
got 4%. It's not very good.

051505.712 -- 051507.791
But the idea is you have some numbers and you wanna try

051507.792 -- 051509.942
to come up with a a target number. 24 in this

051509.942 -- 051510.292
case.

051512.492 -- 051514.652
Okay. So you have some data set. It has

051514.652 -- 051516.722
about 500,000 elements in it.

051518.162 -- 051520.262
And if you train a next token predictor,

051520.362 -- 051521.532
you get

051522.742 -- 051524.802
about one third accuracy. On

051524.802 -- 051526.811
this dataset. Or on

051526.812 -- 051528.832
a on a held out version of the dataset.

051529.772 -- 051531.852
K. So we're not trying to do any extrapolation here.

051531.852 -- 051533.912
All we're trying to do actually just predict

051533.912 -- 051536.062
well And the prediction problem is

051536.062 -- 051538.142
hard because you kinda have to think ahead.

051538.382 -- 051540.502
In order to figure out what to do at this time step.

051541.142 -- 051543.191
And so if you're trying to do next token prediction,

051543.192 -- 051545.412
thinking ahead doesn't work

051545.412 -- 051547.462
very well. Not nearly as well as you

051547.462 -- 051548.042
might hope.

051549.912 -- 051551.992
But if you're using these other approaches, you actually

051551.992 -- 051553.991
start getting some advantage. Right So

051554.472 -- 051555.932
there's the blue state transformer,

051556.552 -- 051558.472
all all these other ones we talked about before.

051558.952 -- 051601.212
I think what's interesting here is that next latent,

051602.142 -- 051603.842
just one step ahead,

051604.481 -- 051606.951
gets a big bump in performance. It's like almost

051606.952 -- 051608.312
it's 20% improvement

051609.192 -- 051610.022
in in accuracy.

051612.352 -- 051614.112
And you get a little bit more from,

051614.921 -- 051617.071
from doing things a little bit further.

051618.512 -- 051620.751
So I think the way to think

051620.752 -- 051622.932
about this is, you get a belief state.

051622.932 -- 051625.011
The belief state has a lot of information about the future

051625.012 -- 051627.332
in it. Because there's information

051627.332 -- 051629.421
with the future in the belief state, it's

051629.421 -- 051631.441
easier for the gradients to connect,

051631.792 -- 051633.772
in terms of what you should do in the current time step.

051634.332 -- 051636.491
So even though we're doing no chain of thought,

051636.491 -- 051638.561
nothing like that, just

051638.562 -- 051640.882
doing a reflexive policy because the gradients have an

051640.882 -- 051642.971
easier way to connect you end up learning

051642.971 -- 051644.422
a a more powerful policy.

051650.292 -- 051652.282
Okay. And then, you also have

051652.602 -- 051654.682
essentially a lot of free look ahead info. This

051654.682 -- 051656.732
is, a graph that had

051656.732 -- 051658.802
tells a lot of stories in it. Have to kinda look

051658.802 -- 051659.472
at it closely.

051702.272 -- 051704.432
We're we're training on, tiny stories.

051704.432 -- 051706.372
You just said a little text dataset.

051708.272 -- 051710.701
And whenever you're you're doing these,

051711.162 -- 051713.141
kind of these different loss functions,

051713.201 -- 051715.361
multiple objectives, you you have some sort of Lagrangian

051715.362 -- 051717.422
between them. So you can kind of tune

051717.422 -- 051719.042
the Ligrangian in various ways.

051720.002 -- 051721.681
Is, of course, one particular tuning.

051723.042 -- 051725.281
But the the tuning here is designed

051725.282 -- 051727.392
so that these other

051727.392 -- 051729.491
approaches compete with next latent.

051729.802 -- 051731.902
In terms of their look ahead information.

051732.042 -- 051734.162
So we're probing the belief state

051734.162 -- 051736.241
with a linear probe to see how

051736.241 -- 051738.542
much information there is about

051739.181 -- 051741.001
future locations.

051741.902 -- 051744.312
And then we're going to relativize that to

051744.312 -- 051746.391
what you get from

051746.392 -- 051748.412
just next prediction So the

051748.412 -- 051750.552
so the baseline here is zero. Is

051750.552 -- 051752.632
is what next token prediction does. And then when you ask,

051752.632 -- 051754.641
you know, how much, improvement in log

051754.642 -- 051756.752
do we have for the future

051758.032 -- 051758.881
Okay. So

051800.981 -- 051803.071
so let's look at this closely. So first look

051803.072 -- 051804.832
at at next token. This is just

051805.312 -- 051807.602
just do one token ahead. Next

051807.602 -- 051809.882
token, alone is optimized for

051809.882 -- 051811.951
this. Next latent

051811.951 -- 051813.762
pays nothing. Surprisingly.

051815.832 -- 051817.841
With the way they we had

051817.842 -- 051819.862
tuned it. These other

051819.862 -- 051821.672
approaches are paying a bit more

051822.152 -- 051823.932
in terms of their, log loss.

051826.742 -- 051828.932
And then and by the way, I think

051828.932 -- 051831.132
that's real. I've You talk

051831.132 -- 051833.401
to some people who play with these things, and often it's

051833.402 -- 051835.422
a little challenging to avoid paying

051835.422 -- 051837.622
a little bit in the log loss. The

051837.622 -- 051838.432
next token prediction.

051841.142 -- 051843.021
And now you look at what happened. So

051844.382 -- 051846.622
MTP and JTP,

051847.681 -- 051849.792
going one ahead are,

051851.352 -- 051853.392
mean, they do very well when you're just going one ahead.

051853.952 -- 051855.412
But they die die off.

051855.972 -- 051857.872
Rapidly. Terms of the information about the future.

051858.992 -- 051901.232
Right The the light yellow and the light green kinda die off

051901.232 -- 051903.492
rapidly. You can do further ahead.

051904.802 -- 051906.921
And and what you see is that they don't die off quite

051906.922 -- 051909.001
rapidly, but they still do die off once you get

051909.001 -- 051910.252
out well beyond eight.

051913.412 -- 051915.551
And then you can look at next latent and and

051915.792 -- 051917.812
even the next latent at d equals one

051918.432 -- 051920.512
k, it's not quite as strong in in

051920.512 -- 051922.642
the position in the next next token. But

051923.122 -- 051925.542
it stays strong and it keeps being relatively

051925.602 -- 051928.052
strong even out to 20.

051928.862 -- 051930.932
And you can go, to the next lady

051931.412 -- 051933.572
eight out, and and it is it's strong all the way

051933.572 -- 051934.072
out there.

051935.991 -- 051938.190
So this is a what we're

051938.191 -- 051940.221
observing here is that we're paying nothing

051941.342 -- 051942.882
in terms of our prediction

051943.922 -- 051945.862
accuracy. Next token prediction accuracy.

051946.342 -- 051948.501
To have quite a bit of information about the

051948.501 -- 051950.691
future embedded into our

051950.691 -- 051951.151
belief state.

051953.072 -- 051955.232
Why is it possible Well, you only need about 18

051955.232 -- 051956.661
dimensions to

051958.422 -- 052000.662
be able to decode to the typical vocabulary size

052000.662 -- 052001.482
of a decoder.

052003.662 -- 052005.971
So there's a thousand more dimensions

052005.971 -- 052007.172
that are just sitting there

052008.042 -- 052010.142
relatively unused in these transformers.

052011.102 -- 052013.262
So there's there's lots of capacity to do much

052013.262 -- 052013.531
more.

052018.652 -- 052020.572
Okay. So I'm about

052020.712 -- 052022.952
finished. The paper's

052022.952 -- 052024.931
here, if you wanna take a look at it. I'm

052025.072 -- 052027.151
excited Oh, not the paper.

052027.151 -- 052029.192
Interesting. I'll

052030.362 -- 052031.982
Paper is on archive.

052032.632 -- 052034.172
Somehow it got changed.

052036.742 -- 052038.942
The compact world of model, I think, is a key

052039.182 -- 052040.482
to having better extrapolation.

052041.572 -- 052043.302
And to nailing this look ahead problem.

052043.701 -- 052045.941
It may also be quite key to the exploration.

052045.941 -- 052047.652
We haven't done experiments for that but

052047.972 -- 052048.912
I think it

052050.432 -- 052052.491
it could be critical or the active learning.

052054.451 -- 052056.042
Next latent is a very easy way

052056.842 -- 052058.947
comparatively, to get a compact world model. So this

052058.947 -- 052100.292
is a generic approach

052101.022 -- 052103.311
to convert a POMDP

052103.312 -- 052105.711
into an MDP. That's that's that's

052105.711 -- 052106.292
very powerful.

052108.852 -- 052111.152
There's lots to do here. People are interested,

052111.392 -- 052113.552
And if if you want to play with these

052113.552 -- 052114.872
things, come and talk to me.

052117.032 -- 052119.202
There's for every one of these question

052119.202 -- 052121.241
marks, I can tell you a story about why

052121.242 -- 052122.691
this is an important problem to work

052123.647 -- 052126.001
I I think

052126.062 -- 052128.271
there's fantastic things that we will discover here

052128.272 -- 052129.671
the next year or two.

052131.032 -- 052132.862
And then there's a related work

052134.392 -- 052136.412
there's this Sikh Jeppa which is at this neurops,

052136.412 -- 052138.471
I believe. They're

052138.472 -- 052140.701
not using action as a target, but they

052140.701 -- 052141.601
are doing something

052143.202 -- 052144.022
pretty similar.

052146.042 -- 052146.782
Approximate information,

052148.312 -- 052150.392
states. Is where

052150.392 -- 052152.512
we are actually learning

052152.512 -- 052154.821
an approximate information state. And I was

052154.822 -- 052156.842
seeing that the ASTRA paper, poster over

052156.842 -- 052158.572
there He's also talking about

052159.292 -- 052201.302
information states. So if you

052201.302 -- 052203.382
know the literature for approximate information states,

052203.382 -- 052205.485
that is indeed the kind of thing that we

052205.485 -- 052206.092
are aiming at.

052208.251 -- 052209.692
Alright. Let me stop here.

052210.962 -- 052213.242
Are there other

052213.242 -- 052213.672
questions

052227.942 -- 052230.022
If you have question, you can come to

052230.022 -- 052231.502
the center.

052232.582 -- 052233.642
Use the microphone.

052238.592 -- 052240.672
Hi. Thanks for the amazing talk. I

052240.672 -- 052242.052
had a question. So

052242.932 -- 052245.352
there's like some work in

052245.492 -- 052247.872
adjacent fields where people

052247.872 -- 052249.982
try to come up with vocabularies

052250.042 -- 052252.171
in the latent space So in

052252.172 -- 052253.882
stuff like as you mentioned, like,

052254.202 -- 052256.592
things being done in pixel spaces, like, the dimensionality

052256.592 -- 052258.872
is too high. People have tried to come up

052258.872 -- 052300.812
with vocabularies in the latent space.

052301.832 -- 052303.742
You think that next

052304.142 -- 052306.382
latent prediction is essentially trying to learn

052306.382 -- 052308.852
that vocabulary itself While Yeah.

052309.612 -- 052311.672
So I mean,

052311.672 -- 052313.932
there is a sense in which you're getting a vocabulary,

052314.152 -- 052316.572
but it it's it's a continuous thing

052316.572 -- 052317.952
rather than a discrete thing.

052319.332 -- 052321.412
There's a strong reason to think about discrete

052321.412 -- 052323.752
things, which is discrete things have a built in error correction

052324.052 -- 052326.311
mechanism. It may or

052326.312 -- 052328.392
may not be necessary. To

052328.392 -- 052330.681
do good long range prediction.

052330.922 -- 052333.162
There's also the diffusion approaches for doing

052333.162 -- 052335.382
error correction. Which potentially could

052335.382 -- 052336.102
be used. But,

052337.502 -- 052339.492
I do think that it's an interesting

052339.972 -- 052341.061
direction to be looking at.

052342.501 -- 052344.691
Right Is

052345.072 -- 052347.312
there particular I guess finding

052347.312 -- 052349.642
discreet a discrete vocabulary could

052349.782 -- 052352.212
be a good multiplier on how far out you

052352.352 -- 052354.762
can do faithful simulations. So like for example,

052355.142 -- 052357.192
even while you train the sort of

052357.192 -- 052359.381
next like, you have some regression function

052359.382 -- 052401.511
in your paper which predicts the next latent. Right

052401.751 -- 052403.821
Yeah. But right now, it

052403.822 -- 052405.832
doesn't enforce, sort of

052405.832 -- 052407.812
discreteness. So the whole thing is still

052408.211 -- 052410.632
like, continuous. I also was, like, curious

052411.862 -- 052414.182
if you have any thoughts on because this is, like, opposite

052414.182 -- 052416.262
of what VQA used to do, right,

052416.262 -- 052418.432
where just wondering if you

052418.432 -- 052420.772
have any thoughts on that. I think it could be powerful.

052421.241 -- 052423.321
Honestly, we haven't pushed the limits of

052423.322 -- 052425.232
how far out you can get the simulation to go.

052425.872 -- 052427.872
And it it I could easily imagine that,

052429.702 -- 052431.782
you come up to me, in a month

052431.782 -- 052433.922
and you tell me, hey. We

052433.922 -- 052436.191
can push out the simulation a 100 times

052436.191 -- 052438.251
further if we use a BQBA

052438.472 -- 052440.692
in the process. Thank

052440.697 -- 052441.197
you.

052444.482 -- 052446.902
Due to the time limitation, we will

052447.702 -- 052449.642
have just one more question. One more.

052449.882 -- 052450.402
Okay. K.

052452.072 -- 052453.842
Hi. Hi, doctor,

052454.287 -- 052456.321
Langford. Really nice talk. I have a

052456.322 -- 052458.422
question which you, I guess, you initially

052458.472 -- 052500.452
pointed out in the first half of the talk.

052500.692 -- 052502.942
That transformers are already world models

052502.942 -- 052505.402
but they're world is not persistent.

052505.402 -- 052507.382
So the states are not persistent, So

052507.682 -- 052509.842
they move around a lot because the

052509.842 -- 052512.142
key and value thing. So

052513.222 -- 052515.281
transform yourself quite a bit already

052515.282 -- 052516.632
solved this long context

052518.302 -- 052520.371
of attention. So this key and values

052520.372 -- 052522.531
have become more persistent because now you can

052522.532 -- 052524.562
have everything in it like, a

052524.562 -- 052526.222
million token size or something.

052526.622 -- 052528.232
The tea has been grown a lot.

052528.632 -- 052530.502
So given that we have achieved

052530.912 -- 052533.191
transformers with really long context, which are

052533.192 -- 052534.732
kind of persistent belief state

052536.962 -- 052539.092
Why where do you think

052539.332 -- 052541.402
that additional relief state which is work that you've

052541.402 -- 052543.551
shown, a persistent memory can play a

052543.711 -- 052545.772
Thank you. Yeah.

052546.472 -- 052548.512
So, think the observation here is that

052548.512 -- 052550.931
people have managed to create

052550.932 -- 052552.471
longer and longer context.

052553.022 -- 052555.342
And reading between the lines of various

052555.342 -- 052557.462
public announcements, they have some

052558.022 -- 052559.402
compactification process.

052602.022 -- 052604.451
Seems like a good idea. I guess the same

052604.532 -- 052606.592
it's it's it's it's aiming at a similar thing.

052606.592 -- 052609.012
Right So you're you're creating a more concise representation

052611.572 -- 052613.932
I I don't know, of course, what's going on inside of

052614.252 -- 052616.251
OpenAI or or these other companies.

052616.732 -- 052618.972
But, what

052618.972 -- 052620.171
I do know is that

052622.172 -- 052623.552
if you get the right objective,

052624.522 -- 052626.842
it makes a huge difference in how easy

052626.842 -- 052629.171
it is to do these things. So,

052631.112 -- 052632.911
this is a relatively easy way

052633.552 -- 052636.042
to do to to create a belief state. Think it's probably gonna be

052636.262 -- 052638.451
widely useful. And I

052638.451 -- 052640.612
could imagine that it composes with whatever they're

052640.612 -- 052642.501
doing internally at these companies.

052645.132 -- 052646.422
But in the public world,

052647.462 -- 052648.872
this seems like it's a great thing.

052649.992 -- 052650.712
Thank you.

052652.122 -- 052654.462
Thank you very much again for your fantastic

052655.002 -- 052657.462
talk. And let's welcome

052657.522 -- 052659.682
the next speaker. Yilung,

052700.302 -- 052702.402
Yilung Du from Harvard University.

052702.602 -- 052704.922
He's currently an assistant professor

052704.922 -- 052706.692
at Harvard and

052707.092 -- 052709.112
Camp Kemptor Institute and the CS.

052709.572 -- 052711.721
He's research interest about building

052711.721 -- 052713.981
models for word using generative

052714.042 -- 052715.852
AI And his,

052716.092 -- 052717.392
top title will be

052718.142 -- 052720.221
building intelligent robots with warped

052720.221 -- 052722.332
models. Let's welcome Yiren

052722.332 -- 052722.722
Du.

052727.452 -- 052728.872
Okay. Okay.

052730.012 -- 052732.251
Great. Thanks so much for the introduction, and thanks

052732.251 -- 052734.352
everyone for coming. So today, I'll

052734.352 -- 052736.412
share some thoughts about how we can to

052736.412 -- 052738.672
build more intelligent robots, using world models.

052739.181 -- 052741.602
So there's a lot of interesting language modeling

052741.802 -- 052743.882
And when we look at language models, they're able to do

052743.882 -- 052745.942
these really fascinating things. Such

052745.942 -- 052748.292
as if you asked a language model how to build

052748.451 -- 052750.532
really powerful AI system, it can give you these step

052750.532 -- 052752.602
by step instructions. And while

052752.602 -- 052754.782
there's a ton of work, on VLA

052754.922 -- 052757.182
VLA models, or models which, use

052757.182 -- 052759.042
language models as a way to build intelligent

052759.262 -- 052801.302
robots, One big issue with

052801.362 -- 052803.642
language modeling that these models actually aren't that

052803.642 -- 052805.731
good at physics. So even when you give it

052805.732 -- 052807.962
a pretty simple scene here where you have two objects,

052808.432 -- 052810.592
want to ask it where should I place a ball so

052810.592 -- 052812.741
that the screen the screen green circle

052812.742 -- 052815.142
can drop in this cup It will just generate

052815.362 -- 052817.402
some nonsense. So I think that one

052817.402 -- 052819.561
big issue is that language ends up not having

052819.562 -- 052821.952
much information about how to act in the

052821.952 -- 052824.092
physical world. This talk will be about

052824.092 -- 052826.212
how we can really use video models as

052826.212 -- 052828.552
a way to really get a lot more physical information

052829.532 -- 052831.852
about the world and how they these can be a basis

052831.852 -- 052832.912
for building more intelligent

052834.572 -- 052836.751
So in this talk, I'll talk about three different

052836.812 -- 052838.961
ways in which we can really use video models for more

052838.962 -- 052841.012
intelligent robot systems. So first

052841.012 -- 052843.092
I'll talk about how these video models can be seen

052843.092 -- 052845.232
as planners. For decision making.

052845.232 -- 052847.552
Then I'll talk about how they can be seen as simulators

052847.552 -- 052849.602
or dynamics models and finally, I'll

052849.602 -- 052851.762
talk about how we can really, use these video

052851.762 -- 052853.907
models in combination with other models

052854.162 -- 052855.782
to do more complex tasks.

052856.302 -- 052858.372
So first, let me talk a bit about how these video models

052858.372 -- 052900.461
can be planners. And the overall

052900.762 -- 052903.001
idea is that, given the starting

052903.001 -- 052905.392
image, and a text instruction, can

052905.392 -- 052907.212
imagine that a video generation model

052907.452 -- 052909.532
essentially synthesize a visual plan of what you want

052909.532 -- 052911.572
to do in a task. So if I

052911.572 -- 052913.552
if I want to, take, pick up this

052913.862 -- 052915.942
get gas, then a video model can

052915.942 -- 052918.042
generate, something like actually picking up

052918.042 -- 052920.161
this nozzle. Or if I want to really pick

052920.162 -- 052922.191
up this tissue paper, the video generation

052922.192 -- 052924.352
model can generate how what it means to pick up this

052924.352 -- 052925.892
tissue. And similarly,

052926.432 -- 052928.582
this type of video generation works across different

052928.582 -- 052930.612
robot embodiments. So that if you want to

052930.612 -- 052932.792
generate something where you want to put the screwdriver

052932.932 -- 052935.031
on top of this tissue box, again, the video

052935.032 -- 052937.152
model can be used to generate a

052937.152 -- 052939.192
plan of this. So video gives you a

052939.192 -- 052941.432
nice way to encode how to solve different

052941.432 -- 052943.642
tasks across a wide variety of different

052943.642 -- 052944.782
embodiments in robots.

052946.662 -- 052948.741
And one important thing about video

052948.741 -- 052950.762
models is that it really allows us to directly

052950.762 -- 052952.960
learn a lot of the physical videos on

052952.961 -- 052955.162
the Internet. There are many videos on YouTube

052955.162 -- 052957.192
about what it means to open a door, what

052957.192 -- 052959.432
it means to clean your room, or to cook,

052959.672 -- 053001.802
cook dinner, And really by training a video

053001.802 -- 053004.032
model, using it as some type of visual planner,

053004.192 -- 053006.452
which describes what you want to do in the future,

053007.262 -- 053009.741
allows you to really explain and use all the information

053009.741 -- 053011.942
on the Internet. And once

053011.942 -- 053014.141
you have this type of video generation model,

053014.302 -- 053016.382
you can just have a separate process that actually takes

053016.382 -- 053018.512
the generated videos and then converts

053018.512 -- 053020.941
it into actions. Whether it's some type of learned

053021.082 -- 053023.412
neural network or a couple other different approaches

053023.472 -- 053025.512
I'll talk about in a bit. And here's

053025.512 -- 053027.432
just a brief example of this. So we're,

053027.672 -- 053029.272
we're about to release a large

053030.392 -- 053032.402
video planning model in the next week. But

053032.402 -- 053033.862
here, we just took an image

053034.562 -- 053036.562
image of this table with a piece of tape,

053036.721 -- 053038.802
And this is a generated video plan. So you can see

053038.802 -- 053041.012
it's very realistic. And then we can estimate

053041.012 -- 053043.092
maybe some hand pulls. We can do some type of

053043.092 -- 053045.170
three d reconstruction. And this allows us

053045.171 -- 053047.251
then to take this motion and really execute it

053047.251 -- 053049.292
on the physical robot. So you can imagine

053049.292 -- 053051.402
that, like, video generation can be directly

053051.402 -- 053053.151
converted into real robot execution.

053057.212 -- 053059.372
So in terms of, how do you

053059.372 -- 053101.142
actually, get actions from video

053101.802 -- 053103.962
one straightforward way is to just have some type of inverse

053103.962 -- 053106.162
dynamics model. Where what you learn

053106.162 -- 053108.632
from your data, another thing you can do oftentimes

053108.691 -- 053110.792
is that you can actually just directly

053111.262 -- 053113.501
extract or determine what you want to how you want to act

053113.501 -- 053115.572
from the video itself. So once you have a set

053115.572 -- 053117.692
of image states, what you can do is you

053117.692 -- 053119.712
can, for instance, estimate the optical

053119.852 -- 053121.882
flow of how, the objects in the

053121.882 -- 053123.952
video are moving and once you have this

053123.952 -- 053126.112
type of optical flow, as well as some depth, and

053126.112 -- 053128.041
maybe you can even have some type of three d flow,

053128.201 -- 053130.361
you can just directly determine what are the states

053130.362 -- 053132.522
of the road that you want to execute to

053132.522 -- 053133.222
accomplish this task.

053137.702 -- 053139.712
And as an example, let's

053139.712 -- 053141.792
say here we want to pick up a run and

053141.792 -- 053143.872
we want to put it on top, like,

053143.872 -- 053145.572
so here we want to pick up this

053147.782 -- 053149.822
We we can simply just take our

053149.822 -- 053151.835
generated video estimate it estimate

053151.835 -- 053153.562
the appropriate optical flow of the robot,

053155.102 -- 053157.322
arm, and then from there, we can solve for transforms allow us to instantiate

053157.382 -- 053159.512
this in the physical world. And this

053159.512 -- 053201.711
works not just for manipulation, but you could also

053201.711 -- 053203.841
imagine applying this for navigation. So

053203.842 -- 053205.512
here you have a generated video

053206.552 -- 053208.677
over here. From the generated video, you can estimate the optical

053208.677 -- 053210.332
flow of how the scene is changing

053210.742 -- 053212.822
from the chain from the predicted optical flow, as

053212.822 -- 053214.892
well as depth map, can infer

053214.952 -- 053217.412
the corresponding robot action that you want to take,

053217.792 -- 053219.141
to instantiate this task.

053221.812 -- 053224.052
And here's just a brief example. So here, this

053224.052 -- 053226.272
is a zero shot approach where we estimate the

053226.272 -- 053228.282
optical flow and we're able to pick up an

053228.282 -- 053229.482
apple and place it down.

053239.112 -- 053241.192
And and another and another and another thing you

053241.192 -- 053243.352
can also do, when you have some type of video

053243.352 -- 053245.481
generated model, you can actually learn a

053245.482 -- 053247.452
policy to actually convert the video

053248.252 -- 053250.151
videos to concrete actions to execute in

053250.712 -- 053252.942
a completely online online learning manner.

053253.262 -- 053255.582
And the idea here is you have your video

053255.582 -- 053257.522
model that generates these visual goals,

053257.622 -- 053259.701
of where you want to reach, and you can

053259.701 -- 053301.792
actually use these visual goals as a way to

053301.951 -- 053303.451
explore it in your online environment

053304.012 -- 053306.192
to gather data, to train your policy,

053306.252 -- 053308.372
to be able to reach those

053308.372 -- 053310.482
goals states. And the idea is, you

053310.482 -- 053312.642
essentially take your video model. You have it generate

053312.642 -- 053314.751
some visual goal. And then you just simply

053314.751 -- 053317.002
have initially a random policy that

053317.062 -- 053319.221
tries to reach those goals. And whatever goals it

053319.222 -- 053321.242
actually reaches, just relabel

053321.242 -- 053323.302
that data a goal of the policy.

053323.461 -- 053325.542
You essentially gather these

053325.542 -- 053327.872
free goal condition trajectories

053327.872 -- 053330.182
in the real robot, And as you repeat

053330.182 -- 053332.210
this procedure over and over again, eventually, you learn

053332.211 -- 053333.402
to reach your final goal.

053337.232 -- 053339.552
Here's an illustration of this. So here we have

053339.552 -- 053341.712
this, goal where we want to pick up this cup and we wanna

053341.712 -- 053343.862
place it on a plate. And

053343.862 -- 053345.941
what we can do is without any rollouts, so

053345.942 -- 053347.952
here you just have a random model While the

053347.952 -- 053350.032
random model isn't able to ever, like, ever

053350.032 -- 053352.192
solve this task, But by interacting

053352.192 -- 053354.251
with the environment for a while, you can learn this

053354.251 -- 053356.490
model that can somewhat shakily put this cup

053356.491 -- 053358.592
on this plate. And by doing

053358.592 -- 053400.752
this for a very long time period, you can gradually get

053400.752 -- 053402.381
a very accurate policy that can

053402.862 -- 053405.052
exactly put the mug top of the plate.

053408.062 -- 053410.172
And then, and in general, we find

053410.172 -- 053412.092
that, this this type of video planning,

053412.251 -- 053414.342
works pretty well in open world settings.

053414.582 -- 053416.661
So here are these, so the next couple videos I'm

053416.662 -- 053418.972
gonna show are just in the wild images.

053418.972 -- 053421.152
We asked, some graduate students to just take

053421.452 -- 053423.662
some images, of their dorm. Again, this is a model

053423.662 -- 053425.892
that we're gonna release in probably in the next week.

053425.972 -- 053428.132
But we we find that if you want to insert a straw into

053428.132 -- 053430.212
the cup, we can find that it's able to accurately

053430.212 -- 053432.272
do this. We also find that these

053432.272 -- 053434.772
models are able to generate these longer horizon

053434.832 -- 053436.912
tasks. So this is again generated, but you can

053436.912 -- 053439.122
put the straw in the cup but now you can actually

053439.122 -- 053441.122
put push the cup to a mouse.

053442.882 -- 053445.122
You can also have this instruction here where you

053445.122 -- 053446.971
can pick up a blue book.

053447.532 -- 053449.562
And actually place it on top of a red book.

053452.441 -- 053454.486
And and in general, you can you can

053454.487 -- 053454.532
actually

053456.772 -- 053458.932
the they tend to be, follow language

053458.932 -- 053500.702
instructions much more accurately than

053501.182 -- 053503.262
VLA models. But we find this, for instance, if you

053503.262 -- 053505.151
have a set of objects here, we can actually

053505.272 -- 053507.501
so this is generated video of putting a cookie can

053507.582 -- 053509.592
on top of green cube. But again, we

053509.592 -- 053511.532
can have we can also take the same instruction

053511.662 -- 053513.902
and, you can put a green cube on the Coke

053513.902 -- 053516.132
can. There's some intuition for this. So

053516.132 -- 053518.172
when you look at VLA models, really

053518.172 -- 053520.252
like you're often just predicting the next short chunk of

053520.252 -- 053522.422
actions, so it's hard for the models to learn as

053523.382 -- 053525.532
Or with your actions. But here with this more video

053525.532 -- 053527.912
based planning approach, actually learn a correlation between

053527.912 -- 053529.932
the entire generated video plan and

053529.932 -- 053532.042
language actions. So so oftentimes when you try these

053532.042 -- 053534.421
things on like the existing VLA models, they simply

053534.421 -- 053536.501
ignore the language instruction and will always do the

053536.501 -- 053538.751
same action. But here's

053538.751 -- 053540.911
another thing you can do. So you can also say, we want to

053540.912 -- 053542.912
put that cocaine on a men's or

053542.972 -- 053545.062
so forth. And and

053545.062 -- 053547.002
just as another couple examples,

053547.362 -- 053549.402
of how you can actually take these video plans

053549.402 -- 053551.562
and actually convert them

053551.562 -- 053553.591
into corresponding robot actions So again,

053553.592 -- 053555.911
this is generated video. So if we say spread coffee

053555.911 -- 053558.042
beans, can do this. And then again,

053558.042 -- 053600.291
we can just retarget on a robot. And have

053600.292 -- 053601.592
it do the corresponding actions.

053604.471 -- 053606.502
With with some lack of precision. That's

053606.902 -- 053608.802
some models are a bit slow right now.

053614.332 -- 053616.331
And we can here's I get another one. So here we can

053616.491 -- 053618.862
this is generated. So we generate close the laptop,

053618.862 -- 053621.362
we can extract some hand poses, do some reconstruction.

053622.072 -- 053624.342
And then correspondingly execute it on the same setting.

053628.792 -- 053629.112
And,

053637.372 -- 053639.532
as as it can be for some of those videos, there

053639.532 -- 053641.612
is still a part that's missing where, like, how do you

053641.612 -- 053643.742
actually convert these videos to, like, the concrete

053643.742 -- 053645.842
robot actions So I think that's an interesting

053645.902 -- 053648.062
direction for future work. But in general, the idea is

053648.062 -- 053649.522
you can kinda use these models

053650.181 -- 053652.341
as a way to imagine what are the what are

053652.342 -- 053654.552
the next stage you want to reach then use

053654.552 -- 053656.972
those as visual goals then to to actually

053658.172 -- 053700.512
to actually execute your actions in the environment.

053700.942 -- 053703.022
That's the first part on how you can really use these models

053703.022 -- 053705.132
as planners. Another thing you can do with these

053705.132 -- 053707.291
models is use them more directly as some type of

053707.292 -- 053707.972
dynamics model.

053709.332 -- 053711.491
And essentially, one thing you can do is you can kinda have

053711.492 -- 053713.532
these models be something similar to

053713.532 -- 053715.792
like a kind of interactive simulator, just given some starting

053715.792 -- 053718.032
images, you can just simulate all types of different

053718.032 -- 053720.172
actions on the world. So you can simulate, picking

053720.172 -- 053722.512
up a bowl, you can simulate putting carrots in the bowl,

053722.671 -- 053724.782
and so forth. So let me show a concrete

053724.782 -- 053726.841
example on the robotics setting. So here

053726.842 -- 053728.962
we have the starting image. And we can just roughly

053728.962 -- 053730.932
simulate this sequence of instructions.

053731.972 -- 053732.032
So

053737.262 -- 053739.582
one thing that's interesting about these models when you

053739.582 -- 053741.814
treat them as these dynamics models that you

053741.814 -- 053744.002
can actually simulate very long horizon rollouts.

053744.242 -- 053746.562
In the world. And this can allow you to

053746.562 -- 053748.482
simulate try to simulate and solve

053748.722 -- 053751.042
longer horizon tasks. And I'll talk a bit more

053751.042 -- 053753.082
later how we can really get these models to really do

053753.082 -- 053754.452
very long horizon simulation.

053757.152 -- 053759.312
But one thing but once you have this simulator,

053759.312 -- 053801.632
one nice thing that doesn't lock is it allows

053801.632 -- 053803.711
you to directly, train your policies in

053803.712 -- 053805.872
the in the space of the generative simulator.

053805.872 -- 053807.892
So by drag by once you have

053807.892 -- 053810.032
trained this, interactive dynamics model,

053810.191 -- 053812.271
you can just directly simulate these

053812.272 -- 053814.352
actions. So the, simulate these actions, block

053814.352 -- 053816.491
pushes, inside entirely inside

053816.491 -- 053818.562
the space of the generative model. And

053818.562 -- 053820.642
you can just directly train a policy directly on

053820.642 -- 053822.852
the simulated data. And once you have

053822.852 -- 053824.922
this policy trained on simulated data, you

053824.922 -- 053827.092
can just directly zero shot deploy them in the physical

053827.092 -- 053829.472
world. So this allows so this type of simulation

053829.472 -- 053831.872
then allows you to really build

053831.872 -- 053834.141
systems that that can do reinforcement learning in

053834.142 -- 053835.241
a very safe manner.

053838.421 -- 053840.661
Another thing you can do with these dynamics models

053840.661 -- 053842.972
is that you can do some type of modeled

053843.272 -- 053845.541
predictive control at prediction time

053845.941 -- 053848.302
to actually infer the actions you want to take. Where

053848.302 -- 053850.642
here what you do is you simply have your policy

053850.991 -- 053853.092
predict different possible sequences of actions

053853.362 -- 053855.542
use a role model to simulate what

053855.542 -- 053857.882
happened in the road in response to each of these actions,

053858.052 -- 053859.952
then you just select the action trajectory

053900.512 -- 053902.732
that has the maximum reward. So here's an illustration of this.

053903.212 -- 053905.632
So here we have a policy or some action proposal

053905.932 -- 053907.862
model that proposes a sequence of actions

053908.022 -- 053909.812
The world model simulates all of them

053910.132 -- 053912.092
You you estimate some reward for the

053912.172 -- 053914.181
for each of the simulations, choose the one

053914.181 -- 053916.592
that has the highest reward, and just repeat this process,

053917.052 -- 053918.671
allowing you to, to refine

053919.282 -- 053921.002
actions until you reach your final goal.

053921.642 -- 053923.802
And one really nice and and you can see that here's

053923.802 -- 053925.772
at the end is when it reaches the final goal.

053926.251 -- 053928.332
And one really nice thing about this is when your

053928.332 -- 053930.191
environment has pretty complicated dynamics,

053930.432 -- 053932.672
so when there are several objects obstructing your

053932.672 -- 053934.832
goal, then this procedure allows you to more

053934.832 -- 053937.231
accurately do these like

053937.612 -- 053940.112
like consider these more collisions between objects.

053940.212 -- 053942.232
So it allows you to, accurately,

053942.852 -- 053945.062
move your move this object to the c,

053945.221 -- 053947.301
even when there are a lot of collisions, where eventually it

053947.302 -- 053949.532
reaches this final goal. We find that in these settings

053949.532 -- 053951.682
where there's a lot of contacts, we find that

053951.682 -- 053954.122
these behavioral cloning methods tend to struggle

053954.262 -- 053956.402
a bit more. So one of these models can be both used

053956.402 -- 053958.430
during training time to,

053958.471 -- 054000.761
as a way to train your policies, but also at test time

054000.921 -- 054002.231
directly through model predictive control,

054003.132 -- 054004.562
to directly control your robot.

054007.201 -- 054009.302
And these models don't have to just be

054009.302 -- 054011.501
in the two d image space. So

054011.501 -- 054013.531
the road is the physical road is in

054013.532 -- 054015.541
three d, you can actually also have these models

054015.542 -- 054017.602
operate in the space as three d. Where in

054017.602 -- 054019.761
this setting what we do is instead of just directly

054019.762 -- 054022.112
predicting the RGB pixels, we the

054022.592 -- 054024.622
both the depth map as well as a normal map.

054024.702 -- 054026.862
And this allows you to essentially get a three d

054026.862 -- 054028.991
surface, which allows you to much more

054028.991 -- 054031.471
accurately, extract the actions necessary

054031.472 -- 054033.642
to solve a task, as well as simulate the three d

054033.642 -- 054033.892
surface.

054035.972 -- 054038.372
And, yeah, and and like and this is roughly

054038.372 -- 054040.402
what does that accounts do. Here

054040.402 -- 054042.512
you just, the model generates

054042.572 -- 054044.732
both this, video. The depth is raw as

054044.732 -- 054046.782
the corresponding normal. And by combining

054046.782 -- 054048.862
all three of these things together, you can get

054048.862 -- 054050.412
a more complete three d

054051.052 -- 054052.071
model of the world.

054055.492 -- 054057.652
And you can also use this not only to get

054057.652 -- 054059.732
a surface, but you can also imagine like training

054059.732 -- 054101.791
some type type of three d latent model. So

054101.791 -- 054104.191
here, so instead of instead of just directly predicting

054104.192 -- 054106.322
the surface, of the future, you

054106.322 -- 054108.022
can actually have the model predict

054108.422 -- 054110.472
an overall three d latent. This allows

054110.472 -- 054112.732
you to get a much more three d, consistent

054112.792 -- 054114.471
reconstruction of the entire scene, so

054115.032 -- 054117.412
allows you to accurately do more precise tasks.

054119.012 -- 054121.251
And and and here's an example of this. So we find

054121.251 -- 054123.461
is, so now now you predict the

054123.462 -- 054125.642
latent states, the latent state actually captures entire

054125.642 -- 054127.781
three d scene. So here is the first, first

054127.782 -- 054130.101
latent that's predicted. So you can see that it's actually

054130.101 -- 054132.102
full fully three d complete. For maybe a for

054132.102 -- 054134.411
a set of partial images. But you can

054134.412 -- 054136.491
simulate, so here the task is to

054136.492 -- 054138.652
pick up the bow and put it in the plate. But

054138.652 -- 054141.032
now it's here it's able to more accurately simulate

054141.412 -- 054143.482
picking up the ball. So now this

054143.482 -- 054145.682
is the third step of placing it down.

054145.842 -- 054147.922
The fourth step of placing it actually in the plate. So you're

054147.922 -- 054150.029
gonna see that by actually directly predicting in

054150.029 -- 054152.031
a three d latin, you can actually also get

054152.032 -- 054152.531
a full

054154.132 -- 054156.052
full three d reconstruction of the scene.

054157.811 -- 054200.051
So so one thing about all of what I've talked

054200.052 -- 054202.011
about so far is all of these dynamics models

054202.412 -- 054204.731
are chunk based in the sense that, I'm just directly

054204.732 -- 054206.522
predicting a sequence of future actions

054206.842 -- 054209.051
or a sequence of future states at once. There,

054209.052 -- 054211.212
in some sense, it's actually nicer if we can have more

054211.212 -- 054213.461
of an autoregressive model. Something similar

054213.462 -- 054215.541
to LLMs, where you just simply

054215.542 -- 054217.822
predict the next image. Given the next image, predict

054217.822 -- 054220.061
the next action, and auto aggressively roll

054220.062 -- 054222.052
out the dynamics model over time.

054223.492 -- 054225.992
And and as as rehashed so far with most

054226.052 -- 054228.271
of the dynamics models we talked about are much

054228.272 -- 054229.741
more of these chunk based models

054230.302 -- 054232.461
where you get this entire Gaussian noise

054232.462 -- 054234.632
trajectory. Just directly generate the entire

054234.632 -- 054235.932
trajectory all at once.

054237.371 -- 054239.531
And in a sense, when we thought when we think about,

054239.691 -- 054241.771
these dynamics models, there are

054241.771 -- 054243.862
some, there are actually quite a few

054243.862 -- 054245.932
advantages towards having the dynamics model be

054245.932 -- 054247.702
a bit more autoregressive in nature.

054248.021 -- 054250.282
And the the nice thing about having more of an autoregressive

054250.422 -- 054252.481
model is that, you can

054252.481 -- 054254.561
actually, do some type of tree search with the model.

054254.561 -- 054256.761
So you can imagine that roll out simulate different

054256.762 -- 054258.762
possible future states and then determine,

054258.842 -- 054300.925
for some type of monoclonal search, which are

054300.925 -- 054303.052
the right trajectories to do, by,

054303.372 -- 054305.692
you can also, by changing the attention

054305.692 -- 054307.652
mask, you can roughly enforce that,

054307.732 -- 054310.052
dynamics model is Markovian or satisfies,

054310.212 -- 054312.192
or, like, or in general,

054312.432 -- 054314.512
is less causally dependent on the other parts of

054314.512 -- 054316.672
the trajectory, You can also make the dynamics

054316.672 -- 054318.532
model have some type of causal uncertainty.

054318.772 -- 054320.852
So that states that are more that you're more uncertain

054320.852 -- 054321.832
about in the future.

054323.062 -- 054325.142
Are are are predicted later. And you can finally, you

054325.142 -- 054327.582
can have these dynamics models have much more flexible

054327.882 -- 054329.952
horizon, predictions. And in contrast,

054329.952 -- 054332.272
all of these dynamics models we've talked about earlier,

054332.902 -- 054335.201
they you they they generate

054335.202 -- 054337.282
everything all at once, so it makes all of these four

054337.282 -- 054339.412
properties a little difficult. Although

054339.472 -- 054341.632
one advantage of directly predicting the dynamics

054341.632 -- 054343.972
all at once is that it allows you to

054344.577 -- 054346.621
directly directly steer your

054346.621 -- 054348.631
entire trajectory if you want to do some type of planning.

054348.952 -- 054351.032
So so you can also try to

054351.032 -- 054352.972
blend a bit more of these autoregressive

054353.192 -- 054355.472
models into your dynamics models.

054356.552 -- 054359.032
And the idea here is we will view

054359.032 -- 054400.892
the diffusion process along two axes.

054401.412 -- 054403.452
So instead of, so now

054403.452 -- 054405.532
we will have two axes in which we do denoising.

054405.532 -- 054407.232
So one is along the time dimension,

054407.862 -- 054409.642
Another one is along the noise dimension.

054411.162 -- 054413.231
And the overall idea here is when you have

054413.712 -- 054415.832
some type of autoregressive model, what you

054415.832 -- 054418.172
do is the auto is this that the autoregressive

054418.392 -- 054420.552
model just simply would just simply

054420.552 -- 054422.602
given noisy image, it

054422.602 -- 054424.682
would just directly generate the next image to

054424.682 -- 054426.632
predict. But,

054427.252 -- 054429.401
similar to LLM, while when

054429.402 -- 054431.481
you have some type of diffusion model, the diffusion model

054431.482 -- 054433.222
will directly predict

054433.541 -- 054435.661
all the future frames that you want to reach.

054436.382 -- 054438.702
But you can, where where here you start from full

054438.702 -- 054440.211
wiggas and noise on everything, and you

054441.492 -- 054443.572
simultaneously denoise everything to generate your

054443.572 -- 054444.872
final clean output.

054445.912 -- 054448.232
And so in this paper, we we try to

054448.232 -- 054450.241
really this type of diffusion road model

054450.242 -- 054452.371
where you're doing things at the chunk level with

054452.371 -- 054454.585
this autoregressive model we do is

054454.585 -- 054456.562
we simply add different levels of noise

054456.722 -- 054458.881
to every single, token that you want

054458.882 -- 054501.121
to predict. So here what happens here

054501.122 -- 054503.342
is is that, you the model predicts

054503.342 -- 054505.322
this increasing increasing

054505.622 -- 054507.162
levels of noise, trajectory.

054507.722 -- 054509.881
So the model takes in a trajectory where the

054509.882 -- 054511.952
next immediate state has very low noise, the

054511.952 -- 054514.362
state afterwards has higher noise, and

054514.362 -- 054516.852
an increasingly higher noise. And then the model

054517.392 -- 054519.463
then learns to predict what will be

054519.463 -- 054521.612
the less noisy version of this entire

054521.612 -- 054523.662
trajectory. But what this

054523.662 -- 054525.822
enables you to do at sampling time is it allows

054525.822 -- 054528.012
you to sample in a more autoregressive

054528.152 -- 054530.191
manner where states are earlier in time,

054530.432 -- 054532.442
denoise them first, into these, into

054532.442 -- 054534.512
these clean images, And then later on,

054534.512 -- 054536.392
you then denoise or simulate

054536.692 -- 054538.891
these later frames. With higher

054538.892 -- 054541.292
levels of noise. So by so by training your diffusion

054541.292 -- 054543.152
process with two axes where you have

054543.392 -- 054545.471
different levels of noise at every single time step,

054545.712 -- 054547.871
you can now denoise or

054547.871 -- 054549.731
simulate the model in an autoregressive

054549.952 -- 054552.112
manner. And

054552.112 -- 054554.532
what we find with this type of autoregressive simulation

054555.222 -- 054557.162
is that it allows you to do pretty long horizon rollouts.

054557.492 -- 054559.572
So at this so here it allows you to move

054559.572 -- 054601.831
around in this, in this environment,

054602.062 -- 054604.402
and and and over time,

054604.462 -- 054606.602
like, be able to, simulate

054606.602 -- 054607.761
a very long time period.

054611.502 -- 054614.002
And and, when you do these very long horizon

054614.062 -- 054616.071
dynamics simulations, what you'll find is that there's

054616.072 -- 054618.222
still a bit of a blow up So what

054618.222 -- 054620.072
you find is as you run these simulations,

054620.392 -- 054622.552
as as time goes on for a very long time period,

054622.552 -- 054624.412
eventually you do get a lot of noise

054624.731 -- 054626.971
So additional thing you can do to try to get much

054626.972 -- 054627.952
more long horizon

054629.132 -- 054631.162
dynamic simulations to do some type of

054631.162 -- 054633.512
more low temperature like sampling based off your history,

054634.952 -- 054637.432
And and this is typically done in diffusion

054637.432 -- 054639.492
models through classifier free guidance. Where

054639.492 -- 054641.432
you combine these unconditional and conditional

054641.732 -- 054643.742
scores. We do,

054644.542 -- 054646.622
so so we, to to be able to do these

054646.622 -- 054647.762
long horizon simulations,

054648.722 -- 054650.701
where low temperature sampling was a history,

054650.942 -- 054653.041
we can actually, use this type of denoising

054653.101 -- 054655.492
procedure to be able to do this more effectively also.

054656.172 -- 054658.512
And the idea is, idea here

054659.441 -- 054701.661
is that, our diffusion model

054702.461 -- 054704.621
when trying to predict the future, which is

054704.621 -- 054706.744
shown here, with this

054706.744 -- 054708.972
idea with this diffusion forcing technique,

054709.332 -- 054711.412
is trained on the history corrupted with

054711.412 -- 054713.432
different noise levels. So

054713.432 -- 054715.032
to so in order to predict

054715.672 -- 054717.692
predict the future, given

054718.152 -- 054720.172
given a pretty long horizon set

054720.172 -- 054722.541
of history, you do is you simply just corrupt

054723.002 -- 054725.172
the history with different levels of noise, whether you

054725.172 -- 054727.531
corrupt it with full noise, a small amount

054727.532 -- 054729.632
of noise, or like, or a short amount

054729.632 -- 054731.492
of history, or over low frequency data.

054731.652 -- 054734.052
And this allows you to essentially decompose

054734.831 -- 054736.911
your generation into these different noise corrupted versions

054736.912 -- 054738.822
of your history. Allowing you to

054739.062 -- 054741.042
avoid going out of distribution as much

054741.122 -- 054743.282
your history is very long. Because now you've decomposed

054743.282 -- 054745.011
it into things that are a bit more familiar.

054745.572 -- 054747.371
And what we find is that by,

054747.772 -- 054750.252
by by adding different levels of noise to history and then composing

054750.252 -- 054752.732
them, we can now do something similar to Classroom

054752.792 -- 054754.862
Fire Free Guidance, along the history

054754.862 -- 054756.941
dimension. And what this allows us to do is it just

054756.942 -- 054759.382
allows us to build these models that can very

054759.441 -- 054801.682
smoothly run over a very long time period

054801.682 -- 054803.750
of time. Allowing you to run very long

054803.751 -- 054804.482
horizon simulations.

054809.251 -- 054810.531
And one thing you'll note is,

054811.262 -- 054812.621
for a lot of these simulations,

054813.342 -- 054815.552
if you look at, if you look at some trajectories and you

054815.552 -- 054817.771
look back, suddenly the entire scene changes. So there's

054817.772 -- 054819.852
still some issues on trying to really build

054819.852 -- 054822.262
a bit more, consistent three d memory into these

054823.532 -- 054825.692
And and we have some preliminary work at at the

054825.692 -- 054827.801
NeurIPS Conference on trying to build this. But I

054827.801 -- 054829.821
think in general this is a very interesting direction.

054829.822 -- 054831.952
Of work. You can imagine trying to build

054831.952 -- 054834.156
something more like a three d three d and

054834.157 -- 054836.322
persistent to represent the world state

054836.322 -- 054838.323
over time. Then there's the question of how do you

054838.324 -- 054840.482
actually simulate the world state world state

054840.482 -- 054842.672
when the world itself is also dynamically changing.

054845.362 -- 054847.472
Okay. So that that was about how like

054847.472 -- 054849.632
these video models can be used more as a

054849.632 -- 054851.972
dynamic simulator. Finally, I'll talk a bit about

054852.112 -- 054854.092
how we can also in a hybrid

054854.152 -- 054856.502
hybrid way, actually combine these dynamics

054856.502 -- 054858.891
models, with other, vision language models

054858.892 -- 054901.028
to do more, long horizon tasks or

054901.028 -- 054903.161
do tasks that require a bit of both high

054903.162 -- 054905.201
level reasoning as well as low level visual

054905.202 -- 054905.592
reasoning.

054907.912 -- 054909.912
And and one idea here is that

054910.072 -- 054912.231
let's say we want to generate some very long horizon

054912.231 -- 054914.231
video generation. We want to generate

054914.232 -- 054916.392
a very long horizon video task for

054916.452 -- 054918.531
something that we, that for for some goal that

054918.532 -- 054920.792
we want to reach. Where we don't really know

054920.852 -- 054922.772
exactly what we should do, reach that goal.

054923.092 -- 054925.291
What we can do is we can really try to combine

054925.532 -- 054927.771
both the vision language model as well as the video

054927.772 -- 054930.112
model together solve these long horizon

054930.172 -- 054932.322
tasks. So as a

054932.322 -- 054934.332
hypothetical example, let's say that I

054934.332 -- 054936.447
want to solve this task of putting all the fruits

054936.447 -- 054938.602
in the top drawer. What I

054938.602 -- 054940.922
can do is I can try to ask a vision language

054940.922 -- 054942.942
model, predict what I should do in this setting.

054942.942 -- 054945.242
So given this image on the left, the model

054945.242 -- 054947.402
can predict to either place the banana in the top

054947.402 -- 054949.392
drawer, or it can predict how,

054949.552 -- 054950.751
to open the top drawer.

054951.872 -- 054953.742
Now the issue is the vision language model

054954.362 -- 054956.442
isn't really able to fully understand the physics of the scene, so

054956.442 -- 054958.572
it doesn't know which of these instructions

054958.632 -- 055000.162
should be the one that should be chosen.

055001.121 -- 055003.461
But you can what you can do is you can simply

055003.792 -- 055005.871
take each of these language instructions and

055005.871 -- 055007.902
have the video model simulate how

055007.902 -- 055009.982
the road will execute, will change if you do this

055009.982 -- 055011.992
task. So if you try to put the banana in the

055011.992 -- 055014.142
top drawer, because the top drawer is not open,

055014.142 -- 055015.762
it will simply drop the banana.

055016.562 -- 055018.632
And in contrast, if you try to

055018.632 -- 055020.711
open the drawer, it will actually correctly open

055020.711 -- 055021.211
the drawer.

055023.452 -- 055025.722
And now you can use the vision language model

055026.062 -- 055028.382
to take each of these video plans and estimate

055028.382 -- 055030.480
how much progress they're making. And

055030.481 -- 055032.502
estimating whether this goal is

055032.502 -- 055034.912
the goal you want to reach is a simpler

055034.972 -- 055037.052
task for the VLM than actually generating

055037.052 -- 055039.182
the plan, it's just simply trying to verify

055039.182 -- 055041.252
whether the final state reaches

055041.252 -- 055043.522
a state that it it wants to reach. This

055043.522 -- 055045.782
allows you to essentially, help the VLM

055046.062 -- 055048.142
understand or estimate what the progress in

055048.142 -- 055049.391
the physical world will be.

055050.191 -- 055052.132
So once you have all three of these components,

055052.432 -- 055054.602
now you can do an overall tree search to

055054.602 -- 055056.271
get a very long horizon task

055056.832 -- 055058.912
that depicts solving this task. So what

055058.912 -- 055101.202
you can do is in the first time steps, you can

055101.202 -- 055103.282
do what I, what we showed earlier, where the vision language

055103.282 -- 055105.332
model predicts two actions. Either opening

055105.332 -- 055107.772
the top drawer placing the banana in the top drawer.

055107.782 -- 055109.882
Would estimate some progress on each

055110.342 -- 055112.371
of these states, choose the one that has the most progress. And

055112.371 -- 055114.362
then from that last state, repeat

055114.502 -- 055116.711
the process having the video, vision, language model predict

055116.712 -- 055118.772
an action to execute. Select the most

055118.772 -- 055121.152
promising one, and repeat it over the

055121.212 -- 055123.692
entire entire time period until you get an overall

055124.152 -- 055126.151
long horizon video plan for solving this task.

055129.732 -- 055131.972
And then after you have this video plan, you can also

055131.972 -- 055134.152
convert it to corresponding real robot execution.

055138.292 -- 055139.432
I'll skip the rest.

055140.352 -- 055142.832
And, and here's a here's an actual illustration

055142.832 -- 055144.292
of this, in more detail.

055145.022 -- 055147.522
So here here we have, the the two, two fruits.

055147.722 -- 055149.582
And we can see that this approach is able

055149.802 -- 055151.872
to get this video plan. And from the video plan, it's

055151.872 -- 055153.722
actually able to infer a set of actions

055153.882 -- 055155.382
to execute in the environment

055155.942 -- 055158.102
so that you can, place both of these fruits in

055158.102 -- 055200.132
the drawer, also finally close

055200.132 -- 055200.701
the drawer.

055204.602 -- 055206.762
And here's another example of this. So here we

055206.762 -- 055208.972
want to we have all these blocks. And we really

055208.972 -- 055211.241
want to move all these blocks to form a line.

055211.882 -- 055213.962
And this procedure is able to dynamically generate

055213.962 -- 055216.341
the sequence of plans for moving the blocks

055216.582 -- 055218.202
allowing us to get this,

055218.672 -- 055220.942
overall synthetic, video

055220.942 -- 055223.022
sequence plan of moving these blocks precisely to

055223.022 -- 055224.282
form this overall line.

055228.902 -- 055230.842
And you can also convert it to a corresponding

055231.062 -- 055232.122
real robot execution.

055242.652 -- 055244.891
And what we find with these pretty long horizon

055244.892 -- 055247.091
tasks is this type of planning you

055247.092 -- 055249.211
can do by using a combination of the video

055249.212 -- 055251.062
model as well as the vision language model,

055251.222 -- 055253.302
works a lot better than these end to end

055253.302 -- 055255.487
approaches, where, like, where you

055255.487 -- 055257.542
just have a VLA model or even just

055257.542 -- 055259.702
you have a semi single video model that just directly

055259.702 -- 055301.642
predicts what to do, because,

055301.762 -- 055303.962
this these before these long horizon tasks,

055304.042 -- 055306.202
this type of planning allows you to more carefully

055306.202 -- 055308.391
reason about the actions to execute so

055308.392 -- 055310.722
that you can directly and finally solve your overall task.

055311.922 -- 055314.081
And the final thing I'll talk a bit about is how we can

055314.081 -- 055316.262
also use a combination of these video models

055316.262 -- 055318.442
and DLMs to really, like,

055318.442 -- 055320.702
improve the spatial reasoning or physical reasoning

055320.992 -- 055323.072
of these vision language models. And as an

055323.072 -- 055325.572
example here is, we have this question

055325.802 -- 055327.471
I sit on the couch on my right,

055328.032 -- 055330.091
and I face the chairs, the kitchen be on my right or

055330.092 -- 055332.182
left And this is a question that is pretty important

055332.182 -- 055334.602
if you want a physical robot to navigate in your environment.

055334.832 -- 055336.912
But tends to be something that is

055336.912 -- 055339.142
very challenging existing vision language models that can't really

055339.142 -- 055340.682
reason for effectively spatially.

055341.332 -- 055343.412
But here, you can again use the video model as

055343.412 -- 055345.472
a way to really help you solve this

055345.472 -- 055347.502
task. Where the idea is you can use a

055347.502 -- 055349.442
video model as some type of mental

055349.662 -- 055351.722
imagination that can imagine how

055351.722 -- 055353.762
how the environment would change if you were

055353.762 -- 055356.142
to navigate inside of it. And by taking

055356.142 -- 055358.461
these, imaginations of how the environment would

055358.461 -- 055400.612
change, this imagination

055400.672 -- 055401.342
in context

055402.782 -- 055405.021
can then be used as data to allow the VLM

055405.022 -- 055407.042
to accurately answer what to do in the environment.

055409.132 -- 055411.112
So So here's the graphic illustrating this.

055411.272 -- 055413.432
So here I have the question, if I sit on the couch and face

055413.432 -- 055415.502
the chairs, will the kitchen be on my right or

055415.502 -- 055417.521
left And now, essentially,

055417.522 -- 055419.492
this what this procedure does is a v

055419.572 -- 055421.811
is it uses a VLM to

055421.812 -- 055423.912
estimate what would be useful actions in this

055424.212 -- 055426.511
environment so, like, so estimates these

055426.512 -- 055428.572
trajectories here. That that might be

055428.572 -- 055430.612
useful for solving the task. Then we

055430.612 -- 055432.312
have this video model be its imagination

055432.662 -- 055433.882
it takes each of these trajectories,

055434.902 -- 055436.647
imagined visuals

055437.012 -- 055439.081
these environments. And now once you have these

055439.081 -- 055441.202
imagined visuals, the

055441.202 -- 055443.542
VLM can take, take this additional context

055443.732 -- 055445.892
and use it to reliably answer this final

055445.892 -- 055447.671
question. So you're using this,

055447.912 -- 055450.311
video generation model as a source of spatial common

055450.312 -- 055452.662
sense that you can then augment the language

055452.662 -- 055454.801
model with. To accurately answer this question.

055457.242 -- 055458.842
And then this is just a more detailed,

055459.481 -- 055501.442
GIF showing this. Where

055501.662 -- 055503.742
essentially the the model predicts the sequence of

055503.742 -- 055505.762
trajectories to execute. Also have a bit

055505.762 -- 055507.812
of a beam search procedure to find the

055507.812 -- 055509.972
right set of actions to execute in the environment to solve

055509.972 -- 055512.242
this task. But we just choose the most relevant

055512.242 -- 055514.322
ones and use them to to solve the

055514.322 -- 055516.552
final trajectory in this task.

055517.912 -- 055520.072
And overall, what we find is that when you have this

055520.072 -- 055522.371
type of spatial imagination, you get a pretty

055522.371 -- 055524.441
large boost in performance. Compared to

055524.441 -- 055526.592
when you don't. Across a wide variety of

055526.652 -- 055528.672
models from GPT, o one,

055528.932 -- 055531.282
these open source vision language models. So essentially,

055531.282 -- 055533.302
these video models provide a very useful

055533.302 -- 055535.262
source of common sense that's orthogonal

055536.322 -- 055538.369
to the high level of reasoning abilities of

055538.369 -- 055539.141
the language model.

055540.501 -- 055542.581
So overall in this talk, I've talked about a

055542.581 -- 055544.672
couple of different ways in which

055544.672 -- 055546.932
how in which video models can really be seen

055547.232 -- 055549.421
to help robotics. At first, they can

055549.422 -- 055551.502
be seen as these high level planners that allow you

055551.502 -- 055553.671
to imagine what it would mean to

055553.672 -- 055555.721
accomplish a task. Second, you can

055555.722 -- 055558.061
use these models, as a type of simulator

055558.282 -- 055600.331
to simulate or to or to

055600.331 -- 055602.492
find the right set of actions to execute something

055602.492 -- 055604.722
in the environment And finally, you can

055604.942 -- 055607.071
really combine them with with a high level

055607.072 -- 055609.222
reasoning from vision language models to jointly

055609.222 -- 055611.302
solve these complex tasks that require both a

055611.302 -- 055613.541
bit of high level reasoning as well as a bit of

055613.542 -- 055615.932
low level or spatial or visual

055616.072 -- 055618.192
reasoning. Thanks everyone for listening and happy to take

055618.192 -- 055618.841
any questions.

055627.351 -- 055629.131
Thank you, professor, for the

055629.372 -- 055631.412
for the fantastic talk. I really learned a lot from it.

055632.132 -- 055634.211
You mentioned that using video models for

055634.211 -- 055636.402
robotic control and you said that it's

055636.402 -- 055638.601
better at instructive following. But

055638.601 -- 055640.852
I really don't understand if it's necessary

055641.152 -- 055643.632
to predict the whole future, the video frames

055644.742 -- 055646.752
actions that could be spontaneous for human

055647.312 -- 055649.392
What's your take on that Thank you. Yeah. Thanks for the

055649.392 -- 055651.461
question. So I think for a lot of tasks, it can

055651.462 -- 055653.762
be helpful to imagine it would

055653.762 -- 055655.782
mean to accomplish the whole task. But then, of course, there

055655.782 -- 055657.861
are many actions where maybe you would not want to

055657.862 -- 055659.992
imagine the whole thing. Right So certain actions, maybe

055659.992 -- 055701.692
it's a bit more reactive and you just have

055702.012 -- 055704.171
policy for it. But I think for a for a certain class of tasks,

055704.172 -- 055706.362
it is very helpful to imagine what it would mean to

055706.602 -- 055707.422
do the entire scene.

055709.182 -- 055711.252
Yeah. Hi. Hi. Thank you for the talk.

055711.412 -- 055713.492
I have question about using video

055713.492 -- 055715.652
word model for planning. So

055715.652 -- 055717.792
do you think that it's gonna be

055717.792 -- 055719.992
break if I

055719.992 -- 055721.822
use this planning for

055722.462 -- 055724.612
highly sarcastic or

055724.672 -- 055726.911
very dynamic environment. So

055726.912 -- 055728.932
because I suspect that first

055728.932 -- 055731.092
of all, it takes a lot of time to do or

055731.092 -- 055733.152
search into to planning using

055733.152 -- 055735.312
video word model, especially when you generate

055735.312 -- 055737.492
frame by frame. And I suspect that it's,

055737.732 -- 055739.752
this gonna break in, like, real

055740.242 -- 055742.582
live environment when you have a lot of, changing.

055742.987 -- 055743.371
Element

055745.022 -- 055747.152
And yeah. Yeah. That's

055747.152 -- 055749.231
all. Yeah. Yeah. Thanks for the question. No. I I think you're

055749.232 -- 055751.342
right. I think for a very dynamic environments,

055751.582 -- 055753.902
it can be hard for the video models to operate

055753.902 -- 055755.932
at a quick enough frequency that you can

055755.932 -- 055758.012
actually respond to the dynamic changes. Like, it

055758.012 -- 055800.052
could be the case that the time the video

055800.052 -- 055802.482
model generates these frames, already too late for you

055802.621 -- 055804.672
to accomplish the dynamic task. I think in

055804.672 -- 055806.822
these settings, might want to use

055806.822 -- 055809.062
a more compact representation of the state, like

055809.062 -- 055811.082
maybe some lower dimensional latent that

055811.082 -- 055813.352
allows you to be a bit more reactive. I mean,

055813.412 -- 055815.572
another thing issue with dynamic tasks is all the video models I've

055815.572 -- 055817.582
talked about so far just in the image space. So they

055817.582 -- 055819.762
don't they don't have any information about contact

055819.782 -- 055821.944
or forces. I think it would be interesting also

055821.945 -- 055824.232
to think about how you can integrate these information

055825.082 -- 055827.242
into the models to more accurately solve these more

055827.242 -- 055829.382
dexterous or dynamic tasks. Thank you.

055832.592 -- 055833.861
Professor. Thank you for the talk.

055834.752 -- 055836.822
Earlier you mentioned that some

055836.822 -- 055838.952
of these they may not have,

055838.952 -- 055841.142
like, object permits. I was wondering

055841.142 -- 055843.512
if these are dynamics models, if

055843.672 -- 055845.912
through different type of training would be able to eventually

055845.912 -- 055848.122
accomplish this Or if you would a

055848.382 -- 055849.512
three d belief or memory

055850.472 -- 055852.772
Yeah. I think it's, it's, not really

055853.012 -- 055855.412
at the moment. I think both are very interesting to explore.

055855.552 -- 055857.632
I think that the current models are still

055857.632 -- 055859.642
quite small, and maybe they they might be more

055859.642 -- 055901.801
similar to like what GPT one was like, many

055901.802 -- 055903.872
years ago. I think it's possible

055903.872 -- 055906.032
that with these much larger models, as well as much better,

055906.032 -- 055908.091
like, post training, they could already have

055908.092 -- 055910.412
very persistent objects. But then it's also

055910.412 -- 055912.631
the the case that let's say you're in a room and you really

055912.632 -- 055914.682
want to model to remember objects for a period of

055914.682 -- 055916.722
like two or three hours. It seems like having

055916.722 -- 055918.422
some type of spatial memory might be very helpful.

055919.142 -- 055919.782
Thank you.

055922.442 -- 055924.521
Thanks for the talk. How much compute does

055924.521 -- 055926.942
these video models add to the entire

055927.162 -- 055929.307
pie Like, if you didn't have the video module

055929.307 -- 055931.322
at all and try to do it with,

055931.322 -- 055933.372
I don't know, BLM or whatever, How

055933.372 -- 055935.422
much does this add to it and what video

055935.422 -- 055937.542
models are you using here Yeah. So

055937.542 -- 055939.712
for a lot of the examples I showed, especially

055939.712 -- 055941.772
in planner section, we're basing it off

055941.772 -- 055943.851
the one two point one model. We are

055943.852 -- 055946.092
using that 14,000,000,000 parameter model.

055946.172 -- 055947.952
So to train it, it did take around

055948.282 -- 055950.521
16 or 32 GPUs, so it is quite

055950.522 -- 055952.682
a bit more expensive. I think that,

055952.922 -- 055955.021
in the future, maybe when we have more powerful models,

055955.072 -- 055957.231
could imagine that maybe you just have to learn a very

055957.232 -- 055959.352
small, like, adapter on top of them, and then maybe

055959.352 -- 060001.432
you only need, like, a couple GPUs. But at

060001.432 -- 060003.202
the moment, it is a little more expensive.

060003.682 -- 060005.712
Was it a division model It was a

060005.712 -- 060006.692
diffusion model. Yes.

060008.532 -- 060010.232
Hi. Thanks for the great talk.

060011.172 -- 060013.250
Video generation involves going from

060013.251 -- 060015.632
low dimensional latence to high dimensional pixel

060015.632 -- 060017.842
space. And the perception is the opposite

060017.902 -- 060020.011
going compressing to some latent I'm wondering do

060020.012 -- 060022.157
you think it's necessary to go through pixel

060022.157 -- 060024.247
space Or can we just cut through the

060024.247 -- 060026.441
latence Yeah. I think that's a great question. And I think

060026.442 -- 060028.468
in a sense, yeah. When you try

060028.469 -- 060030.490
to predict all the pixels, you're wasting a lot of

060030.491 -- 060032.542
your time because maybe, like, for, like, robotics,

060032.542 -- 060034.802
you don't care exactly what the precise textures

060034.942 -- 060037.112
are. Think the reason why videos are nice and

060037.112 -- 060039.282
pixels are nice is at least it does, when

060039.282 -- 060041.272
you have some type of handcrafted latent space,

060041.352 -- 060043.592
you don't know what you're missing, I guess. So, like, maybe

060043.592 -- 060045.862
you trained your latent space on a couple of tasks, and

060045.862 -- 060047.942
something and these things, and, and maybe the

060047.942 -- 060050.150
texture didn't matter, then there might be one task

060050.151 -- 060052.351
where the texture really does matter. So I feel

060052.351 -- 060054.572
that images are a complete space where you don't

060054.752 -- 060056.771
lose information per se. So in that sense, they're better.

060056.772 -- 060058.852
But like for many specific tasks, it may be better

060058.852 -- 060100.392
to have a more compact latent space.

060100.871 -- 060101.742
Thank you. Thank you.

060103.501 -- 060105.631
What you have just shown are mostly

060106.592 -- 060108.912
What When it comes to, like, deformable object,

060108.912 -- 060111.097
what do you think about the word model Can

060111.097 -- 060113.267
do Yeah. That's a great question. So I think,

060113.455 -- 060115.602
I I would imagine that you could also do things

060115.842 -- 060117.912
are somewhat deformable. I guess on the planner setting,

060117.912 -- 060120.071
I did show a couple of things like picking up that coffee meeting

060120.072 -- 060121.732
and so forth where there was it was a bit

060122.292 -- 060124.452
deformable. I think that the control problem gets a

060124.452 -- 060126.661
little harder. When you have these deformable things, and I

060126.662 -- 060129.002
think you really have to have a good learned policy.

060129.002 -- 060131.082
I think, as long as you have a good learned policy, I

060131.082 -- 060132.672
think it should be okay also.

060133.232 -- 060134.522
Okay. Thank you. Thank you.

060136.042 -- 060138.022
Thank you very much for your fantastic

060138.662 -- 060140.282
talk. And let's give

060141.172 -- 060143.441
pause to our speaker, Yilun Du.

060145.592 -- 060147.952
Well, let me introduce

060148.092 -- 060150.592
our next speaker from Google DeepMind,

060150.812 -- 060152.331
Jenny team. And,

060152.982 -- 060155.472
Philip and Stephen

060155.692 -- 060157.152
Spencer from Google DeepMind.

060157.902 -- 060200.142
Oh, you are here. We are looking for we

060200.142 -- 060202.402
are looking for you for a long time. Okay.

060205.322 -- 060206.232
Oh, yeah.

060209.942 -- 060211.172
So just plugging this in.

060251.622 -- 060252.122
Oh,

060253.942 -- 060255.321
I'm sorry. There.

060321.081 -- 060321.562
It's not

060324.541 -- 060325.221
and I'm like,

060329.912 -- 060331.122
Can you

060332.212 -- 060333.672
Can you connect the conference

060340.322 -- 060342.172
Yes. Should

060350.202 -- 060351.812
The second I come on stage, this

060404.621 -- 060407.032
I think so. Yeah, we the presenter

060407.032 -- 060407.532
view.

060442.052 -- 060442.632
I mean,

060444.822 -- 060445.912
so bad.

060455.302 -- 060456.412
Subscriptions on point

060501.202 -- 060501.391
But

060503.222 -- 060505.532
Yeah. I don't know. It's just not showing

060506.782 -- 060508.631
Yeah. Because you're using the

060509.777 -- 060511.222
the extension screen.

060513.042 -- 060513.882
Now I can set

060516.052 -- 060518.437
because you want this speaker note Notes.

060526.322 -- 060526.631
Okay.

060528.392 -- 060530.562
Awesome. Okay.

060559.862 -- 060601.462
I mean, how are we gonna do the speaking

060602.342 -- 060604.052
notes We should just do it live.

060609.512 -- 060611.002
Yeah. I don't really know where they

060617.892 -- 060618.392
Okay.

060620.802 -- 060623.221
It's just honestly,

060623.222 -- 060625.242
should we just use your laptop Yeah. Yes.

060626.662 -- 060628.002
With this.

060804.162 -- 060805.092
Got it.

060808.862 -- 060809.922
Can you help

060811.322 -- 060812.382
I can try.

061011.152 -- 061013.092
Alright. Sorry about that long delay.

061014.272 -- 061015.112
We figured it out.

061017.972 -- 061020.052
So, yeah, I, I'm here today to talk

061020.052 -- 061022.211
about, Genie with Phil. So

061023.092 -- 061025.552
I'm Steven, this is Phil. We

061025.612 -- 061027.851
both worked on G and E three and also on

061027.851 -- 061030.081
G and E two. But,

061030.082 -- 061031.862
of course, we weren't alone.

061032.482 -- 061034.492
This is the Genie team. I'd

061034.492 -- 061036.672
just like to acknowledge, it was quite a large

061036.732 -- 061038.992
team that together to generate Genie three.

061039.392 -- 061041.432
And without the contributions of all these

061041.432 -- 061043.222
people. None of it would have been possible.

061044.981 -- 061047.471
So just to give you an idea of what I'm going to talk about,

061047.552 -- 061050.052
I'm going to try and put Genie in context,

061050.592 -- 061052.552
tell you why DeepMind

061053.012 -- 061055.341
cares about world models and why we're working on them,

061056.792 -- 061059.032
talk to you about the beginnings of

061059.351 -- 061101.512
the Genie three project and then move on

061101.512 -- 061103.612
to the capabilities of the model

061103.612 -- 061105.871
that we discovered, after we trained the model,

061106.032 -- 061108.112
And then I'll hand over to Phil, and he will talk more

061108.112 -- 061109.901
about the possibilities of

061110.142 -- 061111.822
using world models for training agents.

061115.002 -- 061116.782
I'm sure you're all aware of DeepMind's

061117.402 -- 061119.562
success in training agents in

061119.782 -- 061121.412
simulated environments from Atari

061121.856 -- 061124.262
to AlphaGo to

061124.322 -- 061126.362
Starcraft And these

061126.362 -- 061128.762
allow agents to kind of train on their own experience

061128.762 -- 061130.831
and learn interesting things about the

061130.831 -- 061133.181
world the famous move 37

061133.182 -- 061134.802
in, the Lisa doll game

061135.502 -- 061137.591
as prime example of that.

061139.512 -- 061141.942
But we also, train

061143.242 -- 061145.412
agents in simulated

061145.472 -- 061147.882
environments like these physical

061148.262 -- 061150.552
simulations of the world. So this is some fun

061150.552 -- 061152.652
early work from 2017

061153.422 -- 061155.602
training, kind of human locomotion

061156.252 -- 061158.342
in Mujoko, which is a

061158.902 -- 061200.282
a kind of a physics simulator.

061202.252 -- 061203.792
I love these. I love these videos.

061205.912 -- 061208.151
They're crazy, but they they get the job done.

061211.142 -- 061213.222
And lastly, this is a piece of

061213.222 -- 061215.262
work, this environment is called X

061215.262 -- 061217.291
land. And this is a

061217.292 -- 061218.752
a very kind of configurable

061219.732 -- 061221.961
environment for training agents. You can see all

061221.962 -- 061223.822
of this, all of the, the

061224.282 -- 061226.361
setting of the world can be configured as well as

061226.362 -- 061228.512
the tasks. That you can convey

061228.512 -- 061229.972
to the agents. And so this was

061230.852 -- 061232.891
a piece of work called adaptive agents

061233.532 -- 061235.731
and you can train models in

061235.732 -- 061237.782
these kind of more diverse settings with

061237.782 -- 061239.821
more diverse tasks. And

061239.822 -- 061241.962
show that you can kind of solve these tasks

061241.962 -- 061244.007
more efficiently than humans in this in this

061244.007 -- 061246.232
setting. Just to kind of

061246.232 -- 061248.392
summarize here, we

061248.392 -- 061250.472
train agents in in real games. We

061250.472 -- 061252.652
train agents in kind of physics simulators.

061253.252 -- 061255.422
And we make our own environments to try and

061255.822 -- 061257.092
broaden out the set of environment

061259.132 -- 061259.632
But

061301.312 -- 061303.212
in recent years, these

061303.432 -- 061305.672
these foundation models, these large LLM models

061305.672 -- 061307.432
seem to be having a huge impact

061307.831 -- 061309.992
in many different kind of real world domains.

061310.152 -- 061312.262
And so it led us to start thinking, well,

061313.032 -- 061315.482
can we train foundation models to

061315.762 -- 061317.981
generate a kind of unlimited number

061317.982 -- 061319.882
of environments with

061320.922 -- 061323.042
rich, agent experience as a

061323.042 -- 061325.101
way of kind of massively expanding

061325.401 -- 061327.502
the range and diversity of environments

061327.562 -- 061329.912
that we can use to train embodied

061330.132 -- 061330.702
AI agents.

061333.501 -- 061334.961
I'd just like to say,

061336.092 -- 061338.332
just to sort of be more specific about what a world

061338.332 -- 061339.452
model is because I think,

061340.342 -- 061342.502
people use the term to mean different things in

061342.502 -- 061344.352
different contexts. So at

061344.732 -- 061346.811
least for us and the kind

061346.812 -- 061349.032
of world models team, at GDM,

061349.542 -- 061351.751
this is the the definition that we prefer.

061351.752 -- 061353.652
So and specifically,

061353.791 -- 061356.211
this is an action conditioned model. So

061356.422 -- 061358.371
this is quite important, we think, for

061359.582 -- 061401.642
creating models that allow agents

061401.642 -- 061403.791
to interact with and

061405.142 -- 061407.232
have effects on on the world

061407.232 -- 061409.462
that they're they're operating in.

061410.262 -- 061412.422
Also pointing out that this is sort of like the original

061412.422 -- 061414.451
definition from 1990

061414.742 -- 061416.732
Schmidt Huber and then carried on

061417.212 -- 061417.661
in further work.

061421.772 -- 061423.692
Yeah, just to give you this is a picture of,

061424.572 -- 061426.902
what happened in in 2024.

061428.002 -- 061428.502
And

061430.222 -- 061432.471
I think he's trying to paint the picture of

061433.817 -- 061435.732
the the idea of world models

061435.872 -- 061437.922
kind of slowly gaining pace throughout

061437.982 -- 061439.222
2024. So

061440.021 -- 061443.021
Genie one was published at the 2024.

061444.572 -- 061445.471
Then there were other

061446.751 -- 061448.992
world models that were released and announced

061448.992 -- 061451.082
throughout period from World Labs,

061451.082 -- 061453.382
Oasis, NVIDIA Cosmos, as

061453.382 -- 061455.062
well as kind of various

061455.462 -- 061457.782
hype tweets from around the timeline

061457.782 -- 061500.172
from kind of the larger players. So it

061500.232 -- 061502.392
definitely felt at the end of last year

061502.392 -- 061504.192
that this was a active

061504.552 -- 061506.732
research direction, And,

061506.812 -- 061508.112
yeah, we released Genie

061509.312 -- 061510.902
two in December year.

061517.682 -- 061519.862
So Genie two was a model we trained

061521.242 -- 061524.242
and announced towards the 2024.

061525.882 -- 061527.962
It's a interactive world model.

061527.962 -- 061530.042
You can interact with it

061530.282 -- 061532.052
through kind of navigation actions.

061533.252 -- 061535.412
Point out that it's not real time because Genie three

061535.412 -- 061537.772
is real time, but it was it's a sort of a turn based

061539.462 -- 061541.542
and it can simulate a broad range of three

061541.542 -- 061543.412
d environments. This is just one example, but

061543.652 -- 061545.742
you go to the blog, you can see quite a broad

061545.742 -- 061547.842
range of different environments that it can

061547.842 -- 061550.022
generate. Conditioned on an an

061550.022 -- 061550.682
initial image.

061552.102 -- 061553.661
But they are all kind of

061554.782 -- 061557.021
not real world examples. So you can see that

061557.021 -- 061559.262
they're kind of game like environment.

061603.392 -- 061605.632
I'd also bring attention to, a

061605.632 -- 061607.741
paper diffusion models of real

061607.742 -- 061610.191
time game engines. So this was published by

061611.021 -- 061613.052
these these authors, but I'm drawing attention to

061613.052 -- 061615.402
Shlomi Fruchta who is the lead of

061615.462 -- 061617.862
Genie three. So this,

061618.102 -- 061620.262
this is not somebody playing doom. This

061620.262 -- 061622.481
is somebody playing a diffusion model that has

061622.482 -- 061624.172
been trained on trajectories from Doom.

061624.892 -- 061626.922
So this I think, is very impressive because I

061626.922 -- 061628.672
can't tell the difference between this and doom.

061629.392 -- 061631.592
And this is fully interactive and also

061631.592 -- 061633.742
fully real time. So

061635.492 -- 061637.571
it's a very impressive result in a very

061637.572 -- 061639.992
narrow domain. So if you train these models,

061641.162 -- 061643.242
on a large amount of a very restricted

061643.242 -- 061645.449
domain, It's kind of a proof that

061645.601 -- 061648.037
we can generate very long

061648.452 -- 061650.552
consistent trajectories that don't degrade.

061651.282 -- 061653.362
So this was kind of a cool result and something

061653.362 -- 061654.371
we wanted to build on.

061656.852 -- 061658.931
DeepMind also has a video model called

061658.932 -- 061701.101
VO. Which is

061701.101 -- 061703.121
not really a world model. You can't

061703.332 -- 061705.612
interact with it except to prompt it. And

061705.612 -- 061707.851
it's not real time. It takes a long time to generate

061707.851 -- 061708.592
these videos.

061710.642 -- 061712.721
It would be nice if I could replay that video for

061712.722 -- 061714.892
you. But if you can if you can remember

061714.892 -- 061716.952
it, there's a sort of interesting smoke dynamics

061717.032 -- 061719.351
Two people smoking cigars and the smoke kind of drifts

061719.351 -- 061721.041
off in quite a realistic way.

061721.982 -- 061723.942
And I just as another example,

061724.461 -- 061726.442
we like to show for

061726.682 -- 061728.742
VO as a world model, So, I mean,

061728.742 -- 061730.902
if you look at this, it just feels like

061731.142 -- 061733.301
the model has understood something about the real

061733.302 -- 061735.642
world, about the dynamics of snow

061735.642 -- 061737.942
hitting rigid bodies It's not intended

061737.942 -- 061739.802
to be a world model, but it feels like

061740.022 -- 061742.342
it has some understanding

061742.342 -- 061743.572
of the real world in it.

061744.791 -- 061746.871
So we felt like all these three things were

061746.871 -- 061748.982
kind of proofs that we could bring them

061748.982 -- 061750.722
all together into a single model. So

061751.041 -- 061753.162
the goal for Genie three was to make

061754.282 -- 061756.382
a photo realistic by which we meant

061756.382 -- 061758.132
0720 p at the time, but also

061758.452 -- 061800.362
just high quality imagery.

061800.522 -- 061802.702
Real time, action controllable

061802.702 -- 061805.042
world model. And so we were specifically targeting

061805.282 -- 061807.341
being able to generate long

061807.342 -- 061809.282
trajectories that were consistent with themselves,

061810.922 -- 061813.231
And they didn't kind of degrade over time.

061813.712 -- 061815.782
Grow blurry, do sudden

061815.782 -- 061817.942
scene jumps to other parts of the world

061817.942 -- 061819.551
that stayed consistent throughout that time.

061822.896 -- 061824.782
And I guess,

061825.482 -- 061827.961
these guys okay. This is This is an example

061827.961 -- 061830.082
of, skipping forward to

061830.162 -- 061831.842
some of the results of Genie three. So

061832.802 -- 061834.892
I is a very broad range of

061834.892 -- 061837.152
the kinds of things that this model can generate.

061837.501 -- 061839.662
So we have kind of fantastic worlds down

061839.662 -- 061841.741
in the bottom right. But also

061842.302 -- 061844.561
quite convincing weather physics

061844.702 -- 061846.892
if middle video could

061846.892 -- 061849.062
load. I'll give it a minute.

061849.272 -- 061851.382
But you can see the wind acting

061851.382 -- 061853.461
on the trees, the the

061854.192 -- 061856.487
wind blowing the the water into the into

061856.487 -- 061857.981
the path of the road.

061858.702 -- 061900.792
As well as a bunch of other different environments.

061901.652 -- 061903.422
This is to give you an idea of the kind of

061903.972 -- 061906.052
breadth of

061906.052 -- 061907.132
generation that Genie can do.

061914.092 -- 061914.592
Struggling.

061916.992 -- 061919.071
Yeah. I like these videos. So this is a side by side

061919.072 -- 061921.322
of Genie two and Genie three. So on the left

061921.322 -- 061923.592
is, Genie two. And

061923.592 -- 061925.832
so we saved the image prompts we did for Genie

061925.832 -- 061926.632
two and

061928.072 -- 061929.912
played them through in Genie three just to see

061930.231 -- 061932.371
how far we've come. So as you can

061932.371 -- 061934.712
see, Genie two had a length generation

061934.932 -- 061937.072
of about sixteen seconds. So Genie

061937.072 -- 061939.151
three can certainly go much, much further than

061939.152 -- 061941.422
that. But also, doesn't

061941.422 -- 061943.742
suffer from the kind of blurring effects that we

061943.742 -- 061946.012
saw for longer generations in Genie two.

061946.732 -- 061948.602
And also the

061948.982 -- 061951.002
kind of persistence of the world

061951.002 -- 061953.212
is much increased. So if I think

061953.212 -- 061955.291
in a few minutes, we'll turn back around and that kind

061955.292 -- 061957.232
of tower that we started out at the beginning

061957.882 -- 062000.061
is still there and still looks pretty

062000.282 -- 062001.712
much like did at the beginning.

062005.382 -- 062007.542
This is another nice example. In Genie two, we were

062007.542 -- 062009.862
really happy that we could open doors. We thought this was

062009.862 -- 062011.952
a a fun way that

062012.032 -- 062014.202
to show that we'd kind of learned some

062014.632 -- 062016.712
interactions with the world. But I mean,

062016.712 -- 062019.172
if you notice, the new

062019.492 -- 062021.842
room that it generates is kinda janky and weird.

062023.172 -- 062025.171
With Genie three, we can use the,

062026.452 -- 062028.532
the the text prompting to describe,

062028.532 -- 062030.612
okay, you open this door, you go out into a kind of

062030.612 -- 062032.617
futuristic space ship

062033.377 -- 062035.392
bridge. So you can kind

062035.392 -- 062037.652
of wander around here. So the off screen generation

062037.952 -- 062040.072
is been significantly improved by being able

062040.392 -- 062042.792
prompt these models. And also the kind of reliability

062042.932 -- 062045.142
of object interactions been improved because

062045.382 -- 062047.541
you can, not only open a door once, but you

062047.541 -- 062049.621
can open the door twice, and I think even three

062049.621 -- 062051.711
times. There

062051.712 -- 062052.132
we go.

062056.592 -- 062058.792
Yeah. So I just point out some of

062058.792 -- 062100.982
the properties that we have

062100.982 -- 062102.691
kind of observed in Genie three.

062103.572 -- 062106.072
One is the generality. So these are just some fun

062106.772 -- 062108.802
generation. So this this

062108.802 -- 062110.882
is, I think the prompt is something like walking

062110.882 -- 062112.022
around a glacier lake.

062113.002 -- 062115.482
You plug that into Genie and you get this kind of very

062115.482 -- 062117.512
beautiful and almost hyper real

062117.512 -- 062119.772
kind of landscape. It's very

062119.772 -- 062121.832
aesthetic. And this

062121.832 -- 062124.002
is this is a member of the team just playing

062124.002 -- 062126.372
this in real time. This is just footage of

062126.432 -- 062128.472
somebody playing with the interactive

062128.692 -- 062130.752
demo walking around, just kind

062130.752 -- 062132.892
of looking around this kind of beautiful scenery.

062138.851 -- 062140.982
This is I feel

062140.982 -- 062143.382
very realistic. If you've ever walked around

062143.382 -- 062145.412
the countryside in The UK, you

062146.211 -- 062148.371
you will recognize this scene, including the

062148.371 -- 062148.871
puddles.

062150.552 -- 062152.892
Yeah. This is, very dear to my heart.

062153.271 -- 062155.312
And the really nice thing about this generation

062155.742 -- 062157.762
is that the model has some kind of

062157.822 -- 062159.942
understanding of who you are in the world.

062159.942 -- 062202.052
So look down and see our legs. We

062202.052 -- 062203.791
can also see some kind of,

062204.752 -- 062207.212
kind of cool modeling

062207.432 -- 062208.811
of the ripples and the puddles.

062211.371 -- 062213.031
But we've also retained

062213.512 -- 062215.202
an ability to generate, like,

062216.412 -- 062218.571
less realistic, more kind of game like world.

062218.572 -- 062220.722
So this is kind of pixel art

062220.722 -- 062223.132
style world where you're a

062223.192 -- 062225.021
knight and complete with a big

062225.662 -- 062227.461
red dragon walking along the scene.

062229.012 -- 062231.092
So this is just a couple of examples to show

062231.092 -- 062233.291
that, there's a very

062233.291 -- 062235.552
broad range of of of environments

062235.692 -- 062236.692
that we can generate.

062239.492 -- 062241.482
To touch on long horizon and memory. So

062242.482 -- 062244.642
first question anybody asks you, if you train one of

062244.642 -- 062246.708
these models is if I look away from something, and

062246.708 -- 062248.762
then look back, is it still the same I

062248.762 -- 062250.862
think people have a have an intuition

062250.922 -- 062253.211
that, memory is hard in these problems.

062253.212 -- 062255.412
So this is a very

062255.412 -- 062257.792
clear example of that. So try and remember

062257.972 -- 062300.252
red apple cup tree.

062301.132 -- 062302.832
So we kind of walk away

062303.252 -- 062305.331
into this little corner of this classroom

062305.332 -- 062307.352
and just look out the window for a bit.

062307.612 -- 062309.272
So this kind of slightly blank

062309.752 -- 062310.712
scene outside the window.

062312.112 -- 062314.270
And I think we sort of linger here for about

062314.271 -- 062315.572
forty or fifty seconds.

062316.472 -- 062318.062
Just to try and prove point.

062323.642 -- 062325.692
Alright. I think we're gonna go back to the

062325.692 -- 062326.772
black. Board now.

062328.132 -- 062329.592
Slightly grim little classroom.

062330.542 -- 062332.982
And there we go. Red apple cup. Tree.

062333.107 -- 062335.152
So it's a

062335.152 -- 062336.722
nice example that memory

062337.262 -- 062339.581
is, maintained across

062339.581 -- 062341.582
at least a minute. And I

062341.582 -- 062343.902
mean, I was pretty surprised at how high quality

062343.902 -- 062345.812
the regeneration. That black.

062346.132 -- 062348.242
Board was. And indeed, even the scene outside is still

062348.242 -- 062349.762
still there. So,

062351.042 -- 062353.061
that's addressing memory. Another

062353.062 -- 062355.382
example of memory is this cool video

062355.382 -- 062357.462
of kind of painting inside.

062357.462 -- 062359.021
So not only

062359.842 -- 062402.081
will we generate these paint marks and turn away

062402.081 -- 062404.302
and remember the shape of them, But it's also

062404.302 -- 062406.581
a really nice indication of being

062406.582 -- 062408.811
able to perform actions in

062408.812 -- 062410.872
the world have consequences

062410.932 -- 062413.101
in the world, and then have those consequences

062413.242 -- 062415.292
be persistent. And I think that's kind

062415.292 -- 062417.722
of like pointing at the utility

062417.862 -- 062420.111
of these world models for agent training.

062422.592 -- 062424.902
Where okay. So at this point, I'm gonna

062424.902 -- 062427.067
hand over. Phil to talk to you about Crumple World

062427.067 -- 062429.071
Event. Yeah.

062429.072 -- 062431.502
Thank you, Steven. Yeah.

062431.562 -- 062433.652
So the third we wanted to talk about

062433.652 -- 062435.912
or highlight was this kind of new thing for Genie

062435.972 -- 062438.192
three, which was prompt to world events.

062438.192 -- 062438.692
So

062440.402 -- 062442.902
concretely, obviously, you've seen

062443.312 -- 062445.472
like, standard generations where you can prompt it,

062445.472 -- 062447.502
and you can kinda get this scene, which is actually

062447.502 -- 062449.522
quite close to the office, which is walking along Regents

062449.742 -- 062451.921
Canal. And, yes, know, it's nice.

062451.922 -- 062454.162
It's a bit boring, but that's actually kinda how it is.

062454.162 -- 062456.091
There's not much that really happens along this canal.

062456.632 -- 062459.132
Of course, for agent training, we wanna generate counter factuals.

062459.192 -- 062501.572
We wanna rerun scenes that maybe also

062501.632 -- 062503.692
prepare for, like, long tail events. After all, these are, like,

062503.692 -- 062506.182
foundation models. They should have good understanding

062506.402 -- 062508.482
of things that aren't just within this context, but

062508.482 -- 062510.212
things from other, you know,

062511.542 -- 062513.471
areas that it's been trained on. So,

062514.262 -- 062516.422
in this case, we kinda wanna prepare agents

062516.422 -- 062518.712
for like random events that might happen. For instance,

062518.712 -- 062520.282
someone running by in a chicken suit.

062521.482 -- 062523.642
So there's exactly the same scene, but then halfway

062523.642 -- 062525.392
in, we kind of inject this prompt and

062525.791 -- 062527.632
very strange man going for a jog.

062527.872 -- 062529.092
Maybe it's for charity.

062531.871 -- 062534.022
You can also interact with or

062534.022 -- 062536.182
get drenched by really annoying commuters on

062536.182 -- 062537.442
the way. To work.

062539.842 -- 062541.902
So, yeah. We kinda again, yeah. Just injected

062541.902 -- 062543.981
this prompt halfway through and then boom. You kinda

062543.982 -- 062545.702
have this jet ski

062547.192 -- 062549.351
And, yeah. I mean, Game of Thrones might become reality,

062549.352 -- 062551.492
so we gotta prepare for that. So

062551.492 -- 062553.622
again, injecting this

062557.282 -- 062559.712
And kind of as Steven pointed out, there's this you

062559.942 -- 062602.302
know, not a real scene,

062602.462 -- 062604.621
but like you get this kind of like element of like

062604.621 -- 062606.692
realistic physics, particularly with the water, which we quite

062606.692 -- 062608.252
liked. We

062609.472 -- 062611.652
Some more examples would be like here,

062611.892 -- 062613.962
the skiing scene. So, of course,

062613.962 -- 062615.132
you can just interact

062616.001 -- 062618.081
like ski down the slope, as

062618.081 -- 062620.302
a standard, but of course, like many

062620.302 -- 062622.712
things can happen on the ski slope, for instance,

062623.502 -- 062625.652
another skier might appear wearing some merch

062634.452 -- 062636.552
He's gonna encourage us to go. Yeah.

062636.712 -- 062637.592
You very much.

062639.822 -- 062641.472
There we go. It's what we needed.

062643.072 -- 062645.082
But well, like, more sort of

062646.052 -- 062647.741
exciting things might be happening as well.

062700.032 -- 062702.271
And, yeah, we might wanna pick up a little care package

062702.271 -- 062704.082
as we down the slopes as well.

062712.912 -- 062715.091
And you notice the, the shadow cast there.

062715.427 -- 062717.782
Pretty neat even though it is a bit unrealistic.

062718.632 -- 062720.342
We missed out. Alright. Better let me start.

062722.222 -- 062724.082
So, yeah, I guess, why are we all here

062724.492 -- 062726.731
This workshop I think we truly think Genie

062726.732 -- 062728.992
three can help with agentic use cases.

062729.332 -- 062731.162
So, like, let's speak to that a bit.

062732.102 -- 062734.146
Of course, we need an agent to, like, roll

062734.146 -- 062736.291
out. In Genie, so this is where we

062736.291 -- 062738.438
turn to our amazing colleagues in the CIMR

062738.438 -- 062740.502
team you might be aware of some of their work.

062741.272 -- 062743.352
But effectively, they've trained

062743.352 -- 062745.462
and can say there's now Qasimma two is out, but

062745.462 -- 062747.842
effectively a Gemini backbone

062747.902 -- 062750.382
to specifically execute user

062750.577 -- 062752.572
commands within

062753.642 -- 062755.467
various different games that

062756.032 -- 062758.502
we've illustrated on the left there. What's

062758.502 -- 062800.662
particularly exciting is because we've moved over

062800.662 -- 062802.042
to this Gemini backbone,

062802.922 -- 062805.002
we're able to do this kind of multi turn and just

062805.002 -- 062807.321
like much more like dynamic interaction with

062807.322 -- 062809.331
the agent. So instance, you can see

062809.331 -- 062811.351
on the left, it's just like a single instruction

062812.364 -- 062814.392
on the right, you know, the

062814.392 -- 062816.631
SIMR agent starts talking to us. We can start querying it as

062816.632 -- 062818.542
well during the rollout.

062818.762 -- 062821.052
So what does this all

062821.052 -- 062823.351
mean for Genie Well, it'd be really

062823.351 -- 062825.292
nice, you know, as

062825.452 -- 062827.891
researchers we ought to ensure

062827.892 -- 062830.152
our model is generalized. So of

062830.292 -- 062832.312
course, like never seen these

062832.312 -- 062834.172
environments. This is not at all

062834.822 -- 062836.882
the, training set. But, we

062836.882 -- 062838.561
can kinda test out how

062839.002 -- 062841.122
adaptable the similar agents

062841.122 -- 062843.282
are to these brand new never seen before environments.

062843.282 -- 062845.231
This is a nice kind of I guess,

062845.552 -- 062847.711
awesome scene in New England. And, yeah,

062847.712 -- 062849.861
we can kind of query it as well.

062849.862 -- 062852.101
This is the kind of power of, like, training these, like, large

062852.101 -- 062854.207
foundation models. And it can kind of answer

062854.207 -- 062856.422
questions about the rollout and then perform tasks

062858.862 -- 062901.052
So that already

062901.052 -- 062903.211
in in and of itself, even if you don't think about,

062903.212 -- 062905.502
like, Genie creates a

062905.972 -- 062908.191
kind of this open ended, self

062908.192 -- 062910.302
improving loop whereby have these

062910.302 -- 062911.302
like agents

062912.572 -- 062914.731
that are Gemini and then you can get Gemini

062914.731 -- 062916.941
to also set tasks within these automatic automatically

062916.942 -- 062919.021
just based on the context like, oh, if there's a bench, maybe

062919.022 -- 062920.392
we should walk towards it. Right

062921.351 -- 062923.401
And then once a similar agent tries to execute

062923.402 -- 062925.532
that, we can then use Gemini to know,

062925.532 -- 062927.852
ask, like, well, was this a useful trajectory Did it actually

062927.852 -- 062929.762
perform the task that we said it should

062930.081 -- 062932.162
But we can actually expand the scope of this

062932.162 -- 062934.382
significantly greater by great basically going beyond, like,

062934.382 -- 062936.702
the training set, by having effectively

062936.762 -- 062939.000
Genie three being the environment rather than the

062939.001 -- 062941.222
training environments. And and again,

062941.222 -- 062942.041
we can introduce,

062943.312 -- 062945.552
Gemini as basically this environment setter.

062945.552 -- 062947.682
So not only kinda saying what tasks

062947.922 -- 062950.322
should you complete, but actually what should the environment

062950.322 -- 062951.421
be in the first place.

062953.182 -- 062955.272
And I think this we didn't really

062955.272 -- 062957.352
say this when we released it, but actually this

062957.352 -- 062959.672
was all done basically

063000.312 -- 063002.481
in, like, Gemini and

063002.702 -- 063005.001
CIMR and Genie Land. So we basically asked

063005.002 -- 063007.162
Gemini, like, what do you think would be useful,

063007.162 -- 063009.262
like, real world situations where

063009.502 -- 063011.822
maybe we'd need some, like, kind of robotic training

063011.822 -- 063013.882
data and it basically just proposed a load

063013.882 -- 063015.902
of environments. We got some first frames,

063016.152 -- 063018.412
out of, the Genie model. And then

063018.412 -- 063020.492
we said, okay, well, cool. Here's, like, the environment

063020.492 -- 063022.652
you've kind of asked for. Like, what do you think

063022.652 -- 063024.831
we should do in this environment And yet proposed

063024.831 -- 063027.151
a bunch of tasks, which the similar agent was then

063027.152 -- 063029.212
able to, to

063029.212 -- 063031.271
solve. And, yeah, there's kind of like

063031.432 -- 063032.012
I guess,

063036.602 -- 063038.542
purely agentic in sim interaction.

063038.842 -- 063041.242
And, yeah, here probably are aware of reinforcement

063041.461 -- 063043.561
learning. This kind of feels like reinforcement learning.

063043.952 -- 063046.202
You're kinda doing your rollouts. You're gathering your datasets,

063046.202 -- 063048.281
and maybe you can then do some sort

063048.282 -- 063050.712
of step of policy improvement on those generated

063050.852 -- 063052.932
trajectories of course, you can

063052.932 -- 063055.172
update certain trajectories because you can use Gemini

063055.172 -- 063057.202
to determine whether or not those trajectories were

063057.202 -- 063058.111
successful or not.

063101.232 -- 063103.122
So of course, there are limitations.

063103.742 -- 063105.842
You could kind of as you can see with the

063105.842 -- 063107.862
UI, the action space is quite limited.

063108.702 -- 063110.942
We're particularly excited about much more finer

063110.942 -- 063113.121
grained control schema. Particularly

063113.122 -- 063114.822
things around like dexterous manipulation.

063116.092 -- 063118.171
We also firmly believe that multi

063118.172 -- 063120.412
agent is very important. And what we mean by that

063120.412 -- 063122.497
is of course, we can simulate, like, other agents

063122.497 -- 063124.782
within Genie, but we can't really, like, necessarily

063124.842 -- 063127.342
control them apart from prompting. So getting the same level

063127.562 -- 063129.572
of control for multiple agents and having them all

063129.572 -- 063131.501
sort of exist within the same world model

063131.662 -- 063133.792
is, like, particularly exciting. And,

063133.852 -- 063135.901
of course, as kind of Steven was

063135.902 -- 063137.842
talking about, the interaction duration is great,

063137.982 -- 063139.642
but you know, we really want, like,

063140.121 -- 063142.062
multi hours of like, interaction.

063143.212 -- 063145.532
Like great persistence, but of course, that's

063145.532 -- 063147.621
both, like, pretty hard to serve something

063147.621 -- 063150.042
like that and also it's just like a general like memory

063150.652 -- 063152.892
like research topic. So, yeah, these are these

063152.892 -- 063154.952
are things we're deaf definitely like looking to, you

063154.952 -- 063157.121
know, work on and, think about

063157.121 -- 063157.502
a lot.

063159.362 -- 063201.422
So in summary, g three

063201.422 -- 063203.121
is a general purpose foundation

063203.582 -- 063205.731
world model that generates interactive three d worlds

063205.731 -- 063207.672
from simple text prompts in real time.

063208.872 -- 063211.032
We believe world models are on the path to

063211.032 -- 063213.082
AGI because they enable unlimited

063213.462 -- 063215.622
rich simulation environments for training

063215.682 -- 063217.782
embodied AI agents And,

063217.922 -- 063219.942
well, what are the implications of this Well, infinite

063220.002 -- 063222.411
training environments for agents. Really. Enabling

063222.412 -- 063223.852
open ended self improvement.

063225.692 -- 063226.762
Thank you very much.

063236.512 -- 063238.111
Thank you very much for your talk.

063238.992 -- 063241.231
Due to the time limitation, we only have one

063241.232 -- 063242.841
question for Jenny team.

063244.121 -- 063246.202
Thanks. Great work. You share a

063246.202 -- 063248.301
little about little bit about insight on

063248.302 -- 063250.782
how to prevent a compounded error when you're generating

063250.782 -- 063251.942
very long form video.

063259.501 -- 063301.581
Yeah. Look, very difficult

063301.582 -- 063301.982
problem.

063304.621 -- 063306.152
I will say you kind of

063306.792 -- 063308.871
when you scale these things up, right, you

063308.871 -- 063311.041
get things that are more sensitive in the output. But

063311.041 -- 063312.911
sometimes they become more sensitive in the input.

063313.472 -- 063315.422
So I think what's crucial

063316.037 -- 063318.092
is and, like, finding ways

063318.092 -- 063319.492
that you can ensure that

063320.682 -- 063322.831
models become more stable respect to the

063322.831 -- 063324.492
outputs that they generate So

063325.012 -- 063327.172
I don't think I can say much more than that, but, yeah,

063327.172 -- 063329.492
there are ways we can control the outputs

063329.492 -- 063331.442
while still remaining sensitive in the inputs.

063331.682 -- 063333.782
Thanks. I'm trying to understand

063334.002 -- 063336.202
that. Yeah. We can probably talk

063336.202 -- 063336.612
offline a bit.

063340.342 -- 063342.422
Okay. Thank you so much again for your

063342.422 -- 063344.672
amazing talk. And,

063345.452 -- 063347.802
for the next session, we will have

063348.362 -- 063350.382
our panel discussion about our workshop.

063351.162 -- 063351.642
So,

063353.812 -- 063356.292
please the panelist, can you

063356.292 -- 063358.272
come to the stage

063503.042 -- 063503.442
Hi,

063513.972 -- 063514.472
Josh,

063636.212 -- 063638.271
Hello, everyone. We Welcome to

063638.272 -- 063639.972
our panel discussion of our

063640.432 -- 063641.652
World Model workshop

063642.492 -- 063644.592
I'm Mung Yui Yang, the lecturer from

063644.812 -- 063646.932
University of Bristol. And I

063646.932 -- 063649.272
will be moderating this panel discussion.

063650.292 -- 063652.052
We have amazing lineup of,

063652.372 -- 063654.582
our panelists and I will just

063654.582 -- 063656.612
briefly introduce each

063656.612 -- 063658.851
one of this. So first

063658.851 -- 063700.872
one, Yi Leung Du from Harvard

063701.512 -- 063703.932
University and Jia Joon Woo from Stanford

063704.392 -- 063706.212
University. John Lefford from,

063706.532 -- 063708.872
Microsoft Research New York based.

063709.282 -- 063711.362
And, Lin Chao is our

063711.362 -- 063713.532
online member of our panelist.

063713.831 -- 063714.572
And also,

063715.912 -- 063718.272
Gianluca Corrado

063718.272 -- 063719.812
from Weave, our sponsor.

063720.862 -- 063722.802
So, maybe I can ask

063722.942 -- 063724.082
you to introduce

063725.012 -- 063727.012
give a more detail introduce

063727.231 -- 063727.651
about yourself.

063729.402 -- 063730.442
Just one by one.

063732.822 -- 063734.981
Okay. Hi, everyone. I'm Chiang Chiang, our

063734.982 -- 063737.302
assistant professor at Stanford. I work on computer

063737.302 -- 063739.372
vision robotics. And

063739.372 -- 063741.472
I was like, how you can help systems understand

063741.612 -- 063743.682
scenes reason about them, make predictions,

063743.862 -- 063744.652
interact with them.

063747.992 -- 063750.031
Okay. Hey everybody. Glenn Bresseth.

063750.272 -- 063752.432
So I do a lot of I'm a professor at

063752.432 -- 063754.372
Mila, in Montreal. And

063754.512 -- 063756.692
I do a lot of re reinforcement learning and robotics.

063756.962 -- 063758.452
As main research areas.

063759.542 -- 063801.582
Hi. I'm Elan. I'm an assistant professor at Harvard.

063801.782 -- 063803.902
And a lot of my research is on generative models

063803.902 -- 063805.552
and decision making. Making.

063806.012 -- 063807.392
Hey, everyone. I'm Gianluca,

063808.252 -- 063809.871
Workaway, one of the scientists.

063810.412 -- 063812.592
Spent a lot of time working on word models,

063812.862 -- 063814.842
building models like Aya one, Aya two.

063815.962 -- 063817.772
And, yeah, glad to be here.

063819.612 -- 063821.552
I'm John Langford. I've

063821.742 -- 063823.901
worked on many things over time, but I guess

063823.902 -- 063825.302
of late, I'm very interested in

063825.942 -- 063827.382
how exactly you can

063829.081 -- 063831.021
use generative models and

063831.162 -- 063833.562
what capabilities they lack and how you can actually create

063833.562 -- 063834.492
those capabilities they lack.

063836.492 -- 063838.252
Okay. Fine. Today,

063838.552 -- 063840.632
we're going to explore how close

063840.632 -- 063841.681
war models are

063842.961 -- 063844.822
close to real world application

063845.121 -- 063847.621
like robotic and autonomous driving.

063853.732 -- 063855.592
We have a a a jump panelist

063855.942 -- 063857.722
Can you briefly introduce

063858.182 -- 063858.682
yourself

063905.722 -- 063906.592
Yes. Hello

063921.642 -- 063922.051
Ideas.

063925.432 -- 063927.502
Thanks. Thank you

063927.502 -- 063929.202
very much. So briefly,

063929.582 -- 063931.831
I just directly start the

063931.831 -- 063934.061
panel discussion. So for the first

063934.062 -- 063935.842
question, so

063936.142 -- 063938.481
I think most of you are wondering

063938.621 -- 063940.382
that how close are our models

063940.921 -- 063942.572
to real world application like

063943.212 -- 063945.692
and autonomous driving So beyond the

063945.692 -- 063948.182
robots there are other domains,

063948.642 -- 063950.652
where world models have been

063951.132 -- 063951.632
applied

063956.192 -- 063957.652
Yeah. I guess I can start.

064000.092 -- 064002.412
They are very close and I would actually say they're

064002.412 -- 064004.612
probably because, I mean, even today, we

064004.612 -- 064006.851
saw in a few sessions like this morning Sonya

064006.852 -- 064008.932
showed us some examples at NVIDIA. We

064008.932 -- 064011.002
had Wei. We showed some examples on how

064011.642 -- 064013.902
world models that we developed today are actually

064014.062 -- 064015.662
used to help us build

064016.222 -- 064018.481
safer and better, autonomous agents.

064020.092 -- 064022.302
They're not fully there, meaning probably

064022.302 -- 064024.581
we would we'll be done with it once

064024.972 -- 064027.212
we can fully train a policy in a simulated

064027.212 -- 064029.461
world and that policy ends up being much

064029.462 -- 064031.712
better than training on real data. But we're seeing

064031.712 -- 064032.422
this progress

064034.002 -- 064036.222
every month now. There is a new world

064036.222 -- 064038.331
model. It's getting better. It's better

064038.791 -- 064041.222
simulating critical scenarios.

064041.282 -- 064043.054
So I guess they are basically

064043.342 -- 064044.491
they're they're improving fast.

064050.512 -- 064052.032
Sure. I'm happy to at least,

064052.752 -- 064054.912
be a good contradiction, I guess, for all we're doing

064054.912 -- 064057.032
this too. I guess I was

064057.032 -- 064059.042
thinking even during some of the last talk, like,

064059.042 -- 064101.322
what is a good definition for a world model

064101.802 -- 064104.121
Like, one that I also want would probably be faster

064104.121 -- 064106.142
than the real world or something that's, like, less

064106.382 -- 064108.452
resources than actually having to use the real world.

064108.452 -- 064110.572
And that's a bit more challenging to get

064110.572 -- 064113.072
there for maybe some of the world models we're thinking about

064113.132 -- 064115.232
using. And I I

064115.232 -- 064117.272
suppose to wonder if there's

064117.272 -- 064119.351
a bit of a difference here between how well this is working

064119.351 -- 064121.371
for self driving which

064121.372 -- 064123.572
maybe has a few other specific

064123.572 -- 064125.741
conditions, It seems to be a bit more tricky

064125.742 -- 064127.602
to be using world models specifically

064127.822 -- 064129.912
for, like, manipulation tasks and things

064129.912 -- 064131.212
like that with a lot of contacts.

064132.422 -- 064134.462
I don't have access to the today's

064134.462 -- 064136.612
best world model at the exact moment, but

064136.612 -- 064138.771
at least generating some videos, which are versions

064138.772 -- 064140.851
of world models for these tasks, did not go well

064140.852 -- 064141.912
for last week.

064144.851 -- 064146.791
Yeah. I I think that, for manipulation,

064148.121 -- 064150.201
at least the current open source model, I think a lot of the

064150.202 -- 064151.981
models online like Vio or

064153.102 -- 064155.262
or Sora, I think they're very, targeted towards

064155.262 -- 064157.512
entertainment, so they tend to not be very physical

064157.622 -- 064159.672
world. So you need to really, try to

064159.672 -- 064201.732
fine tune them specifically for manipulation for them

064201.732 -- 064203.782
to work well.

064205.172 -- 064207.252
I guess I have kind of a broader definition of a

064207.252 -- 064209.742
world model than just the pixel based things. And so

064209.902 -- 064212.061
if you use the broader definition, I think that you could say

064212.061 -- 064214.191
that there are already world models people routinely

064214.191 -- 064216.102
use there, like, Ted GPT.

064217.222 -- 064219.381
And, there's improvements in these world models

064219.382 -- 064221.452
which are underway. And I expect that many of

064221.452 -- 064223.872
the techniques that people are developing for world models

064224.152 -- 064226.232
will be more broadly adopted over the next

064226.232 -- 064227.802
year or two. So it's it's

064228.422 -- 064230.092
yes. It's there, and, yes, it's it's it's,

064230.572 -- 064232.632
growing, and, the domains are growing as well.

064239.412 -- 064241.492
Well, actually don't let you just said John, you just

064241.492 -- 064243.652
said what I want to say. So, yeah. So don't

064243.872 -- 064245.992
think I have anything to add. Okay. Do we

064245.992 -- 064248.412
have some opinion from online member,

064248.632 -- 064249.132
Linxiang

064250.702 -- 064252.802
I guess one of the the the key differences

064252.942 -- 064254.642
about the warmong involves contact

064256.061 -- 064258.081
for our documentation is really high very

064258.222 -- 064300.092
precise. Modeling modeling.

064300.492 -- 064302.592
But for noncontact noncontact

064302.732 -- 064305.182
contact, which task, like, autonomous driving,

064305.322 -- 064307.581
there's already some of the potential location.

064309.382 -- 064311.622
Yeah. Okay. Thank you

064311.622 -- 064313.722
very much for sharing your opinions.

064314.211 -- 064315.831
And the second question,

064316.371 -- 064318.822
so today, we have a lot of speakers

064318.962 -- 064320.822
mentioned about VLA models.

064321.481 -- 064323.642
So how could the VLA models and

064323.642 -- 064325.472
warm models be effectively

064325.692 -- 064327.932
combined to have some, thinking

064327.932 -- 064328.592
on that

064333.202 -- 064335.601
So one thing you can do is, you can actually

064335.602 -- 064337.692
have that VLA essentially be

064337.692 -- 064339.851
your initial, like, proposal distribution. So you can imagine

064339.852 -- 064341.782
the VLA proposes a series of actions

064342.232 -- 064344.312
a model can simulate the results of each each of

064344.312 -- 064346.322
these actions, and then you can do some type of search

064346.322 -- 064348.482
where you just choose the sequence of actions

064348.482 -- 064350.642
that maximizes your reward or your goal based

064350.642 -- 064352.212
off the world model simulations.

064355.882 -- 064357.952
You can also learn a

064357.952 -- 064400.112
latent world model inside the VLA

064400.112 -- 064400.612
itself.

064402.372 -- 064404.552
So if you have enough data, from

064404.732 -- 064406.742
an external world model or experience, you can

064406.742 -- 064408.822
just learn that latent world model internally

064408.822 -- 064410.112
and then use it effectively.

064413.652 -- 064415.972
There might also be, like, a nice blend between

064415.972 -- 064418.112
these two, because I'm also thinking about where some

064418.112 -- 064420.002
world models are still not quite the best.

064420.402 -- 064422.482
Vision manipulation tasks. How to use this

064422.482 -- 064424.782
combination to basically figure out how to fill

064424.782 -- 064426.792
in and collect the data that still isn't quite

064426.792 -- 064428.822
there yet. Some of the world models that we'd

064428.822 -- 064430.952
use. So even if you do a nice combination,

064430.952 -- 064432.612
like, using a VLA to help

064433.172 -- 064435.032
kinda like warm start to provide the prior

064435.592 -- 064437.972
for some type of world model. Then either

064438.242 -- 064440.122
look at where these things are not doing well

064440.682 -- 064442.762
where their prediction is bad or something like that to start

064442.762 -- 064443.812
to do, like, a bit of

064444.852 -- 064447.092
like, active control and figure what data is missing

064447.092 -- 064449.142
to go collect in the real world. Feed that

064449.142 -- 064450.672
back into the world model where you're missing.

064451.382 -- 064451.832
Information.

064458.042 -- 064500.442
There is a way where, again, if we start expanding

064500.442 -- 064502.592
a little bit the definition of what's a world model

064502.592 -- 064504.851
and we think about it's a lot of

064504.851 -- 064506.901
knowledge of the world as, you know, going back

064506.902 -- 064509.082
to, John's point before,

064509.722 -- 064511.882
ChachiPT may be a world model. It just has knowledge

064511.882 -- 064514.041
about the world. And these VLAs are actually

064514.042 -- 064516.075
connecting a lot of this knowledge. Right Some

064516.075 -- 064518.092
means some comes from videos, some comes

064518.092 -- 064520.331
from text, and this is still knowledge and common sense

064520.331 -- 064522.777
about the world. So it helps the help

064522.777 -- 064524.812
learn a lot of these things. And

064524.812 -- 064526.892
then the action part is also really important because

064526.892 -- 064529.141
that's part of your world model mental

064529.142 -- 064531.212
world model. You wanna know how

064531.212 -- 064533.452
the environment will react to your actions. So actually,

064533.452 -- 064535.572
I think that could be the unifying

064536.132 -- 064538.182
interface to actually add

064538.402 -- 064540.592
information from video, language, and action put it

064540.592 -- 064541.602
all in one place.

064547.412 -- 064549.652
Well, you can also take the output

064549.652 -- 064551.581
of VRA model and then you can

064551.662 -- 064553.822
similar to even what you just said, you see it as

064553.822 -- 064555.932
a proposal, but you can also even train

064555.932 -- 064558.310
it in conjunction. With the war model. Right

064558.311 -- 064600.471
It just take us the VA take the action, but then

064600.472 -- 064602.652
you also same time, you can even join

064602.652 -- 064604.712
train or fine tune a word model. And

064604.712 -- 064605.842
see maybe how help each other.

064607.712 -- 064609.972
Okay. Any opinion from online member

064611.212 -- 064613.562
No. I think I've paid all. And all the

064617.982 -- 064619.602
Okay. I have noticed that,

064620.062 -- 064622.142
some of you agree with that we can,

064622.782 -- 064624.862
we can train a VoIP and war

064624.862 -- 064627.132
model simultaneously. At the same time.

064627.132 -- 064629.212
So do you think a follow-up question.

064629.212 -- 064631.502
Do you think that when we

064631.561 -- 064633.742
train it as integrated

064633.962 -- 064635.992
into the same single system

064636.052 -- 064637.912
will be very computation cost.

064643.472 -- 064645.792
At least when you're playing around with a latent world

064645.792 -- 064647.842
model, your computational cost

064647.842 -- 064650.032
may decline. Because you

064650.032 -- 064652.272
don't need to fill in a bunch of details which turn out to

064652.272 -- 064652.932
be unnecessary

064659.072 -- 064701.282
Yes, I kind of agree probably Learning

064701.282 -- 064702.992
a good latent space to

064703.351 -- 064704.852
learn your word model is

064705.572 -- 064707.731
the best way to get the efficiency. So, yeah, I agree,

064707.732 -- 064708.232
complete.

064712.442 -- 064714.702
Yeah. For most of that makes sense. I just also

064714.762 -- 064717.012
worry about one corner case where I guess

064717.012 -- 064719.092
the assumption in the question is you're also training the

064719.092 -- 064721.171
world model from new data. That's

064721.172 -- 064723.492
being collected in some case. So just be

064723.492 -- 064725.742
cautious that your VLA policy doesn't start

064725.982 -- 064728.302
really poor and you only explore some really bad

064728.302 -- 064730.632
area or really frequent visiting

064730.632 -- 064732.692
of the same space and then basically you

064732.692 -- 064734.722
converge to nothing,

064734.722 -- 064736.502
the same action, the same state the whole time.

064736.902 -- 064738.931
So it also can be helpful with

064738.932 -- 064741.012
some theory that also shows I got at least some random

064741.012 -- 064743.502
exploration things like that that aren't just the BLA

064743.822 -- 064746.022
model in these cases. I think mostly we don't have to

064746.022 -- 064748.222
deal about too much because you start out with a good

064748.362 -- 064750.672
world model. But if you're training from scratch

064750.732 -- 064752.922
or tackling a task that's fairly new

064752.922 -- 064755.002
and out of distribution, just be cautious

064755.002 -- 064755.722
in that case.

064804.502 -- 064806.572
Oh, there Oh, sorry. So

064806.572 -- 064808.532
this is a question for John, I guess.

064809.092 -- 064811.202
For the latent representation, for

064811.202 -- 064813.412
human we learn with word

064813.412 -- 064815.212
modeling. We are dreaming.

064816.172 -- 064818.202
So what space are we dreaming in

064818.842 -- 064820.732
Is it, like, very latent or very

064820.892 -- 064823.101
like, concrete, like, with perceptions and

064824.191 -- 064824.691
everything

064828.822 -- 064831.182
So that's little hard to answer because

064831.182 -- 064832.802
it's an introspection here, but

064833.682 -- 064835.621
we can take an extreme stance and say,

064836.202 -- 064838.041
humans don't have any way to,

064838.282 -- 064840.302
render things in full fidelity.

064840.842 -- 064842.922
And so it has to be has to be while while dreaming.

064842.922 -- 064844.641
So it has to be latent in some sense.

064845.041 -- 064847.482
Right But you

064847.632 -- 064849.762
if you an agent if

064849.762 -- 064851.922
you think about an agent that has a a latent

064851.922 -- 064854.351
representation, if it's a good latent representation,

064854.862 -- 064857.042
it'll be the same as if it's getting whatever

064857.342 -- 064859.832
external stimulus. Creates that latent

064859.922 -- 064902.031
representation. And so

064902.032 -- 064904.192
you can't introspect or really understand the

064904.192 -- 064905.092
difference necessarily.

064906.292 -- 064908.561
Because on one hand, you generate

064908.621 -- 064910.791
a latent On the other hand, you you have

064910.791 -- 064913.132
a a latent projected from an external stimulus.

064913.802 -- 064916.022
And they're they're the same if you have a good latent.

064920.232 -- 064921.451
Okay. Thank you.

064927.632 -- 064929.693
Yeah. The decision is based on the

064929.693 -- 064931.632
latent, then you'll just work in latent variables.

064931.872 -- 064932.531
That makes sense

064935.402 -- 064936.782
Okay. Thank you very much.

064937.652 -- 064939.712
For the jump question. And,

064940.752 -- 064943.092
so the following question would be, like,

064943.092 -- 064945.272
as mentioned several times today,

064945.332 -- 064947.512
that we might learn a kind of latent

064948.291 -- 064950.472
level or models. So do you think

064950.852 -- 064952.942
latent led latent level word model is a

064952.942 -- 064955.002
kind of a compression and it will loss

064955.402 -- 064957.561
a lot of details of the real world.

064957.562 -- 064959.591
So it be a kind of

064959.592 -- 065001.792
a thing that we should treat

065001.792 -- 065003.952
it as a kind of trade off between the real

065003.952 -- 065005.813
world generation and also

065006.382 -- 065007.901
the latent space understanding.

065009.421 -- 065011.570
Oh, this is a fun one. So you certainly

065011.732 -- 065013.811
if you have a latent space, you're certainly losing

065013.812 -- 065015.632
a lot of information about the world. Yes.

065016.032 -- 065018.502
But if you lose the right information about the world,

065019.542 -- 065021.582
you win. You don't not a trade off.

065021.582 -- 065023.822
It's it's a win. And

065024.303 -- 065026.382
the the extrapolation that I was showing is an

065026.382 -- 065028.471
example of this. Right So you

065028.472 -- 065030.632
get an improved ability to extrapolate if you

065030.632 -- 065031.832
throw away the right detail.

065039.102 -- 065041.252
No. I was just sort of like, how do we figure out

065041.572 -- 065043.792
the right thing To leave out is Because, like, the real

065043.792 -- 065046.152
latent space is the location and

065046.212 -- 065048.232
velocity of every atom in the universe. Signal.

065048.392 -- 065050.552
We really don't need to track that, like, in any way. We all

065050.552 -- 065052.331
seem to do pretty good without that information.

065052.882 -- 065053.842
So is there any

065055.162 -- 065057.211
like, rules of thumb or something to understand

065057.211 -- 065059.372
what this right version is. Usually

065059.372 -- 065101.652
for me, it's just like whatever makes my reward

065101.652 -- 065103.733
better, but that still assumes I know a reward

065103.733 -- 065104.582
function for starting.

065106.092 -- 065108.160
So I think the

065108.161 -- 065110.240
general notion of what the right, information is

065110.241 -- 065112.332
is whatever you need to drive your policy.

065114.322 -- 065116.332
It's related to the rewards, but,

065117.252 -- 065119.272
whatever basis you need to to take

065119.672 -- 065121.773
for taking action. Now that's that's kind

065121.773 -- 065123.802
of an implicit definition. But you

065123.802 -- 065125.732
can imagine that if you have an updated data collected,

065125.812 -- 065127.312
you could excite that definition

065127.951 -- 065130.030
in a learning system and actually, get that

065130.031 -- 065130.971
representation out.

065134.013 -- 065136.192
And, yeah, I think I think that comes from

065136.432 -- 065138.772
the actions, right These world models are trained

065139.152 -- 065141.532
taking into consideration actions that agents

065141.532 -- 065143.752
are supposed to take in that world. And

065143.812 -- 065145.861
so once you start modeling the latent space,

065145.861 -- 065146.602
for example,

065148.292 -- 065150.312
trying to model the sky and the clouds,

065150.892 -- 065153.051
probably not gonna help you much deciding what's

065153.052 -- 065155.041
your next next action. That's just in

065155.443 -- 065157.502
in the scene. I'm I'm thinking about the

065158.142 -- 065200.201
the self driving case here, but there

065200.202 -- 065202.322
are always things in the scene that are probably

065202.322 -- 065204.561
not very relevant to your decision making.

065204.642 -- 065206.662
And if you can close the loop between the

065206.662 -- 065208.891
world modeling and the decision making, while

065208.892 -- 065210.982
learning the latent space, you are

065210.982 -- 065213.412
more likely to obstruct out things that you

065213.472 -- 065215.571
don't really need. But, yeah, there is

065215.571 -- 065217.812
there is always like It's always the danger.

065217.812 -- 065220.111
Right Compress too much, you lose too much. You're missing

065220.111 -- 065221.012
out on something.

065225.482 -- 065227.722
I'm just trying to close that loop in my head

065227.722 -- 065229.791
as well. So, does this mean it's some

065229.791 -- 065231.602
sort of information metric

065232.161 -- 065234.112
like mutual information metric over like

065234.353 -- 065236.513
what you can control in the latent space with

065236.513 -- 065238.593
respect to the actions that will be used inside

065238.593 -- 065240.962
of this Over time, and I guess how that relates

065240.962 -- 065242.821
to the real worlds Ish

065244.352 -- 065244.702
Yeah.

065247.122 -- 065248.752
Okay. Cool. I'll do that tomorrow.

065253.892 -- 065256.372
So do you have, some opinion from

065256.372 -- 065258.122
online member

065258.722 -- 065300.763
Link Yeah. Yeah. I think

065300.763 -- 065302.922
it is a very good, interaction, very

065302.922 -- 065304.962
good topic. It's really challenging

065304.962 -- 065307.072
part is how to define and to be, like,

065307.072 -- 065309.393
a good lab space. For example, you know, I know we

065309.393 -- 065311.002
we kind of need for the information of

065311.642 -- 065313.646
for the model. But, you know,

065313.672 -- 065315.912
like, like, for different

065315.912 -- 065318.103
task, probably there will be

065318.103 -- 065319.882
different, important information,

065320.182 -- 065322.331
like, and how to how to know about

065322.331 -- 065324.751
the theory exists and how to, you know, have, like,

065324.812 -- 065326.483
relatively universal representation

065327.023 -- 065329.122
that can, adapt you to, like,

065329.422 -- 065331.741
a like a general set of of tasks,

065331.741 -- 065333.932
that will be the challenge, I will say.

065335.802 -- 065337.882
And, whether we are some kind

065337.882 -- 065340.042
of a transformer that allows us

065340.042 -- 065342.052
to allow the model to

065342.052 -- 065344.393
shift or focus on different, or identify

065344.722 -- 065346.962
some of the most relevant information or different

065346.962 -- 065349.121
tasks, that would be a very interesting direction

065349.121 -- 065351.222
to too. Explore to explore.

065353.422 -- 065355.822
Okay. Thank you very much. Have

065356.462 -- 065358.622
some okay. For the

065358.622 -- 065400.832
next question, I just want

065400.832 -- 065402.693
to ask, so nowadays,

065403.292 -- 065405.372
there are several ways to build up the word

065405.372 -- 065407.472
model. One is building

065407.472 -- 065409.733
up the physical, physical interaction

065410.352 -- 065412.542
with the environment. Building up a kind

065412.542 -- 065414.662
of transition model like that. And the one way

065414.662 -- 065416.682
is just to reconstruct real

065416.682 -- 065418.762
world, like, three d reconstructing, like, a

065418.762 -- 065421.223
spatial intelligence, this kind of thing.

065421.603 -- 065423.382
So, how do you think about

065423.683 -- 065426.013
this two line of the works

065426.661 -- 065429.002
about, how to represent the world

065432.742 -- 065434.902
So my thought is, you should, like, represent the

065434.902 -- 065437.002
world at many different levels of information.

065437.812 -- 065439.972
So for instance, I think a language model like what was

065439.972 -- 065442.102
talked about earlier gives you a very high level semantic

065442.102 -- 065444.152
understanding of the word. World. But then I feel that

065444.152 -- 065446.511
some type of maybe like video modeling

065446.512 -- 065448.751
with dynamics a video model a video modeling would

065448.751 -- 065450.802
really tell you some more of the dynamics of the

065450.802 -- 065452.812
world. But then you probably also need something that's

065452.812 -- 065454.902
much more tactile or force based to really get

065454.902 -- 065457.063
the low level of physics physical details. So

065457.063 -- 065459.302
I guess you would need many different objectives to get different parts

065459.302 -- 065500.262
of the world knowledge.

065504.682 -- 065506.842
I like the latent approach because, it feels

065506.842 -- 065508.222
like it's a little bit unexplored

065509.362 -- 065511.822
rather than speaking. And there's huge potential

065511.822 -- 065512.842
to be much more efficient.

065516.451 -- 065518.712
I guess, I mean, the two approaches can be complemented

065518.972 -- 065521.262
especially right now where we're not

065521.262 -- 065523.472
done with latent word models or

065523.472 -- 065525.682
this word model. So three d reconstruction

065525.682 -- 065527.842
is quite powerful. In

065528.972 -- 065531.142
and has very, very good use

065531.142 -- 065533.292
case for embodied AI applications,

065533.352 -- 065535.502
which is you can three d reconstruct

065535.802 -- 065537.982
a failure case, you can basically build

065538.382 -- 065540.512
what used to be unit tests. Of

065540.512 -- 065542.752
when your software used to fail. Right And then you can

065542.752 -- 065544.742
just build this user array

065545.223 -- 065547.381
of unit tests that you can run all your policies

065547.382 -- 065549.632
in just to check. Is my new policy

065549.872 -- 065551.992
failing at things that an old policy used to

065551.992 -- 065554.273
fail It still has the challenge

065554.273 -- 065556.292
there. Right Because once you reconstructed

065556.292 -- 065558.492
the the three d world perfectly and you want

065558.492 -- 065600.492
to resimulate something, you still have the

065600.733 -- 065603.202
issue of having to resimulate or to simulate

065603.523 -- 065605.393
the correct physics and

065606.122 -- 065608.462
doesn't come for free in three d reconstructed worlds.

065608.892 -- 065610.682
Although that's the kind of promise of

065610.922 -- 065613.162
latent word models and learned word models, they will pick

065613.162 -- 065614.772
up on physics and

065615.223 -- 065617.622
you will get them for free. And, you know,

065617.622 -- 065619.943
in the previous talk, we kind of saw some of these

065619.943 -- 065622.183
examples where the physics were quite well

065622.183 -- 065623.072
represented already.

065627.902 -- 065628.802
Oh, okay.

065630.122 -- 065630.622
Please.

065632.662 -- 065634.522
What actions a lot So I wonder

065635.121 -- 065637.361
we have different definitions of action

065637.361 -- 065639.522
for different applications. For example, for video

065639.522 -- 065641.672
game, it's like simple and discreet

065642.052 -- 065644.342
mostly. And then for robotics, it could

065644.342 -- 065646.502
be more complicated. And one can imagine we can

065646.502 -- 065648.552
have different world models for this

065648.552 -- 065650.572
different deaf But intuitively,

065650.792 -- 065653.022
we also shouldn't build a specific

065653.222 -- 065655.532
world model tailored to specific

065655.537 -- 065657.632
action definition. So what do you what do

065657.632 -- 065658.962
the dependent list think about it

065703.422 -- 065705.362
So actually, I personally think that you do

065705.683 -- 065707.882
role models at different levels of abstraction. So

065708.122 -- 065710.202
if you say something like, like, maybe if you

065710.202 -- 065712.362
abstractly are playing a video game, maybe walk forward

065712.362 -- 065714.492
is just your action. But maybe if you're controlling a

065714.492 -- 065716.732
robot, you really do want a much more low level, like,

065717.212 -- 065719.291
like maybe the way points you want to reach, right, or force or

065719.291 -- 065721.422
torques So it does seem like there there

065721.422 -- 065723.412
should at least be a couple of different road models.

065723.572 -- 065725.312
For levels of different levels of obstruction.

065731.361 -- 065733.432
I guess like, even the the latent space

065733.432 -- 065735.592
way of figuring this out too is also like a nice

065735.592 -- 065737.121
way to preserve some resources.

065737.702 -- 065740.102
But maybe we kind of want like a a latent representation

065740.102 -- 065742.182
where you can generate lots world. Models.

065742.552 -- 065744.852
Like in this case, you can just

065744.852 -- 065746.989
figure out from that latent space which components of

065746.989 -- 065749.162
it actually make the function that's good enough for

065749.222 -- 065751.132
discrete space. Because you don't necessarily

065751.272 -- 065753.392
need a big giant latent space to figure out

065753.392 -- 065755.622
how like a pendulum swings in the world things

065755.622 -- 065757.212
like that. Maybe there's ways to

065757.693 -- 065759.862
factor it in a nice way that's learned especially

065759.862 -- 065801.322
you can get a more efficient

065802.152 -- 065804.312
somehow, like, component version of that latent

065804.312 -- 065806.642
space model even for each task you've might use.

065808.802 -- 065810.862
This is I guess robotics has doing

065810.862 -- 065812.642
this for a while for every task specifically.

065813.302 -- 065815.302
Depending on what the design and constraints are.

065816.582 -- 065818.662
I I guess if we have

065818.662 -- 065820.962
latent action, you can also potentially

065824.292 -- 065826.302
Potentially, yeah. Mhmm. You.

065826.622 -- 065827.262
Thank you.

065831.082 -- 065833.502
So do you have any opinion about

065833.802 -- 065835.931
the previous question on

065835.932 -- 065837.312
the three d reconstruction

065838.531 -- 065840.712
Also, like, the physical dynamics modeling.

065842.112 -- 065843.733
From our online member.

065845.502 -- 065847.662
I think, I think it's it's

065847.902 -- 065850.142
important to, look at the problem from

065850.143 -- 065852.162
different perspective. For the photo, I

065852.162 -- 065854.483
think the the major difference will be the values

065854.483 -- 065855.052
should be

065857.342 -- 065859.422
mean, for the city representation is is

065859.422 -- 065901.722
this kind of provides like a like a like a

065902.042 -- 065903.502
LEGO structured understanding.

065904.682 -- 065907.012
And, for, let's actually,

065907.012 -- 065909.172
it's a challenging part of this, how to do this,

065909.172 -- 065911.292
like, a incremental learning. And how to

065911.292 -- 065912.652
do this command and tutorial.

065913.371 -- 065915.632
Like, things you know, how to, add

065916.232 -- 065918.572
more different different knowledge and a combineings.

065919.502 -- 065921.343
I guess that will be the one to all the challenging,

065921.582 -- 065923.792
things. And

065923.792 -- 065926.032
and and also I want to congratulate you about the water

065926.032 -- 065928.241
model is also like different mortality from,

065928.522 -- 065930.943
I mean, I'm doing I'm doing robotic manipulation. We know that,

065931.162 -- 065933.252
you know, really require different

065933.252 -- 065935.331
type of sensing, what's the, like, the tactile, the

065935.331 -- 065937.492
force, to be incorporate,

065937.872 -- 065939.952
to make the to have the robot to make

065939.952 -- 065941.792
the right decision. So,

065942.193 -- 065944.692
how to combine this kind of, like, different modalities

065945.152 -- 065947.442
or look at the I mean, build the water bottle

065947.442 -- 065947.942
from the

065955.502 -- 065957.042
So any opinion from,

065957.592 -- 065958.092
our

070001.032 -- 070003.432
I I thought there you know, there's a distinction between

070003.432 -- 070005.542
what is the goal. What's the problem you're trying

070005.542 -- 070007.512
to solve, as well as the method or the

070007.832 -- 070010.232
algorithms that are that are solving, and and then

070010.232 -- 070012.281
there's implementation. So at a

070012.281 -- 070014.422
goal level, you know, it's like I I guess,

070014.422 -- 070016.582
eventually, people want more models to have all of these.

070016.582 -- 070018.152
Right So you wanna be able to do

070018.632 -- 070020.871
plausible physical simulation, release the visual

070020.871 -- 070023.022
appearance, help decision making, stuff like that

070023.502 -- 070025.582
And then in terms of approach

070025.582 -- 070027.652
or in terms of, you know, the

070027.792 -- 070029.462
the representations, then,

070030.002 -- 070031.942
no then the two things you mentioned,

070032.682 -- 070034.621
you know, let's say, three d reconstruction

070034.762 -- 070036.812
or versus more like focusing

070036.812 -- 070039.131
on dynamic prediction. I think they're just, you know, sometimes tackling

070039.132 -- 070041.302
different aspects of the problem. But I'm

070041.302 -- 070043.462
pretty sure that, you know, whenever people say spatial

070043.462 -- 070045.542
intelligence, eventually, they also want have predictions

070045.542 -- 070047.552
as well. But you just like how you're prioritizing

070047.872 -- 070049.912
these different sub goals. But in terms

070049.912 -- 070052.062
of method, is a separate question, right, whether

070052.063 -- 070054.412
it should be implicit or or

070054.472 -- 070056.002
latent or more explicit because

070056.582 -- 070058.922
the visual realism or three d consistency,

070058.922 -- 070101.082
you can also imagine a totally latent approach.

070101.483 -- 070103.602
It doesn't it's not a case that if you focus

070103.602 -- 070105.812
on three d reconstruction, you have to use three d Gaussians.

070106.212 -- 070108.302
So so there's a kind of I think, there's a

070108.302 -- 070110.462
separate or disentanglement of a problem versus

070110.462 -- 070111.792
approach that we should be aware

070113.582 -- 070115.902
Okay. Thank you very much for sharing your

070115.902 -- 070117.942
opinions. And another

070117.942 -- 070120.041
question. Just following the questions

070120.182 -- 070121.872
we asked before, so

070122.592 -- 070124.532
so I think some of the participant

070124.832 -- 070127.143
might want to some something

070127.143 -- 070129.382
like if, the current world

070129.382 -- 070131.172
model can capture

070131.792 -- 070133.262
the physical laws, physical rules

070134.132 -- 070136.292
of the real world. For example, in the

070136.612 -- 070138.912
in generated videos or

070139.372 -- 070141.393
generated three d things,

070141.773 -- 070143.902
is this something that we feel to

070143.902 -- 070145.982
capture, like, some physical concept like

070145.982 -- 070148.122
gravity, like, this kind of thing.

070148.202 -- 070149.902
So how could this limit

070150.672 -- 070152.752
limitations be addressed that will have

070152.752 -- 070154.452
some suggestions for that

070157.812 -- 070159.831
I can start just for a second, but maybe part

070200.052 -- 070202.272
of the well,

070202.472 -- 070204.632
I think even for the way some people operate

070204.632 -- 070206.432
is there some latent space that's deemed

070206.832 -- 070209.022
average existence we've been doing for a while, but then when

070209.182 -- 070211.262
well, winter come along comes along for a little

070211.262 -- 070213.480
bit, everything on average gets a little bit more

070213.480 -- 070215.602
slippery. So there's some hopefully, like, at

070215.602 -- 070217.832
least context or something. That's fed into this that's

070217.832 -- 070218.832
from recent history.

070219.982 -- 070222.007
To either help help update the latent model

070222.007 -- 070224.132
or about what would be used

070224.132 -- 070226.142
to better predict or figure out

070226.142 -- 070228.632
gravity on a different planet given some recent information

070231.443 -- 070233.682
So I feel that the middles, the video

070233.682 -- 070235.932
models have some sense of physics, but for a lot

070235.932 -- 070238.382
of especially for rare events, like, for instance, let's

070238.443 -- 070240.452
say you drop a cup and it falls on the ground,

070240.532 -- 070242.773
It seems like these events that are very rarely depicted

070242.773 -- 070244.933
in data, they don't seem to be accurately able

070244.933 -- 070247.111
to learn. So I

070247.111 -- 070249.232
think that I think one way to really get better

070249.232 -- 070251.132
physics knowledge in these models is really to

070251.292 -- 070253.372
maybe try to execute them on physical robots and

070253.372 -- 070255.382
see use, like, real world data as a way to

070255.382 -- 070257.432
gather this physics information. Because

070257.432 -- 070259.532
the data on the Internet is pretty biased towards

070259.773 -- 070301.802
certain physical interactions and not other ones.

070305.322 -- 070307.023
So I guess to me,

070307.452 -- 070309.792
failure to for gravity to work properly is,

070310.853 -- 070312.932
an extrapolation error. Presumably, there's

070312.932 -- 070315.082
some instances of gravity doing something useful

070315.082 -- 070317.132
in the data somewhere. And there's just

070317.132 -- 070319.632
a failure to extrapolate to, other situations.

070320.412 -- 070322.502
And so the solution that we found

070322.502 -- 070324.652
experimentally so far is if you

070324.652 -- 070326.732
have a much more compact world model, then you

070326.732 -- 070328.772
get much better extrapolation. And

070328.772 -- 070330.082
so that'd be my first instinct.

070333.452 -- 070335.442
Yeah. I I I tend to agree because

070335.682 -- 070337.832
it's unclear what like I think for

070337.832 -- 070340.012
me, it always goes back to latent spaces. Right

070340.012 -- 070342.161
If it is possible in your latent

070342.161 -- 070344.472
space, you do you are understanding the physics

070344.632 -- 070346.792
up to the extent that is needed. To

070346.852 -- 070348.853
solve a task. And in pixel

070348.853 -- 070350.102
space, that is hard.

070351.422 -- 070353.642
To then simulate and So it's it's

070353.642 -- 070355.672
unclear whether these models

070355.893 -- 070357.972
actually understand the physics a lot better than

070357.973 -- 070400.252
we think And what we're seeing in image

070400.252 -- 070402.292
space and we can understand that image space

070402.693 -- 070404.552
is what we think it's for physics,

070405.733 -- 070407.442
but they're kind of there, and they're getting

070408.402 -- 070410.502
much better, faster. And that

070410.502 -- 070412.661
that for me is one way of mitigating

070412.772 -- 070414.902
the issue. And then the second is always

070415.217 -- 070417.372
data. Right These models are

070417.372 -- 070419.402
trained from large scale datasets

070419.402 -- 070421.542
and we're still not there

070421.542 -- 070423.983
at the point where can just take all the videos

070423.983 -- 070426.103
in the world and train one word model

070426.103 -- 070428.462
with it. And so data selection becomes

070428.462 -- 070430.821
much more important. So if you want physics

070430.821 -- 070433.062
to be well understood by your model, you need to be

070433.062 -- 070435.052
careful and choosing videos that

070436.012 -- 070438.111
tend to teach the physics a lot better.

070448.032 -- 070450.193
If I remember correctly, I think in your question, you mentioned

070450.193 -- 070452.302
some kind of physical laws. Stuff like that.

070452.302 -- 070454.603
So it's I guess it's probably

070454.603 -- 070456.733
the case that, you know, there are there are methods that we

070456.733 -- 070459.183
can try to incorporate and including

070459.183 -- 070501.202
data collection strategies that Elon and Foresimplify

070501.343 -- 070503.483
suggesting. Can help us to get these models

070503.483 -- 070505.693
better at physics And I also feel like, you know,

070505.693 -- 070507.852
for any sort of applications including robotics, there's

070507.852 -- 070509.942
probably an extent that these models would just be

070509.942 -- 070511.943
good enough and and that's all we

070511.943 -- 070514.092
need because you know, they'd never they'll

070514.092 -- 070516.432
probably never be perfect, but it's okay because, you know,

070516.482 -- 070518.662
know, I understand very little physics. But

070518.662 -- 070520.821
you can still live if it's it's alive in the world.

070520.821 -- 070523.041
Right So so the models don't have to understand

070523.041 -- 070525.112
perfect physics. And if you hear about

070525.273 -- 070527.343
having the models understand theoretical

070527.343 -- 070529.502
laws and, you know, symbolic equations and help you

070529.502 -- 070531.592
do science discovery, a different types of model,

070531.592 -- 070533.672
and then we require something else. But that's sounds

070533.672 -- 070535.792
a bit different from the war models that has

070535.792 -- 070538.062
been discussed in this context for this particular workshop.

070541.852 -- 070544.092
So any ideas from online

070544.092 -- 070544.592
member

070546.241 -- 070547.722
I think, one of the things

070548.281 -- 070550.512
challenge would be how do you, extract a

070550.512 -- 070552.983
representative the the phasing constraints. Your phasing

070552.983 -- 070555.181
constraints are kind of solved in like like part of constraints.

070555.182 -- 070557.262
For example, you cannot like, a

070557.262 -- 070559.462
cup cannot look like it's dropped. To the

070559.462 -- 070601.582
ground, but it cannot go through the ground. Right I mean, it's

070601.582 -- 070603.722
already is is is this sound like like a

070603.722 -- 070605.741
hard constraint on how to represent

070607.081 -- 070609.352
and know, loans this, like, an hard like,

070609.352 -- 070611.272
this hard constraint would be,

070612.392 -- 070614.632
very interesting to think about. I think he's

070614.632 -- 070616.951
also very important

070617.172 -- 070619.272
direction. It's associated

070619.411 -- 070621.502
with some of the interpolation. And and

070621.502 -- 070623.443
then if there's, like, the car constraints,

070624.302 -- 070625.282
what kind of interpolation

070626.762 -- 070628.781
could be, be, learned and represented,

070629.802 -- 070631.893
and yeah, that

070631.893 -- 070633.972
would be, I I I guess

070633.972 -- 070636.031
that would be my of the potential

070636.172 -- 070638.452
ways to to to to mitigate this.

070639.402 -- 070640.542
Like, issues

070643.331 -- 070645.232
K. Thank you very much. So

070645.472 -- 070646.692
the second last question,

070647.791 -- 070649.933
so in the context of embodied

070649.933 -- 070652.012
AI, so what do you say as the

070652.012 -- 070654.263
most promising direction

070654.322 -- 070656.582
for war models over the next two or three

070656.643 -- 070657.143
years

070704.813 -- 070706.832
I think we are seeing a lot of progress

070706.982 -- 070709.042
these days in building these world models

070709.042 -- 070710.502
and they become more accurate.

070711.232 -- 070713.572
But think in the next two or three years, the most promising

070713.572 -- 070715.972
thing to figure out is how you actually use them

070716.212 -- 070717.172
to generate

070718.452 -- 070720.302
data for embodied AI.

070721.002 -- 070723.082
There are so many things that you want to be able

070723.082 -- 070724.782
to cover in the space of possibilities,

070725.392 -- 070727.032
and a lot of these models still

070728.263 -- 070730.422
use interfaces like text prompts

070730.422 -- 070732.502
and such that are quite hard

070732.502 -- 070734.672
to automate. In a way

070734.672 -- 070736.912
that guarantees you kind of cover the space

070736.912 -- 070738.952
of possibilities. So figuring

070738.952 -- 070741.121
out the best way to, like, create

070741.121 -- 070743.541
interfaces for these models to then automate

070744.072 -- 070746.223
the testing and making sure it's

070746.223 -- 070748.702
safe enough, which means it cover the space of

070748.702 -- 070751.102
possibilities exhaustively this is gonna

070751.102 -- 070753.162
be probably one of the most impactful things that we

070753.162 -- 070755.352
can see for Embodied AI because it will

070755.352 -- 070756.692
bring embodied agents from

070759.762 -- 070801.922
being, good to to things in the real

070801.922 -- 070804.023
world to being safe to do these things in the real

070804.023 -- 070806.102
world and therefore deployable and actually

070807.292 -- 070808.712
we can bring them to the world.

070812.872 -- 070815.352
I think that there's a real chance that in two

070815.352 -- 070817.292
years, we'll be using

070818.522 -- 070820.552
latent world models at inference

070820.552 -- 070822.571
time. And that will

070822.571 -- 070824.452
be a quantum jump terms of

070824.693 -- 070826.911
how reliable systems can be Because

070826.911 -- 070828.991
they can roll out the consequences of decisions

070828.991 -- 070830.842
and choose which action to take.

070831.722 -- 070832.861
In useful ways.

070835.122 -- 070837.362
Yeah. I very much agree with that. I think the most

070837.362 -- 070839.482
exciting use case of road models is actually to use

070840.281 -- 070842.422
So because now you can much more consistently select

070842.422 -- 070844.502
which actions to execute in comparison to a

070844.502 -- 070846.512
policy. I think that models are much more

070846.512 -- 070848.572
likely to generalize to new tasks than

070848.572 -- 070848.932
policies are.

070851.652 -- 070853.702
Yeah. The only thing I'd add onto that, but kinda one

070853.702 -- 070855.861
of the comments you're making earlier is maybe also

070855.861 -- 070857.332
having a bit of a mix

070858.052 -- 070900.001
like a short hierarchy of models that are trained.

070900.162 -- 070902.402
Because there'll be one that's really fast that you can run-in

070902.402 -- 070904.342
real time, and that will keep to do some sampling.

070905.012 -- 070907.172
Hopefully have one that's gonna be more expensive, more

070907.172 -- 070909.332
resource constrained. But now it's only if you're like task

070909.332 -- 070911.483
is more critical. Or it's finally passed

070911.483 -- 070913.801
the test through all the other simulators that were cheaper

070913.802 -- 070915.852
and less resource. You know, expensive

070915.852 -- 070917.762
to run including being able to test them as well.

070921.852 -- 070923.582
Well, I'm I'm quite excited about

070924.422 -- 070926.502
having even just video models that

070926.502 -- 070928.701
take real actions as input. It's

070928.701 -- 070930.782
like not just genius grape, but not just

070931.103 -- 070933.211
forward backward, like, real actions. I think there's

070933.211 -- 070935.452
related to of the questions that was asked before, like,

070935.773 -- 070937.853
if you have a different robot actuators and you have

070937.853 -- 070940.273
all the dexterous hands and stuff like that, how can you specify

070940.362 -- 070941.861
actions As input,

070942.402 -- 070944.412
as conditions to these new generation methods And

070944.412 -- 070946.572
I think there's also hope that these methods can be

070946.572 -- 070948.612
made much faster even to real time, so that they can

070948.612 -- 070950.212
also be used at inference time as well.

070953.402 -- 070955.502
Any ideas from link

070957.733 -- 070959.731
From the robotics and spatial modulation

070959.972 -- 071001.652
perspective, we are still interested in,

071002.052 -- 071004.122
like, how to incorporate different modalities like the

071004.122 -- 071004.372
tech

071007.822 -- 071009.682
the the decision making for a bot communication.

071010.802 -- 071012.983
I think that will be also very important.

071015.321 -- 071017.571
Okay. Thank you so much for your sharing.

071017.571 -- 071019.361
So the last question

071019.582 -- 071021.682
maybe some of our attendees really

071021.741 -- 071024.042
curious about So could you please

071024.102 -- 071025.882
recommend one of your favorite

071026.182 -- 071028.511
papers on world models or related topics

071028.512 -- 071030.722
such as embodied AI and

071030.722 -- 071032.801
the video generation or model based reinforcement

071032.802 -- 071034.442
learning or this kind of thing.

071040.152 -- 071042.402
Sure. I mean, I guess I'll suggest

071042.402 -- 071044.483
is like the second last paper that I was including

071044.483 -- 071046.562
a part of my presentation. Was just a

071046.562 -- 071048.962
mix of figuring out how to combine a good vision,

071048.962 -- 071051.002
language, action model, and

071051.002 -- 071053.042
some pipe some kind of model based search.

071053.282 -- 071055.523
But I think the fun part in there, which

071055.523 -- 071057.532
we haven't really talked about is kinda

071057.532 -- 071059.562
make good use of a model. Because the VLA action

071059.562 -- 071101.702
policy is already learned at a whole

071101.702 -- 071103.942
bunch of stuff. And we don't really need

071104.002 -- 071106.081
at that point the world model to predict really

071106.081 -- 071108.332
good for the stuff the vision language action model already

071108.332 -- 071110.571
knows. It should instead, you know, figure

071110.571 -- 071112.693
out how to predict better for the stuff that's on

071112.693 -- 071114.512
the edge of what this VLA policy

071114.912 -- 071117.221
knows. Because that's what it's gonna use to actually

071117.222 -- 071119.492
help correct those actions anyway. So if we

071119.492 -- 071121.512
have to contribute capacity somewhere,

071121.512 -- 071123.662
maybe think about this part of the search and

071123.662 -- 071123.912
expansion.

071129.292 -- 071131.452
No one else has any other papers,

071131.452 -- 071132.122
just me

071134.482 -- 071136.712
Alright. You can also share

071136.712 -- 071138.092
your your paper.

071142.853 -- 071145.172
I'm excited about the paper I was talking about,

071145.172 -- 071147.255
the next latent one. But at the same time,

071147.255 -- 071148.731
I'm even more excited about the next paper.

071153.062 -- 071155.302
I don't have one specific paper I would

071155.302 -- 071157.422
suggest, but I think we're at a stage

071157.422 -- 071158.932
with world modeling where

071159.652 -- 071201.111
there's probably a lot of noise

071201.742 -- 071203.822
going on. Right So it becomes really, really hard

071203.822 -- 071205.992
to even figure it out. What's

071205.992 -- 071208.072
important and what's not. So in this case, you know,

071208.072 -- 071210.161
usually my suggestion is keep an

071210.161 -- 071212.622
eye on all the review papers that come out because

071212.782 -- 071214.932
once you start combining all the

071214.932 -- 071217.222
review papers together, they help a lot

071217.222 -- 071219.263
trying to marginalize out lot of

071219.263 -- 071221.342
the noise and really understand what's going on in the

071221.342 -- 071223.511
field where the field is headed,

071223.512 -- 071225.532
and maybe spot the next

071225.532 -- 071225.982
big opportunity.

071230.802 -- 071232.852
Yeah, I also I

071232.852 -- 071234.931
guess in general, I'm very I don't know if I have a favorite

071234.932 -- 071237.392
paper, but I'm very excited about these papers where you're

071238.332 -- 071240.372
actually using these models at inference time to get more robust. Execution.

071246.473 -- 071246.892
Okay.

071248.672 -- 071250.612
We have to recommend one

071251.032 -- 071253.112
Okay. I'm gonna shamelessly say advertise our

071253.112 -- 071255.281
work that it's not about war model,

071255.281 -- 071257.362
but it's about evaluating war model. Because,

071257.362 -- 071259.682
you know, you have all these war models, and you want to evaluate

071259.682 -- 071301.872
how good they are. So we have a

071301.872 -- 071304.042
paper called ORR score, which just try to

071304.042 -- 071306.321
benchmark how these different word models

071306.321 -- 071308.402
they behaved. But but it was but

071308.402 -- 071310.483
it doesn't apply to your latent word models, so so it

071310.483 -- 071312.723
doesn't require these word models to have a pixel

071312.723 -- 071314.062
output eventually at the end. The end.

071316.462 -- 071318.241
Thank you. And online suggestions

071319.672 -- 071321.752
Okay. Last but not least, I would

071321.752 -- 071323.392
say maybe random management

071323.872 -- 071326.272
some of your paper. I'm just to find, like,

071326.272 -- 071328.362
a recent one, that's, like, called, like,

071328.362 -- 071330.842
understanding word or predict the future, a comfort

071330.842 -- 071332.762
and safe survey for the one about.

071333.082 -- 071335.162
I guess that if, like, find a company and see

071335.162 -- 071337.582
somewhere you're down to the the the program so far.

071337.962 -- 071340.042
So this would be the recommendation on

071340.042 -- 071340.422
my

071342.242 -- 071344.322
k. Thank you very much, and, thank

071344.322 -- 071346.752
you for all the panel

071347.371 -- 071349.531
discussion today. And, I just give a

071349.531 -- 071351.752
brief summarization. So

071351.752 -- 071353.832
today, I think our topic would

071353.832 -- 071356.023
be, like, a way talk a lot about

071356.023 -- 071358.182
latent war model. So maybe in the

071358.182 -- 071400.142
future, latent war model

071400.522 -- 071402.942
is a kind of promising direction about

071403.001 -- 071405.093
word about word model. So thank

071405.093 -- 071406.522
you for all the

071408.111 -- 071410.102
panelists. For your efforts

071410.321 -- 071412.022
today, and also

071412.402 -- 071414.752
thank you for all the attendees

071414.813 -- 071417.302
for, for listening to this

071418.342 -- 071420.751
panel discussion. So our discussion,

071420.751 -- 071422.451
it's, just end.

071423.152 -- 071425.342
So, maybe we can

071425.502 -- 071427.132
move to the next session.

071624.233 -- 071626.202
Okay. I'm sorry.

071626.603 -- 071628.152
Yeah. Oh, is if you come out

071720.702 -- 071722.962
Okay. Let me just give a brief introduction

071723.182 -- 071724.642
about our next speaker.

071726.032 -- 071728.112
Jian Lan Luo from

071728.112 -- 071730.353
AGI Bolt. And

071730.353 -- 071732.433
he is a chief scientist of AGI

071732.433 -- 071734.602
Bolt. And his research interest

071734.602 -- 071737.052
about enabling robust long

071737.192 -- 071739.522
term autonomy for complex real

071739.522 -- 071741.302
world system. And today,

071741.602 -- 071743.721
he's topic is a

071743.722 -- 071745.881
world model powered, robotics

071745.882 -- 071746.382
manipulation.

071749.072 -- 071751.233
Thanks for the introduction and thanks everyone

071751.233 -- 071751.882
for being here.

071753.353 -- 071755.433
I'm Janan. Maybe I should give a quick introduction

071755.433 -- 071757.212
of the companies. Fifteen seconds.

071757.612 -- 071800.362
The company is pretty new. It's founded in 2023.

071800.523 -- 071802.742
It's mainly operated in Shanghai. And

071802.882 -- 071805.092
it's covers basically on the

071805.313 -- 071806.612
full stack of robotics.

071807.562 -- 071809.582
We have lots of humanoid robots.

071809.892 -- 071812.001
I did a research org there. Which

071812.001 -- 071814.372
was founded just this year. Mostly

071814.372 -- 071816.052
focusing on the fundamental research

071816.611 -- 071818.792
in robot learning. We're just we're

071818.792 -- 071820.332
very new, and we just get started.

071821.552 -- 071823.412
From the left, we actually release

071824.422 -- 071826.582
today the largest this is one of Vietnam,

071826.582 -- 071828.762
the largest open source robotic

071828.762 -- 071830.182
dataset is called AEGI award.

071830.902 -- 071831.602
It's online.

071833.302 -- 071835.461
The right is our open source video foundation

071835.461 -- 071836.822
model. It's called Genie Visional

071837.542 -- 071839.132
which we also cover in this talk.

071840.412 -- 071842.592
So I think there are a lot

071842.592 -- 071844.671
of progress on WIDO foundation models in the

071844.671 -- 071847.032
past two years, This

071847.252 -- 071848.792
Cosmos, Weo, and Genie three,

071849.382 -- 071851.462
soar as well. But most of them

071851.462 -- 071853.272
are primarily focusing on

071853.592 -- 071855.412
generating photorealistic videos,

071856.212 -- 071858.412
but what we are really interested in

071858.733 -- 071900.822
at least, is that, we

071900.822 -- 071903.042
want to understand, we want to

071903.202 -- 071905.362
understand the question, how could we leverage

071905.362 -- 071907.452
those models for for robotic

071907.452 -- 071909.472
manipulation problems we're, we're interested

071909.531 -- 071909.792
in.

071911.632 -- 071912.962
So since last year,

071915.582 -- 071917.683
we took our journey on this topic,

071919.172 -- 071921.571
And in this talk, I present two

071921.571 -- 071923.773
ways, which we explored,

071925.071 -- 071927.232
as promising directions leveraging water

071927.232 -- 071929.022
models for robotic manipulation,

071929.292 -- 071931.402
is what we call generative policy learning.

071932.281 -- 071934.233
Basically use trend

071934.452 -- 071936.712
with the foundation models to guide policy learning

071936.853 -- 071939.292
process and the other one is,

071939.513 -- 071941.622
use it as a interactive generative simulator.

071943.022 -- 071943.052
So

071945.382 -- 071947.352
Well, I mean, if you look at the picture,

071947.773 -- 071949.853
he was often in margin first, and and then

071949.853 -- 071951.712
ask if you actually want to shoot a basketball,

071951.892 -- 071954.002
you probably have a have a image

071954.002 -- 071956.042
of a series of image of how you shoot a basketball,

071956.042 -- 071957.952
and then you, and you act

071958.592 -- 072000.111
So in the first walk,

072001.152 -- 072001.652
we

072003.432 -- 072005.512
the the the we basically take a

072005.512 -- 072007.831
first step basically took a first step

072008.542 -- 072010.752
in this direction. So the idea here

072010.752 -- 072013.182
is actually pretty simple. We want we we basically

072013.182 -- 072015.421
ask, can we use the latent

072015.421 -- 072017.632
features Basically, the learn latent

072017.932 -- 072019.912
embeddings from a generative

072020.201 -- 072022.233
water model, essentially, By WER model,

072022.233 -- 072024.392
I really mean video prediction models. To guide

072024.392 -- 072026.352
the policy. And

072026.592 -- 072028.752
what it does is use robot data

072028.752 -- 072030.872
to train a language conditioned video

072030.872 -- 072032.922
models and then use a learn,

072033.202 -- 072034.702
features from the with our models.

072035.342 -- 072037.042
To fine tune to robotic actions.

072038.152 -- 072040.642
In this work, Annivers this

072041.342 -- 072043.442
is actually machine learning center is pretty old.

072044.001 -- 072045.522
Because it's already one year.

072047.132 -- 072049.103
What we have is that we

072049.343 -- 072051.422
at that time, we took some open source

072051.422 -- 072053.662
data and some simulation data,

072053.802 -- 072055.892
and because at the time, we we

072055.892 -- 072056.952
don't have the

072058.402 -- 072100.412
HIBOR WAN dataset yet, we we took

072100.412 -- 072102.492
a lot of real world data that's already open source

072102.492 -- 072104.202
and trained it with a foundation model.

072104.862 -- 072107.183
And then use this WILDL foundation model features

072107.183 -- 072108.622
from the WILDL prediction models

072109.262 -- 072111.312
to basically learn a policy on top of

072111.312 -- 072113.322
it. So, in this

072113.382 -- 072115.002
particular work, we actually

072115.802 -- 072117.992
used the autogenerative diffusion model

072118.313 -- 072120.632
to generate chunk of actions, window chunks,

072120.632 -- 072122.482
not the not the whole videos.

072122.802 -- 072124.581
And it it does contain history,

072124.978 -- 072127.132
but it's it's sparse sparse memory. You

072127.132 -- 072129.212
look at some of the it don't look at

072129.212 -- 072131.292
the whole thing. It picks some key frames

072131.292 -- 072133.542
in the past. Once these video models

072133.542 -- 072135.032
are trained in the second step,

072135.752 -- 072138.012
we fine tune the learned WIDO models

072138.627 -- 072140.852
to to to robotic

072140.852 -- 072143.031
policies, basically to find into robotic action

072143.092 -- 072145.192
data. Because at that time,

072145.192 -- 072147.432
again, we we don't have a lot of real world data, and

072147.432 -- 072149.802
we did actually use a small part of the

072150.192 -- 072152.382
simulation data use Gaussian splattering to augment

072152.382 -- 072154.402
it. To multiple

072154.402 -- 072156.652
views. We here, we're actually using a we're

072156.712 -- 072158.052
actually in a multi view setting.

072159.382 -- 072201.582
So here is this is actually if you if I

072201.582 -- 072203.622
could play this yeah. Great. This

072203.622 -- 072205.872
is actually the generated video from

072205.972 -- 072208.152
two views. From the both the

072208.152 -- 072210.232
head camera and and one side camera.

072210.232 -- 072212.392
For the from the robot.

072213.752 -- 072216.012
Once the the the window

072216.071 -- 072218.452
is trained, we actually have

072218.892 -- 072219.987
additional policy

072220.962 -- 072222.912
the policy head It's a diffusion

072223.052 -- 072225.122
transformer block. And then fine tune this

072225.122 -- 072227.622
block to robotic actions.

072228.652 -- 072230.672
At that time, this is actually, I believe,

072231.132 -- 072233.352
end of last year, the left

072233.352 -- 072235.612
is a figure, the table in the left.

072236.142 -- 072237.932
Is we compare to

072238.652 -- 072240.741
the best method Of course,

072240.741 -- 072242.902
this is actually probably not an

072242.902 -- 072245.352
upper to upper comparison because comparing

072245.352 -- 072246.652
to those VLA models,

072247.531 -- 072249.852
that's fine tune on the same amount of real world

072249.852 -- 072250.502
robotic data.

072251.952 -- 072254.452
In Libro and Calvin, that's a simulation

072254.512 -- 072256.762
robotic benchmark. We actually

072257.002 -- 072258.942
our method actually attained the highest performance

072259.603 -- 072301.902
on those same, on those same benchmark.

072302.882 -- 072305.122
But this is a pretty old result from last

072305.122 -- 072307.451
year. And this is actually

072307.451 -- 072308.651
some of the real

072309.531 -- 072311.692
video, the real experiment we do with the

072311.692 -- 072314.002
actual robot. The the top the

072314.002 -- 072316.022
video I'm playing right now. Is

072316.022 -- 072317.702
a generated

072318.162 -- 072320.382
result from the from the model.

072321.422 -- 072323.462
And on our on

072323.462 -- 072325.552
the bottom, this is actually precise pick and place

072325.552 -- 072327.652
task You need your object sorting. This is as

072327.652 -- 072329.802
a policy route. That I

072329.802 -- 072331.871
can do this precise

072332.172 -- 072333.692
sorting task. Okay.

072336.552 -- 072338.402
So we I talked

072338.622 -- 072340.822
about previously the first step, we're

072340.822 -- 072342.861
actually trying to essentially a

072342.861 -- 072344.642
language and condition of water model. We

072344.942 -- 072346.752
prompt the the model to generate

072347.072 -- 072349.392
realistic videos, and then we fine tune these

072349.392 -- 072351.762
video models to robotic

072351.822 -- 072353.982
policies, The

072354.522 -- 072355.272
second step

072356.532 -- 072358.773
as I mentioned, is that we're also interested into

072358.773 -- 072400.872
using it as a simulator, basically a generative

072401.092 -- 072403.302
simulator. We are asking,

072403.302 -- 072405.282
can we could we actually directly

072406.282 -- 072408.542
condition the continuous robotic actions

072409.192 -- 072411.492
to the water motor. So it could be actually

072412.132 -- 072413.193
used as a simulator.

072414.152 -- 072415.752
So in in the ANIMALS AC,

072416.322 -- 072418.102
I believe one of our colleague will be

072418.661 -- 072420.922
presenting this paper in detail in this workshop

072420.922 -- 072422.946
just just after right after me. As

072422.946 -- 072424.492
well. What I would do is

072425.292 -- 072427.702
we take the foundation model,

072427.972 -- 072430.212
and and we represent

072430.372 -- 072431.992
directly represent the n effective motion, the gripper

072433.252 -- 072435.341
motions, it's we're using

072435.342 -- 072437.682
somewhat interesting representation,

072437.742 -- 072440.232
as you see in the in the images,

072440.611 -- 072442.702
it's the the the the red

072442.702 -- 072444.563
circle and and the green circles,

072444.862 -- 072446.871
the arrows on top of it. That's actually

072446.871 -- 072448.702
directly in the pixel space that

072449.992 -- 072452.012
directly one to one maps to the gripper

072453.092 -- 072455.212
actions in the in the real world. So

072455.452 -- 072457.532
this thing will actually gets bigger when it gets closer,

072457.532 -- 072459.702
and it gets smaller when, when it's

072459.782 -- 072501.802
the gripper is far away. In the

072501.802 -- 072504.122
image. We we we found it actually particular

072504.682 -- 072506.632
effective represent

072506.772 -- 072509.251
an action this way, then you directly contain,

072509.411 -- 072511.313
condition on the continuous

072511.452 -- 072513.492
real numbers of the gripper actions.

072513.722 -- 072515.762
Also actually roll out the

072515.762 -- 072517.772
policies so that we have

072517.772 -- 072519.882
autonomous data. Then using negative data as

072519.882 -- 072521.992
a counterfactual. So the the

072521.992 -- 072524.412
wall model is actually learning the whole support

072524.412 -- 072526.492
of the data distribution, not just on the

072526.492 -- 072527.393
narrow successful

072528.902 -- 072531.182
human demonstrations, rather a mix of

072531.483 -- 072533.621
raw raw data a tons of data, and and

072533.621 -- 072534.762
a part of the demonstrations

072535.962 -- 072538.122
and we're using this simulation simulator

072538.122 -- 072539.582
mainly for policy evaluation.

072542.462 -- 072544.592
In this work, we're actually taking five

072544.592 -- 072546.721
views. So the the model is asked to

072546.722 -- 072548.802
generate the five views, where the robot actually has a

072548.802 -- 072551.072
lot of cameras. To generate five views

072551.132 -- 072553.142
simultaneously. From one

072553.142 -- 072554.162
initial observation image.

072555.291 -- 072557.791
So, here is one video that we actually

072558.222 -- 072600.402
roll the same policy, and then the

072601.842 -- 072603.911
the real world and

072603.911 -- 072605.921
both in the real world and the

072605.921 -- 072608.172
simulator. The in a generative

072608.172 -- 072610.232
sim simulator, So you you could see the

072610.472 -- 072612.712
because this is actually the same policy, you could

072612.712 -- 072614.722
see the the motion

072614.722 -- 072616.942
roughly actually it's actually you

072616.942 -- 072619.362
could not tell the difference. It it matches with

072619.582 -- 072621.702
the same actually much the

072621.847 -- 072623.632
real. And right

072623.853 -- 072625.972
is the table. You evaluate a

072625.972 -- 072628.172
bunch of policies, then

072628.812 -- 072630.852
we compare the success rate basically, the

072630.852 -- 072632.872
performance of the policy, both in

072632.872 -- 072634.983
the model, in the window model,

072635.193 -- 072636.972
and also in the the real world.

072637.292 -- 072639.422
We could see actually the performance

072639.982 -- 072642.058
also actually matches very closely. That

072642.058 -- 072644.002
means our model

072644.382 -- 072646.172
is is relatively,

072646.792 -- 072649.232
effective in capturing the neurons on this precise

072649.791 -- 072650.862
robotic manipulation tasks.

072651.902 -- 072654.082
Here are just more videos. I think this is

072654.082 -- 072656.002
a plane. Again,

072657.152 -- 072659.313
the the circles are actually getting bigger and and smaller

072659.313 -- 072701.432
depending on the distances. These are

072701.432 -- 072703.592
actually two cases of wine is doing

072703.592 -- 072705.831
the, I think, object sorting in a grocery

072705.831 -- 072707.942
store. The other one is trying to make an,

072708.182 -- 072709.562
is trying to make a sandwich.

072710.592 -- 072712.552
From one it's just from one initial image.

072712.792 -- 072714.252
And then we roll out a policy.

072715.622 -- 072717.352
Continuously. Okay.

072717.972 -- 072720.052
We talked about we could use this WILDF

072720.052 -- 072721.662
foundation model or wall model

072722.187 -- 072724.502
as generative policy

072724.502 -- 072726.121
learning, also as a simulator.

072726.773 -- 072729.062
But then the next idea we have is that could

072729.062 -- 072731.172
we actually develop because they

072731.172 -- 072733.332
are separate, we need two models for them, could we

072733.332 -- 072735.102
actually develop one model

072736.142 -- 072738.352
integrated framework encompass all of them

072738.672 -- 072740.382
So in Genie Vision,

072741.722 -- 072743.762
we actually did this. So the

072743.762 -- 072745.812
idea is that we

072745.812 -- 072748.241
have one, what we call, world foundation model.

072749.451 -- 072751.692
That has a base model, but then we could

072751.692 -- 072753.892
use base video foundation model, video

072753.892 -- 072755.912
prediction model to adapt it to

072755.972 -- 072758.062
action models to adapt it to also

072758.062 -- 072800.241
as a simulator and

072800.462 -- 072802.712
then this also serves as a benchmark

072802.712 -- 072804.821
for one model itself. It's a

072804.821 -- 072805.852
benchmark for one model

072807.412 -- 072809.682
rather focusing on the metrics, relevant

072809.682 -- 072812.077
to robotics to embody embody agent.

072812.523 -- 072814.712
Not necessarily means that it has highest

072815.352 -- 072817.752
fidelity image resolution or things like that.

072818.152 -- 072820.312
And by the way, this is all to open

072820.312 -- 072822.352
source. I I believe we are the one of

072822.352 -- 072824.432
the very few works that are actually open

072824.432 -- 072826.722
source arrays and and codes. And everything.

072827.332 -- 072829.592
About the in in a invalid,

072829.752 -- 072831.042
foundation model domain.

072832.722 -- 072833.782
Here is

072834.972 -- 072836.432
what it does. Actually, eventually,

072837.082 -- 072839.322
This is one policy that you fine tune now, a small

072839.322 -- 072841.392
amount of real world data, but by

072841.951 -- 072844.372
by fine tuning the pretrained, with the foundation

072844.432 -- 072846.482
model, the task, is

072846.482 -- 072848.422
to ask a robot doing a

072848.983 -- 072850.862
pretty hard box folding task

072851.377 -- 072853.562
we also ask a robot to

072853.562 -- 072855.352
follow language instructions,

072855.882 -- 072858.062
It needs to place appropriate

072858.202 -- 072900.202
candy with appropriate

072900.502 -- 072902.517
color, in the

072903.152 -- 072905.452
in the box. And also you need to stamp it.

072905.693 -- 072907.773
With, I believe, also

072907.773 -- 072909.862
with we have two two type of

072911.282 -- 072913.443
different colors of the stump. It needs to pick

072913.443 -- 072914.852
the right one and you stump it

072915.491 -- 072917.812
eventually. And it's confusing enough

072917.812 -- 072919.741
for me, Yuan. But I think

072919.902 -- 072921.821
there's also, by the way, one x is is not

072921.982 -- 072924.132
the window is not sped up. This

072924.132 -- 072925.422
is just real time.

072926.702 -- 072928.292
I will just fast forward a little bit.

072929.252 -- 072931.602
So, eventually, it does fold the box

072932.161 -- 072934.412
and, it does rise

072934.808 -- 072935.202
stamp.

072938.492 -- 072939.932
Here, actually more of it,

072941.031 -- 072943.291
this is waterproofing Also,

072943.432 -- 072945.592
because this is actually a cross environment model, it's

072945.592 -- 072947.773
trained on multiple robot

072947.773 -- 072949.792
types, It does further cause

072951.152 -- 072952.693
with different type of robots.

072953.902 -- 072956.312
Which is actually with a Franco robot

072956.312 -- 072958.342
that that that ties

072958.882 -- 073000.802
that folds folds and and and tied it up.

073002.162 -- 073004.281
Okay. So

073004.281 -- 073006.602
for the for the base model, the base video

073006.602 -- 073008.752
prediction model, we're this

073008.752 -- 073010.822
time, we're adopting a pretty lightweight base

073011.042 -- 073012.882
model. It's called LTX. It's it's

073013.122 -- 073015.622
it's open source, but we adopt it because

073015.762 -- 073017.802
you can possibly runs out

073017.802 -- 073019.992
of real time. You use a sparse

073019.992 -- 073022.312
memory as before, it does have a history context,

073022.312 -- 073024.792
but not content. Not throwing off the histories.

073025.902 -- 073027.292
We also adopt a

073028.861 -- 073031.212
multi view consistency by, by using

073032.082 -- 073034.312
coarse attention between the tokens

073034.312 -- 073036.422
of the different views. So that,

073036.582 -- 073038.902
the view generated between other the multiple

073038.902 -- 073040.912
view generated considers a

073040.912 -- 073041.002
cross.

073043.362 -- 073044.992
The action models, we have a separate

073046.152 -- 073048.552
action head. We have a group of diffusion

073048.552 -- 073050.693
transformer blocks that's connected

073052.222 -- 073054.302
properly to the generation diffusion

073054.302 -- 073056.592
block. One by one. And

073056.728 -- 073058.502
then once

073058.822 -- 073100.802
the foundation model was trained, we

073101.042 -- 073103.152
trained the action the

073103.152 -- 073104.821
action block action expert

073105.582 -- 073107.983
as before. And this was

073108.042 -- 073110.142
trend on types of robots.

073110.142 -- 073112.372
When Franco our own robot, and

073112.372 -- 073114.152
the other one is some

073114.452 -- 073115.822
some some platform.

073118.102 -- 073119.882
And for the result, I believe,

073120.262 -- 073122.712
if you look at the table, in general,

073123.132 -- 073125.232
all method was able to actually achieve higher

073125.232 -- 073127.631
performance. Than just using

073127.632 -- 073129.871
the VA models, that are trend that was fine

073129.871 -- 073131.942
tuned on the same amount of real world data.

073132.342 -- 073134.422
Probably because during the window pre training, you

073134.422 -- 073135.961
see small history context.

073136.821 -- 073138.922
And and it captures more neurons of dynamics

073139.302 -- 073141.452
between in the task, so it

073141.832 -- 073143.212
actually attends higher performance.

073144.058 -- 073146.352
So For specific

073146.412 -- 073148.532
training, what we do is,

073148.692 -- 073150.712
this is actually getting into the details. What

073150.712 -- 073152.672
we found actually pretty effective is

073153.232 -- 073155.617
once we have the the video

073155.782 -- 073157.992
WIDO deficient model trend, We

073157.992 -- 073200.072
actually freeze it have an action pretraining

073200.072 -- 073202.082
step. So we just in this step, we we

073202.082 -- 073204.318
just take basically all the robot data we

073204.318 -- 073206.352
have, This is actually generic robot data,

073206.352 -- 073208.241
not particularly one specific task.

073208.522 -- 073210.942
We we fine tune we pretrained the action

073211.241 -- 073213.102
diffusion block, and the second step,

073213.392 -- 073215.552
freeze the action diffusion block, and then

073215.552 -- 073217.652
we fine tune the video diffusion block to

073217.652 -- 073219.902
this one specific target we are interested in.

073219.902 -- 073221.982
And then in the first step, we actually open everything and

073221.982 -- 073224.092
fine tune this end to end, to

073224.092 -- 073226.282
the specific task. Like closed folding

073226.282 -- 073228.452
or or box folding. That I just showed

073228.452 -- 073228.822
before.

073232.103 -- 073234.042
Here is, some more video

073234.152 -- 073236.372
could see, this is actually the video generation.

073236.612 -- 073238.693
By conditioning on the continuous action. This is actually the

073238.693 -- 073241.112
simulator. It

073241.492 -- 073243.581
does actually generate pretty good pretty

073243.581 -- 073245.902
good realistic videos and also physically

073245.902 -- 073248.252
consistent videos. The

073248.252 -- 073250.292
bottom is the actions the gripper

073250.292 -- 073252.742
right now is taking, it's representing

073253.362 -- 073254.222
where to go.

073257.102 -- 073259.002
And here is the

073259.142 -- 073301.452
widow here is showing more

073302.853 -- 073304.851
more cases. When more different,

073305.411 -- 073305.862
challenging task.

073307.702 -- 073309.712
I believe there should be one handling one deformable

073309.712 -- 073311.893
object. It's not

073311.893 -- 073313.973
showing here, but it's the pouring water

073313.973 -- 073315.853
and pick up object. Etcetera.

073318.532 -- 073320.042
Also, for the

073321.831 -- 073324.262
the last last component of this work,

073324.732 -- 073325.956
is the

073326.821 -- 073328.402
embodied wall wall model benchmark.

073329.041 -- 073329.541
So

073331.542 -- 073332.682
rather than evaluating

073333.571 -- 073335.592
the FID of the generated videos,

073337.223 -- 073339.272
In robotics, we really

073339.272 -- 073341.002
care about is

073341.532 -- 073343.612
the generated trajectory, if they are consistent

073343.612 -- 073345.661
or not. And the the

073345.661 -- 073347.667
scene semantics, if it's consistent or

073347.667 -- 073349.852
not. And also, does it actually

073350.152 -- 073352.392
perform consistent across a

073352.392 -- 073354.411
variety of diverse a variety

073354.712 -- 073356.542
of of the of the scenarios.

073357.172 -- 073359.282
So compared

073359.422 -- 073401.442
to the wall models, that's

073401.442 -- 073402.512
available right now,

073404.992 -- 073406.932
we we we we didn't found a

073407.172 -- 073409.392
pretty convincing metric. Like,

073409.612 -- 073411.662
basically, other models are primarily focused on

073411.983 -- 073413.482
generating photoresist videos,

073414.281 -- 073416.361
and this benchmark, what we call it,

073416.361 -- 073418.601
EWMBench, essentially is

073418.602 -- 073420.781
a benchmark evaluating the the model model

073421.002 -- 073423.382
itself. In a robotic setting in a robotic

073423.382 -- 073425.542
robotic context. We have a

073425.782 -- 073428.202
set of metrics that, we propose

073428.752 -- 073430.762
we found actually pretty along well with the

073430.762 -- 073432.772
real world setting we're talking about, what I just talked

073432.772 -- 073434.882
about. And

073434.882 -- 073437.201
I think the the good

073437.201 -- 073439.282
thing is that everything was

073439.282 -- 073441.362
open source. It's it's on our GitHub,

073441.523 -- 073443.802
So if you're interested in we're

073443.802 -- 073445.372
also maintaining this repo

073446.252 -- 073448.582
we'll keep updated. We're come

073448.582 -- 073450.762
to download it and and try it out. It's

073451.603 -- 073453.762
it's it's it's very easy to to

073453.762 -- 073455.512
to try it out. Okay.

073456.527 -- 073458.732
So I talk about, how we

073458.812 -- 073500.272
how could we use it

073501.172 -- 073503.193
for action condition, for generating,

073503.252 -- 073505.362
for permit, for to

073505.662 -- 073507.982
generate windows,

073507.982 -- 073510.282
and also Genie Visionary that actually combines

073510.523 -- 073512.542
two of those, to have one unified

073514.722 -- 073516.182
framework that does evaluation,

073517.322 -- 073519.562
video generation, and policy learning, in

073519.562 -- 073521.742
one loop. But

073521.862 -- 073524.022
we start to ask, basically,

073524.081 -- 073525.862
for the wall model, if it could

073526.502 -- 073528.721
generate a video, then

073528.722 -- 073530.802
you generate the first frame, you generate the last

073530.802 -- 073532.922
frame. And you also,

073532.922 -- 073535.122
of course, generate anything in between. Then

073535.122 -- 073537.031
it actually naturally well suited

073537.523 -- 073539.632
for constructing goal condition policies.

073540.032 -- 073540.992
A goal condition policy

073542.353 -- 073544.522
is you take a

073544.982 -- 073547.222
current observation, and you take a goal image.

073547.222 -- 073549.462
Basically, you specify your intention by taking a

073549.462 -- 073551.441
goal image, you ask the robot

073552.912 -- 073554.732
what to do. Basically, you ask the model

073555.772 -- 073557.821
what to do from my current observation. How do

073557.821 -- 073559.522
I do from my current observation

073600.932 -- 073603.031
So that I can go to my goal.

073603.672 -- 073605.611
Go image. This actually this method

073605.911 -- 073607.212
is particularly useful

073608.622 -- 073610.332
when other means of

073611.052 -- 073612.992
specifying specifying intentions

073613.512 -- 073615.722
are hard. Let's say, you

073615.722 -- 073617.802
wanna draw something, like, draw letter a,

073617.802 -- 073619.982
and that letter a has a specific shape.

073621.022 -- 073623.342
It's probably much easier if you just draw

073623.342 -- 073625.502
that letter a in that particular shape you

073625.502 -- 073627.560
want, and then present it to a robot, and you

073627.560 -- 073628.852
let a robot to do it.

073630.132 -- 073631.892
So in actual goal, this is a work

073632.292 -- 073634.372
we are releasing pretty soon, I believe one

073634.372 -- 073636.853
week from now, We

073636.992 -- 073639.172
really actually take

073639.172 -- 073640.951
a step towards this direction.

073641.672 -- 073643.012
What we have is that

073643.892 -- 073645.982
instead of generating the whole

073645.982 -- 073648.292
video, we basically

073648.542 -- 073650.942
we additionally making

073651.002 -- 073653.021
the warm model become goal

073653.022 -- 073655.041
condition, is condition on the current observation,

073655.893 -- 073658.082
also condition on the goal image.

073658.082 -- 073700.342
You you put you take a image of the

073700.991 -- 073703.392
the thing you want the robot to do, and it was

073703.392 -- 073705.402
asked to generate a

073705.402 -- 073707.422
set of set of frames, basically, a

073707.422 -- 073709.571
set of key frames, between the

073709.571 -- 073711.582
current observation and and the goal

073711.582 -- 073713.662
goal image. The current observation can be can be

073713.662 -- 073714.162
anything.

073716.302 -- 073718.752
So once we train this

073718.892 -- 073721.132
this was towards this step, everything

073721.132 -- 073723.271
is was action free. We don't use

073723.272 -- 073725.222
label action for this step, Essentially,

073725.522 -- 073727.382
this could be amenable to using

073727.773 -- 073729.482
any sort of, videos

073730.041 -- 073732.032
videos on the on the Internet, maybe

073732.412 -- 073734.461
human videos, Eagle Ford, and things like that. And

073734.461 -- 073736.422
in fact, we did use a small portion of it.

073737.603 -- 073739.693
After the pretraining was done, we

073739.693 -- 073741.892
actually additionally attached an action

073741.892 -- 073744.071
expert to the goal condition

073744.132 -- 073745.532
one model we just trained.

073746.342 -- 073748.842
And then this one was actually fine tuned to robotic actions.

073751.513 -- 073753.933
So here is some, experiment

073754.152 -- 073756.102
we did what we can achieve

073756.342 -- 073758.682
this method. It's pretty simple, but

073759.872 -- 073801.491
pretty, pretty use pretty effective.

073802.251 -- 073804.701
So, the right corner, the right bottom

073804.701 -- 073806.692
corner, is a gold condition

073806.852 -- 073809.062
would basically take an image the goal condition,

073809.062 -- 073811.142
and then we present you a robot present

073811.142 -- 073813.352
to your to your model. And and the video

073813.352 -- 073815.222
was the robot executing

073815.361 -- 073816.422
the policy autonomously

073817.392 -- 073819.062
by conditioning on the

073819.462 -- 073821.622
on the go. So the left is

073822.781 -- 073825.092
object sorting task. We want it to

073825.092 -- 073827.482
be a into a particularly

073827.942 -- 073830.252
order and into that particularly position.

073830.842 -- 073832.902
The middle is a writing

073832.902 -- 073834.361
task. We basically present

073834.962 -- 073837.152
a robot with a set of images that

073837.272 -- 073839.081
with a specific

073839.461 -- 073841.701
English word, we want a robot to

073841.701 -- 073841.742
write.

073843.902 -- 073845.402
And and it just write

073845.942 -- 073847.732
a write a ink swab in a in a whiteboard.

073848.922 -- 073850.442
The rightmost is

073851.872 -- 073853.972
we wanted that this task is

073853.972 -- 073856.251
actually pretty precise. It's also object

073856.251 -- 073857.952
is pretty heavy. We want you to grab it,

073858.433 -- 073900.667
and insert the the the

073900.667 -- 073903.072
peg precise peg into a specific

073903.692 -- 073904.902
blocks specific holes

073905.712 -- 073908.132
into the blocks. Actually

073908.432 -- 073910.183
perform pretty well. So

073911.763 -- 073912.202
For

073913.871 -- 073915.411
that's the second step. For, basically,

073916.132 -- 073918.212
if we want to further improve the performance

073918.212 -- 073919.273
of the policy,

073920.292 -- 073922.321
or if the policy is

073922.321 -- 073924.262
actually in a totally new situation,

073924.682 -- 073926.202
we actually have a

073927.082 -- 073929.262
online autonomous improvement procedure,

073930.812 -- 073932.982
in this regard. Because it's a goal

073932.982 -- 073935.422
condition policy then what we could do is

073935.582 -- 073938.082
we if it's in a totally new situation,

073938.142 -- 073940.052
we could take one,

073940.592 -- 073942.991
go image in that situation. Of course, the robots

073942.991 -- 073945.142
will actually initially, if you haven't seen

073945.142 -- 073947.161
that situation, most likely

073947.222 -- 073949.392
will fail a lot. But

073949.392 -- 073951.672
we we leverage a technique called

073951.672 -- 073953.612
HER, or hand side experience replay,

073953.882 -- 073955.832
Essentially, it's practicing

073956.032 -- 073958.306
all the data in the

073958.306 -- 074000.471
replay buffer, treating all the from

074000.472 -- 074002.712
the episode every single observation

074002.712 -- 074004.773
as they go. As a goal as a

074004.773 -- 074006.402
goal image, and then it tries to

074006.802 -- 074008.882
use that as a practice as

074008.882 -- 074010.661
a to to practice on that

074011.262 -- 074012.362
generating the new goal.

074013.432 -- 074015.621
So here's actually one

074015.621 -- 074016.121
video

074017.642 -- 074019.792
that we actually ask a robot to

074019.792 -- 074021.922
write to draw a

074022.482 -- 074024.722
a picture that's actually was never seen in the

074024.722 -- 074026.951
training set. Initially pull a

074026.951 -- 074029.252
web polling, It doesn't know how to draw this

074029.472 -- 074031.252
two square layout in this way.

074031.683 -- 074032.642
But with her,

074033.752 -- 074035.902
and we take

074035.902 -- 074038.322
some and also some, online autonomous

074038.382 -- 074040.773
improvement steps, robot was

074041.832 -- 074043.992
able to draw it. You could say it's actually

074043.992 -- 074045.262
improving on the fly.

074046.222 -- 074048.062
About ten to twenty minutes.

074049.182 -- 074051.232
This window is explaining

074051.232 -- 074053.332
fiber aspect, About twenty to twenty

074053.332 -- 074055.222
minutes, is joined to

074055.523 -- 074057.357
basically to the

074057.912 -- 074059.762
the to the desired shape of

074100.342 -- 074102.412
of the going into that. That. We're

074102.552 -- 074103.562
asking the robot to do.

074105.321 -- 074107.362
If I could go to the end,

074107.362 -- 074109.512
in the end, it's actually a pretty

074109.512 -- 074111.632
good shape. So

074112.632 -- 074114.712
the right is the right table

074115.752 -- 074116.997
of is

074117.812 -- 074120.052
something, is the right the right table

074120.052 -- 074122.071
is, we would test this method

074122.832 -- 074123.892
based on five different situations.

074125.192 -- 074126.872
Drawing and all or five different tasks.

074127.452 -- 074129.492
Robot actually performed decently well. All

074129.492 -- 074131.302
of these tasks. It does actually improve

074131.943 -- 074133.862
with practical range of

074134.023 -- 074136.202
within practical range of real world interaction

074136.262 -- 074138.402
time. And, also, the RIMALS is

074138.402 -- 074140.472
that we also did ablations

074141.031 -- 074143.111
for the replay buffer for the HER2

074143.111 -- 074145.062
practice. If we need

074145.122 -- 074145.922
to include successful

074147.502 -- 074149.741
failure trajectories. I think the conclusion is

074149.741 -- 074151.902
that we just we don't need to

074151.902 -- 074153.932
really care about it. We could just let

074153.932 -- 074155.981
robot practice all the

074155.982 -- 074158.212
data it has on the repair buffer. And

074158.212 -- 074200.393
it does actually improve, steady autonomously.

074201.013 -- 074201.402
Over time.

074203.143 -- 074205.382
Great. That's everything we have. I'd be happy to

074205.382 -- 074206.042
take questions.

074227.942 -- 074229.642
Okay. Let's send to

074230.611 -- 074232.643
the speaker again. And, I

074232.643 -- 074234.882
will, introduce the next

074234.882 -- 074237.022
speaker. Pablo

074237.722 -- 074238.582
Samuel Castro

074239.782 -- 074241.483
from DeepMind and Mila,

074241.922 -- 074244.002
He's a senior staff research scientist

074244.002 -- 074245.781
in Google DeepMind in Montreal.

074246.682 -- 074248.782
He's also an adjunct professor in

074249.492 -- 074250.792
department of computer science

074251.571 -- 074253.762
and operations research at

074253.762 -- 074255.831
the University of Montreal. His

074256.152 -- 074258.552
research interest about fundamental reinforcement

074258.552 -- 074300.892
learning research. And today,

074300.892 -- 074302.432
he will give us an introduction

074303.611 -- 074306.002
about automated reward,

074306.252 -- 074308.272
machines with via

074308.272 -- 074310.132
foundation models for

074310.572 -- 074312.592
conversational reinforcement learning.

074312.652 -- 074314.193
Let's welcome the speaker.

074346.882 -- 074349.232
Okay. Hello, everyone.

074349.902 -- 074351.922
Thank you for being here. Thank you to the organizers

074352.143 -- 074354.272
for inviting me.

074354.432 -- 074356.552
Although when they invited me, my first question was

074356.552 -- 074358.912
like, why me I don't really

074358.912 -- 074400.212
work on world models.

074401.012 -- 074403.172
And that's why the first question is like, why

074403.172 -- 074405.443
am I here And

074405.443 -- 074406.962
as I thought about it more,

074408.772 -- 074411.042
listening to the the previous talks, I was thinking

074411.042 -- 074413.232
more about what is a world model.

074413.232 -- 074415.312
So when we talk about world models, what do we

074415.312 -- 074417.371
mean by world models Maybe I do

074417.371 -- 074419.162
fit in this world. Maybe I am actually

074419.693 -- 074420.982
research in world models.

074421.861 -- 074424.022
So of course, when the natural

074424.022 -- 074426.082
things that come up come to mind when we think of world

074426.082 -- 074428.422
models are things like Genie, which we saw, presented

074428.482 -- 074430.513
earlier. We're simulating

074430.652 -- 074432.972
worlds and and we're able to interact with worlds.

074433.052 -- 074435.292
We can think of world models also as,

074435.531 -- 074436.912
and this is I think traditionally

074437.973 -- 074440.052
prior to Genie and things like that, what people would think

074440.052 -- 074442.242
of with world models, something like Dreamer, where

074442.662 -- 074444.541
where it's learning to reconstruct

074445.102 -- 074447.422
inputs, pixel inputs, but it actually

074447.422 -- 074449.472
does planning in latent space. Right So it's

074449.472 -- 074451.872
no longer trying to reconstruct pixel to pixel

074451.872 -- 074453.552
reproductions. It's using this

074454.032 -- 074456.352
information to be able to then do, this

074456.352 -- 074458.372
imagination planning or the dreaming

074459.002 -- 074501.242
where the name comes from, in order to be able to plan

074501.242 -- 074503.282
more effectively. But

074503.282 -- 074505.362
we can go back further into 2021

074505.362 -- 074506.662
and something like SPR,

074507.532 -- 074509.952
where there is reconstruction loss

074510.142 -- 074512.252
that's happening in

074512.252 -- 074514.412
latent space. Again, we're not SVR wasn't

074514.412 -- 074516.462
trying to reconstruct pixel the pixels. It was trying

074516.462 -- 074518.242
to reconstruct these latent states,

074518.622 -- 074520.832
and comparing things in the future.

074520.892 -- 074523.242
So this is in a sense also a form of world

074523.483 -- 074525.512
model, but not in the traditional way we've

074526.472 -- 074528.732
we tend to think about it nowadays where we're not

074529.332 -- 074531.702
sort of generating new trajectories,

074531.702 -- 074533.402
new new videos, new pixels.

074533.952 -- 074536.132
To be able to plan with that. But we are constructing

074536.273 -- 074537.262
some form,

074539.792 -- 074542.032
of structure or or some form of

074542.032 -- 074544.361
reconstruction in in a sense of

074544.361 -- 074546.452
of the world that the agent is interacting in.

074547.092 -- 074549.412
We can go back even further to 2002.

074549.412 -- 074551.803
So this is the EQ algorithm from

074552.023 -- 074554.523
Kurz and Singh. And here, this this paper

074554.582 -- 074557.052
was basically proving theoretically

074557.111 -- 074559.192
how you can, get optimal exploration

074559.192 -- 074601.552
or near exploration in in polynomial

074601.692 -- 074603.771
time. And they did this by constructing this

074603.772 -- 074605.842
sort of met MDP where

074606.482 -- 074608.742
you have known and unknown states and so the agent transitions

074609.103 -- 074611.122
in this met MDP, from known and

074611.122 -- 074613.172
unknown states and this allowed them to

074613.172 -- 074615.332
to have their theoretical guarantees, which at the time

074615.332 -- 074617.272
that's all we could really do with R l.

074618.212 -- 074620.292
Aside from from toy environments. So all of

074620.292 -- 074622.712
these, I consider them kind of like world models.

074623.462 -- 074624.443
Because we're constructing,

074625.722 -- 074628.063
we're we're taking the the environment and

074628.202 -- 074630.362
and you know, trying to to

074630.733 -- 074632.752
to con have some internal representation

074632.893 -- 074634.322
of it that allows us to do

074634.962 -- 074636.662
learning faster or planning faster.

074637.433 -- 074639.353
Another type of world model that,

074639.592 -- 074641.372
thinking about this are simulations.

074641.672 -- 074643.732
So these are three Nature papers

074643.732 -- 074645.902
that, were deployed

074645.961 -- 074648.022
in real world systems.

074648.022 -- 074650.182
On the left, we have, this is a project I worked

074650.182 -- 074652.582
on, strategic, strategy balloons to deliver

074652.803 -- 074654.832
Internet. Sophie would

074655.072 -- 074657.233
was deployed in in real, Gran Turismo

074657.233 -- 074659.331
games. The tok Tokamak,

074659.652 -- 074701.702
simulator that that's being used in

074701.702 -- 074704.042
in fusion reactors to try to get, fusion

074704.183 -- 074706.443
energy. All of these relied on

074706.662 -- 074708.822
really high fidelity simulators, and that's how they

074708.822 -- 074710.831
were able train in simulation. And

074710.831 -- 074713.252
that's the simulators themselves are world models.

074713.472 -- 074715.522
So what came to mind

074715.522 -- 074717.772
to me is that for me at least, world models

074717.772 -- 074719.792
taking taking a more of a generous

074719.932 -- 074721.942
or or wider net in terms

074721.942 -- 074724.022
of how I define world models in reinforcement

074724.022 -- 074726.042
learning in particular, I view them

074726.042 -- 074727.822
as kind of modifications or applications,

074728.202 -- 074730.312
simulations, simulations. Of the

074730.312 -- 074732.343
environment that you care in. And

074732.343 -- 074734.742
the purpose of of these is to help learning efficiency.

074734.742 -- 074736.362
Right So why do we care about reconstructing

074737.121 -- 074739.322
decisions. Pretty videos, but ultimately,

074740.103 -- 074742.182
for us as oral researchers, what we wanna do

074742.182 -- 074744.327
is train agents quickly and and

074744.327 -- 074746.512
effectively. So under that lens

074746.973 -- 074749.313
of, of what world models mean,

074750.662 -- 074752.712
I I do I guess, consider myself

074752.712 -- 074754.712
a world model, so it's it's okay that I'm here,

074755.392 -- 074757.892
I hope it's okay that I'm here speaking to you about this.

074759.092 -- 074801.252
Okay. So this is the standard thing that you've seen in

074801.252 -- 074802.922
many RL talks. We have

074803.861 -- 074805.962
the MDP, or an agent interacting with an environment, etcetera.

074806.632 -- 074808.871
And one of the key aspects for I mean,

074808.871 -- 074811.192
the key component for why we call the

074811.192 -- 074813.542
the our field reinforcement learning is the reward.

074813.902 -- 074816.381
This is the reinforcement that the agent is receiving,

074816.462 -- 074818.483
and it's incorporating that signal

074818.632 -- 074821.112
in order to adjust its behavior or its policy

074821.112 -- 074823.161
so that it can, the tests that

074823.161 -- 074825.412
we wanted to do. And how do we specify

074825.412 -- 074826.872
this reward This is extremely

074827.652 -- 074829.712
important and it's how we

074829.712 -- 074832.212
get agents to to perform well or fail.

074832.832 -- 074834.932
Like, a very simple example is original

074835.092 -- 074836.872
DQN paper that sort of spearheaded

074837.415 -- 074839.543
that or started the field of DeepRRL that that

074839.543 -- 074841.031
most of us are working in nowadays.

074841.672 -- 074843.832
They had all these, you know, 57 Atari

074843.832 -- 074845.792
games, and they all have very different rewards.

074846.193 -- 074848.352
Scales. So training a single network to deal with all of

074848.352 -- 074850.353
them proved. Quite difficult, and

074850.353 -- 074852.433
so the the approach they took is they clip rewards between

074852.433 -- 074854.322
negative one and one. And

074854.563 -- 074856.643
again, this is taking very generous view of

074856.643 -- 074858.803
world models, but I kind of view this as a world model in

074858.803 -- 074900.962
itself. Because, again, it's taking this modification

074901.102 -- 074903.181
of the environment that they cared of at the

074903.182 -- 074905.222
time and adjusting it in order to

074905.222 -- 074907.302
achieve, learning efficiency. And in this case,

074907.302 -- 074909.402
it was a unified approach

074909.402 -- 074911.482
to to learning, good strategies for all of these

074911.482 -- 074911.852
games.

074913.693 -- 074915.933
Rewards are very critical. This is a a famous

074915.933 -- 074917.872
example from a few years ago from OpenAI.

074918.092 -- 074919.212
Where they had this

074920.732 -- 074922.851
boat racing game, and, the boat the the agent

074922.851 -- 074925.002
found figured out that it doesn't So

074925.462 -- 074927.672
humans play this game, they wanna to finish the race quickly.

074928.273 -- 074930.433
But the the agent, because the number

074930.433 -- 074932.452
of points it got is part of the reward

074932.452 -- 074934.562
function, it figured out that if if it just does this

074934.962 -- 074936.982
these loops over and over again, it gets a whole bunch of

074937.222 -- 074939.302
points, but it's not actually doing the thing we

074939.302 -- 074941.712
wanted it to do, which is race the boat quickly. It's just

074941.872 -- 074943.892
accumulating points because that's what the reward

074943.952 -- 074946.092
function, said it should do. And this is often

074946.092 -- 074948.031
referred to as reward hacking and, of course,

074948.202 -- 074950.362
it's something that, we care about deeply

074950.362 -- 074951.792
especially nowadays where

074952.832 -- 074954.912
many of, the peep of of us and and others in

074954.912 -- 074957.212
the field are are thinking about post

074957.212 -- 074959.152
training or LHF or LVR, etcetera,

074959.446 -- 075001.722
where maybe not LVR, but RHLHF,

075001.781 -- 075003.922
where you're training a reward function, and this is what

075004.482 -- 075006.982
going to adjust the behavior of your of your foundation

075007.362 -- 075009.142
models to them to do the thing you want them to do.

075010.741 -- 075012.762
Coming back to this paper that we worked on,

075013.002 -- 075015.161
with some colleagues a few years ago, we were trying to

075015.161 -- 075017.211
deploy Deep RL in,

075017.451 -- 075019.611
stratospheric balloons to bring, Internet,

075019.772 -- 075021.822
across the world. And

075022.063 -- 075024.222
we had a a good simulator. We had a really, really

075024.223 -- 075026.483
high fidelity simulator for this, but

075026.983 -- 075029.382
the people working on on on there was a company,

075029.382 -- 075031.422
Loon, that were working on this weren't thinking

075031.462 -- 075033.332
of reinforcement learning, and they had handcrafted

075033.712 -- 075035.932
controllers for for the balloon. And

075035.932 -- 075038.092
so what they they asked us to do is

075038.092 -- 075040.112
to see if we can use RL

075040.112 -- 075042.272
to improve on the controls because what they wanted,

075042.432 -- 075044.582
what they needed is this balloon to come over

075044.582 -- 075046.682
a certain area to deliver Internet

075046.742 -- 075049.052
to that area. And the way

075049.052 -- 075051.192
the balloon, navigates was just

075051.192 -- 075053.432
basically by surfing winds. It could only go up and down,

075053.432 -- 075055.562
and then the stratosphere winds push you

075055.563 -- 075057.802
in different directions. And so the balloon has to sort of

075057.802 -- 075100.193
do these fancy figure eight things

075100.492 -- 075102.781
to stay one place. Otherwise, it just gets pushed,

075103.241 -- 075104.202
across the world.

075105.322 -- 075107.422
And this is one trajectory of the balloons

075107.483 -- 075109.571
sort of navigating over

075109.571 -- 075111.807
to this this

075112.262 -- 075114.422
area where we wanted to deliver Internet, and this was

075114.422 -- 075116.211
controlled by our our Deep RL agent.

075116.452 -- 075118.572
So when we started this project, we came up

075118.572 -- 075120.652
with a reward function which we felt was

075120.652 -- 075122.733
was pretty natural, where you want to

075122.733 -- 075124.862
have high reward if,

075125.112 -- 075127.192
you're in the area of interest,

075127.352 -- 075129.568
and lower reward the further away you

075129.568 -- 075131.672
get. So you can see that we want to

075131.672 -- 075133.291
be within 50 kilometers

075134.012 -- 075136.022
of the the center of that circle,

075136.022 -- 075138.063
of that target circle. What

075138.063 -- 075140.303
ended up happening when we use this reward function

075140.303 -- 075142.312
is that the balloon figured it out figured out

075142.312 -- 075144.072
how to navigate to the to this,

075144.353 -- 075146.502
area, but then it just kind of hugged

075146.502 -- 075148.572
the circle. It just stood stayed

075148.572 -- 075150.111
in the perimeter around the circle,

075150.742 -- 075152.902
didn't really go in because there was no real incentive to

075152.902 -- 075154.983
go in. It actually it seemed like,

075155.872 -- 075157.892
it was getting more signal by just kind of

075158.052 -- 075200.132
stepping outside of the circle and coming back in, and

075200.132 -- 075202.232
that's where what the reward function

075202.632 -- 075205.062
led it to produce, which is not what we wanted. And

075205.062 -- 075207.222
again, this is the difficulty and the importance of

075207.222 -- 075208.862
coming up with good reward functions.

075211.712 -- 075214.031
And when we tackle problems that aren't

075214.031 -- 075216.202
given to us in in benchmark or existing

075216.202 -- 075218.542
simulators, this is one of the key issues. How do we

075218.683 -- 075220.911
define an environment is and in

075220.911 -- 075223.107
particular do we define what the reward function is so that

075223.107 -- 075225.183
we get the behavior that

075225.183 -- 075227.342
we actually expect, which isn't always specified

075227.342 -- 075229.002
cleanly in as a reward function.

075229.822 -- 075231.902
What we decided what we're what I'll be talking to you

075231.902 -- 075234.202
about is, work that we've been doing

075234.202 -- 075236.282
with, with, some students and and

075236.282 -- 075238.661
colleagues at Mila on seeing if we can use

075238.661 -- 075240.702
some of these modern foundation models

075241.422 -- 075243.742
to help us in this task of designing reward

075243.742 -- 075245.792
functions that can help speed up the

075245.792 -- 075247.871
learning and make us make our agents learn,

075248.031 -- 075249.172
the tasks effectively.

075250.282 -- 075252.122
So what I'll be presenting is a paper that we,

075252.443 -- 075254.621
put out not long ago led

075254.621 -- 075256.701
by, my student, Roger, who might

075256.701 -- 075258.932
be around here. Glenn was also giving a talk earlier.

075258.932 -- 075301.172
I don't know if he's still around, so you can also go bug

075301.172 -- 075303.522
them. If I don't answer your questions

075303.682 -- 075304.502
to satisfaction.

075305.992 -- 075308.152
And so what we're doing, what we wanted

075308.152 -- 075310.631
to leverage in this work is the use of reward

075310.632 -- 075312.882
machines. Which is a a notion that the,

075313.202 -- 075315.302
an idea that was introduced a few years ago

075316.142 -- 075318.541
and it's to try to specify these reward

075318.541 -- 075320.652
functions. We typically think of reward functions

075321.611 -- 075323.632
as a mapping from states, to to

075323.632 -- 075325.951
real numbers or states and actions to real numbers.

075326.111 -- 075327.482
This was trying to give more structure

075332.182 -- 075334.342
in in a way that's easier to specify

075334.342 -- 075336.452
than these, state action, reward

075336.452 -- 075338.595
functions. So as an example that they

075338.595 -- 075340.683
use in this paper, they have this offer grid

075340.683 -- 075342.922
world where the agent is supposed to bring coffee to

075342.922 -- 075345.172
the office and also, maybe get

075345.172 -- 075347.352
the mail and avoid these, decorations

075347.412 -- 075349.492
because if it steps on the decorations, it breaks them or

075349.492 -- 075351.603
something like that. And so the

075351.603 -- 075354.052
reward function you know, we could specify

075354.193 -- 075356.272
it manually where you get a reward of one if you

075356.272 -- 075358.642
get the coffee and a reward of 10 if you deliver

075358.642 -- 075400.693
the coffee to the office. But it

075400.693 -- 075403.012
starts becoming combinatorial because you have,

075403.652 -- 075405.842
these prepositions of whether you have the cough

075405.842 -- 075407.602
or not, whether you're in the office, whether you expect

075408.002 -- 075409.621
stepped on a decoration, etcetera.

075410.242 -- 075412.442
So even without that, you have already, you know,

075412.442 -- 075414.842
a 108 states with four actions, which isn't

075414.842 -- 075416.962
that bad, but if imagine scaling

075417.182 -- 075419.262
this arbitrarily, and then it becomes quite difficult to

075419.262 -- 075420.852
specify these rewards manually.

075421.332 -- 075423.652
So their proposal is to use reward machines, which

075423.652 -- 075425.763
looks something like this. They're finite state automata,

075425.763 -- 075427.962
which have these prepositions like you

075427.962 -- 075430.041
have a coffee or not Do you did you

075430.041 -- 075432.361
step on a decoration And it has these

075432.361 -- 075434.632
terminal states, and you have rewards.

075434.872 -- 075437.032
So if if you're in the office, you haven't

075437.032 -- 075439.201
stepped on the on the

075439.201 -- 075441.522
decoration and and you and you delivered the coffee,

075441.522 -- 075443.172
then then you get a reward of one.

075444.852 -- 075447.112
And formally, this is the the the formalism

075447.172 -- 075449.290
for the reward machine. I won't go into,

075449.451 -- 075451.711
too much of the details. The only thing I'll mention

075451.792 -- 075453.952
is, the the states u.

075453.952 -- 075456.201
So these are states of the automata

075456.262 -- 075458.172
that define the the reward machine.

075458.332 -- 075500.542
This will come back later, so just wanted to highlight

075500.542 -- 075500.872
that.

075503.612 -- 075505.701
So one of the tasks we started looking

075505.701 -- 075507.621
at, and this will be sort of the running example,

075507.781 -- 075509.822
throughout the talk, is

075509.822 -- 075512.002
a mini grid. Mini grid, we can define,

075512.222 -- 075514.422
these pretty challenging tasks where

075514.422 -- 075516.582
the agent has to pick up the yellow key,

075516.582 -- 075518.752
open this yellow door, pick then get the

075518.752 -- 075520.911
red key. Then open the red door and finally

075520.911 -- 075523.103
get to the the green, the

075523.103 -- 075525.122
green dot. So designing

075525.122 -- 075527.092
a reward function to produce

075527.392 -- 075529.502
this, this behavior is

075529.502 -- 075531.742
quite challenging, and you can do it again just

075531.742 -- 075533.762
by defining, numerical rewards when

075533.762 -- 075536.102
it picks up the different objects, etcetera.

075536.702 -- 075538.943
It becomes a very difficult, sparse reward,

075538.943 -- 075541.241
long horizon problem, which we all know as RL

075541.241 -- 075543.392
researchers, is is quite challenging. And

075543.392 -- 075545.472
also to specify the reward becomes difficult. It's a

075545.472 -- 075547.672
lot easier to do it how I just did explaining

075547.672 -- 075549.732
it with words. And

075549.732 -- 075551.891
that's exactly what what we're after here. So

075551.892 -- 075553.911
what we'd like to do is be able specify things

075553.911 -- 075556.252
in words and get some some type of reward

075556.312 -- 075558.582
machine like this. And this is what what our approach

075558.582 -- 075600.241
is is, aiming to do.

075601.762 -- 075603.842
So we first introduced this notion of language

075603.842 -- 075605.992
a language aligned reward machines.

075606.393 -- 075608.632
So I mentioned the states in this, finite

075608.632 -- 075610.962
state automata that define a reward machine.

075612.142 -- 075614.482
We are, assuming that we have

075615.722 -- 075617.852
a textual description of each of

075617.852 -- 075620.252
these states. And we're we're not this is not assumption

075620.252 -- 075622.282
that somebody's handed it to us actually

075622.282 -- 075624.442
going to be using, these foundation models

075624.442 -- 075626.142
to come up with these textual descriptions.

075626.882 -- 075629.122
And additionally, also so for instance, here's

075629.122 -- 075630.942
an example, rather than specifying these logical

075631.422 -- 075633.922
prepositions, we can, convert them into text

075633.992 -- 075636.152
just like this. And and language models are pretty good

075636.152 -- 075638.252
at doing things like this. And additionally,

075638.252 -- 075640.412
we assume we have an embedding function, which we can get

075640.412 -- 075642.419
from our language model itself. That takes

075642.419 -- 075644.211
that textual description and gives us,

075644.509 -- 075646.642
some some vector in RD. The

075646.642 -- 075648.542
reason we want this is because

075648.782 -- 075650.882
this gives us a sort of a a a latent

075651.582 -- 075653.642
skill space that's semantically grounded. So

075653.642 -- 075655.582
so nearby points will have semantic

075655.722 -- 075657.742
coherence and this will allow us to have

075657.742 -- 075700.062
some form of generalizability, which which I'll demonstrate

075700.062 -- 075700.552
later.

075702.193 -- 075704.273
Okay. So we start with reinforcement learning. This is the

075704.273 -- 075704.972
standard reinforcement

075706.491 -- 075708.622
RL agent that gives actions to an

075708.622 -- 075710.702
environment. The environment returns the reward, and

075710.702 -- 075712.962
the agent learns from that. And the environment

075712.962 -- 075715.202
is typically defined as an MDP, where you have

075715.202 -- 075717.272
states actions, transitions, functions, and

075717.272 -- 075719.772
reward. And the agent's behaviors is formalized

075719.911 -- 075721.992
as a policy that maps states to

075721.992 -- 075724.367
distribution over actions. We

075725.012 -- 075726.712
With with, these alarms that we have,

075727.212 -- 075729.312
we're extending this. So the environment

075729.452 -- 075731.523
now the state goes through

075731.523 -- 075733.611
a labeling function. I'll explain what

075733.611 -- 075735.621
this is. In in a bit. This

075735.621 -- 075737.741
labeling function is what sort

075737.741 -- 075739.571
of is is is making us,

075739.952 -- 075742.132
transition within the reward machine that's also

075742.452 -- 075744.612
going to be automatically generated. The reward machine

075744.612 -- 075746.532
would will produce its own reward,

075747.492 -- 075749.292
and a new reward machine state,

075749.802 -- 075751.502
we can pass through our embedding

075752.152 -- 075754.312
and then the RL agent will will use this. So

075754.312 -- 075756.321
now the policy of

075756.321 -- 075758.492
of our RL agent depends

075758.492 -- 075800.652
not just on the environment state, but also on this

075800.652 -- 075802.742
embedding of the textual

075802.882 -- 075805.042
description of what reward machine state we're in

075805.042 -- 075807.122
at the moment. And the MDP were

075807.603 -- 075809.952
a new MDP that, the state space

075809.952 -- 075812.202
is enhanced is expanded with

075812.202 -- 075814.342
the reward machine states. This

075814.342 -- 075816.482
delta is a transition function for the reward

075816.482 -- 075818.491
machine. And this is the labeling function, which

075818.491 -- 075820.612
I'll explain in a little bit. So transitions

075820.832 -- 075823.072
in here now, you know, we start with some state

075823.072 -- 075825.242
and some embedding of of our reward

075825.242 -- 075827.302
machine, state. And the

075827.302 -- 075829.382
policy, as I said, is is a function of

075829.382 -- 075831.672
that. So we get a new action, we're

075831.672 -- 075833.778
going to transition within our MDP just as

075833.778 -- 075835.772
we do normally with our transition function.

075836.451 -- 075838.432
And we're gonna take the labeling function

075838.672 -- 075840.752
which tells us basically, do you have things like, do you have

075840.752 -- 075842.813
the coffee Did step on a decoration That

075842.813 -- 075844.972
type of thing. Which of the prepositions have become true

075844.972 -- 075847.012
with this new transition And

075847.172 -- 075849.211
this, we we sent to our our

075849.211 -- 075851.291
transition function for the reward machine, and we

075851.291 -- 075853.643
get a new reward machine state,

075853.643 -- 075855.542
which we can then embed again and,

075855.942 -- 075858.262
continue this process. So this is our enhanced,

075858.502 -- 075900.532
MDP m prime. That

075900.722 -- 075902.932
combines traditional reinforcement learning

075902.932 -- 075905.102
with these, language aligned reward

075905.102 -- 075906.742
machines. Now

075908.682 -- 075911.022
the reward, is also, enhanced.

075911.161 -- 075913.332
So rather instead of just taking the reward

075913.332 -- 075915.412
from the environment, we're going to add it with the

075915.412 -- 075917.552
reward reward

075917.552 -- 075919.782
machine, and this is going to be quite important

075919.782 -- 075921.862
because as I as I said before, it

075921.862 -- 075924.162
can be difficult to specify a reward

075924.162 -- 075926.321
that gives you the objective, the behavior you

075926.321 -- 075928.172
want, but also in an effective way.

075929.092 -- 075931.212
You know, you you end up with sparse rewards or

075931.212 -- 075933.302
or long horizon problems. So our

075933.302 -- 075935.462
hope was that these reward machines can can give us a

075935.462 -- 075937.401
denser signal that guides the agent

075937.642 -- 075939.222
in a way that's more effective for learning.

075940.662 -- 075942.512
So let's come back to this this, run

075942.752 -- 075944.812
prompt where the agent has to get this green ball

075944.812 -- 075947.233
and has to open up these two doors before it does this.

075947.372 -- 075949.392
The way our method works, which is

075950.031 -- 075952.432
our RMFM method, is we send

075952.432 -- 075954.522
this, the specification.

075954.582 -- 075956.832
So we're using a VLM that can understand

075956.832 -- 075958.853
these these images, and it can understand

075958.912 -- 080000.942
text. We send it to our foundation model,

080000.942 -- 080002.902
and we ask it to generate three things.

080003.222 -- 080005.382
The first thing is the reward machine's language

080005.382 -- 080007.722
specification. So it's going to try to convert

080007.782 -- 080009.862
this, specification we gave about

080009.862 -- 080012.042
what the objective is into,

080012.202 -- 080014.522
this formal language of, for reward

080014.522 -- 080016.592
machines. So it has the states, it

080016.592 -- 080018.672
has the transition functions, and it has

080018.672 -- 080020.972
the rewards. It receives in each of the

080020.972 -- 080022.282
reward machine, states.

080023.082 -- 080025.242
It's also going to produce labeling functions. So this

080025.242 -- 080027.572
is the labeling function I mentioned before. Which

080028.042 -- 080030.172
basically tells us, you know, did you

080030.492 -- 080032.652
open the key Did you get the, sorry, did

080032.652 -- 080034.352
you get the key Did you open the door

080034.982 -- 080037.242
Are you next to the green, circle, etcetera

080038.162 -- 080040.082
And finally, it's going to give us the,

080040.402 -- 080041.622
the language description

080042.892 -- 080045.063
of these, reward machine states. Along

080045.063 -- 080047.142
with their embeddings. And the embeddings we kinda get for free

080047.142 -- 080049.152
because we're using neural nets, with

080049.152 -- 080050.261
these foundation models.

080051.942 -- 080054.262
In addition to that, so we have our reward machine

080054.262 -- 080056.502
generation. But as we all know, these

080056.502 -- 080058.662
language models don't always get it right the first

080058.662 -- 080100.882
time, and you have to sort of probe them and and

080100.882 -- 080102.882
try to get them to improve. So we have this,

080103.042 -- 080105.062
self improvement loop where we

080105.062 -- 080107.072
have two we have

080107.072 -- 080109.393
the generator foundation model that gives us the reward

080109.393 -- 080111.483
machine, but we also have a a

080111.483 -- 080113.902
critic foundation model that's going to take that reward machine

080114.042 -- 080115.582
and give automated feedback

080116.222 -- 080117.882
to the generator on whether it

080118.442 -- 080120.502
it it actually produced something useful or

080120.502 -- 080122.683
not. We have an optional

080122.683 -- 080124.702
step, which is final human verification

080124.763 -- 080126.292
because everything is specified in

080126.773 -- 080128.933
It's quite easy for humans to look at this

080128.933 -- 080130.502
and say, this is not what I wanted.

080131.592 -- 080133.991
This we we believe is is actually quite useful

080133.991 -- 080136.012
because especially when we're dealing with real

080136.012 -- 080138.252
world problems, where as humans, we can,

080138.252 -- 080140.362
you know, we know it when we see it. If

080140.362 -- 080141.603
we if we look at a specification of,

080142.563 -- 080144.992
this formalism of reward machines,

080145.052 -- 080147.302
it's pretty easy for us to say, no, that's wrong.

080147.462 -- 080149.542
And so we can give that feedback pretty pretty

080149.542 -- 080151.282
easily with an up down,

080151.922 -- 080154.322
but it's a lot harder for us to you know, manually

080154.322 -- 080156.582
come up with these reward machines. So

080156.643 -- 080158.782
we feel like this this optional human

080158.782 -- 080200.872
step is is quite useful

080200.872 -- 080202.952
in in our setup. For some of the experiments

080202.952 -- 080204.962
we did, give some of

080204.962 -- 080207.042
this this human feedback where we basically said, no,

080207.042 -- 080208.972
try again, language model that wasn't

080209.132 -- 080211.462
wasn't good enough. And and it was able to improve.

080212.943 -- 080215.002
Okay. So let's go into the of the

080215.002 -- 080217.321
empirical evaluations. We're going to look

080217.321 -- 080219.102
at four different types of environments.

080219.502 -- 080221.542
Mini grid, craftium, meta world, and,

080221.742 -- 080223.862
excellent mini grid. And these are

080224.023 -- 080226.342
we chose these environments to evaluate different,

080226.582 -- 080228.616
challenges capabilities of of,

080228.910 -- 080230.992
of our system. Mini

080230.992 -- 080233.021
grid, we're going to be we chose because as

080233.022 -- 080235.501
I said, it has this this long horizon sparse reward

080235.501 -- 080237.652
problem that makes it difficult for RL agents to learn

080237.652 -- 080239.702
there. Craftium is a

080239.702 -- 080242.102
is a reduced form

080242.102 -- 080243.683
of of of Minecraft,

080244.622 -- 080246.782
that is three d. It's pretty challenging because

080246.782 -- 080248.922
you have to, somewhat similar to

080248.922 -- 080250.952
mini grid, you have to achieve certain tasks

080250.952 -- 080253.032
in order, like to mine, the ore or the

080253.032 -- 080255.172
the metal And

080255.172 -- 080257.241
if you don't do it properly, then you you just

080257.241 -- 080259.741
don't get to do it. MetaWorld is a continuous control,

080300.371 -- 080302.402
environment. And finally, XLAN

080302.402 -- 080304.821
mini grid, this allows us to test, for multitask

080305.313 -- 080307.462
generation because we can these are procedurally generated

080308.712 -- 080310.871
environments, and so we can, test how

080310.871 -- 080312.331
well our system generalizes.

080314.172 -- 080316.412
So as an example for some of the things that,

080317.443 -- 080319.523
our our method produces, these

080319.523 -- 080321.773
are this is the for this particular task.

080321.773 -- 080323.712
This is the reward machine that it produced in

080323.933 -- 080325.042
the in the language that we specify

080326.072 -- 080328.232
so in the prompt, we we kind of tell

080328.232 -- 080330.632
it this is the the form that reward

080330.712 -- 080332.810
that we want reward machines take. And so

080332.810 -- 080334.943
if it it's pretty capable at producing things

080334.943 -- 080337.241
like this. It produces labeling

080337.241 -- 080339.482
functions, which as you can see, they're just Python functions

080339.482 -- 080341.522
that assume we have access to the

080341.522 -- 080343.832
environment state, and so then it can check

080343.832 -- 080345.912
whether it has the coffee or whether it

080345.912 -- 080348.071
has the the key or whatever. And finally,

080348.071 -- 080350.302
it produces these natural

080350.302 -- 080352.082
language specifications of the

080352.462 -- 080354.612
reward machine states. Along with your embeddings.

080356.132 -- 080358.242
So So we compared for this mini

080358.242 -- 080359.142
grid with DQN

080400.603 -- 080402.943
and, we also compared with an LMM

080403.322 -- 080405.692
agent basically, just seeing whether an LLM

080406.042 -- 080408.122
we can prompt it and ask it to

080408.122 -- 080409.612
to, give us actions.

080410.332 -- 080411.942
And, you know, this is not

080412.501 -- 080414.661
learning, so that's why the we have a flat line, but it gives us

080414.661 -- 080416.862
a a reasonable baseline to see how well

080416.862 -- 080419.062
we compare against just doing anything

080419.062 -- 080421.222
with LMS. As you can see, our our blue line is

080421.222 -- 080423.542
is able to pretty cleanly

080423.542 -- 080425.733
surpass all of the the baselines.

080426.122 -- 080428.142
Most of these environments. And in fact in some,

080428.202 -- 080430.342
like in this environment, it's the only one that's able

080430.342 -- 080432.592
to make progress, the other one's completely flat lined.

080432.912 -- 080434.992
And we tried it with even harder environments

080434.992 -- 080437.062
that have really long horizons

080437.062 -- 080439.382
and very sparse rewards, so none of the other methods

080439.382 -- 080441.582
are able to make any progress because they basically get

080441.582 -- 080443.752
no signal just exploring around. And we know

080443.752 -- 080445.132
this already to be an issue with

080446.022 -- 080448.103
horizon sparse reward problems, you can

080448.103 -- 080450.182
make square mini grid, worlds

080450.182 -- 080452.472
that are very large and just Epsilon

080452.472 -- 080454.172
greedy type approaches won't work.

080454.552 -- 080456.882
But using this idea of reward machines, really

080457.832 -- 080459.772
get some more dense reward signals that,

080459.992 -- 080501.952
enable us to to achieve this

080502.321 -- 080504.531
properly. So craftium,

080504.531 -- 080506.772
as I mentioned, is is, this reduced form

080506.772 -- 080508.983
of of Minecraft where the agent has

080509.763 -- 080512.161
to mine don't remember what these represent. This is like wood.

080512.162 -- 080514.322
This is I don't

080514.382 -- 080516.422
know. Steel. Gold, and diamond

080516.422 -- 080518.462
or something like that. And it has

080518.462 -- 080520.542
to do, mind them in that order. Otherwise,

080520.542 -- 080522.972
it won't be able to to get the

080523.112 -- 080523.862
the diamond.

080525.202 -- 080527.361
Also, obviously, it has to, like, learn how to move

080527.361 -- 080529.422
and and use its ax to to to

080529.422 -- 080531.443
break these things, etcetera. We

080531.443 -- 080533.522
use PPO as a baseline, and you can see it

080533.522 -- 080535.662
does quite poorly. It's, like, barely

080535.662 -- 080537.192
able to to mine wood.

080537.832 -- 080539.852
And it after 10,000,000 steps, it's it's

080539.852 -- 080542.012
unable to to do much. But when we combine

080542.012 -- 080544.282
PPO with our reward

080544.282 -- 080546.523
machines approach, we can see that it is able

080546.523 -- 080548.681
to to pretty cleanly, find the diamond,

080548.682 -- 080550.722
and so this is quite a dramatic

080551.023 -- 080553.072
improvement over what we could do before. And

080553.072 -- 080555.392
it really came from this automated approach

080555.392 -- 080557.531
of specifying the objective in

080557.531 -- 080559.712
words and have, leveraging the

080559.852 -- 080602.172
the natural language understanding capabilities

080602.172 -- 080604.312
of these reward models of these

080604.312 -- 080606.472
language models, sorry, and their capabilities to write

080606.472 -- 080608.562
code to come up with, these reward

080608.562 -- 080610.641
machines, automatically and help

080610.642 -- 080612.272
us in in this learning task.

080613.232 -- 080615.422
In meta world, this one's perhaps a

080615.422 -- 080617.582
bit less exciting, but, we're still able to to

080617.582 -- 080618.882
improve things, you know, just

080620.322 -- 080622.483
But Exon mini grid,

080622.483 -- 080624.662
as I said, this is these are procedurally generated

080624.723 -- 080627.032
environments. And so

080627.032 -- 080629.142
we generated simultaneous environments

080629.142 -- 080631.302
to test the multitask capabilities of

080631.302 -- 080633.332
of of of our

080633.332 -- 080635.733
system. So basically, can you come up with a reward

080635.733 -- 080637.752
machine that useful for multiple tasks

080637.752 -- 080639.991
at a time And you can see that as we increase the number

080639.991 -- 080641.132
of simultaneous tasks,

080642.662 -- 080644.822
using our our full system is really what gives us the best

080644.822 -- 080647.062
performance. And it's able to achieve

080647.281 -- 080649.452
really high success rates. These also

080649.452 -- 080651.693
include ablation, so what happens if we remove

080651.693 -- 080653.722
the embeddings What happens if we don't

080653.722 -- 080655.882
use the the, reward machine reward

080655.882 -- 080658.312
and simply use the the the rest

080658.531 -- 080700.662
of the the the pipeline of

080700.662 -- 080703.082
the reward machines, just not use the additive reward.

080703.242 -- 080705.412
And we can see that really all of them

080705.412 -- 080707.752
are important to achieve the the best performance.

080709.382 -- 080711.232
We Another thing we we

080712.032 -- 080714.232
it investigated was the zero

080714.232 -- 080716.712
sought generalization of our

080716.932 -- 080718.922
system. As I mentioned, we

080719.482 -- 080721.502
part of the the motivation for using these embeddings

080721.562 -- 080723.422
of the natural language specification

080725.682 -- 080727.732
this latent skill

080727.732 -- 080728.951
space that we feel

080729.861 -- 080731.842
allows us to to have these generated

080732.482 -- 080734.722
generalization, capabilities. So we

080734.722 -- 080736.742
trained the agent with these two tasks.

080737.377 -- 080739.462
That know, they're kinda similar,

080739.462 -- 080741.602
but but they're different in the sense

080741.602 -- 080743.842
that, this one has to pick up a blue key, this

080743.842 -- 080746.063
one has to position itself to the right

080746.063 -- 080748.282
of the of the blue pyramid. And

080748.603 -- 080750.711
after doing that, without any extra

080750.711 -- 080752.791
training, we specify this new

080752.791 -- 080754.812
task where it has to pick up a blue key

080755.172 -- 080757.491
and, move to the to the right of the blue pyramid.

080757.491 -- 080759.272
So it's sort of combining objective

080759.572 -- 080801.193
from the two separate tasks.

080802.082 -- 080804.242
And seeing if if it's able to generalize

080804.242 -- 080806.502
well, then it should be able to leverage the

080806.842 -- 080808.781
latent skill space that it that it learned,

080809.321 -- 080811.492
in order to generalize to this new task without

080811.652 -- 080813.092
extra training. And and,

080814.251 -- 080816.462
we were, very happy to see that it that it was

080816.462 -- 080817.922
able to do that, quite successfully.

080819.222 -- 080821.642
So it this is just sort of showing the trajectories

080821.781 -- 080823.871
of the of the agent. But the important

080824.012 -- 080825.552
thing is that these embeddings

080827.482 -- 080829.882
they they have a semantic

080829.942 -- 080832.361
interpretation or they're they're they're meaningfully

080833.233 -- 080835.152
placed within this latent space that,

080835.313 -- 080837.132
in such a way that we can combine

080837.372 -- 080839.452
different skills and come up with new tasks and

080839.452 -- 080841.461
have this this type of zero

080841.461 -- 080842.162
shot generalization.

080844.233 -- 080846.552
Most of the experiments that that we showed

080846.552 -- 080848.772
before were run with, GPT four o.

080848.932 -- 080851.011
At one point we lost access to GPT

080851.012 -- 080853.081
four o, so we started experimenting with other

080853.483 -- 080855.722
LLMs that that we still had access to.

080855.722 -- 080857.932
And this, led us to ask the question, well,

080858.362 -- 080900.603
does it matter what LM you use And in fact,

080900.603 -- 080902.661
it does. One thing that's neat to see

080902.661 -- 080904.751
is that as we use more

080904.751 -- 080906.752
and more capable LLMs, able to

080906.752 -- 080908.562
get more and more useful,

080909.122 -- 080911.361
reward machines. So here we use an, separate

080911.361 -- 080913.682
LLM none of the ones that we used here.

080914.162 -- 080916.321
As a verifier. So we asked the you know, how we had the

080916.321 -- 080918.394
critic that in self improvement loop, we can use

080918.394 -- 080919.582
an LLM to ask it,

080920.502 -- 080922.922
is the reward machine correct Is the

080924.192 -- 080926.292
the the labeling function correct

080926.592 -- 080928.692
And whether both are correct. And

080928.692 -- 080930.712
we can see that the more capable models

080930.772 -- 080932.032
are able to to

080932.912 -- 080934.942
perform better. So bigger models, better models,

080935.202 -- 080937.272
better reward machines, better RL training.

080940.162 -- 080941.622
I already sort of hinted

080942.402 -- 080944.732
at this the advantages of having this well structured

080944.792 -- 080947.082
latent skill space. And here,

080947.082 -- 080949.162
the colors represent, different points

080949.162 -- 080951.192
in the task at hand.

080951.192 -- 080953.252
So, you know, it could be picking up the red key,

080953.252 -- 080955.492
that's the first thing you have to do. Or at the end,

080955.492 -- 080957.611
you're already moving towards the the

080957.611 -- 080959.742
green circle or something like that. And we can see

080959.742 -- 081002.082
that, when we do this PCA

081002.223 -- 081004.532
projection, we do see

081004.853 -- 081007.332
grouping of of the of of these tasks

081007.332 -- 081009.172
in a semantically meaningful way

081009.812 -- 081011.752
which is, I mean, a qualitative

081012.382 -- 081014.542
assurance that that the the method is

081014.542 -- 081016.572
is doing things properly, and

081016.592 -- 081018.722
know, it adds further evidence to

081018.722 -- 081020.931
the claims that we have that generalizes well and can

081020.932 -- 081022.772
potentially do, zero shot,

081023.642 -- 081024.942
have zero shot performance.

081026.062 -- 081028.092
So okay. So that's I'll I'll

081028.092 -- 081030.172
end there. I know we're we're a bit

081030.172 -- 081032.262
over time. But

081032.262 -- 081034.342
this for me has been quite eye opening.

081034.342 -- 081036.353
I I for the past few years, I've been pretty

081036.353 -- 081038.372
skeptical to step into this LLM

081038.432 -- 081040.292
world, but Roger was

081040.773 -- 081042.852
was quite enthused by this and he managed to convince me

081042.852 -- 081045.041
that this is actually quite powerful. And so I'm

081045.041 -- 081047.121
very excited for, I know there's other works

081047.121 -- 081049.142
that are doing similar things. I've of leveraging the power

081049.142 -- 081051.202
of these, foundation

081051.262 -- 081053.612
models to be able to, improve the

081053.933 -- 081056.042
the learning capabilities of our our

081056.042 -- 081058.202
l agents without necessarily having to

081058.202 -- 081100.281
just have the LLM do everything. So we

081100.281 -- 081102.361
can leverage them for those of us that

081102.361 -- 081104.622
still work on on classic

081104.622 -- 081106.782
RL. As I said, Roger and Glenn are

081106.782 -- 081108.832
still around, so if you have

081108.832 -- 081110.992
more questions, you can ask them. And if there's time,

081110.992 -- 081113.152
I can answer a couple questions. Thank

081113.152 -- 081113.192
you.

081124.492 -- 081126.832
Okay. Thank you for the presentation and

081126.972 -- 081127.991
sharing. And

081129.172 -- 081131.332
does anybody have any questions You can stand

081131.332 -- 081132.312
up to the microphone.

081133.472 -- 081133.862
Oh,

081139.542 -- 081141.702
Hi. I I work in robotics. There's been

081141.702 -- 081143.872
a lot of similar work over the last few years,

081143.872 -- 081145.952
like Nvidia's eureka, Text to Reward and such that

081145.952 -- 081148.402
leverages very similar methods. I was

081148.462 -- 081149.432
curious of the specific

081150.637 -- 081153.052
paper why you chose to go specifically

081153.052 -- 081155.052
for a reward machine as opposed to just the

081155.372 -- 081157.425
Python program like the rest of the labeling functions What does

081157.572 -- 081159.592
what additional benefits does the structure of the

081159.592 -- 081201.172
MVP give you out of curiosity

081202.502 -- 081204.902
So the part of the the advantage of going with reward

081204.902 -- 081207.361
machines is that, they're a concise,

081207.902 -- 081209.042
mechanism for specifying,

081210.152 -- 081212.182
the the the evolution or the

081212.182 -- 081214.032
the trajectory of the reward functions.

081214.912 -- 081216.992
And this allows us to to do have this human oversight for

081216.992 -- 081218.872
instance. In a way that

081219.193 -- 081221.292
Python functions, although they are legible, they're

081221.452 -- 081223.532
arguably a bit more complex to to just look

081223.532 -- 081225.722
at and and be able to to assert whether they're

081225.882 -- 081227.342
they're doing the the correct thing or not.

081228.952 -- 081229.862
Alright. Thanks.

081232.922 -- 081235.012
Hi. My question is

081235.012 -- 081237.352
specifically regarding the training of the

081237.912 -- 081240.002
framework you have. It seemed very similar

081240.002 -- 081241.352
to GANs basically.

081242.022 -- 081244.172
Whether you have generator and

081244.172 -- 081245.552
a discriminator

081246.513 -- 081248.672
The offering GANS is a problem of initial

081248.672 -- 081249.172
training

081250.862 -- 081252.941
There's problems in training of

081252.942 -- 081255.102
both things because you're training both them together, right

081255.342 -- 081257.443
This framework Were there any tricks or things

081257.443 -- 081258.962
you used to stabilize the training

081300.132 -- 081302.282
Framework here So it's not quite like

081302.282 -- 081304.442
GANs. So the the critic that we're using,

081305.962 -- 081306.942
I'll go back to that slide.

081308.042 -- 081310.442
We're not training that critic. So we're we're using,

081311.432 -- 081312.717
Here. We're

081313.483 -- 081315.422
This this critic here, we're using a foundation

081315.642 -- 081317.733
model. We specify

081317.872 -- 081320.062
a prompt but the language

081320.062 -- 081322.382
model is pretrained. So we we wouldn't have the

081322.382 -- 081324.581
same type of pathologies that that GANs

081324.581 -- 081326.661
would have. Even the the generator foundation

081326.661 -- 081329.022
model, we're not retraining any foundation. We're using

081329.022 -- 081330.922
pre trained foundation models. It's just

081331.322 -- 081333.642
the the prompt that we use, to generate

081333.642 -- 081335.972
the the reward machine, and then the critic gives

081336.031 -- 081338.152
a feedback that is also included into the

081338.152 -- 081340.162
prompt to to then refine the reward machine.

081340.872 -- 081343.202
And in this loop, did you initially

081343.202 -- 081345.222
use auto training, right For this loop and then at the

081345.222 -- 081347.382
end you use some human No. No. So

081347.382 -- 081349.572
we don't train the the language

081352.693 -- 081354.152
models. Nice talk.

081354.822 -- 081356.902
Seems like your I'd like you to talk about

081356.902 -- 081358.942
how this relates to reward shaping. Because in

081358.942 -- 081401.022
these examples you gave with Mini

081401.022 -- 081403.132
Grid, it seems like there is a

081403.132 -- 081405.372
goal state, but then the other things were

081405.372 -- 081407.322
like procedural hints. Like oh,

081407.422 -- 081409.552
you should first pick up the blue box. And

081410.012 -- 081412.052
and maybe that could be misleading, right

081412.052 -- 081414.002
Like, you might get reward hacking kind of like

081415.232 -- 081417.332
so that was first question. And secondly, your baselines

081417.392 -- 081419.052
when you just have DQN, without

081419.432 -- 081421.502
the reward machine, does it just get the

081421.502 -- 081423.332
reward for the goal or does it also get

081424.212 -- 081426.217
pseudo points for the intermediate steps

081426.992 -- 081428.642
Great points. So

081429.281 -- 081431.442
on the reward hacking front,

081431.442 -- 081433.601
that's that's absolutely correct. So if you have

081433.602 -- 081434.262
a misspecified

081435.842 -- 081438.142
objective because we

081438.242 -- 081440.358
can make errors as well. I would say

081440.358 -- 081442.582
it it's still probably going to be prone to

081442.582 -- 081444.542
to reward hacking.

081445.172 -- 081447.672
You can under specify the objectives.

081448.972 -- 081449.987
And and

081451.362 -- 081453.443
that for something like mini grid, I imagine

081453.443 -- 081455.523
it wouldn't work well. Like, if you just say you need to

081455.523 -- 081456.842
go to the the green dot,

081458.122 -- 081459.822
I would actually be surprised.

081500.292 -- 081501.892
Mean, I've been surprised by LMS, but

081502.372 -- 081504.452
before. But it, having it figure out

081504.452 -- 081506.542
the intermediate steps to do that might

081506.542 -- 081508.841
be challenging. Maybe with

081508.842 -- 081511.002
something with like the self improvement loop

081511.162 -- 081512.712
you could get something

081513.202 -- 081515.352
that automatically is able to figure out

081515.352 -- 081517.852
the intermediate steps. I'm not sure.

081518.292 -- 081520.392
One thing, for the

081520.392 -- 081522.772
for the baselines, so,

081522.992 -- 081525.081
the DQN is with with just the environment

081525.081 -- 081527.102
reward. We didn't give it the the intermediate

081527.241 -- 081529.531
rewards. This is coming

081529.531 -- 081531.792
from the actually,

081532.052 -- 081534.212
should verify that because this is basically using

081534.212 -- 081536.552
whatever reward specification is in mini grid.

081536.857 -- 081539.103
And Mini Grid, the default setting

081539.402 -- 081541.852
is actually reward is not Markovian,

081542.392 -- 081544.632
strangely enough. So, it might be a bit

081544.632 -- 081546.842
less sparse than what we would traditionally imagine

081546.842 -- 081548.962
when you just get to the goal. I believe

081548.962 -- 081551.122
in the paper we put some ablations where we added

081551.122 -- 081553.222
the the reward

081553.222 -- 081555.562
machine reward to DQN

081555.562 -- 081557.862
without any of the other stuff. And that

081558.103 -- 081600.142
helped quite a lot, which is not terribly surprising because

081600.142 -- 081601.972
it's making the the space more denser.

081602.572 -- 081602.852
Yeah.

081604.882 -- 081605.422
There still a 10

081607.182 -- 081609.032
Okay. Hi.

081609.273 -- 081611.433
Have you considered using this method for

081611.433 -- 081613.642
more complex environments like computer

081613.642 -- 081615.742
use And what challenges might you end

081615.802 -- 081618.063
up I mean, we've thought about it. It'd

081618.063 -- 081619.902
be really nice, There's

081620.921 -- 081622.972
this is so I at DeepMind, but this

081622.972 -- 081624.972
work was done with with, at Mila with with,

081625.132 -- 081627.353
with the students there. So there's a limiting

081627.572 -- 081629.652
compute in in what we can and can't do.

081630.662 -- 081632.803
It would be interesting. I mean, this

081632.803 -- 081634.032
is a more general comment,

081635.071 -- 081636.932
but, it's

081637.232 -- 081639.312
a standard frustration or tension that we

081639.312 -- 081641.392
have when we're working in academic research

081641.392 -- 081643.072
and we wanna get our stuff published

081643.792 -- 081645.871
where you often you know,

081645.871 -- 081648.031
you have an incentive to go with standard benchmarks because this is

081648.031 -- 081650.171
what reviewers understand. Ideally, you do wanna

081650.171 -- 081652.192
test on on more complex environments like what you're

081652.192 -- 081654.411
suggesting. So just that the incentives

081654.712 -- 081656.791
aren't always there. That being said, it is something

081656.791 -- 081658.802
that that we're quite interested in. So if

081658.802 -- 081700.992
you want to more, we be happy to chat more. Maybe

081700.992 -- 081702.882
there's something we can do there. There.

081704.462 -- 081705.432
Alright. You.

081712.411 -- 081714.681
Okay. That's that is all

081714.682 -- 081715.661
for our today's

081717.442 -- 081719.702
keynote sharing, and we'll now

081720.082 -- 081721.902
proceed to our oral representation.

081723.362 -- 081724.773
Session. And

081725.502 -- 081728.002
and we have a total of five papers got,

081728.701 -- 081730.861
selected as the oral

081730.861 -- 081733.242
paper, and, the first one is

081733.483 -- 081735.882
why pry video predictions for robot

081735.962 -- 081738.382
actions Let's now welcome

081738.442 -- 081740.527
Sandeep for

081741.152 -- 081742.062
for the presentation.

081818.201 -- 081819.292
Okay. Just a moment.

081839.063 -- 081841.082
I'll just use the HTML, you think.

081902.332 -- 081902.832
Happening

081921.973 -- 081923.032
I'll keep the changes.

081924.172 -- 081925.392
Oh, we should duplicate. Right

081926.432 -- 081928.882
Let's do show only

081928.882 -- 081931.172
on Are you on point. It

081931.172 -- 081933.462
probably won't show on the booking.

081939.472 -- 081940.292
You just

081943.072 -- 081943.532
Maybe

081946.013 -- 081947.122
I need to see. Right

081956.732 -- 081957.232
Extend

082012.861 -- 082014.992
Karen, there is no way to to do

082014.992 -- 082015.822
it without extent.

082017.582 -- 082019.282
We would have to change the resolution

082031.683 -- 082032.392
Yep. So

082039.951 -- 082040.112
just

082043.023 -- 082045.002
If you hit present, it will show up there.

082053.822 -- 082055.843
Hello, everyone. I'm Sandeep. And,

082056.162 -- 082057.702
I'll be presenting my work

082058.692 -- 082100.733
WIPRA, also called Video

082100.733 -- 082102.882
Prediction for Robot Actions. We

082102.882 -- 082105.263
Okay. So let's start with the motivation.

082105.483 -- 082107.643
So this this

082107.643 -- 082109.792
work has two access motivation. The

082109.792 -- 082112.242
fast the first access being

082112.242 -- 082113.972
robot learning these days needs

082114.532 -- 082116.693
large amount of, action label datasets

082116.693 -- 082117.752
to really scale

082118.802 -- 082120.861
learning. But they are expensive to collect and,

082121.102 -- 082123.352
time consuming as well. But

082123.592 -- 082125.911
there are plenty of human plenty of videos

082125.911 -- 082127.973
on Internet of human doing

082127.973 -- 082130.312
tasks or robots or, like,

082130.472 -- 082131.852
simply, like, where videos

082132.473 -- 082134.182
the wild, they're

082134.562 -- 082136.932
abundantly available. But they don't have any consistent

082137.312 -- 082138.802
action levels as such.

082139.442 -- 082141.542
So on interesting question

082141.542 -- 082143.943
to ask here would be, like, can we use

082143.943 -- 082145.452
this, actionless

082146.072 -- 082148.152
videos to facilitate robot

082148.152 -- 082150.012
learning. And the second axis,

082150.172 -- 082151.892
of motivation for this work is,

082152.452 -- 082154.693
have had significant progress in pre trained video

082154.693 -- 082156.982
models. Like OpenSolar,

082156.982 -- 082159.121
CogVideo, Cosmos, and HumanVideo.

082200.571 -- 082202.733
So these these video models

082202.733 -- 082204.692
have been trained on, like, large amount of

082205.251 -- 082207.491
video data, and they already capture some sort

082207.491 -- 082209.642
of physical representations of

082209.642 -- 082211.943
the environment. So a natural

082212.002 -- 082214.452
question to ask here would be like, we turn this pretrained

082214.532 -- 082216.911
video models into robot policies

082216.972 -- 082219.381
So Bipra our

082219.382 -- 082221.542
work, BIPRA tries to tackle both this motivation

082221.542 -- 082223.835
and bring them together. So we prioritize

082223.962 -- 082225.842
a framework, this

082226.082 -- 082228.322
pre trained video models and turn them into

082228.322 -- 082230.572
robot policies while

082231.193 -- 082233.222
simultaneously using unlabeled human

082233.282 -- 082235.352
plus robot videos. That it'd

082235.352 -- 082236.852
be easier to scale. So

082237.422 -- 082239.672
I'll present, the

082239.672 -- 082241.912
framework on a high level here, then we will dive deeper

082241.912 -- 082244.192
into each component. So first,

082246.292 -- 082248.452
we had, like, videos coming actionless videos coming

082248.452 -- 082250.911
from both human and robot. And

082250.911 -- 082251.822
then we learn

082253.432 -- 082255.382
unified latent action representation

082256.052 -- 082258.212
which we call z t here. So what z t

082258.212 -- 082300.222
here represents is like

082300.222 -- 082302.502
a motion represent motion aware representation.

082302.802 -- 082305.072
And, represents the transition happening

082305.182 -- 082306.622
from t to t plus one.

082307.832 -- 082310.072
So, we train this completely in a

082310.072 -- 082312.402
self supervised manner, just from videos

082312.642 -- 082314.972
learn these representations. So once

082314.972 -- 082316.832
we have these abstract motion representations,

082317.212 -- 082319.352
we we use, underlying video

082319.352 -- 082321.353
language model a large video language model,

082321.353 -- 082323.063
and, develop a pretraining scheme

082323.822 -- 082325.902
that jointly predicts the future visual

082325.902 -- 082328.232
step and the latent action that we learn.

082328.712 -- 082330.622
So once this pretraining is done,

082330.782 -- 082332.627
we have a third stage.

082332.882 -- 082334.342
Where we fine tune this

082334.922 -- 082336.902
setup. On any downs

082337.302 -- 082339.702
downstream task with, like, minimal amount of action

082340.082 -- 082342.232
level demonstration So this can be deployed in

082342.232 -- 082344.472
both real so we have deployed this

082344.472 -- 082346.482
in real world in our own

082346.482 -- 082347.983
setup. As well as in various,

082348.782 -- 082350.302
simulated benchmarks. And,

082351.852 -- 082353.952
as you can, as you can see, the cross

082354.682 -- 082356.842
there is cross embodiment generation baked in

082356.842 -- 082359.031
into this because it naturally exploits

082359.092 -- 082401.057
both human and robot videos.

082402.892 -- 082405.212
Okay. So let's move on to the latent

082405.212 -- 082407.282
action model. So the latent

082407.282 -- 082409.553
action model for the latent action model, first,

082409.553 -- 082411.712
we take, like, a sequence of frames that

082411.712 -- 082414.162
are sampled at, like, three to six hertz.

082414.162 -- 082416.212
We ensure the sampling, temporal

082417.331 -- 082419.752
coarseness in the in the in the videos.

082420.302 -- 082422.501
Then we pass it through a latent action

082422.501 -- 082424.522
tokenizer, which is essentially an inverse

082424.581 -- 082426.911
model. Which we call I beta here.

082427.292 -- 082429.402
And it learns to infer,

082429.563 -- 082431.082
representation z t, which is

082431.722 -- 082433.802
representing transition from o

082433.802 -- 082435.582
o t to o t plus one. So

082436.802 -- 082438.642
so this I beta here is,

082439.201 -- 082440.572
like, which is, like,

082441.483 -- 082443.582
user design for inverse models. We have, like,

082443.742 -- 082446.063
non causal bidirectional attention within

082446.063 -- 082448.232
this. And we also have an information bottleneck,

082449.502 -- 082451.582
ensuring that the latencies are capacity

082451.582 -- 082453.802
limited, and we will use we will see

082453.802 -- 082456.012
shortly why this is So then

082456.012 -- 082458.172
we have a forward decoder. The forward decoder

082458.172 -- 082500.482
takes in like, the history

082501.451 -- 082503.712
o o zero to t. O zero

082503.712 -- 082506.132
to t and zed zero to t, and tries to reconstruct

082506.193 -- 082508.412
the future frame t plus one.

082508.553 -- 082510.733
And this this is a causal, causal decoder.

082511.482 -- 082513.432
And we train this using reconstruction loss

082513.952 -- 082515.912
some per perceptual loss,

082516.152 -- 082518.572
which is the LPS loss here, and a optical

082518.792 -- 082520.312
flow consistency loss

082521.861 -- 082523.892
so this optical flow consistency

082523.952 -- 082525.902
loss ensures that the reconstructed

082526.102 -- 082528.331
the optical flow of reconstructed frames

082528.572 -- 082530.813
and the optical flow of the ground truth frames

082530.813 -- 082532.522
match. So the reason for having this

082532.842 -- 082535.092
optical flow consistency loss is like it is like an

082535.092 -- 082537.252
explicit loss ensuring that, our model

082537.252 -- 082539.443
learns to encode motions from d

082539.443 -- 082541.683
to t plus one, t plus one to t plus two, and so

082541.683 -- 082543.892
on. And the reason for having this information

082543.892 -- 082545.911
bottleneck here is to ensure that,

082546.052 -- 082548.282
since if if the

082548.282 -- 082549.943
latents are here here are, like,

082550.452 -- 082552.692
have high capacity, they could simply learn

082552.692 -- 082554.722
to copy the future directly from

082554.722 -- 082557.182
the model since it already has access to the future.

082557.342 -- 082559.201
So ensuring that it's capacity limited,

082559.672 -- 082601.362
it is necessary to

082601.932 -- 082604.132
make sure that the loan only motion and

082604.132 -- 082606.021
not, like, irrelevant background noise.

082607.302 -- 082609.321
So so we learned this end to

082609.321 -- 082611.172
end in a sub self supervised manner from

082611.732 -- 082613.812
actionless videos. And once we have this,

082613.812 -- 082615.332
we we we then

082615.892 -- 082618.052
also also before before moving on there, we have

082618.052 -- 082619.672
some, illustrations here.

082620.247 -- 082622.342
Of the latent action

082622.342 -- 082624.683
model actually encoding motion centric dynamics.

082624.983 -- 082626.972
So here we see, like,

082627.932 -- 082630.252
ground truth video. The video

082630.812 -- 082632.892
is ex exhibiting in motion or moving

082632.892 -- 082635.032
up, we pass it through the

082635.032 -- 082637.512
I beta, and we infer the latent,

082637.752 -- 082639.602
for upward motion.

082640.162 -- 082642.062
Then we have another ground truth video.

082642.321 -- 082643.742
This is a ground truth video where

082644.462 -- 082646.632
the person is person hand is moving

082646.632 -- 082648.802
down, what we do is we take

082648.802 -- 082650.882
the first frame of this video, and we use

082650.882 -- 082652.902
the upward latitudes

082652.902 -- 082655.012
that we extracted. And,

082656.491 -- 082658.472
autoregressively decode audio video.

082658.712 -- 082700.972
And we will see that this new video

082701.752 -- 082704.142
has upward motion in this. So

082704.532 -- 082706.693
what this, implies that our latency

082706.693 -- 082708.612
that we learned actually had

082709.117 -- 082711.152
the up up the

082711.152 -- 082713.552
notion of up encoded in them. So similar

082713.552 -- 082715.642
example here. Showing left

082715.642 -- 082716.661
and left to right.

082720.342 -- 082720.842
Yeah.

082722.531 -- 082724.672
So Okay. So let's move on,

082725.212 -- 082727.032
to the pretraining and fine tuning framework.

082728.072 -- 082730.281
So we have a video

082730.281 -- 082732.602
language model. So with the video language

082732.602 -- 082734.652
model takes in a history

082734.652 -- 082735.542
of, visual frames.

082736.812 -- 082739.242
And, it it it predicts

082739.462 -- 082741.723
a frame like after k steps.

082742.382 -- 082744.542
And then, we take the related action model that

082744.542 -- 082746.621
we learned I beta, from the

082746.621 -- 082748.781
earlier stage, and we predict the sequence

082748.781 -- 082750.782
of latent action latent action,

082751.962 -- 082754.412
like encoding the transitions happening from t

082754.893 -- 082757.193
to t plus k. So here it is. So

082757.193 -- 082759.581
this is this is the pretraining

082759.882 -- 082802.112
part. And note that, here, up

082802.112 -- 082804.542
until now, we have been using actionless

082804.542 -- 082806.782
videos. There is no action required in

082806.782 -- 082808.813
any of this till now. So once

082808.813 -- 082811.060
this, pretraining is done, can deploy

082811.060 -- 082813.262
this model, on any downstream

082813.322 -- 082815.483
environment of our choice by just simply, like,

082815.483 -- 082817.572
fine tuning this model And here,

082817.572 -- 082819.652
we used, like, a flow matching decoder with

082819.652 -- 082821.172
action chunking to

082822.132 -- 082823.082
do the fine tuning.

082825.222 -- 082827.642
So we we, here are some simulation results.

082828.407 -- 082830.542
On the simpler environment,

082830.682 -- 082832.572
and, we'll go through this

082833.853 -- 082836.232
slowly. First, we have,

082836.232 -- 082838.532
like, first key takeaway here is, like,

082838.532 -- 082840.612
we have two variations of our model. One is a

082840.612 -- 082842.933
discrete action model, well, which predict,

082842.933 -- 082845.132
like, like, discretized

082845.132 -- 082847.342
action. Through binning. Autoregressive decoder.

082847.342 -- 082849.532
And we also have a continuous version of our model

082849.693 -- 082851.773
which predicts continuous actions. A

082851.773 -- 082854.193
flow matching decoder. And in both variance,

082854.332 -- 082855.952
we outperform other baselines

082856.711 -- 082858.302
like existing in the field.

082859.071 -- 082901.392
So in particular, the scratch

082901.392 -- 082903.443
baseline here is like we simply

082903.443 -- 082905.752
take the video language model and, fine

082905.752 -- 082907.992
tune it on the domain instead of doing the

082907.992 -- 082910.442
pretraining. So the improvement over the

082910.442 -- 082911.722
scratch based lines indicate that,

082913.012 -- 082915.122
our pretraining is an

082915.122 -- 082917.217
effective procedure in learning good

082917.220 -- 082919.252
representations. So next, we have,

082920.142 -- 082922.321
LaPanda and Univule. These are some

082922.372 -- 082924.491
like, state of the art latent

082924.491 -- 082926.571
action works existing in the field, and we

082926.571 -- 082928.752
also managed to, like, improve upon them.

082929.312 -- 082931.012
And then finally, we have

082931.313 -- 082933.302
comparison with OpenVLA and Pi Zero.

082933.782 -- 082935.942
These are, like, state of the art VLA

082935.942 -- 082938.002
models trained using large amount

082938.002 -- 082940.161
of action label data. And

082940.161 -- 082941.652
we also, like, improve

082942.262 -- 082943.452
upon them as well.

082944.922 -- 082947.082
So these are, real world results,

082947.082 -- 082949.193
and we test the real world results in

082949.193 -- 082951.512
our own back end world setup. And we,

082951.512 -- 082953.821
again, have, like, the scratch model

082954.142 -- 082956.162
and the pie zero, and our model, like, trained

082956.162 -- 082957.672
here. And,

082958.292 -- 083000.552
we obtained, like, better performance than

083001.563 -- 083003.722
all the baselines. So we observed

083003.722 -- 083005.962
some emergent behavior like generalization

083006.262 -- 083007.321
to unseen objects

083008.782 -- 083011.202
tasks, and visual variations, and, robustness

083011.422 -- 083013.602
like some retry behavior and,

083013.922 -- 083015.222
adaptation under perturbations.

083015.962 -- 083018.122
And, high like, we we also

083018.122 -- 083020.132
do kvCashing to ensure high frequency control

083020.132 -- 083020.522
here.

083023.222 -- 083025.722
So I have some videos, like, for example,

083025.782 -- 083027.842
like, the upper

083027.842 -- 083028.772
video here is, like,

083030.102 -- 083032.262
like, a task which was seen in the training set,

083032.262 -- 083033.722
like, covering Apple, but

083034.331 -- 083036.571
it can also do covering baseball, which was never

083036.571 -- 083038.422
seen in the trading set. Similar

083038.722 -- 083039.542
examples here.

083041.962 -- 083044.302
And here as well. So

083045.852 -- 083048.332
And we we we did did

083048.472 -- 083050.802
absorb some emergent robust

083050.802 -- 083053.062
behavior. Like, for example, if

083053.412 -- 083055.482
initial attempt for grasping

083055.482 -- 083057.042
fails, it goes again.

083057.842 -- 083100.102
And attempts to grasp the cloth here.

083100.102 -- 083102.112
And the cup. In the other

083102.112 -- 083102.362
example.

083106.161 -- 083107.861
And, it is also like

083108.393 -- 083110.553
if if there are some external perturbation here,

083110.553 -- 083112.492
it can adapt itself to the perturbation.

083113.781 -- 083116.062
And handle it. Finally, also

083116.062 -- 083118.351
test this in, challenging bimanual setup.

083118.802 -- 083121.193
Which are significantly more challenging

083121.193 -- 083123.362
than single answer. Answer tops.

083126.571 -- 083129.052
And, finally, this is the final example,

083129.052 -- 083131.152
we show demonstrate that. It can

083131.152 -- 083133.032
also do high frequency report control.

083133.592 -- 083135.672
And we use kvCashing to attempt this. So these are,

083135.672 -- 083137.722
like, side by side, rule outs. One

083137.722 -- 083139.882
is at 3.5 hertz. One is at seven

083139.882 -- 083142.102
hertz. And you can see that, there is

083142.102 -- 083144.272
significant delay, slag and delay. In

083144.272 -- 083145.861
the right example.

083147.752 -- 083149.977
So yeah. So yeah. Thank

083149.977 -- 083151.102
you. Any questions

083208.313 -- 083210.111
I'm okay. Thanks. For the,

083210.432 -- 083212.772
sharing. Of the speaker.

083212.832 -- 083214.792
And our next paper

083215.148 -- 083215.648
is

083218.773 -- 083221.223
and let's welcome

083222.781 -- 083224.802
Yuxin Zhang from Ichabod to

083225.462 -- 083226.312
share his paper.

083451.732 -- 083452.212
Okay.

083515.183 -- 083515.842
Because it's

083638.792 -- 083640.502
But that would be the same.

083648.563 -- 083650.422
Sorry for the technical problem.

083652.002 -- 083654.102
Hi, everyone. I'm Lee Liang Chen

083654.241 -- 083656.362
from AGI boat. And

083656.362 -- 083658.443
today, I'm so glad to hear to

083658.443 -- 083659.742
present our work

083700.622 -- 083702.662
and inverse AC it's a

083702.662 -- 083703.162
generative

083704.762 -- 083706.942
simulator, recently always be called

083707.002 -- 083707.902
the war simulator.

083709.531 -- 083711.882
So And first, let's, briefly talk about

083711.882 -- 083713.741
why we need a wireless simulator.

083714.302 -- 083715.821
A key application is about,

083716.742 -- 083719.162
policy evaluation. You know,

083719.402 -- 083721.483
evaluate a policy robot in

083721.483 -- 083723.982
real world always consume a lot of

083724.122 -- 083726.252
time, and always cost a lot of

083726.252 -- 083728.282
materials. And sometimes

083728.282 -- 083730.443
may fear meet some,

083730.922 -- 083733.162
safety problem, make it a little bit

083733.162 -- 083735.662
dangerous. And also a very important

083735.722 -- 083737.102
problem is about reproducibility.

083738.902 -- 083741.281
And it is almost impossible to reproduce,

083742.102 -- 083744.181
exactly the same value you

083744.182 -- 083746.262
last time meet. So that that's maybe

083746.262 -- 083748.462
a big problem for

083749.502 -- 083750.802
evolve your policy.

083752.052 -- 083754.512
And common use of ways to evaluate

083754.812 -- 083756.672
your policy in a physical based

083757.672 -- 083759.342
simulator like ISAC C.

083800.142 -- 083802.562
But so far, this kind of simulator

083802.622 -- 083804.842
still struggle for some

083804.842 -- 083806.393
complicated simulation like

083807.962 -- 083810.092
deformable objects non rigid objects,

083810.092 -- 083811.542
and, fluid,

083812.571 -- 083814.852
sometimes complicated lightning

083814.852 -- 083817.192
and shadow. This kind of

083817.331 -- 083818.582
domain gap make it

083819.702 -- 083821.882
always the conclusion from

083821.943 -- 083823.832
the simulator is

083824.072 -- 083825.562
not the same as a real world.

083828.042 -- 083829.812
And the other problem is about

083830.451 -- 083832.353
scalability. The simulator

083833.022 -- 083835.082
whenever it is created, it cannot

083835.082 -- 083837.103
be better or it cannot get better.

083837.492 -- 083839.572
But for a wall simulator, we can

083839.572 -- 083841.692
make it, better and smarter

083841.832 -- 083843.702
if we feed more data into it.

083849.523 -- 083850.023
So

083851.563 -- 083853.642
So what kind of model we need to

083853.642 -- 083855.262
build such a world simulator

083857.122 -- 083859.202
First, it should, generate a

083859.202 -- 083901.281
subsequent vicious state. So it

083901.281 -- 083903.781
makes sense to be a a video generation

083903.842 -- 083905.531
model. Sorry.

083908.032 -- 083910.362
So And so far, a lot of

083910.362 -- 083912.103
modern policies requires

083912.402 -- 083914.422
a multi view observations to do

083914.522 -- 083916.552
some press precise

083916.693 -- 083919.002
manipulation task. So we think

083919.002 -- 083921.052
our simulation cover this kind of

083921.052 -- 083923.111
features. So we extend it to

083923.111 -- 083925.222
be, multi view video generation

083925.222 -- 083927.402
model. And

083927.402 -- 083929.722
the most important part, the generation model

083929.722 -- 083932.192
should be conditioned on the robot action.

083934.572 -- 083936.701
So Okay. The

083936.701 -- 083939.036
problem setup is clear and then

083939.036 -- 083941.382
the most important problem

083941.682 -- 083944.103
is about how to represent the continuous action

083944.837 -- 083947.201
condition. Because for robots, action

083947.262 -- 083949.472
space is a virus, and, a

083949.473 -- 083951.392
lot of robot type, a lot of,

083951.712 -- 083953.592
expression for action space, like,

083953.992 -- 083955.853
joint and effector, velocity,

083956.342 -- 083957.962
a lot of kind of, representation.

083958.662 -- 084000.272
We want to represent the

084000.831 -- 084002.772
condition in a universe way and

084003.331 -- 084005.122
the more important thing is to

084005.603 -- 084007.912
make it easy to learn for our generation model.

084011.092 -- 084013.291
Find some inspiration from

084013.852 -- 084016.193
such, anime generation work,

084016.592 -- 084019.071
You might have been you might have seen such,

084019.232 -- 084020.772
amazing results for

084021.502 -- 084022.402
like, animation.

084024.273 -- 084026.773
The secret for such, strong controllability,

084026.992 -- 084029.432
it comes from the pixel

084029.652 -- 084031.861
aligned control. Like the post

084031.861 -- 084033.002
map on the left.

084034.072 -- 084036.162
So And we think, we can

084036.722 -- 084038.822
try try to convert the action

084039.882 -- 084042.001
condition to be the pixel aligned

084042.001 -- 084043.902
condition. So

084044.142 -- 084046.642
Specifically, we use the camera

084047.082 -- 084049.321
intrinsic and the extrinsic to render

084049.321 -- 084051.502
the robot's continuous

084051.502 -- 084053.702
action. Especially for the

084054.262 -- 084056.342
60 and any factor pose,

084056.342 -- 084058.362
like the position and the rotation.

084058.802 -- 084101.062
Together, sir, with the openness of

084101.062 -- 084103.402
the gripper. Render them into

084103.722 -- 084105.802
the two d image plan. So we call

084105.802 -- 084107.162
that action map.

084110.842 -- 084113.071
So Then we can train a video

084113.071 -- 084114.892
generation model We use

084116.052 -- 084118.222
diffusion model manner, and we

084118.281 -- 084120.252
simply concatenate the action

084120.312 -- 084122.442
map with the noisy

084122.442 -- 084124.563
hidden state. It's and it works

084124.563 -- 084126.802
where Here,

084126.802 -- 084129.143
I have to mention about the training

084129.202 -- 084131.222
data. Not only use

084131.222 -- 084133.541
the expert demonstrations to train

084133.541 -- 084135.603
such, video diffusion

084135.603 -- 084137.162
model, we also add some

084137.962 -- 084140.462
imperfect value data, and that's very important.

084141.152 -- 084143.492
Because you cannot expect a generation

084144.032 -- 084146.012
model which never seen

084146.152 -- 084148.012
value data to generate

084149.012 -- 084151.157
visually plausible value interaction.

084151.763 -- 084153.922
But that is very important for

084153.922 -- 084154.582
a simulator.

084157.502 -- 084159.672
Here I show some multi

084159.672 -- 084201.292
view video generation results.

084201.822 -- 084203.762
You can see for each view,

084204.582 -- 084206.712
it presses link follow the action

084206.777 -- 084207.277
condition.

084214.832 -- 084215.332
Okay.

084216.852 -- 084219.071
When we have this, video generation

084219.712 -- 084221.792
model as a simulator, how can

084221.792 -- 084223.971
we evaluate if it's

084223.972 -- 084226.372
a reliable simulator for policy

084226.372 -- 084228.432
evaluation Think the

084228.432 -- 084230.412
most convincing way is to

084230.972 -- 084233.152
evaluate the policy in our simulator

084233.933 -- 084235.952
compare it with the sim

084236.286 -- 084238.632
the evaluation result in the real world

084238.632 -- 084240.672
under the almost the same visual

084241.962 -- 084244.023
initialization. Here, I

084244.023 -- 084246.071
show some evaluation process

084246.542 -- 084248.132
procedure. For

084248.991 -- 084250.902
a policy model in our

084251.222 -- 084251.702
simulator.

084257.182 -- 084259.432
Our experiments demonstrates

084259.652 -- 084301.802
that the the result

084301.861 -- 084304.222
in the real world and the result in our

084305.532 -- 084307.772
simulator is very consistent. You can

084307.772 -- 084309.712
see the left. The left left is

084310.122 -- 084312.182
the generation video

084312.622 -- 084314.642
compared with the real world in

084314.642 -- 084316.852
the in the real world. And the right

084316.852 -- 084319.032
side shows the success rate

084319.032 -- 084321.122
for the evaluation in the real

084321.122 -- 084323.062
world and the success rate in the

084324.821 -- 084326.792
simulator. I mean, our war simulator.

084327.102 -- 084329.541
We try different

084329.842 -- 084332.262
policy, and we evaluate different tasks.

084332.483 -- 084334.552
And the results are very

084334.552 -- 084335.052
consistent.

084337.611 -- 084339.572
Okay. In conclusion, there are some

084340.612 -- 084342.642
So first, we prove our

084342.642 -- 084344.661
action map is effective by pixels

084345.032 -- 084347.512
based alignment. And second, the data recipe

084347.512 -- 084349.652
is very important, especially for

084350.052 -- 084352.452
the negative samples that mean the value data.

084353.172 -- 084355.353
And the last one, we demonstrate

084355.492 -- 084357.592
the evaluation with our

084357.592 -- 084359.832
model can be consistent with the real world.

084401.112 -- 084402.282
Yeah. That's all. Thank you.

084416.252 -- 084418.112
Okay. Thank you for the sharing

084419.832 -- 084421.912
And our next paper is

084421.912 -- 084424.387
BLAOS, and

084429.782 -- 084430.882
Yes. Welcome.

084432.422 -- 084434.522
As welcome from AUS to represent

084434.581 -- 084435.222
the paper.

084437.062 -- 084439.142
Hi. I think I will present to you,

084439.142 -- 084439.642
Aman.

084443.233 -- 084445.393
Or if you if you would like to, you can

084445.393 -- 084447.602
open the camera and or

084447.852 -- 084449.692
and the PPT share it.

084450.331 -- 084452.622
Okay. Okay.

084455.012 -- 084456.452
Yeah. Yeah. Yeah. Please begin.

084457.492 -- 084458.952
Okay. Thank you.

084500.012 -- 084502.172
Hello, everyone. My name is Tong Kai Gao from the

084502.172 -- 084504.472
National University of Singapore. Today,

084504.473 -- 084506.632
I'm very happy to present a BOE OS,

084506.632 -- 084507.933
Structuring and Detecting

084510.312 -- 084511.532
language engine model.

084516.683 -- 084519.002
And they are pretty good because they can

084519.002 -- 084520.702
solve non horizon and complex

084521.291 -- 084523.382
and delicate tasks. And show

084523.382 -- 084525.402
superior spatial and object generalization

084525.622 -- 084527.662
ability. And it does have some

084527.662 -- 084529.902
data scalability. And, recently,

084529.902 -- 084531.943
some papers are incorporating the

084531.943 -- 084533.982
task planning models into

084534.092 -- 084536.143
various through either language

084536.143 -- 084538.242
planning, video planning, or word models.

084538.612 -- 084540.853
And they are making way becomes better by

084540.853 -- 084542.972
improving the performances and

084542.972 -- 084545.142
the data efficiency. Enable

084545.382 -- 084547.442
enabling conversational generation

084548.081 -- 084550.161
and making where it becomes explainable, safe,

084550.161 -- 084552.223
and controllable. However,

084552.282 -- 084554.262
these task planning methods in VAE

084554.662 -- 084557.082
exhibit a significant divergence across various

084557.622 -- 084559.862
aspects these methods vary in

084559.862 -- 084601.612
planning representations they use.

084602.172 -- 084604.302
The paradigms, the bill

084604.302 -- 084606.402
and bank phones. The

084606.402 -- 084608.572
training data sets. The network architectures,

084608.572 -- 084609.952
and the training strategies.

084610.972 -- 084613.212
This all makes it very hard for us

084613.212 -- 084615.062
researchers to determine which

084615.382 -- 084617.542
component is more important and which one needs

084617.542 -- 084619.772
to be further improved. So

084619.772 -- 084621.951
in this work, we present a VLAOS

084623.581 -- 084625.982
unified framework for evaluating VLE

084625.982 -- 084628.122
OS task planning. We provide

084628.122 -- 084630.222
a standard and a unified implementations

084630.842 -- 084632.547
across VLAN backbones.

084632.922 -- 084635.082
Data sets, network architectures, and training

084635.082 -- 084637.411
strategies. And explore the

084637.411 -- 084639.571
effects of different VOA paradigms and

084639.571 -- 084640.392
planning representations.

084642.223 -- 084644.242
This makes VAOA OS a strictly

084644.382 -- 084646.491
controllable experimental investigation.

084646.692 -- 084649.111
We first divide the VAOA

084649.331 -- 084651.111
into four categories of paradigms.

084651.763 -- 084653.842
The first category is planning only

084653.842 -- 084656.062
VAE, which only performs task

084656.062 -- 084658.082
planning. Using a per train VOM.

084658.402 -- 084700.721
We do not compare this paradigm in this work

084700.722 -- 084702.821
since the no trainable low

084702.821 -- 084703.982
level action networks.

084705.422 -- 084707.812
The second category is action only VAE.

084707.812 -- 084710.012
Which is trained in an end to end fashion.

084710.252 -- 084712.492
And, directly generate a robot actions

084712.492 -- 084714.111
from visual and the language inputs.

084715.772 -- 084718.193
The third paradigm is integrated BLE,

084718.332 -- 084720.751
which use a monolithic network to simultaneously

084721.132 -- 084723.402
generate a task planning output and

084723.402 -- 084724.322
the robot actions.

084725.683 -- 084727.822
And the last one is hierarchical bill,

084727.822 -- 084730.032
which employs a hierarchical structure

084730.353 -- 084732.432
First, generate a task of planning results.

084732.832 -- 084734.911
From some high level planner. And then

084734.911 -- 084736.182
use them to generate actions

084737.161 -- 084739.152
with no gradients between these two levels.

084740.991 -- 084743.491
Then we build a series of composable network

084743.711 -- 084745.552
modules with unified structure.

084745.872 -- 084747.712
We first, retrain

084748.412 -- 084750.412
a series of bank phones with

084750.472 -- 084752.632
the same network structure and only different

084752.632 -- 084753.452
model sizes.

084755.041 -- 084756.742
Then we design the action heads

084757.382 -- 084759.462
for for for the action generation that

084759.462 -- 084801.662
can extract the VOM keys

084801.662 -- 084803.602
and values from the VOM

084804.032 -- 084806.122
with blockwise cultural attention, just

084806.122 -- 084806.532
like

084808.192 -- 084810.332
So And finally, we designed

084810.332 -- 084812.741
three different, planning heads for

084812.882 -- 084815.201
different representations, including the language planning

084815.201 -- 084817.652
head, Visual planning head includes

084818.111 -- 084819.502
the bunny box,

084820.382 -- 084822.652
planning, any factor

084822.652 -- 084824.892
flow planning, and affordance area planning. And

084824.892 -- 084826.842
the go image generation planning head.

084827.482 -- 084829.882
Which is used for, in many world

084829.882 -- 084830.382
models.

084832.702 -- 084835.042
Then we can use this modular network modules

084835.422 -- 084837.502
to implement different VOA paradigms

084837.502 -- 084839.682
just like building in with LEGO bricks.

084840.103 -- 084842.262
And all of them supports both l one house

084842.262 -- 084844.112
training and the flowmetrics training.

084844.672 -- 084847.013
And finally, we manually label

084847.292 -- 084849.312
large scale of multimodal manipulation

084849.702 -- 084851.772
planning datasets. With the help of

084851.772 -- 084854.192
some foundation models. And, this dataset

084854.491 -- 084856.991
is built across different variable dimensions.

084857.852 -- 084859.332
Object types. Robots,

084900.052 -- 084901.592
fingers, and, platforms.

084904.393 -- 084906.692
With all of these preparations, we

084906.831 -- 084909.012
perform comprehensive evaluations to investigate

084909.152 -- 084911.242
a different paradigm and

084911.242 -- 084913.542
representations of very we first do a

084913.542 -- 084915.962
sanity check on the labor benchmark for our model.

084916.282 -- 084918.222
And we can see that the VAE

084918.443 -- 084920.782
OS performs better or comparable to other

084920.922 -- 084920.932
exist

084922.991 -- 084925.411
Note that our model is trained from scratch.

084925.733 -- 084927.273
And, utilize only,

084927.893 -- 084929.182
0.5 b

084930.222 -- 084932.362
backbone. This result

084932.422 -- 084934.822
shows that a larger video is trained on larger

084934.822 -- 084936.772
scale data size. Do not

084936.992 -- 084939.152
necessarily outperform smaller models than

084939.152 -- 084941.542
are trained from scratch. Model

084941.683 -- 084944.002
architectures and algorithm designs are still

084944.002 -- 084946.032
important. The time have

084946.032 -- 084948.532
not yet come to scale real world model status.

084951.132 -- 084952.912
Secondly, we compare different

084953.532 -- 084955.622
planning representations. On the labor benchmark,

084955.622 -- 084958.042
we show that a visually grounded planning representation.

084958.603 -- 085000.611
That is a virally planning. And the

085000.611 -- 085002.652
image foresight planning. Work better

085002.792 -- 085005.152
than language planning representations. And

085005.211 -- 085007.291
also have faster inference speed and

085007.291 -- 085008.492
smaller training cost.

085010.893 -- 085012.933
Then we also designed experiments to show that

085012.933 -- 085015.112
a virally grounded planning representations

085015.172 -- 085017.193
are easier for the low level action

085017.412 -- 085019.222
to follow. Than the language planning

085019.861 -- 085020.301
representations.

085022.062 -- 085024.260
Then we perform continued learning experiments on the

085024.782 -- 085026.853
liberal law with sequential learning

085026.853 -- 085029.272
algorithm. We can see that a fairly grounded

085029.492 -- 085031.733
planning representations deliver superior forward

085031.733 -- 085033.982
transfer. And, exploit a slower

085033.982 -- 085035.982
forgetting. Relative to the language

085036.602 -- 085036.962
planning representations.

085039.922 -- 085041.932
Then to compare different variant,

085041.932 -- 085044.082
paradigms. We train all three kinds

085044.082 -- 085046.402
of paradigms on six different benchmarks and

085046.402 -- 085048.572
show that the integrative VOE

085048.572 -- 085050.902
and the hierarchical VOE outperform

085051.202 -- 085053.483
action only AOE across a broader

085053.483 -- 085054.372
spectrum of tasks.

085056.853 -- 085059.273
Then we want to know which paradigm can generalize

085059.712 -- 085101.822
better and can benefit from the

085101.822 -- 085103.872
planning head of pretraining. We use a

085103.872 -- 085106.193
cornerstone benchmark and a liberal benchmark and show

085106.193 -- 085108.152
that the hierarchical value generalize

085108.312 -- 085110.652
the best And both the integral

085110.652 -- 085112.342
VAE and hierarchical VAE

085112.742 -- 085114.512
benefited similarly from the

085114.832 -- 085116.102
planning ahead of pretraining.

085118.741 -- 085120.442
Then we show that when employing

085121.103 -- 085122.802
multiple planning repetitions simultaneously.

085123.922 -- 085126.183
As a hierarchy, we outperforms others.

085126.662 -- 085127.122
Paradigm.

085129.842 -- 085131.622
We also show that on the liberal

085131.922 -- 085134.032
benchmark, the Heroku VAE

085134.412 -- 085136.192
performs the best in the high level

085136.751 -- 085137.542
task planning alone.

085139.943 -- 085142.002
And finally, for the continued learning,

085142.242 -- 085144.202
show that the integrated Bayou and the

085144.683 -- 085146.943
Arakawa BOE achieves the higher forward transfer.

085147.302 -- 085148.842
But incur faster forgetting.

085149.523 -- 085151.102
Compared to the action only value.

085152.701 -- 085155.002
And lastly, we also perform data and model

085155.002 -- 085157.212
scalability experiments which show that

085157.212 -- 085159.152
all VOA paradigms have a data

085159.372 -- 085201.622
scalability. However, we show that the four

085201.682 -- 085203.693
tasks that are trained from scratch with

085203.693 -- 085205.052
no more than five k

085205.772 -- 085208.022
demonstrations. There is no model

085208.102 -- 085209.822
scalability. On it.

085211.103 -- 085213.202
So here are the summary of our conclusions.

085218.193 -- 085219.762
Thanks very much for the listening.

085231.852 -- 085234.171
Okay. Thank you for the sharing of

085234.171 -- 085236.492
Yixin. For

085237.372 -- 085239.452
thank you for the sharing of Chung Tongkah.

085239.452 -- 085241.662
And our next

085241.662 -- 085243.442
paper is Latent Weight Fusion.

085244.353 -- 085245.822
And now let's welcome

085247.762 -- 085250.212
Shahshank from USC. To

085250.542 -- 085251.042
representative

085257.342 -- 085259.332
Awesome. I can do HDMI.

085259.652 -- 085259.851
Okay.

085329.442 -- 085331.482
Okay. Surprisingly,

085331.861 -- 085333.942
Linux was the easy Ubuntu was the

085333.942 -- 085335.142
easiest to set up a display

085336.502 -- 085338.632
from ironically. Right Alright.

085338.692 -- 085340.931
So I'm I promise you I'm gonna keep this short. I get

085340.932 -- 085343.103
it. It's the end of a very, very long.

085343.742 -- 085345.791
So I'm not gonna take a lot of time. But having

085345.791 -- 085347.531
said that, I hope at the

085347.912 -- 085349.922
end of this six minute conversation, you

085349.922 -- 085352.081
guys are as excited about the implications of this

085352.081 -- 085354.142
work as I am. Alright.

085354.693 -- 085356.773
Cool. So this paper originally was called latent

085356.773 -- 085358.832
weight diffusion. But

085358.832 -- 085400.902
through a few iterations, we have

085400.902 -- 085402.982
a more, up to date version of

085402.982 -- 085405.232
this, which we call world

085405.232 -- 085407.222
model assisted reactive policy diffusion.

085407.781 -- 085410.102
You can find it in this archive link and this

085410.102 -- 085412.121
QR code, which I will share show it

085412.121 -- 085414.172
again at the end. Alright.

085414.712 -- 085416.812
So people who are familiar with robotics

085417.332 -- 085419.432
are familiar with this massive

085419.733 -- 085421.772
dataset, the cross embodiment dataset.

085422.172 -- 085424.342
It's a massive dataset that

085424.342 -- 085426.762
has a lot of people contributing teleoperation

085427.063 -- 085429.241
data from different

085429.241 -- 085431.431
locations. And so a very big aspect

085431.432 -- 085433.662
of this is because they're multiple

085433.662 -- 085435.762
people and because each person never behaves

085435.902 -- 085437.922
the same way twice, we have a lot of multi

085437.922 -- 085439.392
modality in the action space.

085440.432 -- 085442.512
And also, we have, like, a distribution of

085442.512 -- 085444.762
experts because we don't have just all this data coming

085444.762 -- 085447.053
from one person. But we have it coming from different

085447.053 -- 085448.862
people in different labs in different countries.

085449.983 -- 085451.702
But we ask ourselves,

085452.332 -- 085454.412
a lot of people who try to train

085454.412 -- 085456.432
models on this train diffusion models

085456.542 -- 085458.377
where your action

085458.862 -- 085501.102
distribution is modeled by a diffusion model.

085501.102 -- 085503.202
So they do diffusion policies. So

085503.202 -- 085505.162
they sample actions using a

085505.222 -- 085507.252
diffusion model But we say, hang on.

085508.052 -- 085510.272
A lot of people have contributed to this data

085510.752 -- 085512.852
set. So you in fact have a distribution of policies

085513.242 -- 085514.943
not just a distribution of actions.

085515.922 -- 085518.242
And so in this work, we actually try to see if we

085518.242 -- 085520.402
can retrieve that distribution

085520.542 -- 085522.792
of policies rather than distribution

085522.852 -- 085523.602
of actions.

085525.382 -- 085527.402
Alright. So very briefly, during inference,

085527.712 -- 085529.892
this is what our model is capable of.

085530.292 -- 085532.402
So if you have can I see my

085532.402 -- 085534.443
mouse Okay. Yeah. If you have this

085534.443 -- 085536.802
initial state, you have a

085537.122 -- 085539.362
our model that takes this initial state as input,

085539.362 -- 085541.422
and then takes Gaussian noise

085541.662 -- 085543.432
and denoises it to give you

085544.152 -- 085546.252
a neural network, a policy which is a neural network

085546.572 -- 085548.652
which then will interact the environment for the

085548.652 -- 085549.962
next foreseeable future.

085551.152 -- 085553.172
At the same time, we can also condition

085553.232 -- 085555.252
our diffusion model on a task,

085555.252 -- 085557.512
and we can actually generate a policy

085557.792 -- 085559.742
based on a task. So we are diffusing in

085600.882 -- 085602.452
parameter space. Here in a sense.

085604.042 -- 085606.281
Okay. So we derive

085606.281 -- 085608.342
in this paper but

085608.512 -- 085610.592
since we are still training on a dataset of

085610.592 -- 085612.441
trajectories of state action data,

085612.922 -- 085614.382
like in like we do in robotics,

085615.081 -- 085617.572
we derive that you can have this

085617.632 -- 085620.082
elbow function, elbow objective function.

085620.642 -- 085622.722
Which has an encoder here which

085622.722 -- 085624.902
maps your trajectories to a z.

085625.452 -- 085627.592
You have a decoder here which maps your z

085627.592 -- 085629.933
to a tilde. And then you have

085629.933 -- 085632.172
a behavior cloning loss here which says, if you have a teter,

085632.172 -- 085634.241
which is what you parameterize your policy with,

085634.241 -- 085636.172
and a state, what should the action be

085636.652 -- 085638.732
You have a word model here which says, suppose

085638.732 -- 085640.802
you choose an action given

085640.802 -- 085642.792
a tata. Then like,

085643.032 -- 085644.892
are you making sure that this generated

085645.112 -- 085647.262
policy tracks the, desired state space

085647.982 -- 085649.732
And then we have a Kale regularizer here.

085650.852 -- 085653.032
Alright. So what does our model look like

085653.353 -- 085655.693
We take a simple trajectory encoder.

085656.052 -- 085658.212
Which takes state action pairs, over

085658.212 -- 085700.071
a trajectory and maps it variationally

085700.522 -- 085702.572
to a latent. And then

085702.572 -- 085704.842
we have a hyper network decoder

085705.207 -- 085707.232
which takes this latent and maps

085707.232 -- 085709.582
to parameter space. So, these are hyper

085709.643 -- 085711.952
networks which are networks that are capable

085711.952 -- 085714.312
of estimating weights of another network.

085715.512 -- 085717.672
And then, in, when we

085717.672 -- 085719.892
have this teacher forcing, part

085719.892 -- 085721.952
of our algorithm, we we

085721.952 -- 085724.132
take these, this original

085724.132 -- 085726.472
state information, pass it through our estimated

085726.692 -- 085728.821
policy, and see these estimated

085728.821 -- 085730.892
actions, and then we have BC loss

085731.212 -- 085732.342
over these actions.

085733.702 -- 085735.882
At the same time, we also

085735.882 -- 085737.742
train a word model in parallel

085737.943 -- 085739.962
to make sure that we can track the dynamics,

085740.422 -- 085742.652
that are defined by these state action pairs.

085744.742 -- 085747.001
And then mid training, sometimes

085747.001 -- 085748.842
we also pass an additional gradient

085749.082 -- 085751.162
to our policy generator by freezing this

085751.162 -- 085753.563
world model and making sure that our generated

085754.023 -- 085756.122
policy tracks the desired

085756.122 -- 085757.582
action, desired state trajectory.

085758.531 -- 085800.951
So this is inspired by model based imitation

085801.012 -- 085803.432
learning. It's a paper by Wave.

085804.443 -- 085806.523
Which, where they actually show that you can make

085806.523 -- 085808.622
sure that you can use a world model to

085808.622 -- 085810.202
kind of mitigate covariate shift.

085811.277 -- 085812.741
Alright. So,

085813.361 -- 085815.442
this entire thing can be trained end to end.

085815.442 -- 085817.402
This is the VA section of our

085817.802 -- 085820.102
of our model. And then once you have this

085820.102 -- 085822.192
latent action as access

085822.192 -- 085824.302
to this latent space, you can train a

085824.302 -- 085826.272
latent diffusion model to kind of

085826.752 -- 085828.971
sample in this latent space. So during

085828.972 -- 085831.152
inference, what you're actually doing is sampling

085831.292 -- 085833.242
in this latent space based on

085833.483 -- 085835.562
the previous states and a task identifier and

085835.563 -- 085837.582
then decoding it to the policy space.

085839.081 -- 085841.241
Alright. So what are the benefits of

085841.241 -- 085843.692
doing something like this The first

085843.732 -- 085845.702
is because you're actually generating

085845.842 -- 085847.792
a policy, you're generating a function

085847.952 -- 085850.112
You're not actually generating a sequence of

085850.112 -- 085852.532
actions which you usually do

085852.933 -- 085855.023
in robotics. Because you are generating

085855.023 -- 085857.202
a function, you are more robust to stochastic

085857.263 -- 085859.402
environments, you are more closed loop

085859.802 -- 085901.902
If you were to generate an action trajectory

085902.041 -- 085903.852
like most of our baselines do,

085904.571 -- 085906.652
then during the trajectory tracking phase,

085906.652 -- 085908.662
your policy is pretty much open loop.

085909.622 -- 085911.682
Whereas here, since we generate functions, we are closed

085911.842 -- 085914.082
and we see when we have adversarial perturbations,

085914.642 -- 085916.772
we are more robust to it. So this

085916.772 -- 085918.932
is on the push t task where you have to align the

085918.932 -- 085920.962
gray t over the green t. And

085920.962 -- 085923.382
the DP is the diffusion policy benchmark

085923.443 -- 085925.532
here. And because we

085925.532 -- 085927.612
are generating in function space and we are

085927.612 -- 085929.782
more robust to these perturbations, we can actually reduce

085929.782 -- 085932.062
the number of diffusion calls that we need

085933.102 -- 085935.121
so we can actually run this with

085935.693 -- 085937.332
for a far longer action horizon.

085938.612 -- 085940.773
Now, similarly, we can show also

085940.773 -- 085942.852
condition our policy generation

085942.852 -- 085944.882
on on a task. And we

085944.882 -- 085947.142
can we actually see that since we are generating

085947.282 -- 085949.442
policies for just the task you

085949.442 -- 085951.522
care about at hand, the generated policies

085951.522 -- 085953.532
tend to be really, really small, like

085953.773 -- 085955.932
orders of magnitude smaller, and so

085955.932 -- 085958.172
there's a lot of computational benefits for doing

085958.172 -- 085959.072
something like this.

090001.552 -- 090003.942
This is the result that I'm most excited about.

090003.942 -- 090006.092
Which is we have a

090006.092 -- 090008.382
latent space. Right And if we take a

090008.382 -- 090010.563
TSNU of that latent space, we

090011.183 -- 090013.342
see clustering in that latent space, and we believe that

090013.342 -- 090015.563
these clusters actually map behavior

090015.563 -- 090017.632
space so this was

090017.632 -- 090019.777
trained on the Robomimic dataset. Which

090019.777 -- 090021.752
is we have six operators.

090021.842 -- 090023.062
Trying to

090023.922 -- 090026.082
manipulate a can from one place to another

090026.082 -- 090028.132
on the table. And these six

090028.193 -- 090029.972
operators are of diff different,

090030.512 -- 090031.572
degrees of

090032.982 -- 090035.292
proficiency in completing this task. And

090035.292 -- 090037.532
when we apply our method and we when we visualize

090037.532 -- 090039.832
this latent space, we see very clear clustering.

090040.233 -- 090042.482
Based on, which point

090043.602 -- 090045.812
person. And like our method

090045.812 -- 090047.962
was never given explicit information about this.

090048.522 -- 090050.781
So during test time, for example, we could

090050.781 -- 090052.942
actually just sample a few latents from

090052.942 -- 090054.462
this red region over here,

090055.182 -- 090057.422
which is Yeah. So just this red

090057.422 -- 090059.451
region because maybe we like the behavior of

090059.451 -- 090101.822
operator one more than any other operator

090101.983 -- 090104.032
when we want to see our robot control something.

090104.672 -- 090106.832
So we have actual access to what kind

090106.832 -- 090109.012
of behavior we want. When

090109.012 -- 090110.232
we generate our policies.

090111.982 -- 090114.023
So in conclusion, we

090114.023 -- 090116.523
can now generate parameters rather than actions,

090117.143 -- 090119.382
and these parameters are modeled by their

090119.382 -- 090120.362
behavior distribution.

090121.652 -- 090123.832
One kind of limitation that we saw is

090123.992 -- 090126.392
if you can run diffusion at a high enough

090126.392 -- 090128.542
frequency, where you're just generating

090129.023 -- 090131.142
actions at every time instant, that's

090131.142 -- 090133.382
really hard to beat because you you have a really

090133.382 -- 090135.432
good probabilistic model that

090135.432 -- 090137.432
works with a very easy task here.

090138.472 -- 090140.792
But if you have a compute constraint platform

090140.792 -- 090142.402
and if latency is an issue to you,

090143.201 -- 090145.281
then our method can actually alleviate a lot of

090145.281 -- 090147.332
those issues. Now the open

090147.332 -- 090148.552
directions, are

090149.482 -- 090151.562
we want to see how we can expand this

090151.562 -- 090153.722
to vision based policies. So all the results

090153.722 -- 090155.332
I've shown so far are on low

090155.892 -- 090158.052
dimensional state spaces. So we assume that we have

090158.052 -- 090200.081
some kind of image encoder that kind of

090200.081 -- 090201.942
gives us these embedding information.

090202.162 -- 090204.242
Or we have proprioception, which gives us

090204.242 -- 090206.262
a lot of state information. But we

090206.262 -- 090208.602
want to see if we can somehow estimate generate

090208.662 -- 090210.512
entire end to end vision policies.

090211.962 -- 090214.122
We wanna see if we can use a richer hyper network.

090214.122 -- 090216.202
So there's a lot of lot of work on

090216.202 -- 090218.232
hyper networks lately where you can actually

090218.472 -- 090220.251
scale it up to even vision transformers.

090220.733 -- 090223.072
You can estimate weights for even vision transformers.

090224.592 -- 090226.672
And now since we have access to this behavior

090226.672 -- 090228.752
distribution, we wanna see if we can do some cool things

090228.752 -- 090231.103
with it. Say, we want this kind of behavior.

090231.103 -- 090233.242
We don't want that kind of behavior. In

090233.242 -- 090234.552
our generated policies.

090235.912 -- 090236.652
And also,

090238.112 -- 090240.273
also remember Glenn saying this, earlier

090240.273 -- 090242.371
during the discussion panel. Wherein

090242.371 -- 090244.451
he wants to see where we have a world model

090244.451 -- 090246.462
that can plan but then

090246.603 -- 090248.342
but then the output of that plan is a

090248.822 -- 090250.902
small locally optimal policy that we can then

090250.902 -- 090252.912
fine tune This is a method that

090252.912 -- 090255.192
actually can give you that because we can plan

090255.192 -- 090257.352
with our pretrained world model, and then

090257.352 -- 090259.702
we can generate a policy based on the outcome

090259.702 -- 090301.942
of that world model's plan. And then because

090301.942 -- 090303.992
it's a neural network that we generate, can

090303.992 -- 090305.592
even fine tune it if we want to.

090307.652 -- 090309.922
Yeah. So those are the open directions.

090310.382 -- 090312.542
And if if you're kind of interested in this or

090312.542 -- 090314.192
if you wanna get in touch yep.

090314.592 -- 090316.702
You feel free to Any questions

090325.812 -- 090328.312
Thank you. Thank you. Thank you. Thank you.

090328.422 -- 090330.582
The next speaker is Chunghao

090330.582 -- 090332.611
Ni. She will get up to the

090332.611 -- 090334.542
final paper robotic model.

090351.497 -- 090351.997
So

090418.252 -- 090420.513
Awesome. Yeah. Thank you, Urs.

090421.063 -- 090422.902
Thank you all for staying this late,

090423.142 -- 090425.172
for my paper. That's what

090425.172 -- 090427.252
I told myself. And I like to stand very close

090427.252 -- 090429.102
to the screen so I can be more engaging.

090429.821 -- 090431.871
Great. So this workshop this workshop

090431.871 -- 090433.962
is about Embodied Work Model for Decision

090433.962 -- 090435.992
Making. And a lot of people

090436.292 -- 090437.981
like present very nice ideas of like

090438.782 -- 090441.132
video upward models. But we

090441.193 -- 090443.233
wanna show this actually

090443.233 -- 090445.402
works in real robot. So we're very

090445.402 -- 090447.722
proud of, this work. It's kind of one of the

090447.722 -- 090448.822
first to really

090450.042 -- 090452.282
you can't distill a policy as innovates

090452.282 -- 090454.392
by previous speakers from the work

090454.392 -- 090456.602
model owner. Great. So we

090457.012 -- 090459.042
presenting a robotic work model, a neural network

090459.042 -- 090501.062
black box end to end simulator.

090501.092 -- 090503.053
Robust policy optimization

090503.112 -- 090505.193
robotics. Great.

090505.512 -- 090507.791
So if you come from the background

090507.791 -- 090509.871
of robotics, especially for, like,

090509.871 -- 090511.812
legged locomotion or quadratic humanoids,

090512.082 -- 090514.102
What people do nowadays is, well,

090514.342 -- 090516.821
have a robot, just throw that in as high fidelity simulator,

090516.821 -- 090518.852
you identify all the parameters, and

090518.852 -- 090520.861
train a PPO policy on it. And you

090520.861 -- 090522.672
deploy the policy on the robot.

090522.912 -- 090525.142
And hope it transfers. This

090525.142 -- 090527.172
works relatively well, but you

090527.172 -- 090529.512
know, after deploying a policy on the real hardware,

090530.192 -- 090532.272
well, our hardware is running in the real world. Right

090532.272 -- 090534.382
So since real world data, we

090534.382 -- 090536.402
have state action transitions, we're not

090536.402 -- 090538.523
just used to use these data, to,

090538.523 -- 090540.642
you know, keep improving our policy because, well,

090540.642 -- 090542.902
the simulators are never good enough, and

090542.902 -- 090544.862
we can always keep improving.

090545.922 -- 090548.292
So Yeah. This is like

090548.352 -- 090550.583
the typical other training pipeline,

090550.583 -- 090552.662
and there's no online adaptation anymore for

090552.663 -- 090553.332
the current pipeline.

090555.522 -- 090557.882
So what we're thinking is, oh, and

090557.882 -- 090600.222
since we're doing RL in real world,

090600.462 -- 090602.962
and in our real world of RL, we have very limited

090603.102 -- 090605.261
data. Then a very simple

090605.482 -- 090607.612
strategy is train a work model or a dynamic

090607.612 -- 090609.672
model, and that can directly

090609.672 -- 090612.091
give us infinite amount of data. And with that data,

090612.253 -- 090614.652
synthetic data or people say imagination,

090615.292 -- 090617.512
we can improve our policy. It's a very

090617.512 -- 090617.902
simple idea.

090620.542 -- 090622.672
Yeah, we after we have the model, we

090622.672 -- 090624.692
train a policy with purely imagination,

090625.532 -- 090627.762
and we can deploy the policy back into real world.

090628.083 -- 090630.162
So you can see, if the model can be bad, it's

090630.163 -- 090632.301
fine. The policy will that that's

090632.302 -- 090634.382
be bad, but it's fine after we deploy on

090634.382 -- 090636.430
real world. Can collect more data to

090636.431 -- 090638.601
update the model update the policies. So

090638.601 -- 090640.352
both the policy and model are trained

090640.862 -- 090642.912
online like, this kind of continual learning

090642.912 -- 090645.261
setup. Yeah.

090645.402 -- 090647.562
But why are people not doing this This is

090647.562 -- 090649.562
so simple. Right Like typical

090649.620 -- 090651.962
model based RL, you see a lot of great results

090652.021 -- 090654.251
in simulation at least. It's

090654.252 -- 090656.141
kind of hard because no matter

090656.431 -- 090658.562
well, I wanna motivate why do we need

090658.562 -- 090700.722
to train a policy and planning doesn't really

090700.722 -- 090702.801
work. Because we're dealing

090702.802 -- 090705.102
with, lack of locomotion, and we have

090705.342 -- 090707.292
control frequency of at least 50 Hertz.

090707.722 -- 090709.882
And planning, if you have a lot of, you know, zero

090709.882 -- 090712.102
short planning like Sam, you have a

090712.102 -- 090713.902
lot of particles this doesn't

090714.122 -- 090716.260
give you 50 hertz. So we do need to

090716.261 -- 090718.372
learn a policy from this

090718.431 -- 090720.422
work model. But

090720.662 -- 090722.902
matter whether you use policy optimization, like,

090722.902 -- 090725.391
no matter whether you use policy gradient methods,

090725.872 -- 090727.352
or the value function based method,

090728.632 -- 090730.493
To get the unbiased value

090731.032 -- 090733.403
estimation, you have to roll out your dynamics

090733.732 -- 090736.072
kind of long enough, you know, to do the TDE bootstrapping.

090736.552 -- 090738.942
Right But this is especially challenging

090739.423 -- 090741.620
on the learn dynamics. Because, you know, errors

090741.620 -- 090743.962
compounds. When you do it, when you imagine autoregressionally,

090745.382 -- 090747.422
like, in the end you will deviate, you'll

090747.422 -- 090749.632
fly. So that's kind of challenging.

090750.931 -- 090753.413
And the second thing is for real

090753.472 -- 090755.492
hardware, you know, when people do model

090755.492 -- 090757.562
free RL with a similar layer, a bunch

090757.562 -- 090758.790
of domain randomization, like

090759.782 -- 090801.952
like dragging the robot, like pushing the robot,

090801.952 -- 090803.692
and you can change it to terrain.

090804.092 -- 090806.572
But this is also a little bit hard to do on the learn

090806.573 -- 090808.652
dynamics, Again, like, once you perturb

090808.652 -- 090810.661
the dynamics, dynamics will

090810.662 -- 090812.122
fly away, right, will deviate.

090813.572 -- 090815.592
So the model have very limited robustness

090815.652 -- 090817.892
of noise, which in return

090818.112 -- 090820.132
have very limited robustness

090820.191 -- 090821.672
to the policy that transferred to real.

090823.673 -- 090825.532
And finally, for lack of locomotion,

090825.833 -- 090827.712
especially, the dynamics itself, as we are

090828.673 -- 090829.832
we is

090830.870 -- 090832.762
inherently discreet. Right So

090832.923 -- 090834.782
when we wanna learn an end to end neural

090835.003 -- 090837.242
network policy, it's kind of smooth. This is really

090837.242 -- 090839.432
not really there. So the main

090839.812 -- 090841.816
contribution of this work is

090842.462 -- 090844.701
to stabilize long horizon auto

090844.702 -- 090846.082
regressive model rollouts.

090846.772 -- 090849.032
And to make it robust, against

090849.332 -- 090851.352
noise. Also,

090851.352 -- 090853.471
like, you know, there are a lot of very advanced

090853.472 -- 090855.132
or, you know, fancy architect

090856.644 -- 090857.224
like, well,

090910.382 -- 090910.782
Sure.

090917.932 -- 090920.673
Sorry guys. Workshops were over at 5PM.

090921.482 -- 090923.112
So this session has gone overtime.

090923.662 -- 090925.902
And we'll have to ask you to pull your posters

090925.902 -- 090928.232
off the wall and exit the

090928.292 -- 090929.311
room please. Thank you.