Number one, welcome
to our workshop So
please take a seat, and we will
going to start our workshop
this morning's session.
So I will give, like,
I will wait for, like, one minute.
Okay. For organizing our workshop,
we are gathering, at a really
exciting moment of this field.
War model have become a very
popular concept these years, and,
especially in embodied AI because
it help agent to infer and
predict the real world dynamics
by modeling the external
environment. More importantly, we are
we have increasingly, powered
progress in decision making and
planning for interacting agents.
The motivation of this workshop is to
push that further. We want to
bring together the researchers
working across different domains
like generative modeling,
reinforcement learning, computer vision, and robotics.
To explore, the next generation
of embodied world models or
general war models. And,
we are going to discuss about
how to enable agent not only
to understand, but also
how to predict the word and how to allows
the agent to in, to
modeling the interaction of the word.
So the key theme throughout the day is moving
beyond the world model as a
passive predictor, and
our work is emphasizing
the following domains, like
based reinforcement learning and long
horizon planning.
And also aligning simulation
and the real world physics for
robotics learning and the interactive
scene generation video language action models.
And for border, we are going to discuss
about open war games and also
autonomous driving as our application.
And, for today's
schedule, after opening remarks, we'll
move into some outstanding lineup
of
invited talks, we are honored to host
a keynote from
Elias Berenbaum, Sanjay Fiddler,
Nicholas Hanson, Chelsea Finn,
Peter Stone, and
first Birthdays in the morning
session and after lunch and post session
too. We are especially excited
for the industry demo from our
sponsor, Weave, and highlight
how world models can empower evaluation
and validation as a school.
In the afternoon session, we will hear
from, John Lanford,
Yilun Du, and,
Philip Ball, followed by some,
panel discussion. And
we also received strong submissions
in both research and opinion
formats, and all of the
accepted papers will be presented in the poster
session. So I hope you will
enjoy spending time with discussion,
with with the author
of the posters and
chat with the author according
crossing the day.
So before we begin, I'd like
to offer a special thank to our sponsors,
AGI boat, and, we've for
their general support for our workshop.
And the border embodied the world model's community.
With that, let's get started. I hope you
have a fantastic fantastic and inspiring
day at Embody War Model for Decision
Making.
So our first speaker
is Elias Paringpoint.
He is, associate professor
at the department of computer science
and a director of Artificial
Intelligence WAP at Columbia
University. His
talk today will be
towards causal AI. Let's
welcome him.
Okay. Good morning, everyone.
I'm Elias, happy to be here.
I changed a little bit the title of my talk, but I think
it's suitable as to art causal AI. I
think it's very interesting kind of angle
the point of view to have
embodied bundles. And,
and I'll try to elaborate on that.
Most of my talk is in the my forthcoming
book, that is, the
draft is available in this website.
Cause our air book .net.
And I think we are all here today because we're
excited about some kind of recent
breakthroughs in the field of AI.
Systems are able to
perform extremely well in making predictions
in high dimensional settings.
In particular, there's,
huge progress in fields such as
NLP, computer vision, and enforcement
learning. And building this
capability of doing high dimensional predictions.
Applications are everywhere. I don't feel I need to
talk much here in the audience.
With this audience. And this is kind of very
exciting. I think we should be few
or we are blessed to be
living these times. A question
that I'm asking I ask,
myself and in the lab is is
does this means that that we are done
Get Duncan, experiment here.
Thought experiment. Even if you assume that you have
infinite amount of compute
and data, I know that you don't,
but doing yeah,
this experiment I were
done, in reality in terms of,
the fundamental problems here, and
all what we need is scale more. And,
the more a little bit more of the same.
In this neighborhood. And if not, what is
missing So
Now, the other side of the coin here in terms of
the breakthrough, and what this technology
led us, there are still
very serious foundational
issues. I have been saying that
for maybe a few years now.
I'm glad to see that it seems that there
is a growing feeling
from people even in the field that help
us to reach here, acknowledging
this point, For example,
Ilya, said that just scaling
just recently, this is last last week. Just
scaling is not enough We
need new ideas the next jump.
So Le Cun Yang says scaling
alone will not get us
to systems that are that understand the world.
With the keyword here is understand as well. What
does this mean, and how this relates to
Khosaleid
Francois said that intelligence is not an emergent
product of bigger models. So
OSHA say robust and safe AI
requires causal understanding.
Without it, system will fail, fail unpredictably.
Again, I think I'm
super excited and that it
seems that the
thought leaders in the field,
it seems they start to get
this feeling that there is some some kind of
important missing component to the puzzle.
And I will elaborate a little bit more. We're
not completely lost in terms of the
what what is missing and maybe a solution if we
don't have all of them. Breaking news here
or or to to be upfront,
I think you have ways of thinking about that. At
least. And, and some kind of robust
theory or language to to think
about that. And here are a few kind of
challenges for the current AI. They suffer
from lack of,
understanding the keyword there that was mentioned, and
ability articulate the
explanations. I I would defer making
paper reviews here or a few reviews in terms
of the literature of explainability,
but I think we
don't have this capability at the moment.
Potentially, also don't need
a maybe a PhD in in AI
to see that,
data in reality is a picture of the
current reality that you are sampling from.
Breaking news as a human,
reality is kinda messed up. A
way, they have many problems.
And, and if you if you're sampling
from this reality, these problems
are in the data. This is queerness or the biases
or your fingers is in the data. Then
you are training a model, that is,
there's a pattern recognition model
in high dimensional high
dimensional pattern recognition.
What do you think that the model will be learning The
the model is in some way trying to learn or
replicate something that is in the data,
that is just a mirror of what is in the world.
Which you usually I'm teaching. If I'm teaching AI or
ML, non causal
course or I'd say that this is a punch in the
face. Of the the paradigm
I put this hat. Because
there is a belief that, data
is everything, give him more data,
and eventually, things will be okay.
And, and this observation here shows
that this is not not the case.
Maybe you're trying to move to a better world,
counterfactual world that it doesn't even exist
yet. And then you need some type of language
to be able to articulate that. And I
I could elaborate, we spend
a good amount of time on that. There
oh,
They are very data inefficient. Right We need
a kind of billion interactions to learn
something that maybe a infant
or something you you you you learn very
quickly, Nothing
against me excited about results that we
have in terms of beam sample efficient,
but they they are kinda neat and
amazing in in their ways. Given they
appreciate the mathematics. But the question is like, is
it just a signal for us there
is something wrong in the data structure, and what
we are trying to fit something that doesn't really
fit. We are kinda trying to hammer our way
out. Of it. Then doesn't
it doesn't it exist some kind of more natural
parameterization that maybe is more aligned
aligned in terms of English word here It's kinda
more related to the real world that,
that is simpler in a sense.
That doesn't make us doesn't
require us to dance so much.
Or to try to use our hammer
this way. They're very, nonrobust.
Generous abilities are very poor. Right These systems
break very easily. Think I don't need to tell
the robotics or the people who use that.
Yeah. I think this audience is
acknowledges this one. The
and they also lack controllability. You
kinda train this billion dimension
billions of orders of billions of dimension
space, and then
we can go there inside this space and then,
try to change something, and then we end up
doing kinda splash. Right It's very hard to go
surgically in this space trying
to change one thing to bring about the change that we want.
We end up changing multiple things. Also
try to give example. Those are
kinda torny problems,
long standing. It didn't appear
here only because of these
models, but We're carrying over that from decades.
It just amplified it in a very
very crazy scale
given the scale of the models. And
scary in their ways the sense of also
the scale of penetration. Right And and
scale of use of these systems
by society. It's no longer been the
rips when I started my PhD. As I don't
know, much less people
carrying or using
what we are trying to do. Now,
the the and that's good,
We'll have we'll have a little bit more.
To start move moving from there. The
but question that also that I ask myself
is like, do
these problems have anything in common
Or just have a bag of different types of problems there
And, and now,
he'd be here, I believe that
they do have something in common. At the core
of these challenges is
the absence of a robust causal understanding.
So They are never I haven't seen it.
You can send me the paper. And
any any paper,
any work that, the systems
are truly trying to optimize
or trying to to go towards understanding
the world a causal way.
Now, given that we are a scientific
conference as well, would
like to to get this label science. Right We'd like to
move towards a science
of AI. And And here
is an idea
Why not try to model
the agent and viral relationship truth
causal language
Then we have we have our
cartoon here, that we have the
world, and we have the agent
being borrowed,
And then, the agents see the world, and the agent
acts in the world. And this is kind of
following recipe. I like to trace it back
more than seventy years ago,
seventy five maybe, to
to touring himself that he throw
that, but we want to to build a is a
machine that can learn from experience.
Then I usually say, Turing
was right
as he almost always was
then, like he's our hero here,
the the idea of the
experience is very underspecified.
The the whole touring, the whole idea of the touring
that is remarkable that has passed,
was under first the avoidance
of having to try to defy intelligence,
and under the idea that if you can kinda
be asked, if you can kinda mimic or
pretend that you are this entity that you feel that
is intelligence, things will will be
able to kinda generate intelligence.
We didn't expect that the idea that
we can kinda disentangle someone
that is able to be asked is
different than someone that is intelligence or acquired
his intelligence skill.
And I see that about when I'm interacting with these
systems, Everything that I don't understand about
the world world that is
very little about the world, very specific topics.
All topics that I don't understand about,
if I chat with the models, it's amazing.
I think, wow, I'm learning. I almost want to
love it. Right It's so coherent.
And, so compelling, then I'm
almost like you don't you hard the system in a way.
Every topic, very few, but that I understand
something about or relatively well, there
there is always kind of things like, wow, man, no human
will say something like that. We got completely
off. Then I think
must choose Mark maybe to realize that you
could you could disentangle intelligence from BS
ing. Right The idea of
pretending
many help that the the But the the
but idea is good
The idea of experience is good.
And and they specified. Now we would
like to kinda open up a little bit and
refine what experience could mean.
And here are different types of modalities
about the how the agent could interact with the
world. The first one is through senior or through
passively observing the world unfold and kind of
collecting data. The the second
one is to doing, to intervene in the system,
and the third one is to kinda
hands off just kind of thinking about the different
versions of reality that you could have
They then, in some way, would like to unpack
the idea of what is experience. There are different
types of experiences. That the
agent could face or
experience. Now, it turns out that
these different types of experience is tightly
related to a structure, a mathematical
structure that we have in causality, have been
studied from some time. Now that is called the PCH,
the pro causal hierarchy.
Is a hierarchy of languages related
to each of these types of experiences.
Then here's the the the headline, I
guess, that is like not all experiences created
Then I would like to elaborate a little bit more
about these different languages.
Now, result here, number one in causality,
and this is kind of popularized in this book, The Book
of Why, I recommend. Second,
recommendation of the day,
The and the result here is about
once we have a I had the word
world here, but once we have a causal model that is a
world model,
this induces this hierarchy, the potential
of the system that you
can, have different types of observation of
the We have one observational distribution.
You have different types of ways of intervening,
with the system, the kind of typical RL.
And we have different, counterfactual
distributions.
I will fast here, spend a whole semester
doing that, The the
in class but the first
layer of this hierarchy, this is why it's
is a containment hierarchy, like p versus
NP, or Chomsky or many
that we have in computer science and mathematics,
the the first one is associational,
very related to the symbol p
of y. Oh, where is my mouse here Oh, it's
here. Yeah. Of
y given x, highly related to
observing the world in a way. It turns out
that there are different ways of,
different contact part of that the
world of machine learning, is called the
supervised and unsupervised learning or semi
supervised learning as well.
Depending on the decade that you live, some of you
are just arriving here, but if you
have been more time in the community,
depending on the decade, we have a different
formalism that is trying to evaluate
expressions like that, p of y given x.
But You can date this
back even before before,
machine learning existed. Have been doing that
for two hundred, three hundred years.
But now the different aspect here is
that this variable x here can be a 1,000,000,000
dimension. Kind of
variable. So then we are kind
of trying to predict, and the y is a big sequence.
And this is why it's kind of very good for the
computer scientists because kinda our business, we are trained
with the idea of like how to scale up.
And this is what we have been doing for decades.
And transformers is the current or the the
slash the fusion is the current one that we are able to
do in inference inferences on top of this data structure,
this object. This abstraction called p
of y given x. But now there is the
other one that is related different type of experience,
that is related to interventions, symbolically
we can't extend the language here, and
we have p of y given do x.
Comma c. And the do is that the agent with their
own free will or their own volition
or from outside the environment, it decides
to intervene in this variable x. It's not
observing other agent, and what will happen
with why had the other agent sat
have access to tracks, but the agent know
is doing that because know the causes. Right It's driving
kind of the process. Sees the context
here related to doing, what if I do
x
There is also a counterpart for that in
machine learning that's called reinforcement learning.
Also, there are different formalisms that you can
have to handle that different data structures,
MDPs, POMDPs, and so on,
or causal, version of that is a
causal Bayesian network, and so on.
The specific way I'll try to to show
or or give a few examples about my
own is great, but it's a very specific way of thinking
about causality. It does allow us to get some causality,
would like kinda to broaden that
The the word is a little bit, yeah.
Bigger. The the the third
layer is the counterfactual layer.
That is related to the idea of imagination,
introspection, retrospection,
The the why type of question was with this book of why
that I recommended. Is very tied
to the counterfactual layer,
very related to the premise
what if I hadn't acted differently I
didn't. I did a particular thing, but
what if
dots dot dot Then they here, for example,
Joe took the drug, that
is x prime, and Joe is dad that is
y prime. This happens in
this world, as fact. And then you
wonder, would he be alive That
is why, the opposite of y prime,
had he not taking the
drug. That is the x, that is the opposite
of x prime. And very hard type of questions,
but we are kind of asking, I
will, suggest you to try
to introspect try to catch your own
thought process, and see the variations. Maybe
you're not using exactly the same language but how
do you use, in your own lives,
the idea of counterfactuals Very
related to attribution, blame, responsibility,
very important notion.
For society and for humans in terms of understanding.
K Keeping the word you are understanding.
That I mentioned earlier. The there's no
counterpart for that. In
ML. Not do paper review here,
but there's no no
no counterpart, but there is a
structural causal model that is a type of
language or model that, gives
semantics to quantities of this
type, and then you can kinda unravel from there.
Observation that the formalization
of the PCH provides a way
to measure the capabilities expressiveness
with with different formalisms with
respect to increasingly complex
queries or questions or modes of
reasoning that we like the agent to have
or to able to articulate relative to some
environment
Now, what's the challenge here
Elias, what's the challenge If it's true
that a structure called a model
a world model induces all
these probability distributions, then why the problem
is not trivial In a way
And here is a a a cartoon here
why why it is not trivial. Most of
the data that we have in the world, Elias
numbers, 99% of the data is
coming from layer one. Is
observational data. And most
of the queries or the interesting queries or questions
that you can ask about the world
lives in layer two and layer three.
I Then you have a fundamental mismatch
here. That we are sampling from a distribution. Let's call
layer one distribution p. And you're
trying to make a statement about another
probable distribution p prime. Attached
to the other layers. How the world
would be had we done dot dot
dot. And we didn't collect data from this world.
Then there is a intrinsic methodological
here problem or struggle about how
to connect these different worlds.
Again, data is from layer one. Then
is about how you observe the world, most of it.
And then and the question is, are most of the inference
are about cause or effects The effects
of policies, treatments, decisions,
so on. This is about layer two. Same can
same picture for layer three. Now,
question that kinda started the field
maybe twenty five years ago plus, is like,
how to use the data collected from observations
from layer one to answer
questions about interventions at this layer two.
Now how to dance in the field that tries
to answer that is called causal inference.
Now I would like to put the kind of mother that addressing
here to the same question, that is
the idea everyone is using the word world
models I'm not the most excited at
the moment because it loses meaning when people use
them so casual not so causal, but
very casual way. Everything is
world model as well. Now, they
but challenge here from a kind
of, hopefully, more mature causal perspective
about what is world model. Don't like the name also
cause a world model. Because a causal model
is a world model. It's not a model
model about what The and a world model
does not cause us, which kind of, what are
you saying in reality It doesn't doesn't parse for
me, but I digress.
But then you have some kind of unobserved world
there that exists, There is this underlying
collection of causal mechanisms
and and and border conditions
m star, that is called causal model m star,
we don't observe that either.
Just results from the previous lay slide
that it does induce has the potential to induce
these different probabilities or different
kinds of experiences The first
one is about observations, the agent just passively
observing. That's red.
Intervention, that is the yellow, and,
and counterfactuals that are green. There's
one observational, agent dependent. Each
agent has his own kind of point of view,
and then you have multiple intervention distributions
and multiple counterfactual worlds that
could exist. Now, it comes
we don't observe these different
realities. The yellow and green, we don't observe, but we
observe the current world that is factual,
observation of distribution. Now
it comes humans here, us,
and you are trying to learn a model in the right
let's call a neural model, a transformer,
m hat.
I This m hat is a bunch of functions. You can
also add probability distribution there. It's already there
depending on the phone and transformers.
Then there's something that could look like
a collection of functions and probability
distribution is great, which means that in principle,
it has the potential of
generating different types of distributions or
experiences. If you wish. Now
what we'll do What means training
Training means that we'll get the data that we have that
is from the left side that is red,
and then we start kinda training. We go to the cluster,
many GPUs, go there, you
spend one week one month, sometimes two.
You dance, you jump, you pray.
And eventually, the guy in the right side
is able to match
or kind of, mimic the distribution on
the left. That is the one from the data.
Then that's great. And you say success.
Great. Are great. Amazing.
But now the questions about what does
this the the this paradigm
or this model m had That
m hat that is trained through this idea, tell
us about the yellow. That we haven't
observed.
I call this a fundamental problem
and the question here just as likely
more explicitly, under what conditions
inferences in the m hat that
is in the right side are valid.
When do they recover the distributions
induced by the true model in m
star That is in the left.
Of course, you can ask ask the same question
related to layer three.
And the same cartoon holds
if you common question that I get, if
you observe part of the yellow
distribution. In the left.
People come and say, Elias, I have a
system It's enforcement learning. It's collecting
some data. Which is experimental.
In principle. Then I have a piece of
the yellow distributions there.
Then what you do is exactly the same cartoon.
You you still have the chance about
everyone else that you couldn't intervene in the world,
that it is the rule, by the way.
Even though in artificial environments,
you can make the Mario Bros to jump
through the cliff, half a dozen times,
get a very low reward and say, oh, I
will not jump again. Or I will shoot my
friend Many times, seems a
bad idea, get low score, I'll not do again.
In the real world, we cannot do that. Right
In other words, in the real world, the key assumption here
that made most of our systems work
and did it very beautiful results
empirical,
are that you have abundant amount of, interventions,
which in the cartoon here means that you
can collect essentially whatever you want in terms
of the yellow in the left side.
Now, in the real world, you get very
little fragments of the yellow. We
should leverage that 100%. But
challenge remains the same. How do I make
statements about the other parts of the yellow
even about the green type of distributions
So Then
that's the the the the picture. Now I would
like to say what
is causal AI or what is our goal in causal AI Our our goal is
to develop more general types of AI endowed
endowed with the following capabilities.
They have the capability of doing causal and counterfactual
generation.
They have some kind of causal understanding
of the world, and they are able to articulate explanations.
I they have
more efficient and precise, more surgical
type decision making.
They are also more generalizable and robust.
And they're able to act as a science.
Right Cop take like a
a a baby as a scientist do
experiments in the world and eventually learn some things about
the world and discover.
Then each part here, there's a each part
each of these kind of tasks or capability
to expect is a different part of the book. There's
a few 100 pages written about each one of
them because I think they are important.
Cannot make a claim here that they
are, sufficient for the most
type of intelligence that you like, but I'm sure
that they are necessary. And,
and I don't think that anyhow, I think you you
should make some progress here. Now
I will just get some examples based on the time that
I have.
About, some of these,
problems. I'll start with the the counterfactual
generation. Seems to be
a topic of interest. Let's start with cartoon.
A cartoon that we have some kind
of amnest and then you have the digits
that is deep and the color that is c,
and there is some kind of correlation between that that is
shown here. Like, the zeros
distributed around the red. It's very the red.
Redish. There is very very few,
zeros that are blue. The
the five is like cyan. Distribution
around there, and so on. Now it is shown
in the bottom.
And then those are kind of example of digits
that you have that are found found in
nature in the wild. The agent can perceive
that. If you want to do generation,
you can say, I would like to sample from this.
There is the c and the d that is something in the world,
and then it'll take a picture, and you got
this image I. 100 by
100, let's say. And,
and the task here then
becomes how can you sample from the distribution p
of I given d is equal to
zero We're doing some kind of
filtering we would like to get the natural in
words here, it was the natural zeros.
And things that looks like what I can see in the
real world, in the current world. And
we've got got things like that because zeros
usually I didn't say something important. Sorry.
The the correlation where is my mouse
I cannot see it. Is it there
Yeah. I don't know.
The the the
I don't know why they're masked, but there there there is
a bidirectional dash arrow between
d and c there. What this means here, the
semantics about there is some kind of latent variable
there. You that is both generating the digit
and the c. That there is some kind of
common cause that is generating the correlation
between B and C. The digit and
the color, which is the shown in the bar in the
bottom. Concrete instantiation.
Now if you ask your sample from distribution, this
is kind of what we get that in reality,
zeros tend to be red.
But now if you ask for the layer
two distribution, they do distribution as
what I want to sample from the distribution p of y,
given du,
d is equal to zero. Now
do it d equal to zero, it means I'm from out
outside as the agent intervening in the digit
d, and I'm kinda cutting. I
the u variable that is hidden there that that generate the
correlation is no longer active. Because
we are I'm the agent, and I went there
and set the digit to to zero.
And I don't whatever is the viewers
don't know. They it's hidden.
Then you get kind of zeros from all colors.
Right Because kinda averaging over the space of
c. For any sea that is there.
This is a layer two type of quantity. That doesn't
happen to see from the rare distribution of
zeros you cannot get that in principle.
They're completely different distribution.
What about the counterfact Example of a
counterfactual here. The
given that the digit was a a five,
given that d is equal to five,
we kinda wonder,
what would be the image had the digit
being a zero Had I
made the digit be a zero,
Naturally, the digit is a five, it means
that I was originally cyan.
Then because the u that
generated five is the u that generates
cyan. Right U that's there,
got a value, generates c and generate d.
D is equal to five, c is
equal to cyan, more likely. But
now you want and you go there and do the
intervention and set the digit that will pop up
as being, zero.
Then you get in nature, you cannot
see zeros or very rarely or cannot see
zeros that are cyan, but now this
is the distribution that you would like to be sampling for.
Completely different distribution.
I hope it makes sense. Cartoon
here about the layers. Of the passage
of the Khazadir Iraqi. So just
to get intuition, move to some things a little bit
more real, I guess. The what would a
person look like counterfactual had they
been dot dot dot, these
papers that appear appear in the ICML and
NERIPS this year. They
maybe you ask AI to generate a human face, and
then you get the first lady there
and you ask what would happen had the person been,
I don't know, ten years older
then in reality, if you use a known causal
method, the you get a guy on
the other side. Which is totally
off. Little bit older but is
totally off. Right Because change there there's not
even the same person. State
of the art was generating that. Didn't
try this week or in this is
the original I sent out paper.
You get the picture of the lady there that is
blonde, AI generates, and you ask, what
would have happened had the person been a different
gender And then you get a guy
there which seems okay,
but very hard to understand the the
the the consistency or the counterfactual
here between these two people. They seem to be disparate.
It doesn't feel that the guy is in another age
group. In other words, it doesn't feel the the counterfactual
payer. Here in a way. Let me skip the
other one about the grey hair that is a sensitive
issue here. They
when you have causal, what we'd expect is
about gasket the picture of someone and
then you get the guy there with the glasses.
How do we have been older This seems to be the guy
that is the counterfactual pair,
the older version of the guy on the left.
And it kinda makes sense.
You know, where he has more gray hair, less
hair, and so on. Or you
ask a picture,
Where is it We got the picture, and then we got the lady
first. And then we asked, what would happen
had you been a male And then you
got the I don't know what's the relationship.
This is AI generated it
seems kind of even almost like twin brothers.
But it seems plausible that this is the counterfactual
pair.
I'll skip I'll skip the great heritage.
The huge commotion here important,
another one this is with language that was in the literature
before. Have the pair there with two
people and farmers and you and you ask what
would happen if they had been flight
attendants There's a whole drama here about this one,
and we ended up changing the gender
of the guy to become a a
lady. Why Because strong
correlation between gender,
job type, in the real world, in the data,
then AI is just kind of replicating that.
Overwhelmed by the correlation in the latent space.
Or you can ask them, make them
look like doctors then you change the
gender of the other one. To become
a male. Again, there's a stronger correlation
in the data between gender and job type.
Totally off, of course. A lot of drama,
from causal lenses not not right, of course,
off. But not surprising. Correlations
couldn't make it. It's like
this is another one from this is from one or two
weeks ago. It's not published, but we ask,
generate the the carrot with the
rabbit then we get the non causal
and essentially we kind of replace the
carrot with the wolf.
But now, if you use a causal version of that,
got a thing that is counterfactual plausible here,
that the the rabbit is kind of
running away from the wolf. It makes more sense.
Than being a docile kind of
cat rabbit as in the first. This is an unexpected
one, that you should, and I'm not sure if you're familiar with
the student that brought me this example, You
asked Jared, what if is a wolf I
thought it's cute because didn't expect to generate
a baby wolf. Then it's okay that,
it will be still playing ball.
The the the rabbit with the the baby wolf.
Just when you are too big that maybe
it's not okay. The
anyhow, this is an example of counterfactual human
generation. We can do examples
like that with text as well. I I
would defer, but it's this exactly the same
principle. Let
me just Oh, my
mouse is here. Let me skip this one, but it is about
causal understanding. It's part two
of the book. Let me move, just given that we are
in the RL, let me move to the next one.
Give me a second. Where am I Yeah.
This is now yeah.
I would like to do example about efficient and precise
decision made. Even the URL dimension here.
And we spend don't know, the last ten
years thinking about causal RL. It's about
what's the relationship between decision making
and causal knowledge. And,
if you want to know more, part
three of my book or the website is crl.causalai.net.
Now, this is the cartoon that we have before,
which we have the environment and we have the agent
on-site. Now the observation here
here the here that we started from touring
conversation is about two cube observations to
move from the RL to the causal RL.
First one is that the environment and the agent are
are tied to a formal pair,
SCMM collection of causal
mechanisms, are the underlying kind of data generated
model. And in the left side, we have some type
of causal model g. A causal graph,
for example. Second, observations, we
will define different types of experiences
or interactions of the system to avoid ambiguity
we are using the PCH, seeing, doing,
and imagining. Then those are the two
things that can unravel as
ten years old. Maybe my life and some
of students.
And we just released, before new reps,
for new reps, there is some kind of GitHub
with the colossal gymnasium.
Is kind of our attempt to try to build the infrastructure
to allow people to benchmark the
causal capabilities of the agent.
Very exciting looking for collaborators here.
This is version 0.1. It's huge effort.
Terms of coding and infrastructure building.
Mainly, I think it's exciting.
And, happy to get your feedback and
any interest. But in any case,
there are different once we take this idea
of modeling experience in different
ways, there are many new challenges and opportunities
that appear.
Our first task that we studied
is causal offline to online learning. We call
cool that is some kind of high generalized policy
learning that we are trying to combine
offline with online learning. Had
been going on for a few years. Those are the references there.
And here is an example of coup. This is the
context that we we we have
done the the experiments. I will not read
the whole thing, but this is a cancer can't
context, and we have the we are trying to do chemotherapy
and other kind of treatment, The
this is a randomized controlled trial.
Then this is a two stage trial in
the I have the x one and x two that is
the the two shots. The patient get the the
drug, the treatment, He comes back
later, a month later, and this we decide what
he'll do. There's a verbal ass that is
the state of the patient, and
different than the MDP or POMDP, there
is this variable u there. Is
affecting everyone in the system. It's a confounder a
global confounder.
And now this is the kind of results that we can
get. If you're doing some kind of
usual or Susan Murphy's, if you know some
kind of dynamic treatment
allocation, treatment regime, the
plot here, x axis is the number of episodes
y axis is the cumulative Higrat.
And if you do the standard in the real literature
that we are just doing RL slash randomization,
you get the red curve. So
On the other hand, if you use this causal knowledge,
the structure that we have there, that is saying nothing more
than what happening in the world,
Right That, you got a treatment x one and this affects
the state. And these states affects the
outcome y at the end. That is the survival of the
patient. After one year of taking the treatment
and so on. They you get the the
blue curve. This is, of
course, much better. And,
the the great is already a huge improvement related
to the the the red just using basic
causality and some type of some
learning period not crazy. The interesting
one here is you can prove results like that.
Formally. Interesting one to me is the the
green one. This is the one if you have observational
data, now
you can pretty quickly, the problem becomes
super easy. Right Even
though observation layer one is different than layer two,
if you use observation naively, I'm not
showing here, this will slow down.
Or even, harm the convergence.
Hinder the convergence. Or if you use
the the observational data properly,
the problem becomes easier. We don't
even need to do experimentation. I don't even have the scale
here. The Zoom is not enough. And,
the speed up is huge. Then I think it's a good one.
They they are not a task. When
when and where to intervene
Usually, if I put my RL hat,
and I have all these degrees of freedom where I should
intervene, if I one, x two,
x 100, I'll try to intervene in all
of them. Zero zero zero zero zero
one, All the combinations to see
what will be leading to the highest the
the highest outcome.
The the the
but sometimes and this is the line of work that I'm
not talking much, but sometimes I can just go
to the variables x two and x 27.
And leave the other 98 degrees of
freedom there not touch, just let it
naturally vary, and these will lead
to the highest outcome. Then this
was a kind of mind blowing for at least for me
result that wasn't NewRePS 18.
We kind of carry over. There's many ways of
generalizing that with a question about when should we interfere
when should we inter when
intervene Yes or no Maybe no Never
Or when Then if it's when, where should
we touch the system Should Should do the
intervention to bring about the changes Not all the
variables as we previously
believed.
Another kind of task is related to layer three,
we also learn It's called counterfactual
decision making.
That we are I I would just say that it's very related
to the idea of free will And,
yeah. I think I I and,
and the different, different way of there's a different
optimization function than
the one that we usually use in
RL. I will defer, given the time, but it's very
interesting. Other's calls are
imitation learning.
Believe that, assumption there
that we are doing some type of behavioral cloning
IRL, and, the key and everything
is good. If you give more data.
Key assumption there that the two agents
are kinda identical in their causal models.
In their causal capabilities.
Is almost never the case. No true agents
are not the center. No true agents are created
equal. Now we started this line of
work saying, if one agent different than the other,
just change one bit. From
to another, can do situations that you have perfect
cloning and the performance of the agent.
The learning agent is already completely is a complete
disaster. Then it's a kind
of challenge of the key assumption
underlying the known cause, I would say, or the the the the
previous version of the imitation
learning. We did a causal layer there And,
towards going to
should go towards the real world. In reality,
to be real a way.
There's curriculum learning as well that is kinda
more, recent
results. There is
keyword shaping as well that we like to use with
offline data.
Me see how much time
How much time do I have
So
you are shaping This is just an example.
Cartoon here for class, but is in the
simulator too that a robot is in a maze.
Robot is in a maze and the the
walls are made of lava.
Which is highly little.
Important here, the agents' movements
are, affected by the wind.
That is indicated by these symbols, the the
circle and the arrows, but the
agent doesn't see it.
And, and the question is,
how can the agent escape the maze without getting
hurt
And we are trying to generate this bread of crumbs.
Right The the hero in a way that assuming
that there is this
systematic source of bias, is the
wind that makes the the agent the the sensor related
to wind, I guess,
is damaged.
And more broadly, how can you design a HE watch
already said function that enables the agent to minimize
online experimentation. In other words, you don't
want the the agent in the real world to be
experimenting. We like to leverage the data that you already
have collected. It's coming from layer one.
Where is the agent already moved
here Yeah.
Okay.
And and and there is the baseline that is
non causal and the causal one. The
is in the bottom. And, the difference
here is like of course, the agent that is,
on the top is, like, very easily following in
the lava because it's not taking into account the
wind in a way, and the causal method in some
way is able to protect against that. The
first kind of column shows that. The second one, the agent is
very temperature got to this tunnel, There is
these coins that are very valuable.
Of course, the causal agents avoids it.
There is no way, based on this huge
amount of partial observability, that
he can go through the tunnel. And there's some type
of conservative policy based
on causal constraints that the agent is able
to to to do well. Same here in
the on in the last column. There is a little
coin there that is very close to the lava,
The known causal agent is trying to get this
coin, causal agents already know that it's
impossible. Because it's too dangerous. A
way. Given that he doesn't know
huge amount of the environment.
Of course, the question is how to do that I'll defer
to to read a paper or to talk
with me offline.
This is one task, another one that's kind
of very exciting. That is about,
everything that I'm saying here is single agent.
Very recent, maybe two years ago or one year
ago, two years maybe, we start seeing
strategic settings in which we have,
multiple players. And there's a kind of version
of game theory is based on causality.
Here is just an example. There are two prisoners
that suspect for a crime, but there is no
evidence convince convict them. PD. Right
Prisoner's dilemma. Then
they may either cooperate or the fact, the the
c or d, and this is the variable
x I. Also, they don't know what choice
the other fellow is doing the other guy is
doing. But now in addition to the
setting, have a new new part here
in the causality sense
that the interrogation process by
the police police officers
may subconsciously affect
their decision and contain formation about
their faith. That is
that is shown in this variable, u one.
Then there is the police officer one that is
affecting the decision x one and is affecting the
outcome. The guy's kinda trying to bully.
Maybe the the the the prisoner to the
or to cooperate.
Now the this can be described
by other labs. Maybe you have a we we do the experiment.
That you kinda wrap other lambs around each
around each of the prisoners and the police officers and
so on and the judge.
It's kinda cute to see how the alarms are
discussing between themselves. I'll skip for the time that I just
have one minute, I believe. Or less.
But now, interesting enough,
the the usual game theory in the Nash,
the the the
the the x axis here is the probability
that the prisoner can naturally read
if the situation is favorable or not favorable.
What is the intuition of the agent He doesn't know
how to explain but he just got the gut feeling.
The and the y axis here is the reward or
expected utility in this case.
Now the red line here is the game theory of the Nash
equilibrium. The blue one is the
one that if the agent is acting like layer one,
like automata. Gut feeling.
Then after sometimes, if the reading of the room
is not very good, can see that if
the engine the the blue line are below the
red. The agent starts getting
screwed. Then it may make sense
to use the game theory in the settings. But then
you have the green one here that is layer three.
That is the counterfactual. It turns out to
dominate the agent that is doing ASH.
And dominate dominate the agent that is doing,
following their gut.
Are the grass just ruled the conclusions here for
the time. I think it's quite exciting and re
new stuff. Happy to talk about that.
Let me skip this one. Give me a second. I'll go to the
conclusions.
The our general goal, the
causal AI research program to
develop more general and trustworthy
AI systems and download the following capability.
They have the capability of doing causal and counterfactual
generation. Having
some kind of causal of
the system and the ability of articulating
explanations,
being more efficient and precise in terms of their decision
making, being more generalized
when robust, when you have changing
condition and moving across environments,
and they do some kind of model learning
a world learning and and discovery.
Again, this is in the book. Happy to get your feedback.
Also check the LLM of salvatori.org.
I didn't have time, but it's a different thread here.
Related to benchmarking LLMs.
There's a top of the dimension to most. We can kind of
combine them, but all the problems here I still in
the current system.
Thank you. Also, the NSF, DARPA,
and so that support in the lab.
And the and the club the the students
that are doing the work. I'm just
presenting here and the the
collaborators. Thank you.
So thanks professor Birrah, for the
wonderful talk. And for the sake of timing, we we
might just skip the question for this, but feel
free to reach out to the person. And our
next speaker today will be professor Sanjay
Fiddler from University of Toronto. Who's
also affiliated with the professor faculty at the Vector
Institute and also the VP of AI
Research and In Media. And today her
talk will be towards world
models for autonomous driving. I'll give
it to you, professor.
Sorry. Too many cocktails during the week.
Let me connect.
Okay. Can you hear me now
Okay. Great. Yeah.
Thanks for inviting me. I'm Sonia
and today I'm gonna talk about,
world models for autonomous driving
and I'm just a humble representative of the
team, the spatial intelligence lab at in Birrahood.
Who's doing all this great work.
My talk is gonna be super different than the previous
one, it's just gonna be lots of videos.
You guys have had a lot of math during the week,
so I'll just so spare you that.
I'm running to the airport after this, so
start waving at 10AM. If I'm still here,
super chat. Okay Code red.
I Okay.
So, can you guys see anything
Yeah. So a couple of years ago, right, everyone
has seen kind of conquer the
world. Right That was the era of of
generative AI, and really unlocked
so many different applications. And
since then, we we have multimodal
model, actual language models, and they're
all kind of working synchrony
together towards agentic AI to to
solve complex problems.
This was a super exciting area, but
us work on computer vision or
three d or robotics,
were kind of sidelined. We weren't
we weren't in a school anymore. But that
that's changing. So the the next
arrow that's coming is is physical
AI. Robots are coming. And, of
course, robots has been around for for
many decades. Right So
you might be wondering what what's different this
time. And it's actually
these big foundation models are going to have
a huge impact in actually making this
a reality. Right Because you're packing basically
the entire human intelligence into these
models, that we just need to get
working on on the robots.
Reasoning, the visual understanding, it's all
kind of there. We just need to kind of connect it to
the control and and make it work.
Sounds easy, but maybe not. Alright. So this
is kind of like the plot of how we're thinking while robots
Right So on the bottom, we have environments,
so kind of the increasing complexity of
the environments. And on the
y axis, we have skills that these robots are expected
to perform. And, of course,
like, the simplest environments we have, these robots
are kind of limited in first of all, they're limiting
the skills that can actually perform, and they're also
not, like, super useful. Right So if you
have manipulation robots, they're
kind of, like, stuck on a particular table,
and they they can do whatever you know,
interact with the environment that's around them.
And then we have autonomous driving. Right
Which is really gonna becoming a a
reality these days.
The environment is entirely more complex.
Right It's traffic. It's like so
many things can happen on the road that I think
is overtaking. Anything can kind of jump in front
of the car.
Super complex. So these robots actually need
a really strong understanding
of the world. Around them and also
what's going to happen next so they can convert it
into an action. And, obviously,
kind of at a at a north star, there's
a humanoid robot which you're represents
kind of the general robotics. Right Like, I've done the
most general robots the more generalized
skills. Meaning that they can they need
to do most complex
environments. You know, work in a factory, drive
you home, and then cook dinner.
Right So, the expectation on the skill
just grows.
So today, I'm gonna mostly
focus on autonomous driving, where kind of we
have been focused on. And
the reason why I chose this application, actually,
when we joined NVIDIA is because
it was an application with a lot of data.
So you know, robotics is is coming,
but a lot of the times, you're limited in the
data you can collect with a robot. That's
not the case with a car. Probably almost
everyone in the room has a car. Right And
we can actually collect data at scale. So
feels like the first application with all this
technology can actually come to fruition.
Okay. So these days, right,
a lot of the autonomy companies are
are working on this kind of, like, architecture
of the policy. Right So it's an end to end
model. That takes some sensor
reservation, maybe high high level action
or high level kind of navigation,
by the user user.
And then it produces a set of action tokens.
Right For example, a trajectory of where it should
be moving next. And that kind of controls
the car. Right And in
the last maybe year or so, we've
seen the transition towards including reasoning
as part of this this
action policy. Right For
example, a lot of the if you if you
wanna go to toward l four and l five,
there is a lot of cases where maybe the car
just gets stuck. It's very to kind of solve with traditional
systems. It's even hard to solve
with end to end models because end to end
models typically train with,
imitation learning. And maybe you just haven't
seen some of those tail scenarios.
This is really where kind of the
premises or people think that reasoning models
can actually just use
know, human intelligence to solve those problems.
I don't need to go to the cloud and query a
human to get me unstuck
from behind, you know, like a double parked car or what
whatnot. I can just actually
just ask an LLM what I should be doing.
Okay So this is basically
kind of where the community is trying to go.
Now, obviously, this is like one
model that in the end of the day is gonna run on
the road. It's gonna run on the car.
However, to make this work, actually,
there is a gigantic infrastructure
around making this model work.
Just on its own, just collecting data and kind
of, like, training this is obviously not gonna cut
it. And one
important part of that infrastructure
is obviously simulation.
Right So if you have end to end models that
go from sensor data to action,
and now I'm starting to tweak these different parameters,
I need, like, some very quick signal of
am I actually doing the right kind of choices
How well is my model performing
I just can't go to the real world to do that.
That would be so inefficient Right Especially
as we're starting to solve the long tail
problems, just cannot go out and
get all that, evaluated in the real world.
Right So simulation is actually a key
part of this,
pipeline. Right So we
want this policy to actually
be driving in a completely simulated world,
and this world should behave exactly
like the real world. And if we can
actually operationalize this,
this can really be, like, so much faster
development in the future. Know,
kind of my dream is that maybe a ten ten years
from my fifteen years from now,
a grad student would be able to train an autonomous
car. Right We just wanna
kind of provide the core infrastructure and models.
So So today, I'm going to talk a little bit about
our journey on building this simulation. I
pretty much started working on this as as
as I joined NVIDIA.
And in 2020, Nerf
came out and and
then Thomas Miller and Nvidia already started working
on making that fast Right If you
guys remember, Norforce actually rendering pretty slow,
so it wasn't, like, ever useful technology at the
time. But then we started seeing
signs of actually this can go much, much faster
and even real time. And
we always the next day, we switched, and we said, okay.
The killer app for that technology
is going to be simulation for autonomous driving.
Why Because it's actually a robot
that doesn't need so much interaction with the environment.
It's basically the physics that's on the ground.
Whenever you hit something, you stop the simulation,
obviously. Just bad, bad, bad car. Right
And then everything else, you know, maybe
maybe it's just visuals, basically. Right
So we started working on this pretty
much maybe 2021.
And at the time, we were kind of an academic lab,
if you will. And through this, we kind of
learned a journey to become an industry research
lab. And, a lot of this technology
I'm gonna show you now is actually running in
the production enabling the our
autonomous vehicles at NVIDIA.
Alright. So this is basic we're
taking we're gonna take recorded data
from the car and the goal is to turn it
into simulation environment.
Right Why Because what's
better better than simulating the real world than
actually collecting the real world Right
We just wanna make those kind of like
static collected videos into something
I can modify. I can
actually make the car change my action and get,
like, a reaction from the simulator.
Okay. So what we're building basically or
what we have built is, like, kind of like a dynamic
scene graph representation where you have the map, and all the
cuboids are going to be controlled
outside of this NERF wall.
And then, and then
we have the I guess, these days, Gaussian particle to
represent the scene in each of the agents.
Okay. So basically, on the top left, that's
kind of the captured data. And then
as you do reconstruction, one important capability
is the three sixty sixty reconstruction.
Why Because on the car, you actually have a lot of
sensors, not just one front camera.
You actually have maybe 10 or even more sensors.
LIDAR included. And
as you know, right, this this, Gaussian
spots or whatever allow you to change viewpoint
is super important way because, for example, NVIDIA
is working with multiple OEMs.
Each one is going to have slightly different sensor
configuration. Both in terms of position, but
also the know, the actual sensors.
So you wanna be able to kind of take
one capture and kind of resimulate it
in different different maybe car
models, but also in different camera set.
So here is like maybe we can even
make it like a van or something.
So And then another important aspect, obviously,
modifying all the other agents because
the in the end of the day, if I'm
making new action in my simulator,
then every other agent also need to be reacting
to me. So I need to be able to control them.
And, obviously, when I record from a video,
I only see, like, sides of the cars,
right side of other agents. So I need to kind
of hallucinate the other part such that later
when I'm modifying these other agents, I
can actually kind of recover and
not show, like, the empty space of, of these
agents.
And here is just an example of how now we
can drive multiple times just show it's
not a video. It's for the same scene.
I'm gonna drive two times taking
different actions.
So some challenges obviously
include
scale, Right A car is driving
what I I like sports car. I actually will
drive pretty fast. So don't know. I'm
not gonna say how much, but you can go quite fast on
the highway. So
even in thirty seconds, you can actually drive
maybe kilometers. Right
Typically, nerve settings in papers that you see,
it's like a phone. Right So maybe maybe
this room you can scan, but here we're talking
like really large environment.
You know, at the scale of, like, a whole racetrack
or a a few blocks
of a city. So
that's one of the challenges. We actually built
a really cool spatial framework for you guys in
case someone is interesting to use it.
That uses as far as data structure called VDB
that we borrowed from the VFX community.
It's all released open source.
And, allows you to actually scale these
environments much larger than with typical frameworks.
The other aspect is just like, oh my god. You know There's
so much stuff that can happen on the road.
Right And it's so visually challenging to
actually reconstruct this. And you
don't really understand how challenging it is
until you actually try it. I know,
you know, in papers, you show a couple of results
or so on. But if you actually wanna
deploy this in a production system, there
are just so many, so many things you need
to think about. Right For
example, like, the the traffic
light you actually need to get it perfectly
when it switches from red to green.
And if you're not paying attention,
maybe you could just do, like, a blend of the red and green.
Right Because maybe that's how training. Right
Text on signs, super important, right, for
the cars to make decisions. It cannot
be blurry. Right You actually interconstructed
at a very high resolution.
So these are, like, small things that actually
really matter in practice.
The other thing that matters is actually going,
kind of off rails. Right Like, typically,
when you drive, you go on a you you
move forward. But if the car
is now making different decisions, maybe it can actually
change a lane So you actually need to support
novel view synthesis maybe five meters or
even more potentially. Right
And that's typically as we know, these methods kind
of start performing worse.
So we build this maybe, like, a very tiny
introduction of generate AI for this
that actually fixes novel views
and it then we can actually go
up to five meters, very easily.
We also kind of complete the cars. So even
if you see them from just the side, we're able
to with something similar
to Sam three d, I guess.
And then the other thing
is harmonizer. Again, a very tiny
little generative AI network.
Which allows us to be super stupid
in how we're actually composing scenes.
No light transport. No that. We're basically just
reconstructing cars or other objects
from different scenes. Compose them in
the most naive way, It looks
super bad. And then the generator is
just gonna insert shadows and make it look like
it blends in. So it's like a
super nice hammer to actually
make this all this modification to the scene.
Here, I'm just gonna show you two examples
of Probably, the the guys in the in the back
won't be able to see it. But basically,
the just to kind of show you how this, like, fixing.
The generative AI that's factually fixing all
the views is is operating.
Originally, the car just kind of drove forward,
And in this example, we're gonna do
a u-turn. Which is obviously not what we're gonna
do in real production when we're gonna be testing.
But just to kind of see how
the novel view just goes really off rails
on the left, and then with some fixing
it starts looking pretty good.
So on the left side, we're gonna take a
u-turn You're gonna see it's already
like a a you already maybe see some
artifacts on the rendering.
On the right side, it's still pretty good.
Now we're gonna return.
But It's
not perfect, but you almost don't don't
have any observations here. Right You were
just driving forward. So it's pretty
cool that you can actually even do that maneuver.
We also made some modifications to the
actual formulation of Gaussian's plots. Right
Which is based on the rasterization,
and we kinda retrace it. Which allows
us to do all these secondary effects. You get
all the reflections right. And because
we also reconstruct all the geometry and
stuff, and it's all in three d, we can
edit it. So you can get
nice, like, edited scenes with
whatever, physics effects we wanna
add.
This is pretty cool too. We can also relate
to the scenes. Even though, you know, maybe I
recorded the the capture in
the early morning, I can actually simulate
all days all, times
of the day. Which is really useful
for, like, structured testing.
Okay. So how is this this actually used,
when your test Right So you have the policy
on the left. It's producing action
our simulator is basically kind of composed of
three things. There's a behavior
model that's kind of moving all the cuboids around
in the map. We run
physics because we reconstruct the ground
in geometry so you can have all the collisions. If I go
over a speed bump, I can actually simulate
that a effect. And then, which
is kind of this stack, is then rendering
the scene. Right Because you can recompute all where
all the agents are, where you you are based
on all the kind of the the ground dynamics.
And then you're rendering out, and that basically
gives you kind of the next state. Can compute
the rewards or basically some evaluation.
And then this process repeats.
So just a couple of examples here is, again, the
policy. So
driving multiple times in this scene.
I wanted to show this plot because this whole slide
which, of course, is pretty cool. So so
far, we had to construct the 75,000
scenes We're going after a million.
And around 2,000 are constructed
every day. Automatically,
and then there's some queuing on top.
And there is about 1,000,000 simulation
rollouts per day that all the developers are doing
in the AV. And
the bottom plots are basically showing kind of
our KPIs that we were he'll climbing when
we were developing our tech. So for
example, the plot on the right is showing
kind of where we started on the left,
which is you don't really see, but
reproducibility of what happened
in the real world. So, for example, imagine I draw
somewhere and I had a ghost break, Now
I do my reconstruction, and I resimulate the
car the say exact same soft
drives, exactly the same in the same
scenario. And we wanna
then see a ghost break. Right If it's
not reproducible, then obviously something is
wrong in our simulation. So when
we started, the reproducibility wasn't super
high. Not just because
of the visual effects, some was also our homework we
had to do and actually improve our our
reconstruction, like traffic lights was definitely
one of the problems and the tax.
But also all these other
effects, like just simulating the latency
of different cameras when the policy
interacting with the wall. The physics,
kind of the whole hardware in the loop needs to kind
of be like the real world. And that makes
it more and more realistic. But
the cool part is that every week, we was like,
clear progression. Of how how this was, like,
closer and closer to actually being
like, a really good measurement of reality.
I'm just gonna show you this video.
Physical AI.
Transforming the way we move and
simulation is driving the field's latest advances
To test autonomous vehicles, real
world data is transferred
based three d scenes.
Using Omniverse neural reconstruction
or NURAC. Converting video
into photo realistic
digital environments.
The reconstructed scenes are then simulated
with variations. To assess the
AI driver's ability to generate safe
trajector Physical
AI isn't just driving on roads,
it's operating in kitchens,
offices, and warehouses.
Each of these robots needs to be simulated in
virtual worlds reconstructed
from multiple sensors.
With NURAC, can rapidly render these
simulations in real time with high
fidelity.
New capabilities like physics
bring realistic interaction to three
d Gaussian scenes. Letting
robots engage naturally with virtual
environments. Generative
AI uses the visual properties
these simulations to add diversity,
scaling a single scene into many.
Neural reconstruction is already advancing
the world's
AI. And now, with just a text NewReck
and Cosmos can generate three
d simulation environments
that refine testing and validation
for the next generation of robotics.
Okay. Cool. When
really speed up to the airport, but I'm gonna give you a
few more minutes. I just wanted to show what
we're working on towards now. So
basically, we're kind of kind of finished in the research in terms
of you know, that that's simulator is already
in action. So we're thinking, like, what's next
Right Like, what can our research team actually help
and build next And we were
kind of looking at this this plot. Right
So in the bottom, you'll have a
scenario as a commonality of a scenario. And on the
kind of, like, the the y axis is
the volume at which you recorded when
you're driving around with the development fleet.
And, obviously, just the data collection
fleet is kind of on this left side. Right
You're you're typically collecting common
maybe higher volume of common scenarios.
Maybe you have a very targeted data collection
when you go out and you want a particular
scenario. Is it kind of hunting for that
Maybe you even get some, you know, customer free
data or acquire some data.
NURAC allows you to do some variation of
that, so it adds a little bit more
to whatever you recorded. But, obviously,
we wanna go here. Right Especially
for l four or l five, and
Drago is sitting here, so he definitely knows
this. You need to simulate
rare cases. Right It's just cases where
you're almost never gonna record.
And ideally high volume of that. So
question is obviously, the premises
go after generative wall models.
So for that purpose, we have been building,
cosmos. We announced it
at CES, this past year. That was
in January. And it's a platform
basically meant for physical AI developers.
So we've been building, several foundation
models that are hopefully enabling everyone
to build you know, robotic
systems. From the reasoning model, visual
reasoning models to basically
world simulation models.
And, obviously, there's just, like, so
much work, so much data, so much compute
going into making this a reality.
Pretraining, basically train on twenty million
hours of kind of real world
physical, videos. And
then there's post training
that, you know, you kind of use the data
for a particular robot, and then you post train
the model to make it really,
successful for that domain. And
for that, we're basically throwing the entire
or a subset
selected from the entire NVIDIA collected fleet
and we're just contributing that, in the models
we have been releasing. That's twenty
thousand hours, plus we also acquired
a couple of datasets.
Maybe I'll skip this part.
So Cosmos Drive was, I think, released
maybe more April. That was
our kind of first post trained Cosmos
models specifically for for driving.
And kind of what we did here, we took Cosmos
that was kind of pretrained
and we built a foundation model for driving,
which was then fine tuned on this twenty
thousand hours of data plus some of these acquired
data sets. And
then we, of further post train it
with with further
conditions, which are very used
which is basically cuboids maps
and even lighter. So these are all optional inputs.
And then the video or multiple
camera videos are coming out of these models.
And I should say here, at least until
these models maybe mature more,
this kind of conditioning signal is really
a boost to quality. So without
that, it's very hard. There's a lot of hallucinations
going on.
Here are just a couple of
examples. Of kind of
conditioning on a particular scenario and creating
variations
and simulating that with multiple cameras.
Since we've been working
on making it, much longer,
The first models were five seconds.
Now we can go minute a minute
or longer. I'm just
we'll just do the crazy prompt here just
so that you can see it's generated.
This video is one minute. I'm not gonna play one
minute. But it can actually also go longer.
The key trick actually in making it longer,
we made it work in almost a week. Is
basically just a longer foundation model
So cosmos initially did five second
generation at a time. Right
And we basically just took that and post train
it to be more than that.
I think ten ten seconds. And with
that that model, like, you can actually roll out
much, much longer So it's really
key to just kind of post train that model to
do, larger batches of
frames at one time.
And we also can do multiple cameras
now in the same duration.
So it can go much longer.
Not five seconds. Again, this is five
twenty seconds here, but it can actually do much longer.
I wanted to show a limitation slide. This was
from March and a lot of this has been
way improved. But I
think the the the challenges are still
kind of valid. These are the things that we actually
need to work on. To to kind
of bring this to production to real value.
So one of the first problem that everyone in navy
complained about and they really didn't believe that this
model is ready yet is, on
the top left, The, the model
symptom simulates both red and green
lights to be on at the same time.
Obviously, that's super bad. You know How how will
the policy even interpret that You know, such
a small just a few pixels wrong
like that can actually make this technology
super useless. So that's definitely something
we it's already much, much better in the latest
models, but it's something that, you know, you need to pay
attention. With.
Hallucinations, you're gonna see this car just
kind of goes into thin air.
Obviously, with that, we cannot have
that happen. With conditioning, that's
a little bit less of a problem, but it still happens
to some extent.
The top right is an interesting one. So I'll tell
you in advance what to pay attention to.
It's basically wrong logic.
So basically, here we weren't conditioning
on the on the traffic lights.
State. We just condition on kind of the
map and where the traffic lights are.
And then the video comes out. And then
the VLM is obviously generating a prompt.
Right And that goes into the model that generates
the actual video. So what you're gonna
see here is that there
the the traffic light is still showing
red, but the cars are already being
navigated to drive. Which means
that the logic there something went wrong in the
logic of of actually producing
this video. So the cars are driving,
but actually, the the light was red. Which
wouldn't happen in the real world. Again,
something that we need to solve for a better reasoning
inside, the video generation models.
Text remains kind of a problem, especially
for faraway text. Right And that's super important.
All the traffic, like, you know, traffic text actually
needs to be read.
And pedestrians, I think, have improved a
lot, but the especially with scenes with a lot
of pedestrians, it's, it's still kind
of like a a jungle. Sometimes you
see someone with multiple legs or people disappearing.
This is a funny one.
I talked in the beginning that we need
to simulate kind
of these OD scenarios, right, like, very
rare cases. But those are actually
hard to simulate. So here you just
prompting with you know, here in a highway scenario,
there was a tire that breaks So let
let's see what the model generates.
Super weird.
The other one is a traffic officer
signal. It's a turn around.
At a closed street. So the street is closed,
and the video model is gonna paint
that correctly. But then the logic just kinda
breaks. So
Goodbye. It just goes
through. This one is gonna be hard to
see, but there is a basically
a cyclist that doesn't move.
He sitting on a cycling pedaling,
but he's just stationary. Which
is kinda funny. And in
the bottom right is a
the the prompt was ego car crashes into
another car. And then, you know,
this kind of stuff comes out.
Yeah. Not not quite there yet.
But, actually, we are making progress.
And so why does this happen Is
this kind of failure cases The
issue is that in our in our the
collected data that we have from the fleet,
actually, you'd never see these edge cases.
Right You we actually never recorded
a tire breaking off or or
or crashing into someone. Maybe for
Joao, that's gonna be different. But for Nvidia, we
don't actually have those examples.
So it's kind of up to whatever the model
still has from that world world
knowledge. So it needs to kind of extract whatever it
has learned from the Internet with very
nicely behaved AV videos we collected.
And it's just not kind of there yet.
So the way that we basically solved
it not solve it, but at least
improved it it's actually
super simple. We just acquired some crashing data.
And now the model's actually successfully
crash. And simulate
this much much harder cases.
You can just go to a Dashcam provider that's
collecting this for a living.
And throw a little bit of that data inside
And then the remaining challenge of how we're training
this model is is really dashcam
data looks a bit different than what we collected from
the fleet. You need to kind of bridge that
visual gap, but you can actually make it work pretty
well. Is another one where you can just,
like, drive. Crazy.
And the physics also looks pretty correct.
Let's see if I have something more to show you.
Yeah. So I think, like, the first use case we are
thinking right now just because some of
these limitations remain is,
just generating these videos and actually
turn it into just reconstruct
them because from there on, we know how to simulate
closed loop and everything. So we
can actually just that video that we generated
before we can now
just lift it to three d and
plug it into, our simulator. So that
basically unlocks our team
that has never collected a particular
scenario to just kind of hallucinate it and then
put it in d and have perfect control of it.
And the other one is editing. Editing
is super, super useful feature because that
means that I have maybe collected
an example, which is perfectly, like,
normal, I can type some prompts
and can start editing and making that scenario look
like much more challenging.
So that's Corona edit. We also released it a couple
of weeks ago. Just wanted
to show you just and then I really need to
go. Our latest research
is just probably we're gonna release this sometimes
next year, but it's very
early work. We're we're trying to make this,
so all the stuff that I was showing before,
was actually open loops. So you write some text, and
it generates, like, a minute long video.
And the car is kind of
driving in whatever the prompt was there, right,
which is not really what we want. We want the actual
policy to drive in this role model.
Right You guys have all seen Genie. I don't need
to preach here. We all kinda wanna
make it here work for for the car. So
make this model interactive.
Which basically means I don't
not as human, I'm gonna be operating
this with a keyboard control, I want
the policy to be produced in the actions,
and then this model to be
reactive. Then maybe you know, generate
one second of video, then policy takes over,
predict and then I kind of go like
that. So we're working towards the system.
So we're still gonna make it condition the
same as we have it in in Yurag with the boxes and
and map. And and text
prompt. You're gonna generate now a very short
snippet of video, maybe just like half
a second. This is thrown
to the policy model that's producing an action.
And then you you obviously iterate.
And we also wanna add a
memory cache so the model
actually has, like, long history.
Example, especially, you have dynamic scenes where
some cars come, slow down, come
again, like, you don't want them to completely lose
identity. And especially for uterus,
it's super important to have, like, some sort of a
three d spatial cache. And
the other super important thing
is a reasoning in the loop.
Right So before I talked about, hey. The policy
needs to reason about the how
the world is kinda behaving and what I should be
doing next. Here is a little different.
Here is, like, okay. The policy has
decided to make this action. What
is everyone else going to do
And that goes back and into the prompting and
generating extremes.
Alright. So I'm just gonna show you, like, a
couple of very early results. In the
bottom is, bottom left is our conditioning signal.
So that's basically the input the model, press
some text prompt. On the bottom right is kind of the policy
that's gonna be making prediction.
And at the top left, we're gonna run
So the the model's condition on the first frame.
We're giving the first frame, and the and
the kind of the HD map and the and the boxes.
The top left is going to be the world model. So that's this
generating model is gonna hallucinating
from there on, from the first frame on.
And on the right side, we actually took that full
scenario where we construct it
with those Gaussian plots. So and then we're
gonna start comparing how the two star deviated.
Does it make sense And the policy is
basically reacting to whatever the world model
is painting, and then we're rendering in both worlds.
And as you go forward, they're gonna start deviating
more and more. But the difference here is
that the policy is actually driving.
It's actually driving, and the video model is
reacting, painting, and extremes.
So here we are already kind of deviating
but it's still pretty good in how it's
what is creating.
The text is not great. As you
can see on the road.
Here it's easier to see. It's bigger. I'm gonna
just be swiping.
And I would like to close, actually, because
I I need to finish. With
a failure case. This is just gonna show you
how important is to actually have VLMs
in the loop So
here, maybe I'm just gonna start
because the mistakes are just all over
here. Okay So here in the top,
there's gonna be a car that's gonna come
here just very unrealistically.
Just disappear.
And then the text prompt is mentioning a
grandma on a bike. And because we're
not changing the prompt as we go,
this grandma just gonna appear
in every every few second
chunk, which is kinda funny.
And the other failure cases, here,
we we haven't used the crash data yet.
We're gonna try to crash the car
And on the right side in Europe, you just go
through this car and the video model just
doesn't want to crash. So it just starts
painting something different.
Okay. I'm gonna stop here. Thank you
very much.
Yeah. Thanks, Those are wonderful demos
and amazing talk. And, so for
time being, we might just keep it a q and
a. And next, our speaker today
will be Nicholas Hanson from UC San
Diego. And,
so Nicholas received, like, the NIMVIA
graduate research fellowship and done many amazing
works in in robotics. And
their topic today will be massively multitask
word models for continuous control. Welcome.
Alright. Good morning everyone.
I wanna talk about some of the work that I've been
doing during my PhD on on world
models for continuous control. I wanna
talk about some of the older work that we've
been doing, and then also some
new work that we just released last week.
But first, I want to zoom out a little bit and just
talk about AI more broadly. We've
seen a lot of progress in digital creative
tasks we have models now that
are very capable. You can,
snap a photo with your phone. You can ask
it to give some tips for
decorating your room, and it will give you pretty
sensible ideas. Also have
models that are able to generate high fidelity
on video that matches your text
prompts. And
the commonality between all of these, models
that we now have is that or the progress
that we now have is that it's usually just
one algorithm or one model that can be a
to lots of different problems. Without
the user necessarily having, expertise in
those specific domains. So this is
really what's driving the progress.
So There's roughly three things that go
into such a generalist model. We need
lots of diverse data, We need
scalable architectures that can consume all of
this data. And then we need some,
general objective that is general enough that we
can repurpose the model for
new problems without necessarily updating
the model parameters.
So So a question that I get a lot is like,
what might this actually look like for Embodied AI
I would argue that predicting the future is perhaps
the most general objective that we can think of.
So And building world models
specifically allows us to do reasoning about
the physical world.
We seeing a lot of progress in
this already in industry. There's one x technologies
have built a a world model for their
specific robot. It's able to generate pretty
short horizon videos of their robot
interacting with the world. So
WAVE has designed, similar world models
for autonomous driving that is able to
simulate here, short term videos.
And same as what we just saw with with NVIDIA.
And Google D Mind released the Genie
model, multiple
iterations of it where you see here the the world model
simulating digital,
video game like environments.
The commonality here is that all of these
works are very, very impressive, but none of them are actually
open source. So that's what I want to focus
on.
Overall goal of my research and also the talk
today is advancing the
research in open world models and RNN false
learning. I wanna talk about
some of the work that we've been doing in developing data
sets and benchmarks for researchers to use.
Some of the algorithms that we have been proposing
that have some kind of scaling,
capabilities now. And I also
wanna talk about some of the work that we've been doing and just
demarcate the tools to
make it, more accessible and easier to use
for for researchers more
broadly. And so this includes, for example,
sharing, research papers, but
also, open sourcing all of the checkpoints
and everything that we have used to produce
our results.
So
my working definition of a world model for for
control specifically is that it's
a model of the transition function. So it's
a model, that takes a state and action
as input and it produces the the future
state. We want to do,
reinforcement learning, we also have usually a reward
signal, might look
in many different ways, but we always have,
like, one objective that we're trying to maximize.
The way that we do that with a world model is that we can do planning.
So we can sample lots of different actions
We can simulate what they would look like with
our world model, then we can maximize
the cumulative reward, so the reward over
over a longer horizon using,
this planning algorithm.
So The first work I wanna
share here is, TDMZ two. This is
a work we released a while ago.
It's the second iteration of our algorithm.
And really the focus here in this work is to build
a model based RL algorithm
that is scalable. So more
data and more parameters
gives us better performance, like we see in in
language models for example.
And we also want it to be robust, meaning that
we have a single algorithm that can be applied to
lots of different problems without necessarily
the user having to do any high perimeter
tuning.
I want to show first here how the planning with TDMC
two works. We
receive observations from the environment.
There must be this dock here, for example.
We encode that into a latent representation.
And then condition on actions that we are sampling,
we can simulate in the latent space what would
happen, in the future by taking those
actions. Then we also have
some kind of reward and value, model
here that is conditioned on the latent, state.
Predicts the rewards and values over the planning
horizon. And so here, we use,
short term rewards when we roll out the model
at each step, and then we use a value
to bootstrap our returns
for the more long horizon, estimates.
So that's how the planning works.
During training, we do something very similar. We
have data sequences of of state
and actions that we have collected from the
environment. We trained a model
by first taking the first state
in a sequence, and code that into
a latent representation.
Predict quantities like, the optimal
action, reward that we will receive,
at that state, and also a value function.
So And we
conditioned the model on the actions that we actually
took when we collected the data, and we
used that to recurrently roll out the model and predict
these quantities.
So The TDMC
two world model does not have a decoder. So it's not a
reconstructive world model. Way
that we ground this model in the dynamics
of the real world is
that we minimize the difference in the latent
representations. So it's a self predictive
world model. That works, for example, is
that at the time step three here
in the right, we minimize the difference
in latent we would get from encoding
the first observation and rolling that out
into the latent space versus just taking
the ground truth of observation at that time
step and encoding that with the same encoder.
Ideally, we would want these latencies to be
the same.
So So we can use this world model
for, online RL.
This is an example of a single task
just doc from DM control.
We want it to run forward as fast as possible.
And initially, we don't have any
data, and our model is just random parameters.
But as the world model interacts with environment
and collects more and more data, it keeps improving
its policy, with the
planning algorithm. And eventually, after 10,000,000
environment steps, we have a pretty,
proficient policy. So And notably,
this is just about twelve hours on one GPU
without any parallelization. If you do
parallelize the environment, you can, get even
more speedups.
So So we repeated this process on
hundreds of different tasks. Here's examples
of some of the tasks that we have been working with.
Many of them are DM Control, MetaWorld,
MySQL, Maniscal, and and
some other ones that we have, applied the algorithm
to. And we want to train
world models on all of these tasks, separately.
A way that doesn't require high parameter
tuning.
We have some numbers here. This is
all the tasks that I just showed. Break
in, we break them down by domain.
And we can measure here the average performance
across these different domains. So
the y axis here is is score, and that's where we
want to maximize. Have
a few different baselines like, self-service
critic, Dreamer v three. The first
iteration of TDMC.
This first version here, the blue bar, it does
do high parameter tuning.
TDM two world model here we released does not require
any parameter tuning, so it's the same high parameters
across all the different environments.
And it seems to be, comparable or
better across the the task that we tried.
So something that I'm increasingly interested in
is trying to take the TDMC2
model based RL framework and not just do
multiple tasks, with training individual models
there, but also trying to train multitask
policies. The way that
we do that is we collect a big offline RL dataset
for 80 different tasks, Disbands both DM control and
meta world. So many different
embodiments. Then we,
just train increasingly bigger models on this fixed
dataset. So And we see
something interesting now here that as we increase
the number of parameters in the model, we get better and
better performance. This was at the time,
fairly
unheard of in model based RL.
And I also wanna emphasize here that even
these models even that they're bigger, they're
actually not too expensive to train.
Mostly because we don't have the reconstruction objective.
You can train this 48,000,000 parameter model
in just three days on one GPU.
Now this experiment is state based.
We we also can extend it to vision,
and it adds a little bit more compute, and the numbers
are a little bit lower, but it it kind of works.
So if you're interested in anything like
this, we have open
stalls pretty much everything about this project.
We have released every single checkpoint for every
single experiment that we ran. We
have all of the data available, so you can train your own
offline RL, multitask,
agents. Also have code and, and
training and emulation of your own TDMC
two checkpoints. So you can all find
it at at tdmc2.com.
So I wanna zoom out
a little bit and just think
about more broadly what are the trends that we're
seeing in language models and how does
that look different from the work that we've been doing in model
based around
If we look at the most
common LML recipe that we have nowadays,
very significant part of this is self
supervised pretraining on lots and lots of data.
We have some amount of supervised fine
tuning, which is more instruction based,
it's it's curated dataset.
And then our we also still do online RL,
but it's a much it's a much smaller
part of the overall pipeline. What I've been
doing in the previous, experiments.
And one thing that is very
important when you do the online RL is that
with all of the pre training that we are doing in language,
we get really, really good strong
priors for doing RLs. So the exploration
problem becomes much more feasible.
And also another difference is
that throughout all of these three different,
training segments, we have always thousands,
if not million, of different tasks that
we're training our models on. And this allows
them both to get really, really good priors, but also
to be able to generalize the new tasks.
This is not really something that we're doing in model based
RL at the moment.
The RL environment lends
in Embodied AI looks a
little bit more like this. We have a lot of different
environments like DM control and meta world
and Atari. They're served for a lot of purposes.
Most of the purpose of these algorithms
or the benchmarks is that we are training single
task policies. We train
maybe, our RL policy
algorithm here on lots of
different tasks from the same benchmark, and we have
some aggregate numbers of how well our algorithm
is doing in general.
But this is very different from what we're doing in in
language models.
So I had a discussion with my advisers
and just had a little bit of a crazy
idea. What if we just combine all of the RL benchmarks
that we have access to right now and just try
to train policies on all of the different tasks at
once
So in order to study this, we developed
what we call Bench. It's a
benchmark for massively multitask RL.
It's essentially a combination of all the existing
task domains that we could find. So there's 10
different task domains and about 200 training
tasks in total. We collect
single task, checkpoints for all of these
different tasks. So you wanna work
on this benchmark, you have expert policies, and
we also have, a demonstration
data set that you can use, to to bootstrap
your your online RL learning.
Here's some examples of what the different tasks
in the benchmark look like.
So many of them are, probably
familiar to you. We also have some new environments like
the the Mini Arcade and DM control
extended are just a little
bit of a new set of tasks that that
we've been working on.
Here's some, examples of some of
the new tasks. We have, new
embodiments for a DM control environment, and we also
have a lot brand new tasks, that you can use.
So let's move on to
training world models on this benchmark.
Propose what we call a newt, it's a massively
multitask world model that is trained with online URL
on all of these different tasks. At the same time.
Way that we do that in practice is that we instantiate
one parallel environment per task So we
have 200 environments.
We have a unifying API that
that maps all of these different tasks to a common
observation and action space.
We provide, state observations that are
zero padded to match, the
largest observation space in the
dataset. Then we also optionally
have RGB observations. So if you wanna
work on visual RL, you can can do that with it. This
benchmark. You can also disable it if you wanna
be more, compute
efficient. So
We condition our new world model here on
these two inputs. As well as a language
description that describes the embodiment that we're
working with, as well as the task.
So for example, here in this instruction, it's a
quadruped and we want it to
run as fast as possible. We encode
these tasks, in descriptions
with a pre trained clip and to produce
embeddings. So
And then the world model here, as I showed previously,
we do planning so we're predicting actions
here over multiple time stops, and we also
model the rewards of all of these tasks
as as supervision. So
And the backbone here is again the TDMC
two world model that I that I showed
previously.
So how well does this actually work
We have some experiments here where we train
a single newt world model
on 200 different tasks at the same
time. The x axis here is the
environment steps, and then the y axis here
is the performance.
But first, we train some B policies
on the demonstrations that we collected. And
we see something interesting. We see that the multitask
BC policy here, which is the gray dash line,
actually underperforming compared to 200
single task BC policies.
We suspect that this is because a lot of the task
domains are very disjoint, So if you just measure
in domain performance, you will probably see that
a multitask policy is a little bit worse.
We also tried PPO in this environment.
We do see that it is struggling with the exploration
and it's not actually exceeding the performance
of the initial BC policies.
Tried a newer algorithm, Fast TD3. This
was released about six months ago.
It's actually able to exceed the performance of
both the multitask and the single task BC
policy. We also
tried the new, world model here without access
to demonstrations. You'll see that it learns
faster and it also seems to converge a little bit
higher than model free baselines.
What's most interesting is that when we add
demonstrations and do pre training of our world model
before doing online RL,
we see that we actually start higher So doing world
model based pretraining outperforms
the BC baseline. And we also see
that it both learns faster and converges to a higher
performance. This would indicate that
probably there's something useful in that pretraining
that allows us to to do more efficient exploration
when we do online RL.
Again, this is pretty cheap to train.
It's about four or five days
on just two consumer grade GPUs.
So it should be pretty accessible.
We look at the effectiveness of doing online
RL also on a per domain basis.
This is still the same model trained on 200 different
tasks. Now we just break it down by domain.
And we'll see that for some of domains, newt
here is doing a lot better than the baselines.
For things like meta world, it's a little bit more
mixed. And for many of the other environments,
it also kind of mixed. We
see, for example, for the box two d environment
in the bottom left corner, we see all
the algorithms that kind of
performing on par with the BC baseline.
So there's definite definitely still room for
improvement here. This is also true for Atari,
for example. Where again a
lot of the tasks are very disjoint.
So
We do some ablations in the paper
just studying different abilities and
and scaling. Of
the new agents. And we generally
see that there's a threshold for a fixed number
of tasks in which performance
or the size of the model is good enough.
For example, here when we scale the model size, we
see that for 200 tasks, 20,000,000
is actually sufficient, and
you can still scale more, but it doesn't really add
much to the performance. We
do see that is not shown here, that if
you decrease the number of tasks, the
threshold here looks different.
The same for batch size. We see that bigger batch
sizes generally help when we have more tasks.
But again, there's there's a point here
in which the batch size is big enough.
So We see that also including
demonstrations is important for performance.
And we tried a few different ways of doing that,
but it seems that it doesn't matter that much.
Just the fact that you do have demonstrations and you do pre training
is the most important.
We also see that including
the language instructions is actually really helpful
even for in domain performance.
See here that when you provide the clip embeddings, it's
able to more distinguish the different
tasks than if you just provide the observations alone.
So What I'm most excited
about is that we can use these world models to do open
loop control. And we actually
trained the world model on just three time steps into
the future. But at test time, with this
world model, we can actually do six teams
times longer, planning than than than that.
Here's an example of the DM control working
agent. You'll see here that it's able to walk
fairly well. And then the dynamics
kind of diverge over,
as we do longer and longer planning. And you'll see
here that it folds over eventually.
But it's doing something useful.
Able to do simple manipulation tasks.
Also open loop. Are
able to do two d navigation here, also open
loop. And we also have this little
lunar lander environment where, the agent
is supposed to hover at this target
and it's able to reach the target but it overshoots
a little bit because the dynamics are
just slightly off. This is
all generated by one model.
So We still just have 200
tasks, so we can't expect too much generalization
to new tasks and embodiments.
We do have some early signs. We're able to do serial
shot manipulation, where we change
either the task or the object to
something unseen.
And we're also able to, again, do the walking task
here on an incline. It does kind
of fold over eventually. So
there's definitely some
limitations here. If we do online RL
for just 50 trials, we now have policy
that that runs on this incline.
We repeat this fine tuning experiment
over about 30 different tasks.
And this is what the results look like.
See that if you train the world model from scratch
on just that task, you
do, worse on average than if you take the
pretrained multitask world
model and you fine tune that on that new task.
So We expect that if you increase
the number of parameters and you increase
the dataset, then eventually we will
probably get better and better future fine tuning performance
as well.
So if any of this is interesting to you, it's
also available. We have released
all the checkpoints, the data, everything should be there.
On this link here.
I also wanna just end on an
positive note, on getting involved.
So I
believe there's not been a better time to
get involved in world model research than now.
There's so much open SOLUS work going on.
And you can actually, with pretty limited resources,
get involved. And contribute
to the to the research.
If you don't know where to start, we do have
in our paper we have about two pages
of just open problems that would be super interesting
to work on. I don't have time to work
on all of them myself, so
please give it a read. There's lots of ideas
for how to, improve the visual RL,
how to improve the language understanding.
To change the architecture, stuff like that.
So definitely check it out if you're interested
in in working on this.
That, I wanna thank my advisers also for all of
their support in in these papers.
Yeah. Happy to take questions.
Nicholas. Maybe we can't afford one question.
Any question from the audience
Yeah.
So I was wondering if you thought about the,
partial setting. Seems like you're you're kinda
set up for a markup setting right now.
We don't have a lot of capability for partial observability.
We've done some experiments with frame stacking, and it
kinda works in the short term. So you can
estimate things like velocity and acceleration.
But more like long term, memory for a task
like navigation, which is not really something we have
touched.
Sure. Thank you.
Yeah. Our next speaker today will
be, Chelsea Fing from Stanford University.
She's also cofounder of physical intelligence.
And her topic today will be developing
long term autonomy. Thank
you.
Awesome. So
hi everyone. I'm going to be talking a
bit about autonomy and,
kind of set the stage for this, I think that
imitation learning has played like a
really major role in a lot of
advances in robotics and
embodied intelligence recently. I think
we've seen really, like, admittedly,
like really really cool stuff that robots can do
through just like vanilla imitation learning
including a humanoid tying shoelaces,
a surgical robot tying a knot, sauteing
a piece of shrimp, tearing off a piece of tape,
and so forth.
And I'd like to talk about today is
something that imitation learning struggles with,
which is developing long
term autonomy. In, embodied
agents and specifically in real world
robots. And
as some examples, I wanna talk about tasks that
are pretty complicated and
much longer, horizon
than the typical tasks that we see
robots, do with kind of an adaptation learning
including dusting shelves and replacing
them, packing items into a plastic
bag, performing a procedure on a gallbladder,
and tidying a novel bedroom. And these are
things where if you just kind of apply vanilla imitation
learning out of the box, you're going to get
pretty poor performance because of how long the
task is. So
should mention also that I think this is a really important
problem because that oftentimes
the
a lot of the AI systems we interact with today,
it's okay if they make mistakes if they're
not operating fully autonomously. For
robots to actually be useful in
many real world contexts, they need to be
operating autonomously long periods
of time. Now, why is
this hard There's
a couple reasons why this is hard and I'll focus on
two of them in this talk. The first
is that to operate over long periods
of time, robots may need
memory over long periods of time.
And that sort of long term memory introduces a number
of challenges, and the second is that
you're operating for longer periods of time, there's more opportunities
to make mistakes, and more opportunities to get
stuck that prevent you from even
moving on to the next part of the task.
Cool. So let's talk about memory a little bit.
So, we've seen some
really cool robot foundation
models and all of the robot foundation
models shown here lack,
basically, lack any notion of memory.
And maybe this is okay, like a lot of fundamental
motor skills don't require memory.
So maybe we can kind of develop
a lot of kind of physical intelligence
without that sort of memory. But
memory is also really useful for a lot of different
things, whether it be finding objects,
operating when there's occlusion, counting,
how many objects you've kind of
done or kind of how many times you've done something,
keeping track of steps, Also
tasks where there isn't a visual impact,
of of what you did, like if you're sanitizing
something, for example. And
so in this first part of the talk, I'd like to develop
I'd like to talk about whether we can develop memory
in a way that is compatible with robot foundation
models.
And there are a couple
challenges with this. The first key challenge
is that as you introduce
long term memory, you're
increasing spurious correlations between
past observations and future actions.
And the we already have issues with distribution
shift with imitation learning and basically as you
add history, you're basically
exacerbating all of those challenges that come up.
With imitation learning. And in
fact, that pre really cool previous work has shown
that, if you add history,
your perplexity gets better. Your
your on the validation set, you're getting better generalization.
When then you actually look at performance when you're rolling
it out, performance is a lot worse.
For example, you have, like, almost double the number of
collisions.
And the predominant hypothesis for why this is
the case is what I talked about. It's these kind of spurious
correlations between history
and next actions.
And the second challenge that
I also don't wanna understate is that as you
introduce memory, it actually
introduces substantial
compute challenges, both in terms
of the amount of memory,
during training, as well
as just the kind of how long
it takes to to train these models and how long it
takes to run them at inference
we Cool. So let's ground
ourselves in a couple example tasks. So,
say that we want the robot to,
kind of put six scoops of ingredients
into the green and and blue bowls.
And we'd like it to remove the items from
the shelf, dust the shelves, and place the items
back. Where they were.
These are both tasks that require significant
memory, actually extending to
the very beginning of the episode.
So So there's two core
ideas that we're going to introduce to try to
tackle the challenges I talked about.
The first idea is that
a lot of the relevant information the robot
needs is only present in
a small number of previous frames. And so,
you don't actually need to have complete memory
of the past. And the
second is that a lot of the decisions that a robot
is making really only
require memory at a higher level of
abstraction, not the lowest level of action prediction.
And so what we're gonna do with these two key
ideas first, we're going to retrieve only a few
key frames and use those key frames
in memory rather than using the full
video as memory. The
second is that we're gonna when we retrieve these key frames,
we're gonna give this memory to the
high level policy and not to
low level policy.
And then the low level policy will give either no
context or or a short context.
And potentially, this could address both
challenges, both the challenge of of reducing spurious correlations
because we're only gonna be giving memory to
a high level policy that's predicting kind of
at a high level what the robot should do next.
And second, it can help address some of the compute
challenges because we're only gonna be using
a few key key frames of history,
at test time. So
So, specifically, what this looks like is we're gonna
have a high level policy and a low level policy.
Where the interface between them is the language primitive
that the robot should complete next.
And And, this kind of
hierarchical architecture has been done in a number of
prior works. The
high level policy is going to have
essentially, have full memory, but it's going to be
kind of using its memory
selectively to kind of select out which keyframes
it wants to use
readily it's making a prediction.
And so these keyframes, it's going to be
based on the past pre preframes, it's gonna be selecting
which which of those keyframes it wants to remember into
the future. And we'll also have an
aggregation step to remove redundancy in
keyframe predictions.
And then that high level policy is going to use its
kind of memory of those keyframes
to actually predict what the robot should do next.
And then, the low level policy will,
get actually in this case no memory, just the
current frame and the joint positions
and output target joint
positions, for the next, like, half second.
Using the language primitive.
So This is trained using demonstrations that
are segmented and annotated with
language instructions. This is also following previous
works that use this kind of hierarchical architecture.
That you can say, okay, this part, I'm gonna
pick up the scoop. This next part, I'm gonna scoop
some M and M's. This next part, I'm gonna put those M
and M's in the green bowl, and so forth.
Now And then the last key question that I haven't
talked about is how we actually
decide which frames to put,
which key frames to store.
There's a number of different ways you could do this.
We actually opted for in some
ways, like, something that's not particularly
sophisticated. In principle, you could
kind of annotate this, you could
try to derive some method that
figures out which key frames are gonna contain the
minimal information for predicting the action.
We actually just found that using the segment boundaries
in the dataset seemed to work well
for all the tasks that we studied, and so we
used that for simplicity. Think that future
work could explore other ways of deciding which key
frames to use.
So that's the gist of the method. In the
experiments, we're using only 50 demonstrations
per task. A pretty small
number of demonstrations.
And the reason why we can do this is we're gonna be
using pretty powerful pretrained models for both
the high level policy and the low level policy.
So the high level policy is a pretrained,
QEN model, this has video data
in pre training and so it has kind of relevant
information we could run this asynchronously
during inference is another
benefit of using memory for the high level is
you don't have to run it quite as frequently.
And, as immediately as the
low level. And then for the low
level policy, we're going to be taking the PIO five
the open source PIO five droid model and
fine tuning it on the 50 demonstrations.
This model, out of the box, can already do something
pretty good, and that means that we don't need
as many demonstrations for fine tuning.
So We then compared the approach that I talked about
with these kind of keyframe prediction and and
and so forth to different high level policy designs.
The first is just using no memory,
to test that we actually need memory for these tasks.
The second is just kind of vanilla
long context memory. As much
memory as we can fit into
memory for reasonable inference time.
We can also just do vanilla short context
history that basically just kind of a blading away
using those key frames. And lastly,
we'll use a human high level policy as an upper
bound of performance.
Oh, and then lastly, we'll also consider GPT
five and Gemini out of the box
as high level policies as well to test
where are kind of these frontier models are at.
Now we'd like to actually first make sure that we
are doing tasks that require long term memory, and
so this is a policy that only has short
context memory. It needs to pick the items
off the shelf, dust the shelves, and replace
them back. And we can see the high level predictions
basically the predictions of the high level policy in the top left.
And what we see is the the policy successfully kind of
takes the items down, dust the shelves, and and
then it kind of doesn't have memory, and so it will keep on
dusting the shelves again.
And kind of repeat that process.
So it gets quite confused due to the lack of memory.
So in contrast, when we
incorporate memory using the approach
that I talked about, it's
able to not only remember that
it has dust the shelves at this point,
but also remember, which shelves the
objects were located on.
So Second, we can look
at the scooping task that I talked about and
here, we can see that the policy, is
able to
able to perform a long horizon task that requires, like,
knowing that it has put three scoops in, count that.
It also recover from mistakes
and know that it actually hasn't put a scoop
in even if it kind of made a mistake during the
scooping process. So it's quite robust
to those sorts of things.
Quantitatively, we see that first, memory
is really important for performance.
Second, vanilla history is helpful,
but skills poorly
to the the kinds of tasks that we're looking at.
And with our approach, we're actually able to get performance
that's very similar using
a human as a high level policy.
So Now, we're using
visual history here. In principle, there are some scenarios
where you might only need text history.
For the task that we looked at, long term visual
memory is actually helpful.
Compared to using text history of the commands
that you have seen before. We see that in
this comparison.
Then lastly, the comparison I showed before
didn't include GPT five and Gemini.
And the reason for this is that it was in practically
slow to run on the robot. The latency was around
ten to fifteen seconds. Which is kind of
far too slow to kind of be
running repeatedly as an out of the box
high level policy. So instead,
we did an offline evaluation to test the accuracy
of these methods, kind of separate from
the latency concerns. And
we find that, we're kind of at a point
where these models are not yet
accurate for this kind of high level policy
prediction task, and significantly
less accurate than, than
this MIMR approach. Now, in some
ways, this is an unfair comparison because these models
are being applied out of the box, and so they don't get
access to the fine tuning data, on
the flip side, these models are also far larger
than, the kind of open source based
model that we're using, and so it's basically just a test
of, where are these kinds of out
of the box, kind of API based models
stand.
Cool. So in summary, we see that
feeding history is the level policy avoids
issues with spurious correlations,
It also reduces with frame
retrieval, we can reduce compute, and allow
high level policies to focus on important bits
of information.
And, kind of from a practical standpoint, it allows
robots to remember object locations, count,
keep track of task steps, and so forth, which I think
is gonna be really important as we
hope to develop, intelligence
in physical robots.
So Cool. So that's kind of the first bit
about long horizon a autonomy, which
is needing memory over long periods of time.
The second aspect of,
long horizon autonomy is that when
you're operating over long periods of time, the robot
has more opportunities to make mistakes and get stuck.
And that kind of can really tank performance
you're looking at longer horizon tasks.
For this part of the talk, I'm actually gonna be be
talking about a paper that's fairly,
I guess by machine learning standards is ancient.
I think it's like a year or two old.
And, the reason why I wanna talk about it is actually
something that we've been using technique that we've
been using in in, like,
actually, like, all of our recent
works that are doing long horizon tasks.
So Cool. So,
the motivation for this is that,
if you train a policy with imitation learning, you see mistakes
like this, where the robot kind of repeatedly
is unable to pick up the Sharpie. When it
does, it kind of drops it and so forth.
We can also see, mistakes here
where it's able to pick up the Sharpie, but it
doesn't put it into the bag successfully.
These are the kinds of mistakes that really tank
performance over long time horizons.
And, we'd like to enable the robot to
improve and recover in those
situations. I And
so, the setup that we have
in mind is one where we're also gonna
have a hierarchy, kind of a high level policy and a
low level policy.
And instead of kind of providing interventions
at kind of a low level to help the
robot recover, we're gonna actually be providing
interventions at a high level. So we'd
like to be able to tell the robot if it's in this
state, and is trying to put the sponge into the
bag, tell it instead,
use the sponge to open the bag wider. That
might be a way to recover
from the state that it found itself in.
And that could then allow the robot
to complete the task successfully.
Now, this kind of intervention, kind
of requires a human in the loop. So,
it's not kind of a final solution,
we'd like to do is we'd like to take this language correction
data, pull it into our data set and
and use it to improve our high level
policy so that we get higher and higher
performance when the robot encounters these
kinds of states and gets stuck.
So So essentially we can leverage this we wanna be able to
leverage this high level supervision both on
the fly and for kind of iterative
improvement our policy for long term
tasks. So how do we
do this Need to
connect our robot behavior with language and so,
like in the previous work, we'll be using a high level
policy that predicts language
instructions, and then that language instruction along with
the observation be passed to the low level policy
for it to predict target
joint positions. And the
key insight in this work is that if you
have a good low level policy,
you can actually get improvement just
with a better high level policy.
And the high level policy can be updated just with
language supervision.
And so as long as the level of
a policy follows a wide range of different
instructions, including instructions
that it it won't get stuck with, then
you can improve this full system by only
updating the high level policy.
So you if you're familiar with something Dagger,
this is essentially doing Dagger on
the high level policy rather than in
the action space, and kind
of a form of language diagram because we can provide these sorts of corrections
in language space.
So So specifically, what this looks
like is we'll freeze the low level policy,
a person will intervene and say what the robot should
be doing in a particular state.
And then this could override the high level
policy prediction, which as we're running the robot.
Can also do this in an offline procedure as
well, which we've done in some of our our future works where
you kind of take a rollout and then say instead
of this high level policy prediction, the
high level policy should have said this.
And then we fine tune the high level policy on this
language correction data.
With kind of standard supervised learning.
So, Cool. And so
in our experiments, we
after fine tuning on these language corrections, the
robot can learn how to self correct.
So is an example where it made the
same mistake that we saw before, and now
it's gonna say move towards me, go
higher. So it's kind of self correcting itself with these
kinds of corrections then putting the Sharpie into
the bag successfully.
We can also kind of correct for things
like, grasping
so that the it
can kind of move to the right, instead
of making a mistake. And then lastly, this example
of putting the sponge into the bag
the robot at this point is trying to shove the
the sponge in, it's not successfully doing that.
And so, instead, it tries to release
it and instead tries to poke it into the bag and
is more
and really the kind of key point that
I'd like to make here is that these kinds of language
corrections allow for
better long horizon performance. They allow the robot
to recover from the kinds of mistakes and the
kinds of times it's getting stuck
so that it can complete tasks that,
are over a
minute length.
So the only thing that's kind of nice about
this is it also provides an interface
for humans to be working with robots.
And so a human especially in
safety critical domains where you don't wanna make a mistake,
see right here the robot's, like, about to spill a
ton of, a ton of peanuts onto the the
table. So, you can intervene and say,
okay, stop, move move
the left arm to the left, go higher, move the
scoop into the bag, and then prevent
that sort of mistake.
Okay. And then after fine tuning on that sort
of data, you see kind of the robot ability to
autonomously correct for that, where it's about to make the
same mistake here and instead
it learns how to correct for that.
Cool. So on these long horizon tasks, we
see a 20% gain in
quantitative performance just from using the verbal
corrections. And it closes a lot
of the gap to actually
using these human corrections or using kind of an oracle
human high level policy.
And in this particular work, the the low level policies
have a lot of for improvement. But if you
apply this sort of technique to settings where you have
better low level policies, you can actually get
really high performance for long horizon
tasks. So the kind
of the takeaway from a technical standpoint is
that with this sort of language feedback, robots
can, improve significantly
by fine tuning the high level instruction policy.
This can be a lot more data efficient than trying
to provide corrections at the low level.
But this relies on a a performant instruction
following policy.
So Now, I mentioned that we've
used this work quite a bit or this
technique quite a bit since. And
so let's see how we can then translate these
capabilities to more complex tasks.
And so in the first case, we'd like to apply this to
surgical robots. This is collaboration with Johns
Hopkins University, and in particular,
there's a procedure of removing
a gallbladder, and the most complex part of the procedure
is where you need to kind of apply these ducts
and ultimately kind of cut
cut the ducts so that the you could
then remove the gallbladder.
And here's a video of a
held out gallbladder bladder, as you might
imagine, that's fully
autonomous.
And, this is You can see the high level policy predictions
on the top left. Is a completely different
domain for making trail mix.
And, we see that the robot is
able to kind of fully, autonomously complete
this part of the procedure.
I'll speed it up a little bit.
Applying the different ducts. Here's kind of an irreversible
step of cutting the ducts.
Continuing, for the second one.
And repeating.
So Cool. So the same sort of procedure is
useful for surgical, it's kind of
relevant to surgical robots. And the other thing that's nice
here is that because we have this
interface between the high level policy and the low level
policy, this could be useful,
with, kind of
surgeons and and people with medical expertise
that may not have the kind of dexterity
to operate these kinds of tools, to intervene
and kind of prevent bad things from happening.
And then second, we also use this technique
in, the PIO five model. So
we wanted, in this case, to be looking at
mobile manipulation problems. So we're not
just controlling two arms, but we're also controlling
the wheels of a mobile base. To complete
long horizon tasks like tidying a kitchen
And and
like before, we'll use this technique to post train
the high level policy on verbal
instruction data, like
telling it to, telling it to kinda
put the the cup in the sink, place the pillow on the bed,
and so forth. So
And here's kind of videos of the final
policy. So, on the left, the task
is to clean the bedroom.
A pretty long horizon task. I can't remember
On the order of minutes, I think it's Yeah. On the order of
like four to five minutes. And the
policy is after using this kind
of high level instructional data, is able
to better recover from mistakes and
better complete this long
horizon task. So, it first puts
the laundry into the hamper, then it makes the
bed, or or tidies the bed, and then
it, kind of throws away a couple pieces
of trash. And
on the right, the robot is
completing different tasks in tidying a
kitchen like closing a cabinet,
here you can see kind of Lucy is providing language
commands, high level language commands to the robot.
That's kind of showing examples of the kind of,
like, verbal interface
you could have these robot policies.
So Cool. And the robot
eventually cleans things
up. So
Cool. I'd like to highlight, the folks who
who led the work I talked about. Specifically, Lucy
led, the the Yay robot project, Brian
led the surgical project, and Jenny and Ajay
led the memory project that I talked about at the beginning.
And the whole physical intelligence team played
a big role in, developing
the pie o five model.
I'd be happy to take questions and while I do
that, I'll also mention that same sort of technique
was also used in kind of the
PIO six work, that we recently released
specifically for this coffee making
task. And so, while we take questions, we
can watch the robot make espresso. Thanks.
Yeah. Any questions from the audience Yeah,
please. Hi, Charles.
There's a microphone. Yeah. There's a microphone. Over
there. Chelsea.
Quick questions. In MER,
you think the key frame memory is
more like brief state in
Palm DP Or is more like a high
level retrieval prompt for planning
Interesting. I think that so
for it to be a belief state, you would need to actually it wouldn't
just be each key frame, but it would be kind of the
aggregation of of the information. And
It's certainly less compact than what you would typically
think of as a a belief state, I think.
And it's hard to guarantee that we're getting Markovian
ness even we do incorporate this
memory. So I think
of it, I guess, maybe more closer to what I
was presenting in terms of just like information
that that might be useful. But perhaps some ideas
from that formalism could be for selecting keyframes.
Thank you.
Hi. Yeah. Thanks for the talk. I think I have
a related question only. So
I can look at it, you know, in memory could be like
infinite memory, I may need it, ideally. Right
So how do you look at it, to store
it in some of our concepts Like, if
I like to have an infinite kind of memory,
in robots, so how the memory will be organized
Like, will you look look at, like, concepts
being stored and use it in a high
the high level policy or what's your thought on
that Yeah.
I think well, it's definitely one thing that we haven't
explored that you need to do to scale this up is
also deciding if you want to, like, remove some items
from memory as well, because right now,
the memory will just grow and grow and grow.
I also think that
if it does grow, like, kind of significantly
larger, then ideas from
retrieval might be useful.
Where you don't just use all of
the key frames in your memory, but you actually
retrieve specific things that are useful for a downstream
task, like if you're in the kitchen remembering
where the the, the blender is, for example.
From your long term memory and you don't need, like, all
sorts of memory from the bedroom, or from
the dining room, when making that sort of prediction.
But, yeah, it's definitely an an open research
problem, to, yeah, figure out how to scale this completely.
And just one more related question. If the robot is doing
multiple tasks, it may need different kind of
memory for each of the task. Right Yeah.
So will you be able to extend that as well for
the multitask robots or
Yeah. I should actually mention that in the MIMR work in
those results, we did actually fine tune on all three tasks
at the same time. And so it was actually a policy
memory module and a policy that was
applied to all three of them at the same time, and it was able
to figure out for the high level prompt,
which which frames to to keep.
Thank you.
Hello, Cholcee. That was a great talk. I think
my question was, do you think that, like,
we're lacking, like, reasoning traces on
using memory, like long horizon reasoning
traces, like for example, if do something wrong,
we will remember, like, our mistakes and probably
just try something else instead of
trying the same thing or, like,
just remember where things are, just remember
to, like, try to remember like, where we put stuff
very like, just for that key frame, like, just
like, kinda like marking it. You think that
we're just lacking, like, really long horizon
supervision on, like, kind of recent
choices And that's why we don't have, like, long horizon
abilities or like maybe exploration
I do think that kind of thinking carefully about
the kind of supervision we provide to these systems
is important, especially for long horizon tasks.
And the in actually, in kind of
the PIO six work when we were doing reinforcement learning, you kinda
need to think about, how do you provide that supervision,
that reward signal And I think that Yeah. Especially for
long horizon tasks, don't wanna just provide
like a sparse reward at the very end, for example.
So yeah. I think that there's a
a lot of work to figure out in terms of like
and and a lot of things to explore also in
terms of different forms of supervision for long horizon
tasks. Don't know
if that's like think it kinda depends on what problem domain
you're talking about in terms of like whether that's the
bottleneck or not. But,
yeah, I think it yeah. Definitely lots of things to explore
there. Yes.
Time. Maybe just one last question.
Okay. Hi. Hi, Chelsea.
Thanks the great talk and also sharing the
vision of long horizon task. So,
I have a question. So, want you
think one step ahead. So
when let's say, when we have Dacher's hand
with a tactile sense of feedback,
in the future, Do you envision
that that change would
incur another major paradigm
shift in this long horizon
task that you shared or you
think that it's more or less
likely that will be same
as what you are sharing right now And also,
Professor Reid Sutton recently shared
that these are
these are these are new vision of
continual learning. So could
you share a bit more, like, also
thinking the context of physical
AI Yeah. Thank you. Yeah. So
architecture. Yeah. Yeah. I think that the on the tactile
front, I think that a lot of these same ideas
are very applicable to to tactile sensors.
And so I don't imagine there being particular
paradigm shift. I think the you can
also implicitly get tactile feedback in these kinds
of systems because the the camera on the
wrist can see how much the the the fingers
are and so they can see kind of the
deformation in the finger to get information about
how how it's gripping the porta filter. For
example, and then on the
second front in terms of continual learning, I think
yeah, some of the ideas that we explored in p I o
six where we're trying to iteratively improve
I think is really important, especially
as we might want robots to,
be reliable in the real world.
We've also have some work on trying
to enable that sort of iterative improvement
and continual improvement in language models,
not just in weight space, but in text space.
As well, that I'm quite excited about. So, yeah,
I think it's a really interesting solve.
I think is really critical, especially as
we, try to put these systems in the real world.
Thank you. Thanks.
Yeah. Our next speaker will be professor
Peter Stone. From UT Austin.
Peter is like the founding, director
of Texas Robotics and also the chief scientist
at Sony AI. And today, his
talk title will be beyond scene to real,
leveraging simulation support of embodied world
models for real world robot learning.
I'll give it to you here. Thank you.
Nothing
Plugged in. It says mirror.
Looks like that, it accepts
it.
And this is,
Always what's gonna
coming up.
Try to
Maybe we can try another
excellent chapter.
To mix
You know, everybody crowding around here does not help.
I mean, there's clear that the computer
is
That's a HER question.
Somebody else connects a computer and makes a Zoom, then I
can I mean, we
can
My computer's my computer knows that it's sending
something out It's just not getting to the display. Okay.
You have Linux or Yeah. It's a
Linux. Maybe there any resolutions
I think or things The resolutions
are right here, so we can do any
Resolution, do you want So
this is within the night
and then I can apply
my computer thinks it's sending.
Yeah. I can connect it. If you give me a Zoom link, I can connect.
A computer, so
Okay. Thanks,
thanks for your patience, and, thanks
to the organizers for putting together
a fantastic workshop. This is a great topic
for a workshop. Embodied World Models.
And as as I was trying to figure out what to
to talk about today, there's a lot of different
things, that that I I could have discussed and that
you've had some already some talks
up to this point, that talk about causality.
Elias's talk, which is really important for for
world models
and long horizon tasks and and, and language
conditioning and a ton
all of those are sort of things that that
that I that I could talk about to some degree.
But what I decided to focus on
as as suggested by this title
is work that is using a simulator
or a world model in some in some
ways but is not neither
pure sim to real nor pure imitation
learning. But is sort of using a world model
in a more more creative way.
And, this talk is gonna be in two parts.
At the in the first part, I'm gonna tell
you almost nothing
about a whole bunch of things. Sort of introducing
where I'm coming from and
and sort of the kinds of things we do in my lab,
This will be the a great part of the talk for people
with ADHD. It'll be like flashing right in
front of you, lots different things, but and you'll you'll have
to go to the papers to get more detail.
And then at the end, I for the second half, I'm
gonna dive a little more, in a little more
detail on some recent work that appeared in coral
just last month conference on robot
learning, that's exactly on the theme
of this, of this workshop.
Okay. So here we go. Starting with that
really, fast part, first of
all, I'm coming from UT Austin. A lot of exciting
things happening we happening. We actually have
two AI institutes there now and NSF funded.
One that was one of the first ones headed by Adam
Clivens, the Institute of Foundations of Machine Learning.
We have a great,
ethical AI initiative called Good Systems.
As Bo mentioned, I was the founding director of Texas
Robotics and now the chair of computer science.
And a lot of innovative AI education going
on including a public avail publicly
available AI literacy course and an online
masters in artificial intelligence for $10,000.
The things that all the things going to my research,
the the research question that connects
and is this question, to
what degree can autonomous intelligent agents
learn in the presence of teammates and their adversaries
in real time dynamic domains Now not
all the research answering this
question is embodied. Not all uses robots.
And not all of it uses world models.
But a good chunk of it does. And,
and so, we we publish
in a bunch of different sub areas of AI as shown here.
And some of the sort of use inspired
research that's that's inspired a lot of the research
in my lab is a lot
about world models
and in many cases, multi agent world
models. So robot soccer requires
a model of teammates and their adversaries,
I'm not gonna talk about that today, except
to show you a clip from, Robocop this
past this past summer in
the human versus robots game. So I have the
ball here, you're gonna see something that
oops. Why did that not work
Try that one more time. Something
that couldn't have happened even like four or five months
ago. I left the ball for the robot. I was on the team
of the robot. And,
we've saw, a goal that
we're seeing the robots having
some sort of model to be able to
find the goal, find the ball, and and kick it into
the goal. We're getting to a point where not quite yet at
the where there's competitive games
between people and robots.
But it's, you know, there's been a lot of progress over the
past twenty five years or so.
I also do work also a lot on on
general purpose service robots. So,
Robocup isn't just about soccer. There's also
Robocup at home, which involves
robots that have to do things like,
like, acting as a party host.
Or putting away groceries oh, this is the video
I meant to show. This is a much
shorter one. So
introducing people to each other, putting
away groceries, setting the
table. Here, this is it putting away groceries.
There's a for those of you interested in in
embodied embodied,
competitions, this Robocop at Home is fantastic.
I also did have a car in the DARPA urban challenge,
and have been inspired a lot by autonomous driving,
And in my, in my role at at
Sony AI, there's been,
some really interesting developments
there. In fact,
we've had it was a really exciting
week for us at Sony this
this week. Some many of
you know about our work from a couple years ago,
GT Sophie, that was featured on the
the cover of Nature where we
had the first AI agent that could beat the
best human
humans at a real time control task in a
task that people in a physically realistic
task that people really care, you know, care about
enough to practice a lot,
We, for those of you who aren't, familiar
with it, this is a sort of a demonstration
of how realistic the simulator
is. This is a an expert
driver, Ken Chan, driving in the real world
on the bottom and in the simulator with the same
track the same car. The
simulator, And finishes the
lap in about a tenth of a second different time in the real
world versus the simulator because it's modeling all of the physics
in in very high fidelity. And then
we did beat some of the best
drivers with an end to end reinforcement
learning agent, and
and learning things
just from, again, no
and and oh, I should say in this video, the colored car
is GT Sophie. The fork
human ones are driving the white cars.
It's learned through the skills of defensive
driving where it's know, trying to get into
a position where the other cars can't pass it.
But this was all model free, or at least with
no explicit model. This was end to
end deep reinforcement learning
starting from from random behavior.
Meanwhile, at, at Sony AI,
just just today, or I guess,
this week, two days ago, we had
another paper featured on the cover of Nature,
so it's the current issue. This has nothing to
do with embodied world models, but it's so recent that I
can't I can't help sharing it with all of
you. Because, it's
called Phoebe, a fair human centric image benchmark,
which is the first globally diverse,
consensually collected fairness evaluation
data set for human centric computer
vision tasks. So what it is is
basically a evaluation
set for vision models where all of
the subjects that appear given their consent, have been
compensated, have, themselves
contributed the labels for where,
you know, for all of the features about them, and there's
subjects from 81 different
countries. It's it's, diverse in
many different ways. This is not
at all the the subject of the talk, but
there's, you know, it's it's not enough to train
models, but it's it's, we we show in the
paper that it's very useful for evaluating
models. Okay. So
that's the sort of the whirlwind introduction
to the kinds of things that that are going on in
my lab, most many of which I say are are
embodied and and have world models
involved. Some other things I'm I'm one
other thing I'm not gonna talk about, which is very
subject in central to this topic,
is social navigation. So we have work
on robots trying to navigate
through crowds, and that requires
an embodied world model of people,
and so, you know, that that was something that that didn't quite
make the cut for for this talk, but I had I had
a talk at ICRA not too long ago, and
that's online that you could you could see if you're interested.
What I am gonna focus on in
in the balance of this talk is three
three works. Three papers.
Again, two of them I'm gonna tell you almost
nothing about very quickly, and then the third
one, I'm gonna dive into in,
in much more significant detail. So that's where I'll
give you sort of all of the all of the
technical details.
So first, from a couple years ago,
we have, have some work on self supervised
environment synthesis. And
the this is, first author here is Yifan
Xu, who's a Ph. D. Student in my lab.
And the the idea here is that
we want to do training
and simulation,
and of navigation policies.
And, but in general, the real world
is not the same as the simulator.
We can and and I think it's, you know, too much to
simulation. And so
there's gonna be completely different environments
and we want them to, you know, nonetheless, generalize.
Many people have done sort of pre training in simulation
and then fine tuning in the real world,
the idea of this this paper
and I think this was very clever of Zafan, was to do
some pre training and simulations, sure,
but then do a small deployment in the real
world and find the environments
where the navi where the behavior fails,
and then try to generate more simulated
environments that
that fall that are similar to those failure environments.
To then be able to, again, fine tune in
simulation, but in environments that are more reflect
And so, you know, sort of the the the overview
is you have your small real world deployment,
you find your, your
database of challenging environments,
then use a GAN to sort of synthesize those
create a whole bunch of new environments for
your for your simulation training. And the best way to get a sense of
this just to watch some watch this,
on a video.
Okay. This happened again. Let's see.
I think this was because of the setup at the beginning.
Let me see if this if I can quickly get to the
to the video. Good way to appreciate it.
Let me fast forward.
I don't wanna show you the whole video yet. So this is where
it's. So the
idea is that, you know, you you deploy your
robot in a bunch of environments,
and then you try to find the ones where it
failed and generate a whole bunch of
new ones using this this scan,
and then, you know, synthesize
the environments where the the agent is having
trouble. And then after training, using
our method, it's able to come go through those the
the sort of distribution of environments, that
you care about, much more
much more efficiently. So in some sense, we're
we're finding a way to give an embodied
world model to the robot, that's reflective
of of the task that it really, that you
really care about, and, and
then, you know, this is deployed. Everything I'm gonna show you
today is is meant is shown to work on real
robots. So this is, this was
for navigation through constrained spaces, where
the robot, you know, has to go through spaces that
are barely bigger than the robot. And so
that's where especially this method is is
shining.
Okay. So that's the that's the very brief
that can that you can find all the details in
the paper. Like many,
we're also very interested in
we're
for partially observable task and motion
planning is the
is the domain that we're looking at. And so
this is first the first author of this is is
Yoon Woo Kim. And the idea here is that
we're gonna try to have a robot doing
these sort of long autonomy tasks,
the kinds of things that, that Chelsea was talking
about as well, the robot has to go search
for things in an in an environment, and
the using the the foundation model
not to do the actual
low level task and motion planning, but
rather to provide some of the
common sense of where should we be searching for the for
the objects. So, and there's a
couple of different inductive
biases that we that we inject. One is that you
know, as you might imagine, we can we can
have knowledge of where objects tend to appear,
what's in the living room, what's in
the refrigerator, and what's in the cabinet. And also,
objects tend to be collocated. If you see a spoon
somewhere even outside of the kitchen,
there's more likely to be a fork nearby that spoon
because ob there are certain objects that tend to appear
together. And so by by leveraging
that, we're able to have the
robots much more effectively
search through an environment for the kinds of objects
that it's looking for. And so this is again a world
model, coming from a foundation model
that's that's helping a robot plan, and
we have some, again, real world experiments. I'm not gonna
show you the videos here, but, again, all
the the details are in the paper.
Okay. That's the end of the sort of
whirlwind segment of the of the talk.
So
now I'm gonna dive a much much more deeply
into this paper that that, appeared at
Coral just a month ago. That
is, I think, perfectly on theme for this
for this workshop called Simulation Pretain
Trained Latent Action Space
for real world policy learning.
And this is the PhD thesis work of
of Jiaheng Hu, and it's also collaborative
with Roberto Martin. Martin, we're
both co advisors of, of Jahang.
And so the, the goal
of of Slack of this of the system that
we that we published is to learn visual
motor contact rich, whole body,
mobile manipulation. So that means, you know,
robots that are moving around and manipulating
things.
With real world RL, without any demonstration.
So neither using sim to reel
nor using imitation learning, but doing
the reinforcement
starting from all in the real world.
And so here's, you know, here's the task
after training. So the kind of task we have, where the robot
has to you know, navigate around an obstacle
in the environment, and be able to wipe
a wipe a whiteboard.
And so that's just one of the tests. I'll show you a couple
others that we, we look at as
well.
Okay. So so why do we wanna use, you know,
RL in the real world Learning
this whole whole mod body manipulation is difficult,
and zero shots seem to real. It's first it's costly
to build high fidelity simulators. And
I think often, you know, you can't you
can't get them just right.
And imitation learning, it can be very costly
to to collect the high quality demonstrations
and also difficult. And so
the idea is we wanna directly learn
via real world interactions without
high fidelity simulation or demonstrations.
And so most methods, if you just give them their
action space in the real world and tell them to try to
learn, they're gonna do what I what you saw in this video,
just sort of motor babbling,
and never really make any meaningful
progress towards the the task.
You're not you know, in the long horizon task, you're not
even gonna get the reward signal once,
through random behavior, and so there's no way to
to create a gradient and to try to
try to learn. And so the idea is that the challenge
here is to overcome the the
efficient the sample efficiency needs and also the
safety concerns. You don't want the robot if it's
learning real world, to to, you know, drive off
a cliff, when we're
working in in complex with complex robot
embodiments. And so,
so the the insight
from Slack is that we what we
really need is a good action space.
An action space that facilitates both
sample efficiency and safety. So rather than
this random motor babbling in the
low level joint space, we wanna start
by giving the robot
some some high level knowledge
about what the or or information, or have
the robot itself discover are the
what's a good action space. And so for this,
we use a low fidelity simulation, which is
a lot easier to generate than a than a high fidelity
simulation. It doesn't have the requirements of of being
really faithful to the robot.
And do unsupervised latent action learning,
to try to, to learn an
efficient action space for the for the robot.
What do I mean by that So here's here's what the you know, a
low fidelity simulator of that robot
that I showed you. And unsupervised
reinforcement learning, what that means is is having
the robot explore
the environment without having any reward signal at all.
It's not trying to do a task. It's just
sort of the analogy of a baby in a
crib. You give them give the baby a bunch of toys.
It, you know, sort of gravitates to the things that
are surprising or interesting or gives it
empowers it to do things.
But it doesn't really have a task yet, and
yet the skills that it learns there are gonna help
it do tasks in the future. So
that's sort of the analogy here. So
we're gonna learn task agnostic behaviors
in that low fidelity simulator, How
do we do that We learn a latent action space, and this is
based on work from
the the diversity is all you need paper from Sergey
Levin's lab, of gives this the
sort of first
I think, idea
in this direction was learning a latent
action space where you're just trying to have the
robot figure out how different actions lead
to different behaviors. Right
So a a a low dimensional latent action space,
that allow for temporary temporally extended
whole body motions.
But we we wanna go a step further than that.
We want to not just learn a low
dimensional action space, but we wanna learn one that's
controllable. And so we we,
we introduced a method to do
disentangled unsupervised skill discovery. So
what that means is in this load action
space, want each
dimension to only control one dimension
of robot action. So maybe one dimension controls
the motion of the base, another dimension
how the camera moves, another how the
arm moves. And
so, and so we have we add sort of an
extra
diverse and temporally extended behaviors.
That are structured, which makes it easier to learn
the downstream tasks, and also we
can we can lead learn a safety reward,
that that penalizes moving in this
low fidelity simulator into, like, self collision
state or any anything else you define.
As being unsafe. And so,
and so if we have a dimension for the camera,
shown in red, for the for the
arm shown in blue, and the base shown in green, we
wanna try to, as I said, learn different dimensions
that that that control those
those different action components. And
so we have a this what we call a skill
empowerment reward, and then also the safety
reward that penalizes self collisions and anything
else. And that allows us to learn this
low dimensional action space
in this low fidelity
low fidelity simulator.
Then in the next step, we can take those
that abstract action space, and
use it for downstream task learning
with all of it working, all of the learning happening
in the real world, and
so we we, develop a factorized
version of soft factor critic, that
leverages this structure. And a key
thing here is that we assume there's a
factorized reward function. So there's different
terms, you know, that that,
are dependent on where the base is, where the arm is,
where the camera is. And
and using a mutual information
reward signal, we can sort of separate
which action features,
the z's, impact which
reward signals. So we can come up with sort of a
much more sparse representation
which allows for a more stable
value function estimate and more efficient
reinforcement learning trading in the real world,
We infer these dependencies, this is a different
paper from RSS on causal
causal policy gradient, so this connects
to Elias's first the first talk this
morning. But it's basically underlying
it is a conditional
mutual information reward signal,
that's that's, having the robot learn for
itself which action which of these latent
action dimensions impact which
component of the reward signal and then coming up
with a masking matrix that allows, when you're trying
to maximize one component, to mask
out the other action components.
And then we can use that data to initialize the
replay buffer for for efficient
learning. So so the kinds
of tasks we look at, I'm now gonna move to show you
the show you results, and all the details are are there
in the paper. But in the,
you know, we're we're gonna just use onboard
observations from the robot's
camera and proprioception,
We then are gonna learn a task policy
with our frozen latent action space
that was learned before the task was
given.
And do this, to to
try to and and then we'd give a a factored reward
signal that that, is
dependent on the the camera,
the arm, and the and the and the base
for each of the tasks.
And so and that's, you know, leveraging that causal
matrix from the previous slide, and then up we can update
the policy. So here's we can
now at the beginning of training, it looks sort of
like other other robots, except now
it's the actions aren't quite as motor babbling.
It's doing some more reasonable things.
But still not getting any task reward, of course,
at the beginning. But
then, if we let the robot go for,
about forty minutes, of real
world training, here's the sensor in the top left, and
it's you know, given that the reward
here for wiping the the
board, and it's figure it has fig
oops. Didn't show you the rest of that video. So
here's that whole video. The eighteen second video, but this
remember, this is forty minutes into training.
Starting from the random behavior. And
it's able to successfully wipe the board. It's getting
all all of the letters. And so
and re recall that it's,
why is it stopping
Well, you might have to take my word for it that it does finish
the board. I'll show you the other tasks as well.
Here's here's the robot,
pushing garbage into a into a tray.
Okay. What is never
had these videos stop partway through.
One more time.
See if this is deterministic or nondeterministic.
I'm doing some real world reinforcement learning here
too.
It's the in Yeah. The Internet's flaky, so it's gonna stop
right around here. But there there did get through that task.
Let's see if this one is able to
to get through. So here's pushing garbage into
the tray, and again, this is all happening with
all of the learning happening in the real world.
And then here's sweeping garbage into a bag.
And so it's using that latent action space with
these reward components to
and this is gonna have to load again.
Okay, you'll take my word for it. It does fit it does
sweep it into the into the bag.
And the key here is that, with
all of these tasks, it's less than an hour of real
world training, it's doing better
than if you're using just the raw
action space for sure. It
has a higher success rate and fewer safety violations
if you're using just plain straight sim to
reel. With all of this the, you know,
sort of zero shot sim to reel these
are sort of the the other state of the art
methods that you would, compare against. And again,
the key here is that all of the policies
trained in less than one hour because it has this embodied
world model that it's learned in the low
fidelity simulator. So you can find details
on the paper here. Again, this is the work
of of Jiaheng Hu.
And, so just to remind you,
the theme of this of this talk was
to tell you about some of the work that we've been doing
on going beyond
pure sim to real, still no imitation,
using simulators in creative ways, either
to do environmental synthesis
to be able to to get a a richer simulation
that's more reflective of your real world environments,
to be able to do common sense reasoning
about where to to search in long horizon tasks,
and then finally, to learn this latent
action space real world policy learning.
So with that, thank
you for thanks for your attention. Sorry about some of
the technical
Yeah. Let's see if we have any questions from the audience.
The microphone is over there.
Go ahead.
Great talk. So I had one
question about the synthesized environment.
Approach you mentioned. Yes. So when you're injecting
those learned stochastic process for fine
tuning, right, are you doing that
online during training or pre generating
No. That's pre generated. So it's using it's using
a GAN. So it's basically we're we're identifying
the environments where the robot
fails. Getting some features
of those environments, and then
generating features that that's, you know, that are indistinguishable
from
generating environments that are indistinguishable
from those environments where the robot had
trouble. So it's not just the environments where
it had trouble, but it's sort of learning a distribution
and then we can
do retraining in the simulator still offline
before putting it onto the real robot.
Just one follow-up. So if if you wanna do that
online is like, is there is
it very hard or how do you counter
the computational challenges Yeah. I mean, I think that's
the key would be like know, how because
in this case, we're still doing the learning in simulations.
So if we're doing it online with real robots,
it's gonna be hard to generate enough data
to learn to improve the policy. So it's it's more of
a mostly a sample efficiency challenge
than Thank
you. Yeah.
Hi, Peter. Thank you for the great talk.
Have ADHD myself, so the one already
hooked me in. Thank you so much for
it. I have one high level
question maybe. So I noticed that nowadays
when people are talking about like,
generating data or simulation for
training agents, Most of the people are focusing
on single agents. So I was wondering,
I have this idea in my mind times ago, so I want
was wondering, like, what's your opinion if we can
gather some data for, like, multi agent
interaction Right Yeah. That's a that's a fantastic
question. And and actually, I didn't talk about it. My
a lot of my research in my lab is about multi agent
systems and about multi agent data.
And I often show this video of, like, you know, what traffic
signal symbol traffic intersections
should look like in the future when we have autonomous
cars. It's gonna be, you know, you you
can have there's gonna be a lot of interaction. And
to do this, you either need a a
centralized control control algorithm,
or you need a predictive model of what all the other
agents are gonna do. And, you
know, in traffic, we can do it with,
you know, with with some kind of a controlling signal
like we do in that video, But if you're doing
social navigation, which there's like I say, there's a talk
on my website from ICRA in the
spring, which was all about our work
on social navigation, and that's a a robot
moving an environment that has to do a lot
of prediction of, like, where are the people gonna move,
What are other you know, what are and it's not just where are they gonna
move, It's also how is what you do gonna
influence where they're gonna move. And so
that's that's very much a multi agent model, and
there's a whole stream in my lab on
ad hoc teamwork, which is very much
about teammate modeling, opponent modeling, didn't feature it
in this talk because most of it is not embodied. I tried
to talk mostly about robotics things in this this
talk. But think, you know, ultimately,
we need to put those together and and have, you know,
multi multi agent models
that are embodied in the real world. And,
so, yeah, I appreciate the question. Thank you
so much.
Yeah. Hi. A quick question. What is
really the benefit of learning
a compact latent action representation
Instead of just designing it myself, like, say, the end
effector poses for instance. So we're just trying
to bake in the experience inductive
bias. To the loss functions here Instead,
why not just design it Yeah. So I mean, you know, if you
have it's a question of
you know, how much human intuition is
gonna be correct. Mhmm. Right So you could
you know, just say, well, there's gonna be a component for the base
movement, one the camera, one for the arm. We've actually
found in the in the initial work on this where we
learned the latent action space,
Dosti, that it's not always it's
not you know, those intuitive dimensions are not
what come out from this method. So so we
want controllable, we want latent,
but we want them to be reflective of
the environment Okay. And,
you know, human intuition is for is not
always reflective of what's actually gonna
be the low dimensional action space for the robot.
So you could, but I think it's gonna be more effective
to allow the robot to in sort of the, you know, the bitter
lesson concept as well. The more you know, we can try to put in
human you know, human biases,
but the more we can let the robot figure
out what's there for itself, the more, I think, the more
accurate it's gonna Right. So the intuition was easy to
bake in in the loss function of the objective of
the unsupervised learning rather than going and and testing
Yeah. Okay. Thank you. Yeah.
Sorry for the sake of time that we are
running out of time now, but you you probably can
reach out Peter offline. You have more questions.
Thank you. Thank you. Yeah.
Thanks, Peter.
And, yeah.
And now let's welcome our today's last
speaker, from Mila
University, the Mont Trello. And, his
topic today will be using foundation models
for embodied control. Thank you.
Problem.
Let's see if this goes smoother than it did.
For some other
Alright. Great. How's everybody doing
Fantastic. Everyone loves robotics. Good to
be in this room. So I'm gonna
cover a couple of, like, really recent pieces
of research out of the lab in this area.
And I guess, basically cover a lot why I'm
always still and will be a big of
using deep reinforcement learning to do all these things
unless try to make that easier and easier
with every project. And
this is really just a large belief of mine because
robots can still be much better tools
I want them to be able to help people
do a lot of different things.
Make a lot of, like, better agency for the things
we can have these days, especially as we get older.
One that really gets me these days is we're still
not fantastic at being able switch our
means of production. Another, like,
electric car company comes up, it takes a lot of
time to automate that entire process.
And in particular areas of things like being able to grow
food, avoid a lot of waste,
and things like that, and recycle much better.
So then how can we basically make
some good automated systems that can do these
easier faster, less direct
human involvement at every step.
So this is where I'm gonna talk
today about figuring out how to
understand how to integrate skills a little
bit better, and then figure out
how with each iteration, we can get these agents
to learn faster and faster. I mean, we're getting better
at making foundational models
making world models, things like that.
So these are good ways to be able to, like,
bootstrap basically the agents that we can
learn. So they should require a little less data every
time we've collected some data. We
should really be using RL to go collect stuff
for that we don't know the solutions yet to,
and we can use a lot more of the foundational
models for where we have data already to build
on. So this is basically
a version of this. So the world is
fantastic and really diverse and complicated.
Bunch of examples like multi agent environments
here on the right and sports.
And basically go through this process, and what
I'm gonna talk a bit about today is
also incorporating in a little bit of a step
for like, almost internal thinking and planning
that's built into the agent. That helps
it maybe deliberate a little bit on
each step. If it works a little bit on
this thinking and planning process, can help
save us a little bit of data at that time. And
then maybe we just save that data for later
things become more reactive and less thinking
and planning on newer tasks that Aiden has
to solve. Okay.
So then my favorite tool here, just in case
people aren't totally familiar with this,
is deep reinforcement learning. It's great because
we can dump whatever sensor information we
have from the robot into the deep network.
Figure out how to estimate what is a better action,
you know, using advanced or something, or some
human comes along and pushes a button and I
prefer what the robot did there instead of what it did
last time. And in this case, basically,
propagate these gradients back such that that
thing has a higher probability the next
time the agent visits that same state.
Or observation.
The, the challenging point though
is this still struggles to
generalize basically outside of this distribution.
And some ways, and it requires an
enormous amount of data. So
what might be your first guess at things
that you might try on this Since we're in this
workshop on embodied stuff
Let's maybe try some sort of
giant video model. And still
trying to kinda like did a little poll in the lab.
How many people think that we should just do this for
everything and it'll solve all of our solutions
Maybe a quick show of hands. How many people think the
videos I'm about to show are gonna be perfect
and there's gonna be no issues for any of the
robots in any of these videos
Nobody. Okay. So maybe people still need
more coffee. Whoops.
Okay. That still works for you even though it just went upside
down on my screen right now. So this was just
a fun example of trying to figure out, okay,
can the robot screw in some particular
examples
This is it, like, just pick up one of the tools in
the lab and start using the screwdriver. And
this was
was kind of prevalent across many different video
models we train, but these are just the ones my students were
generating last week.
This one was a bit better. We're getting a little bit better
at prompting these models in the last few weeks.
Although, the use of screws
was a little confusing. You might also get a
idea of this problems I'm interested
in in the current future. And this was just try to pick
up objects basic inside of the background. And
there's a lot of things that transform objects
and basically all of these videos
so it's basically this might be
a way to go in the not too distant future.
I think here there's still a bit of a struggle
because there's actually not a lot of, like,
first person robotics data. Tons of
data. Robots have really only been
around not too long There's actually many,
many of them these days, but there's not really enough
at least to make well, me happy,
and the people in the lab right now that are working on these
problems. So then,
what's maybe a slightly better solution where we can
extract maybe representations out of these
models and then use them for a little bit better, like,
post training in order to get them to be more
skilled. Okay.
Hold on. You get to
see that one again in case you missed it. The
first time.
Alright. That'll work a bit better.
So then I'm gonna talk a bit about this work, what
I'm gonna call it's called seg dac is how
it's pronounced, but it's basically how to overcome
visual reinforcement. Learning.
With a, basically, a clever way to figure out how to
extract segments from basically pre trained
good segmentation model. And use those
such that we're basically doing something really
related to kind of like object centric reinforcement learning.
Using some of the recent pre trained models. So this is a
picture of Alexander's
bedroom. Joking. I'm
pretty sure he generated this one as well.
But we have really good segmentation models to basically
break this up into the component pieces.
Kind of like the last question that was just asked for
Peter's talk too, like why build in some structure
I don't think all of you like, perceive
things on a pixel level, certainly not an atom
atom by atom level when you go to actually do
operations. There's some built in structure
there that is going to pretty much always be
reused. So basically in this project,
we're looking at this type of structure and how
can we make a more object centric representation out
of this So
the challenge in doing this though is trying to
make this fast. So segmented revenue
is great. It does segment things
pretty well. But we basically
developed a way to be able to
extract the object representation for each
image really fast so we can still do online
reinforcement learning. So here we
really combine, like, the YOLO world model
So this will give us a nice bounding box, basically,
for every object inside the scene.
We don't really want the bounding box. We want the segments
for how everything actually look in the image.
So from that, we then give this to Sam.
To then segment out the object inside of
that. And then we kinda
put this two step process together.
To just get a segment here.
We've got here on the left. Basically, an
embedding per object where it's
located inside the scene relative to where the
agent actually is in the scene. So
here's that basically process a little bit more
in two steps. So first,
you can basically use segment anything
to actually like, this last chunk
here is gonna remove and extract
out Right now, it's just, like, class
label or not, basically, for the segmentation
concept.
And you're gonna get a patch embedding for whether
or not that of embedding is going to be matching
for that object. From that, because
we've got the bounding box, we can chop
out from the whole image, which should definitely be corresponding
to that object. Then
from there, basically do this average pooling step,
which hopefully everyone can see since it's actually
pretty small. In the back of the
room. This so that we get one
segment embedding per object. By
being able to kinda overlap these processes together.
So this was some fantastic
work that Alexander was doing. He's like a magician,
basically. Being able to get some of the stuff to work
well together. And to get it to work fast.
Then, basically, now we're gonna really
the next step is just we've got this embedding.
Let's just use this in our RL
policy. So there's
a few more details. So this ends up
just as an example here. You know, this is like the Frank
arm and the many skill
I think simulator for being able to move objects
around. And here, these are basically
the three or six segments that come out. You've
got one for the arm, target, the
some background, the teeny little
blue object that actually needs to move.
I was kind of impressed that these models were still detecting
such tall op small objects really well.
And this is just background.
Basically. Stuff that isn't really an object and can
be ignored.
Alright. So then there
at least there was one step about being able to design
the policy here. Because there's a
possibility to have a different number of these segments,
like a different number of objects basically
at any point in the scene. So although
this is like a policy design, it's
not too complex here in terms of the other
model. But there is at least a transformer
in the first layer because it has to process a
variable number of segments that could come
in for each possible task.
So the most of these policies though
are pretty much exactly the same you
know, just the the critic here on the right has
the extra input for the actions. But
besides that, the input for the each of these models
now is just these list of
segmentation embeddings then
the actual action at least for
the critic, and some of the proprioceptive state
to also helpful.
Okay. So that was really
good. And then we created so for
some of the experiments for this, so
we used a lot of the environments that are
already in many scale for able to move some
objects around. Even got some of
the ones over here for the g one robot being
able to manipulate some of the
simulated fruit around in the room.
But then Alexander was still
not happy enough with how difficult the tasks were, so
he decided to create a bunch
of really hypnotic stuff from the seventies
and basically change all the text
to see, okay, how good are these
models at generalizing and being
able to be robust to these types of changes
So I mean, I would probably
have a bit of trouble with the version on the right,
especially if it was like this morning when I really
tired. But this adds just
another layer of actually looking at the type of
robustness we can start to get from
maybe being using these different types of models.
Okay. So after training a policy like on
this task, we can actually start to look at,
like, the queue values for attention embeddings
that we're getting for each object.
So they're really small here, but maybe I'll
zoom into this one and these two steps. So
like this is the Frank arm here.
On the left and here it is on the right. And
you can see that, like, the the Q value
for that actual segment for that part
of the model is going up. The
arm is pushing the very small
little block that you can hardly see because it's
teeny in the screen. As these things start
to come together. So it's also giving us a
little bit of an intuition, okay, is this model
learning something useful and related
to the Q value It is. At least
here, looking at the attention values and how they're related
to the Q value for what's being seen for the
model. Alright.
And then from that okay. A whole
ton of experiments because we're very
thankful for the detailed thought
algorithm they could think of under the sun that we'd run experiments
on. But
this is so this analysis is
in two steps. So there's a little bit of a
lighter bar here So this is the
original many scale environments
such that there wasn't any extra, like, perturbations
or changes of color or anything like
that. And then the solid bar performance here
is like so these are kinda like the original
ones plus these visual perturbations of
a bunch of different types of changes in
the environment that we've described from before.
You can see there's some similarity, especially
for these easier environments
with only small amounts of changes.
But then as we start to get down
to the much harder environments, where
these are much more drastic changes, much
more like that kind of hypnotic pattern
that was on the past screen rather than just, like,
switching something from green to blue.
Here, we can see that on this
bottom row, our use of being
able to use CEGDAC is much more robust
to any of these types of visual perturbations
that people can supply. And that we
created because we're also basically
the challenge of being able to do, like,
DM control suites and changing the color in
the background wasn't really that challenging. We wanted
to add a lot more variation to every object
every part of the robot.
Because I guess a lot of what we're also interested in is having this
generalized to other robots as some part
too and other objects really well.
So this was really good visual robustness,
especially for these harder tasks.
And this is a comparison to pretty much any
kind of recent visual algorithm that
we could find in the literature
Okay. And then we
did also a little bit of a comparison for sample
efficiency. So some might be wondering,
alright. Great. Glenn, this sounds awesome. Does this
take three weeks to train So it doesn't.
Thank goodness. Because of a
lot of the coding and the use use of those specific
models, it only takes about a day to train
one of these. At least in simulation.
And as well for these cases here,
the sample efficiency for this model is about
on par basically, with any of the other kind
of visual based RL methods.
Most of them are doing types of image augmentation
and some of the other ones are using
some pretrained models, but require a lot more
assumptions in the way that data needs to be labeled.
Really, basically, this
SeGDAC model is fairly flexible.
Such that we can also use it right now in
other visual r l problems. So we decided
just to load up Vizdoom. We created
what I'll at least call right now for
seg d q n, because it's just the same segmentation
model. You can go plug it into whatever
RL algorithm you feel like after this.
And then here, this is just on Viz
Doom right now trying to collect health kits as fast
possible. And this ends up
working out pretty well. Here without
any really additional details for it to learn.
And you can also see here I'm visualizing
the Q values for, like, the attention,
objects that are actually in the scene. So
you can maybe read that that says, like, closest health kit
is the one that's got the highest queue value, and you
can see the other queue values for other objects
in the scene as the agent is navigating.
Okay. So that was the first
project where we're basically figuring out, okay,
how do how to adapt basically
some good pretrained models directly into our
It's been a struggle really to do that
really smoothly, especially for being able to do do it
for online RL recently. Yeah. If
you wanna learn more about this paper, you can see
the updated version here off of the
link for SeggDek online. It's been out
for maybe a month or so right now.
So Okay.
Right. This is gonna happen again, it
seems. Okay.
So then this gets into kind of the second chunk
of the work. That I'm gonna talk about a
bit briefly.
I'll click on that. Where there's a lot of
recent work instead
Alright. So the first part was being able to
basically plug in some good pre trained model and
do RL right on top of it. But we're getting
a lot of great progress in what we're seeing in the research
lately for adapting vision language
models in some way to use them for imitation
and for robotics. And
this is just really, you know, people, you
grab some vision language model, you chop
off the end of that model, and you start training that for
some robotics task. Could be,
like, a diffusion model that's doing torque,
something like that at the end. And then
this next piece of work trying to figure out,
okay, what's good way to really make use of
these models and some model based
embodied planning as well.
So here in this next chunk of work, where we're
looking at improving pre trained vision
language models, with model based search.
So I'm really we're and this work really
trying to get a really good trade off between what we get
from these pretrained models and what you can
also get from search, which help
for some other things downstream, including like safety
and adjustments. Things like that.
But basically, one of the
many things that's nice about a good vision language,
action policy is it's a
great prior. Over like, good
possibilities for actions that are likely
to be successful for this particular task.
It's not perfect in all cases.
But if we had a model can basically
use this in some way to figure out
now it can search over, like take that prior
rather than searching over what could be a really large
search space, can basically seed, you
know, that training or basically the action
to take at that particular step. With the output
from a good vision language action model.
And this is what I'm gonna talk about here for the next
steps. So this is what's called
a VLabs here. So it's going
to take in, just for some background on the input,
still using a vision language action model, so it takes
in text. Be able to do a large number
of tasks. It's gonna produce
some action sequence to specify
And I'm gonna talk about this chunk here about
how to get the best use of combining these
two things together with some
planning where we want the plan to eventually be
successful, The better
the VLA that we have, the better we can basically
sample down this trajectory on the left and get
to a successful state fast.
And we'll show an interesting example about that
in a minute. So
first part, planning and continuous space.
Really, really painful and kind of infinitely
complex if you wanna get down to the details.
But as I was saying, so we can use a vision language
model here to basically now let's
sample what's gonna be a likely good,
actions from that model to start this planning at
least at this stage.
So it's a good place to start. One of
the challenging aspects for at least doing this is that
the VLA is still really, really
slow. So that means you really don't
wanna do a ton of samples through the Vision Language
action model if you wanna be able to expand a lot
of nodes. So what we actually ended up doing
is we would sample a number of actions and then
we would actually build a of, like,
local quick distributions around those actions,
things like little Gaussians around all those action
sequences. And sample around those
as well. In order to be able to
basically expand the tree. Here in
a well informed way. So this is
just basically take the v VLA, the
image of the text, do a bunch of sampling
according to this distribution, and that
you get a bunch of whatever you wanna call
them these days, macro actions,
action chunks, a whole bunch of names for
these, but basically a number of those. That
can then be used for sampling
for the model based process.
So this is just an example of
so here's a bunch of these action chunks that have
been sampled from that first state. So
it's just first, basically, root node
expanded here.
Okay. So From this step,
we need to figure out if everyone's
really, you know, remember everything from
your early AI class, to do the expansion
and selection step for model,
like Monte Carlo Tree Search, We need
some way to actually be able to select better
actions. So we did
a kinda clever trick in this case, because we
basically need something to compute the value.
Here, we actually used to use the VLA
again. So after sampling these actions and you get
into the next state, how likely
were those actions under the VLA
given the ones that were recently generated
Pretty quick actually to compute that likelihood.
So, basically, we use that as
our next selection mechanism here.
So this is just, you know, whether or not you've
visited that node already many, many times,
kinda multiplied by how likely is that
action sequence given by the VL
that was generated, and then you can just run
And this part on the bottom is much easier to parallelize
because you're keeping the same, like,
cache and everything for, like, the image and
the text, and you're just sticking in the actions at
the end. So it's much faster. To be able
to do that kind of evaluation step
Okay. And then from this,
combining this, we've already if you just take
some VLA that exists already, add
this model based planning step, you already start
to get boosts in performance for
the liberal environment, which does couple
of data sets for spatial and object oriented.
So you can see here
Okay. So this is just the baseline trying to be
able to grab one of these bowls and be able to pick
it up But really what we started to
discover is like where VLA's kinda
break down is they kind of know most of the behavior
to grab the object, but they might make kinda
silly little mistakes just like missing
by quarter of an inch on stuff, and it's not
good at recovering from those.
Instead, this is like really one of the nice parts that
the model based search helps
us from. Here. So in this
case, now this, I believe, is now using
our method because now
it can do a little bit of planning just a few steps
ahead to know, okay, I was gonna make a mistake there.
I can easily adjust that action a little
bit, and that's really the kinda easy part
provides the extra performance that's the game here.
Okay. Did a kind of fun
analysis on this too. To figure
out this. Because there's basically a trade off here. Because we're
doing something kinda like test time compute,
basically combining these two together. But
if you start with some VLA policy,
and you can start doing some additional,
basically, post training on the data set that
you have, then this
starts to basically the VLA policy
on the o alone at the beginning is terrible.
Even, like, a somewhat okay
VLA policy gives really large gains in
performance. Because it's already helped
like, basically provide a good prior for the search space
really fast. But another
interesting trade off is
here, basically, as you get
a better VLA, performance also starts to go
down. So depending on how much data
you have and how much time you have do some
of this planning, you can basically play around with
using this in the space or maybe you have no data
collecting enough to start to get started on being
able to train your model, But, sure, more data
always helps, but it also improves the
performance either for, like, this planning VLapse
model or it basically helps reduce the
time it takes to do the search. As
you can train these models better and get better
performance.
Okay. Those were the two, like, main
projects I was gonna cover about for a bit, and then
kind of some extensions of these,
because here, this kind of ends on
we can do this really well. It also depends on how well
we can fine tune vision language models.
So I've got a recent piece of work that was
at the reinforcement learning conference on, like,
what is the best way to fine tune basically
your vision language model you can use
LoRa. You can use input adapters. You can
use prefix tuning. You can use
direct adaptation. And
we're trying to figure out, okay, what's the best way to
go about given how much data that you have
The, basically, short story out of this
is it really depends on how much data you
have. So if you don't have a lot of data, there's
not a big difference between doing
LoRa and some prefix tuning And as your
new data that you're trying to integrate in your model
is gonna start to grow, you definitely
can start to use things like LoRa, increase your
prem, count. If you think there's a lot more to be learned
from your model, and
there's a bit of a mix of some other options
here that can be more data efficient
if you go ahead and use them as well.
Okay. And then this kinda
gets a little bit towards so fine tuning works well.
There's always still some challenges. Maybe
some of you have also read like John Shulman's recent
blog post about how to do good, like,
post training with RL, and you should
use Laura in pretty much most cases,
at least for language models. It's a bit true for
robotics as well. Long as
you have a lot of data. But this
also led me to a couple of questions on
basically how good is deep reinforcement
learning at learning from its own data
Because I if you take basically
basically in this case, the plot on the right is I trained a
policy in like a
deterministic environment This is basically
this line is the best stuff
that the policy was able to generate. It's
really great. High value stuff. A replayed
in the environment again so we know it
as it can absolutely be learned and used.
But usually, the policy, what it actually
learns, there's quite a large gap between how, like,
the best stuff it actually finds in the environment
and the average policy it gets from learning.
And this is a bit concerning if we're gonna end
up, you know, trying to get the most out of our data we're gonna
hope our policies are gonna learn from this stuff. We're gonna
finally deploy in robot in the real world and
use all of our time for. So
this has led to at least a couple of recent pieces
of research I'm looking at better optimization
algorithms for re reinforcement learning
in order to basically close the distance on
this gap. Basically, I want the policy to
always be able to learn the maximum and,
like, best stuff it generates during learning
and not some kind of weird mixture
of it because deep learning is quite
painful. It does not like non stationary distributions
for the most part. This
led to a recent
work that I presented yesterday with some of
my students on being able to
do this, but also scale up deep learning
models, things like ResNet, and things like
that for more complicated tasks.
And this is very difficult because there's a lot of
non stationarity in the network. With
respect to how the deep learning policy model sees
it. Every time the policy changes,
distribution and your replay buffer changes,
It's not the same distribution anymore and it's
kind of like a successive new
task distribution problem in terms of just
how the deep learning model sees it.
So this basically gets worse whenever your
network gets larger. So the two
things that was found for that piece of work that
seemed to work really well. So one was being able to do
multi skip connect So
this is basically kind of like a ResNet,
but instead of it just being in blocks of three,
you can skip over pretty much from the
encoding model from your computer
your convolutional network. Many
steps forward So you can kind of skip large chunks
of the model in case they're not being too useful.
But the other thing we're also looking into and what worked
really well is looking at second order
optimizers. Actually really helped
policies be able to recover and scale better for
much larger models. So
here, we ended up using Kron as an example
for this. And then we can basically
apply this and get good,
like, monotonic improvement performance from
these deep learning models. Policies and models
for much deeper networks. And this is
better than some of the prior work that was
doing things like resetting networks, because
it's difficult to use a resetting network
task to be able to get I mean, you don't,
especially if you've already spent a lot of work on your
foundation to model training. And you're gonna tell someone
you're gonna reset a big chunk of it.
But also that resetting only really worked
for off policy based methods that had a replay
buffer that could then learn from that good stuff
in replay buffer again.
This now works on any RL algorithm. So
we ran it for PQN, for PPO, DQN,
I think SAC owned Rainbow as well.
And it's nice. I like these solutions because they work
in the kinda deep learning model space.
And not specifically RL. So then it can be applied
to wherever your model is using deep learning.
For the future. Okay.
And this is just basically just kinda wrapping
things up. This overall recipe that we're going with
is basically train your large foundational
model, You've got a lot of data somewhere.
Should help with generalization. It's kind
of the definition of doing these foundational models.
Then basically integrate this with reinforcement
learning in some way because it'll save
you a lot more data to up to actually
collect in the environment for your agent.
Maybe these two parts where you'll bang your head
against the wall that why it's not your agent isn't
learning from the data it's generating very well.
So look at some of the better optimization methods
that maybe I've been working on lately.
Discover some new high value data. Go dump
that new high value data into your origin
so you can train your foundation model again and
kinda iterate in this process in
engulfed all the data in the universe and it catches
up to how fast we generate data. Maybe.
Okay. So then just a bit of acknowledgments for
the team that's helped cram all the stuff together and
do a lot of this research. And I'm happy to
take any questions now.
Thank you for the great talk. I had one
question on how do you handle temporal
credit assignment problem across
sensor latency Have you done anything on
that For sensor latency,
is that what you said Or and latency
Sensing latency, like there's a lag between
the sensors and the actuators. Right So have you
like, one of one of the limitations of DRL techniques
Right So I just wanted to know if your
your take on that. Yeah. Well,
the I mean, so, Chelsea
was talking a little bit early about incorporating
some form of, like, more flexible memory.
Which can also be a good solution here.
Usually the first thing that people try though is some
amount of kinda stacking of the recent
history So keep your
model doesn't take in the most recent sensor reading.
Given, like, the last five times steps of re
sync, history for that sensor
information. And just let them give the model a lot of
data, and it'll start to figure out how to
approximate what that sensor reading really
means in order to do good control.
Yeah. Makes sense Thank you. Yeah. Sure.
Hello. Thank you for the great talk.
I have a few questions about the
first project on the SAM
Sure. Two questions. First question is,
I really liked your motivation on
automating certain tasks and we know that the success rates we need
are extremely high for those for those tasks in
production. Mhmm.
What are your thoughts on accumulation of errors
for foundation models For example,
Simon, we know that you know, if looks very impressive,
but in many cases it's still quite brittle.
The other question I have about the project is,
there's this kind of trade off between abstracting away
sort of non important information like light,
lighting conditions, shadows, but in the case of Sam,
we might be missing park level information.
Can you think of ways to strike
a better balance between those two two
aspects Yes. Although I think
I'm gonna give the same answer for both of these.
I mean, again, the the nice part
is is that, like, the same model gives
a most likely a much better representation
than just starting from nothing.
That's already a really good start. So even if Sam
isn't perfect here and there's some challenges
in the way it can learn good embeddings,
you're using RL on top of this,
you end up still being able to learn from that
information from the embeddings. Like we
didn't end up having any trouble in this project even
when Sam might have some issues.
Because in like when we were running this, the number of embeddings
would change because Sam would have a couple of errors
and it wouldn't quite capture, objects really
well. But because of the way we were training
the policy that was kinda taking a
variable length number of embeddings, it learned to
maybe deal with the fact that of these
were either, like, occurring or disappearing or coming back.
This maybe goes a little bit towards your second question
where obviously,
maybe I'm an RL and kinda control person,
and with a little bit more compute in the future,
I would also train basically the segmentation
model, and that would all be trained together and it can improve
on the task. Overall, this
should save a lot of data
needed to learn on these visual RL tasks.
Thank you. Sure.
Thank you. Nice talk. I had a question about the VLA
stuff. You could go to that slide, please.
Yes.
So it seems like, what you're proposing
is to sample a set of
action chunks and then score them
by the likelihood under VLA, And
I was a little surprised by that. I I would have thought you'd
use a value function for scoring. And
then secondly, if you're just scoring by the likelihood,
you're basically trying to find
like the top k most likely sequences.
Right And there might be other algorithms for that
beyond just generating test Mhmm.
Yes. So the first question so it's tricky
to be able to train a value
function. Here. So in this case,
at least it's it seems to have
been tricky the community to train a value function on
top of some foundational model, and people
haven't at least gone that route very
much. It seems to be easier to train a policy
So it doesn't have to incorporate in as much information.
And there's also been a couple of good, like deep
reinforcement learning papers lately
that have been scaling to better performance than what
they've been doing is adding like hundreds of
layers to the value function, not to
the policy. There's probably
more challenging to understand how to do a good, to
like predict a good value function for those.
So it's a it is an interesting trade off
There was a bit of a difference because we generate a
couple of action chunks then
we actually make a Gaussian distribution
along that, and then we generate some more from
those, and then we evaluate those from the actual
policy just to see, I don't know, and
the generation process was just
noise was added in the wrong place along
that little action chunk that's gonna cause the
error, that way, if you go and ask for the likelihood
of the VLA again, it'll say, okay. No.
The one that's more towards the
mean is actually a little bit better. So
we also even though we did some experiments where
we turned down the temperature for the VLA,
that didn't quite solve the problem either.
So the stochasticity and resampling
does seem to help. So you find that
likelihood of the VA is a good indicator of
success It actually was really well.
I mean, yes. As long as
your VLA has been trained over some data,
And I think that's sort of what this
plot here is indicating. So
if if your VLA is
terrible, then no, it's basically not
even a any
kind of approximation for how well that's doing.
But after a little bit of training on what should
be expert data, so if it's expert
data and you train something to generate that, this
sounds like it should also be essentially a
value function as long as you can get it to train pretty
well. We all assume it is expert
data, even though it's it's not really most of the time.
Then it becomes at least a fairly good
approximation But we're looking instead
we're doing some methods to be able to improve
the evaluation here. We're also looking at
being able to use a much better kinda
model for our model based search into the future as well.
Yeah. Thank you. Sure.
Let's, thank Professor Keng again
for the wonderful talk. And,
thank you everyone for attending the morning session.
We have forty five minutes up before the lunch break.
And there's the poster
like, at the side. And please
come come back before 1PM,
and we'll have five more keynote
talks this afternoon and a wonderful, like, panel
discussion. Thank you.
Thank you. Sure. No problem.
Later, And,
will let another organizer
to introduce our sponsor,
WAVE, and also their demonstration
part.
Okay. So now we are very pleased to
welcome the speakers from Wave
to represent their industry
demo and topic is scaling world models
to power evaluation and validation.
So So let's welcome Gianluca
and Lorenzo.
All right. Well,
thank you everyone for coming here just
right after lunch. So
we'll wave. And today, we wanna just
demonstrate
how do we think of, scaling world models
a way for the purpose of
powering our evaluation and validation.
But let me start with a brief introduction,
maybe don't
know who we are. So Wave is a British startup.
We build self
driving intelligence And what we're building is this
product called the Wave AI Driver.
Which is a product built
to be integrated into any vehicle because
it works off, pretty lean
and flexible AV stack. It's sensor agnostic.
And it can it's basically ready to integrate
into any vehicle that has cameras and compute.
Doesn't rely on any on any HD maps, so it's
quite scalable. It's
adapting to any new place that
it goes extremely well. And it's built
for the automotive systems today. So it's
versatile, It can cover
all applications from ADAS to full IV.
And because of
the way we've been building this product,
this year we really started stress testing
it in this thing that we call the
AI 500 series. Where
the way by a driver is currently driving around
the world, through 500 different
cities. And, you know, if I'm not mistaken, we might have
a couple of cars right now being at the Arctic
Circle trying to drive there. And, you know,
we keep seeing this great generalization
coming from these models. And
this is just one small example
of places where we tested almost
zero shots, so we went to Paris and
we tried to drive to one of them most
complex roundabout we could find
in Europe. This is the Arc De Triomphe.
As you can see, know, this model
is built to work in a variant structure
environment. Like, this is a huge roundabout. There are
no lane markings, There are many exits.
Cars are coming from all the sides.
And the way that I drive it, it just capable
to generalize, understand how to negotiate
this kind of behaviors. And
just safely go around such
roundabouts.
The big question is, we kind of
seen huge
acceleration in the way we can
build and improve our driving systems,
which poses like a new problem
which is do you evaluate these things at scale
And, you know, real world testing is essential.
But it's also very expensive. And so here I
put some examples of things we encounter
on the road almost
on a daily basis.
But that's still not enough. Like you want to
make sure that when you validate such
products, you're very thorough covering all
the space of possibilities.
And, also, there is this other property that
has your model improves, on
road testing starts to become more and
more uneventful because you fail a
lot less. And
for this reason today, we wanna
talk about GAIA three. So
Gaia three is the latest version of our Gaia model
series that is starting few years ago.
And this is how we think of
approaching this problem, and Lorenzo will show you
a little bit more about GAIA three.
Hi, everyone. So let
me deep dive on Gaia three
and its applications. First
of all, what is it Gaia three is
our latest, latent diffusion
transformer. It's 15,000,000,000
parameters model. So it's twice the
size of the previous iteration, Gaia two.
And we train it with 10 times the amount of
unique data of our previous
version. And when we look at the
training data, we have a proper data
set of many cars driving around
the three continents and covering
different embodiments, different type of vehicles.
With a focus specifically on
safety critical scenes vulnerable
users, and traffic con traffic control signs.
So let's take a look at how Gaia three
compares with with Gaia two first.
So this is this video is a video of
Gaia two, and even before I start
start playing, you can take a look at the
billboard in the front image.
And you can see that the text is quite blurry.
We know these are big problems for world models,
things like test or science are quite crucial
for driving. So
now while playing the video, focus on the train
just behind the billboard and on the cyclist
in front As
you can see, the train is disappearing,
so the cycling partially.
I'll play it one more time.
So you can take a look. And
now, this is the new iteration with Gaia
three. You can already see again before I play
the video, that this the
design, the billboard, is much
more clear and it's written the same as
the actual white billboard on the on
the left part of the
front image. Now that I play the video, you can see how
the train is much more consistent than before,
and so is the cyclist.
One more time.
Another interesting, element
is vulnerable or user. In particular,
in the presence occlusions. So
in this case, we'll see a pedestrian passing behind
the car and coming from the
other side. We can see the model
can also deal with short occlusions,
in generating the pedestrians with the same appearance.
As before. The occlusion.
And then, let's take a look at snapshot
of, again, very small details.
Crucial for driving. So can see
here, vulnerable users, in particular, motorcyclist,
can see a police car rendered in the in the
but in the bottom part, we see
Gaia three, and in in the top part, Gaia two.
You can see the details, for example, the police cars
are much more crisp, much more clear.
And so you can even read the
driving plate in the in the second
example in the bottom wall.
Why you can't for the for the top one.
And and as I mentioned, one important element
for driving is signs traffic control
signs, traffic lights, and you can see how
here we render correctly the
arrows for going straight, turning left,
is they are quite small. These are
basically, screenshots of a small
part of the generated video.
And now let's talk a little bit
about as Gianluca started to mention
about the applications and how we use
GAIA three for evaluation and validation.
We have all sort of applications. Let
me start with the the
capability to reuse the data
that we collected on a specific
embodiment, on a specific vehicles and
recreate scene from another sensor
configuration. This is extremely useful
because you may change the vehicle
type, and you want still want to reuse the same amount
of data. So as you see on the left
part of the of the video, you have the
original embodiment with different
camera field of view, with key different camera
heights and positions. Then you want to
recreate the scene from a different embodiment.
So if you look at the again, on
the front camera specifically, there is no,
there is no car, for example, part of the
car, there's no boner, there's no internal part.
And this is due to the camera configuration.
This is a scene in Japan.
And let's take a look
also at the foliage,
quite high high frequency details.
When we look at the bottom video, you even before
I play, you can start already see that there is a
a little part of the car as this scene is rendered
from a different point of view. Similarly,
all the other cameras are actually rendered
from a different point of view. Let
me play both of them another time.
Another application is targeted
testing. Want to be able
to take a scene and perturb it
in a controlled way, so that you can
test the driving model into the scene.
So in these examples, we will see how
we want to generate a scene
and and see how driving model respond to
this per turbine.
Particular, the green trajectory is the
conditioning we give to Gaia, our
model, to generate a new scene.
And the ping trajectory is the
response of the driving model
to this perturb scene make the the model drive
on. So we see, for example, on the on
the left one, will generate a scene where
we try to drift left and see the response
of the driving model with it.
So here, the scene now starts to diverge from the original
as we are drifting left, and we see that the driving
model correctly predict
to push on the on the right side to come back
to the original lane.
And if you look playing it again on
the right example, is the opposite happening.
We are ask we're generating a
imperturbacine drifting right,
and we see the driving models trying to push.
To come back to the original lane.
In this case, we're actually going off
road in the generated scene.
And now, I mean, these are just two examples. Right We can
start to generate, an entire suite
with many, many more examples.
Going too fast, going too slow, and many other variation
in different scenarios.
Now a third application now
linked to the previous one is robustness
testing. May generate scenes
where you're drifting left and drifting right, but you
never know whether the driving model will be
robust to variations of the scene. We
it be robust to lightning Change
of lighting, change of semantics, or change of,
for example, time of the day, or weather
as in this case. So here, we're taking the
same scene make sure
the route configuration in the agent's
behavior the same way, but we are changing in
this case time of the day, and weather.
So, again, here, we can compare model
performances on different
conditions, see whether the the performance
is robust. To these variations.
So then finally, one of the most interesting application
in my opinion is safety critical testing.
So, how do you test on rare
events that are unpredictable and also quite dangerous
It's like near misses or collisions.
It's obviously
in the real world. So
one approach one common approach of the industry
is to use, test tracks, for
for example, One example
of them are end cap scenarios where the specific
target seems are reconstructing
and regenerating in the
to test the the driving model.
Now so we start with that. This
is a scene generated that
reproduce the end cap scenarios
of you know, moving into the oncoming
vehicle and crashing on it.
But the beauty of the the fact that
this is a simulation is generated by a world model
is not real, is that we can
actually create way
more realistic, scenes that are actually
more representative of the complexity of the
real world. So here, you'll see the same
exact scene in terms of, behavior
of agents, but in the
real world. And not in
a test track, which is something obviously very hard
to do in real. Instead of in
a simulation. And I I think
one element to highlight here is that the other
agents is not reacting to it. This is
World Rail, which is a specific
element. You want to test for.
And and again, we can
recreate differences,
different variations of it, I found
this to be particularly interested because we have a
construction track in front
of us another construction truck
on the other on the other lane, and we can
still recreate the same type of scenarios.
So here, we are colliding with
the the incoming the incoming truck.
This gave gave you a little bit of overview of the type
of applications we are we are
are dealing with. When using IAA three.
And, I want to conclude
with a video of the of a coastal
path that could look like, probably similar
to the type of coastline
we are in here in San Diego.
We released a blog post just a few days
ago, the link is below. So
thank you for your attention.
Thank you Luca and Lorenzo for
their representation.
So now we are now proceed to our
next keynote session, and our next speaker
is John Longfer. He's
a director of
learning and Microsoft research and
he leads cutting edge work in machine
learning and large scale predictive learning.
And he's talk today,
it's titled Next
Latent Prediction Transformers
Learn compact world models. So let's
welcome John.
Alright. Very good.
Okay. So this is, something we've been working
on over the summer.
And it's related to
several of the talks that have been going on
One of the things that's been bothering me for a long
time is in in reinforcement learning,
this notion of harshly observed market decision
processes And then, of course, you have a market
decision process. Where
decision process, because it's easy.
But the real world
is always a partially observed Markov decision
process where you don't have all the
information you need right up front.
And it turns out that
this is a trick which allows you
to convert from a partially
micro observed market position
process to marker position
process. In practice.
This is kind of a key thing related very
much to compact world models.
That's what this is really about in in assessments.
But I wanna kinda start with, what
is a world model. Because I think there's
there's two definitions running around.
And,
and and they're very different.
And they both have, points, and they kinda connect
together. In import. Ways.
So for many of the
talks that we've been looking at today,
the notion of what is the world model is,
simulation of video.
Essentially.
So for for I just
I'm just kinda curious what where people are on this.
How do people think of world models as
ability to simulate video controlled simulation
of videos Yeah.
Okay. There's
another notion of world model.
Which is which which I'll get to. But if
you think about this let me give
let me give you a little bit of pause. Which
is,
yeah. So realistic video generation, which
is if somebody's blind all their life, there's
no way they're actually simulating a video.
But it seems a little
strange to say that they don't have
some sort of world model internally
So there's there's another definition of world model which
is very natural. Which is something about
kind of, having a simulation
an internal simulation of what's going on in the
world. Right
And so that's,
this is not like a it's not a gotcha in the sense
of, oh, you discriminated against a blind person. This
is a we should really think about what the definition means
here. And I think that this
definition actually gets at a really key
element of of the limitations
of current, like, large language
models. Current large language models
are, you know, wonderful in many ways, but
they learn very slowly. They require
a lot of data. A lot more than a human.
And and they're also kind of brittle in sort
of funny ways that are little bit hard to
codify.
Thinking about what is this, a different kind
of world model. That blind people
can have. Think, helps address this.
Okay. So let's go back to why
you want a world model. And
I think there's there's
several possibilities people think about.
One of them is you wanna do better generalization.
And better generalization is a very generic thing to
say like, oh, I wanna generalize better. That's great.
I think we there's a there's a very specific kind
of better generalization, which is possible if you have a good
world model. Which is you can extrapolate better.
Right So you can you can,
drop a rock on
the earth, and you can extrapolate to
estimate what would happen if you dropped a rock on the
moon. That that that that that that's an extrapolation which
is possible because we have some kind of world model
that we built as
a civilization. So the
the the extrapolation ability is kind
of a key element of what it means to have
a a good internal world model.
There's another, which is used by several
of the posters over here, which is essentially
model predictive control. If you
have a good moral world model,
then you can roll out the future
You can look at the consequences of actions
that you take. And then you can you can
try that several different ways, and you can choose to take
one action or or another based upon the
consequences that you compute.
Okay. So this is kind of a distinct one.
And then there's a third
notion of why you want a world model. Which,
is kind of informed exploration. And
I think this comes up in in two sort of
distinct approaches.
One of them is,
if you are moving
around in a world it's good to
know when you came back to where you were. This could
be in simulation. It could be in in in in the actual
moving in in in in life. Because when you
come back to where you were, often things are actually
a little bit different. The signs have changed. The the the
the trees blown over, whatever.
Things are different, and yet still it's the same you you
are where you were before. And so that ability to
kinda test data quality is
kind of a critical aspect of having a
good world model. If you don't have
a good world model, it's it's it would be very hard for
you to to realize that, oh,
this place that I see right now is the same as the
place that I see at night, unless you
specifically train for that kind of thing or something like that.
And the other part is if you're if
you if you actually just don't know things
and you wanna gather information.
Then having a good world model is very helpful
here. So, like, you walk into a house, you've never been
in the house before, You want
to, know where the bathroom is so that
later you can go to the bathroom. Or maybe you can go to the
bathroom now, can
systematically explore the house because you have
a world model based upon what you've seen so far.
So these kind of these these are
kind of three different categories. And I guess I'd be
very curious Hands up, anyone.
Are there other reasons to have a world model
There other things that people want from world model
These are the ones that I, see
fairly often. Nobody
If you come up with one, I'm very interested.
A hold of me. Oh. Have one.
Causal manipulation.
I think that the causal manipulation has a lot to do
with a model predictive control.
You are, you you you you
checking to see what happens if you do something differently.
And observing
the consequences of that. Yeah
Yeah Active
learning. Active learning has a lot to do with that informed
exploration. The second version of
it. You have you understand some part of the
world and you want to,
come to understand another part
of the world where you you don't have a full
understanding yet. Yes.
Say again
Evaluation.
Interesting. I don't
quite understand that one. Let's
Using role models to evaluate a policy.
Yeah. So I guess I would think of this as as
kind of the model predictive control.
Type angle. Mostly Because
you're you're using you're you're rolling
things out in your synthetic world
to try to understand what
your policy is going to do.
Another. Yes.
Say again Surprise. Surprise
So being able to measure surprise.
Yeah, that seems interesting. That
might be
new in some sense. It it it
it's related to the informed exploration,
it it's it's it's a little bit different because
it's at least a new line under the
informed exploration, I think. Because what you're saying
is you could be going through the world, and you could just
you could be surprised, and and you could have an informed
notion of surprise. And that
could, you know,
trigger something about your data gathering policy,
for example. That you could then use,
later on.
Interesting. Good. Yeah.
What about
Understanding concepts like gravity.
I guess you would hope that
the better generalization, the ability to extrapolate
is, one of the key aspects
of like,
you if you have a good world model, then maybe
you understand gravity. And because
you understand gravity, you can even
about what happens if you have less gravity or more gravity.
On other planets.
So the the the gravity is is kind of
a key element of extrapolation.
I think, as far as
way you would use it in a world model.
Alright. This is very good.
Okay. So we would like this inner world model.
Let's think about just at
a semantic level, what we'd like from a world model.
So what you would like to have is better extrapolation.
And this is this is an example of
of a data set where you can actually measure
extrapolation relatively easily.
This is a simple data set where you
have taxi rides in Manhattan.
It's kind of here's a start, here's
a goal, here's a path.
And now you want to learn to predict
a good pass given the start and the goal.
Right And if you use a
next token transformer,
then, everything in red a
hallucination.
We're gonna do something slightly off distribution,
just to be clear. So if
you if you measure on the distribution that you trained with, you
you have no error. But we're gonna be slightly
off distribution. We're just gonna take random start
goal pairs. And ask for a good path
between the start and the goal.
And then, all
the red is solutionations. It's like, oh, you
go from this intersection to that intersection, and there's
no real road there. Doesn't work.
Right And so you can see there's a
a lot of errors. So it's not in
a huge amount, but you there's still a substantial amount of
errors where it just kind of of teleports
from one intersection to another. Doesn't make any
sense. And yet there's
a way to train slightly
differently which has
about one third as many errors.
And they tend to be more local.
K. So this is a small change to the way
that we train. Which
causes underlying transformer
to develop a better world
model. Which is what we're talking
about here.
Alright. So,
what it was a world model semantically
What does it really mean to be a world model
for a blind person Right
And here's what I think it means. I
think it means that it's you you you're
developing a belief state, developing
a transition model. Alright Those are
those are the key fundamental things. And so what what
I mean by belief state, what I mean by belief
state is you have all the
information from the past, that,
is relevant for the future.
Alright. So you can try to predict actions in the future
and you would like to
you know, like to be able to do that effectively. You
need all the information from the past that's gonna help you predict things in
the future. Future.
And then if you're going to think about a transition
model, well a transition model is in which takes a belief state,
and an action, and it gives you the next belief state.
State. K. So I I'm building into
the definition here since you're converting
my POMDP into an
MDP. And
then the assertion is going to be that we can do this
systematically.
Alright. So
to learn a world model, you
need to learn a belief state, and a transition
model. And these need to be compatible with each
other.
Okay. So,
now now a question. Is a transformer a world
model Right So just, think
of a next token transformer is
is this a world model
And I think a lot of people here are gonna say, no.
Because they wanna do something new.
Right Who who says
no
Okay. Who says yes
Yeah. So I'm
gonna go with yes here.
Because you they they are, in some sense,
a world model. You have
all the keys and values. Say that
it's your belief state. And you
have your current observation. It
gives you all the information from the past that you
need as
long as you optimize well. And
in the in the action is your next token.
And and you can even do this in a pixel centric
way. There's this world in human
action model stuff that some folks in,
Microsoft Cambridge published in Nature.
But, you know, I think there's a
a is is a but.
Right
And the but is that if we
go back to earlier, we see that
just a simple transformer
tends to produce a lot
of these hallucinations that it's sort of
incoherent. In terms of what his world
model is internally. So
for the definition that work a world
model that works for a blind person,
it's it's a it's a little iffy.
Somehow, doesn't
extrapolate very well. It doesn't generalize as well as you
would hope to.
So so what do we wanna do to change things
Well,
what goes wrong with a transformer as a
world model And what I would
say goes wrong with transformer as a world model
is that it's extremely loose.
You have all these keys and values,
and they're and every time you get
a new token, you get a new set of keys and
values.
So that it's the the the notion
of what is the model
of the world is is extremely
expansive. There's a lot of extra capacity running around.
And that's kind of
that's that's the issue. What you would really
like is a compact world model.
And so, what what is a compact
world model A compact world
model is a world model with a minimal sized
belief state. So we'd like to have
a belief state which is not too large.
And the reason why we want a belief state
which is not too large is because
you'll end up helping
us extrapolation. And
of course, it's easier to compute and easier to work with
and and all of these things.
Alright. So then, minimal can a belief
state be Well, there's something which is essential
that information that must be in a belief
state. Right So the information
that must be in a belief state is
all the information that you're
going to use to take actions in
the future. So
whatever whatever basis upon
which a policy is going to make to take action in the future,
you need to have that information in your belief state.
State. And syntactically, you know, may
maybe instead of having all the key values,
and and and the current observation, you could
have a few of your latent
activations. Or one of your latent
activations be your belief state.
I I think it's really the the semantics is really the
incorrect important thing. We would
we have to have all the information necessary to take action
in the future. Now,
it was kind of interesting watching the the wave folks,
because there there there are stimulating
things at the pixel level.
And that's, it's very high precision.
There there's is a is a
definition of progress there, which is you get higher and
higher fidelity, and eventually you have to kind of cover
everything. But
you may not actually need to know
exactly what the signage is on various
billboards. Because it doesn't actually
affect what a safe driving policy
is.
And so there's this differences
in how much information you need to simulate
for the policies that you
care about. Versus
versus the the general program of just simulating everything
perfect.
Okay. So we're gonna
make a small change to the way you train a transformer.
And I'm gonna claim that this
creates learn to believe states.
And transition models.
K So the the changes
so you have a transformer. It has multiple layers.
Top layer you have
a a head which decodes, a token.
It's gonna predict the next token.
We're also gonna have another head which
predicts the next late.
Okay. And then implicitly, the
next latent is gonna predict the
next next token. And the next next
latent and and so forth.
And then there's only one little trick here which is a
little important to understand, which is we're going
to predict
the next latent using the
true decoded token. So
this is a teacher forced
a prediction of the next late
Okay. So why do we do that Well,
turns out that
well, turns out that h three ends
up being a a belief state, first of all. And second
of all, turns out that the head which
predicts the next latent
is a transition mode.
Right Because it takes a belief state as input,
and an action which is the teacher
force token, and it predicts the next belief
state.
Alright. So, we can prove a theorem.
This may be the only theorem in this workshop.
If you do next latent, and you kind
of assume infinite
data and compute as necessary,
then you're going to learn a compact world model.
Model. The proof is by induction.
Our base case is gonna we're gonna be at the end of a sequence.
And the next latent is trivial.
And so inductively, we're gonna say,
oh,
I'm going to predict the next token
So,
my latent
must be a belief state for the next
action. T plus one action,
And I'm also gonna predict the next latent
which is a belief state for
the following actions. So this is the inductive
hypothesis here.
And so I might getting a belief
state for actions t plus two,
two plus three, and so forth. From
predicting the next latent.
And I'm getting a a
belief state for predicting the next action,
off the next token prediction.
And those are the that that prediction is coming
from the same latent and so
the latent is now a belief state for
all the actions going forward.
Simple claim.
As long as you have support over this and
you have a a good
ability to approximate, which transformers are wonderful
at, you can converge
to having a belief state.
Okay. So that's that's kinda cool.
Transformers traditionally have had a bit of an
issue compared to RNNs
because RNNs
kind of naturally have a belief state because it's it's whatever
bottleneck you have in in a recursive
neural network. Transformers don't have that.
This is a way to make transformers do that.
So we can make transformers dance in a new way.
Okay. So now what what goes wrong with this Let's
start with, the downside. Downside is you're gonna
do a little bit more compute.
So I'm going to compare
several different approaches. For
computing
the next token belief states
and belief state like things.
So GPT is standard next token
prediction. We're gonna be able to train at,
3.72 iterations per second.
This is on, some little data set.
That's tiny stores.
And then, if you do next latent,
you just go one ahead, which is
sufficient you
get 3.26.
So this is about a 12% slowdown.
And you're training I expect that as
you scale up, this becomes more and more negligible
because the head complexity becomes
a smaller part of the overall compute.
As you, scale up a transformer.
There's several other possibilities here. One of them is this
belief state transformer. This is our first attempt
to create belief state with transformers.
This is a more complex approach. I don't wanna
get into the details of it, but, oops.
It it's it's a lot slower. To compute.
It involves quite a bit more computation.
It works, but it's heavy.
This is multi token prediction. This is,
the Facebook style multi token prediction where
you put you have you have to kind of predict each
of the following tokens. Each each of
the tokens. So you get the the marginals
of the next tokens.
And it's it's a bit
heavier, compute wise.
You also see later that, next latent
with one
ahead is about as good as multi
token prediction eight ahead.
In terms of how much information they can suck in
effective.
In token prediction is a is a variant,
where instead of just predicting the
marginals of the future tokens, you predict
the joint of the future tokens using a
teacher forced version of things.
It's, also I
mean, it's a little bit faster computationally, but
you'll see that the next latent is
indeed more effective.
In practice. So we're
paying 12% at most, probably less when
we scale up. This is the downside.
Of of doing. This.
Alright. So now what goes right So we
talked about extrapolation error reduction.
Why do we get extrapolation
error reduction if we do this
I've kinda hinted at this. But
I I think about this
by an analogy.
It's sort of miraculous. How how do you
minimize errors on things that you don't
even optimize for Right I
mean, it's sort of weird.
It used to be the case
for many centuries over a
millennium The people predict
where the planets would be in the sky.
And they had a system for doing this.
Right That system involved this kind
of epicycles,
and it could predict where plants would be in the sky.
That worked. So we had a way to explain
our observations effectively.
Later on,
Copernicus came up with the Heliocentric
theory. Right
So earth was no longer the
center of the universe, instead it was the sun
and you had orbits, and you would calculate things
based on the orbit.
It turns out that Copernicus
theory works on Mars.
The epicycle system does not. Or
you you can design a new version, but you have to,
like, have all new constants. Things like that.
We switched, of course, to the heli centric approach
before we ever observed anything from Mars.
And the reason why is because it's a simpler.
So there's this this
understanding in science that you want a simple correct
theory. And I think that's what we're
driving for here. We're trying to get a simple correct theory
of observation.
That's kind of the key driver of
getting a good compact world model.
Alright. So there are other things that go right as well.
We discussed Monte Carlo
search. We haven't actually done that. But we've done
something close to that. Which is
implicit look ahead planning.
There's a there's a problem there's several problems which
are kind of combinatorial problems that
are known to be hard for
next token prediction.
This is the countdown problem. Kinda
famous because I think on GPT four
got 4%. It's not very good.
But the idea is you have some numbers and you wanna try
to come up with a a target number. 24 in this
case.
Okay. So you have some data set. It has
about 500,000 elements in it.
And if you train a next token predictor,
you get
about one third accuracy. On
this dataset. Or on
a on a held out version of the dataset.
K. So we're not trying to do any extrapolation here.
All we're trying to do actually just predict
well And the prediction problem is
hard because you kinda have to think ahead.
In order to figure out what to do at this time step.
And so if you're trying to do next token prediction,
thinking ahead doesn't work
very well. Not nearly as well as you
might hope.
But if you're using these other approaches, you actually
start getting some advantage. Right So
there's the blue state transformer,
all all these other ones we talked about before.
I think what's interesting here is that next latent,
just one step ahead,
gets a big bump in performance. It's like almost
it's 20% improvement
in in accuracy.
And you get a little bit more from,
from doing things a little bit further.
So I think the way to think
about this is, you get a belief state.
The belief state has a lot of information about the future
in it. Because there's information
with the future in the belief state, it's
easier for the gradients to connect,
in terms of what you should do in the current time step.
So even though we're doing no chain of thought,
nothing like that, just
doing a reflexive policy because the gradients have an
easier way to connect you end up learning
a a more powerful policy.
Okay. And then, you also have
essentially a lot of free look ahead info. This
is, a graph that had
tells a lot of stories in it. Have to kinda look
at it closely.
We're we're training on, tiny stories.
You just said a little text dataset.
And whenever you're you're doing these,
kind of these different loss functions,
multiple objectives, you you have some sort of Lagrangian
between them. So you can kind of tune
the Ligrangian in various ways.
Is, of course, one particular tuning.
But the the tuning here is designed
so that these other
approaches compete with next latent.
In terms of their look ahead information.
So we're probing the belief state
with a linear probe to see how
much information there is about
future locations.
And then we're going to relativize that to
what you get from
just next prediction So the
so the baseline here is zero. Is
is what next token prediction does. And then when you ask,
you know, how much, improvement in log
do we have for the future
Okay. So
so let's look at this closely. So first look
at at next token. This is just
just do one token ahead. Next
token, alone is optimized for
this. Next latent
pays nothing. Surprisingly.
With the way they we had
tuned it. These other
approaches are paying a bit more
in terms of their, log loss.
And then and by the way, I think
that's real. I've You talk
to some people who play with these things, and often it's
a little challenging to avoid paying
a little bit in the log loss. The
next token prediction.
And now you look at what happened. So
MTP and JTP,
going one ahead are,
mean, they do very well when you're just going one ahead.
But they die die off.
Rapidly. Terms of the information about the future.
Right The the light yellow and the light green kinda die off
rapidly. You can do further ahead.
And and what you see is that they don't die off quite
rapidly, but they still do die off once you get
out well beyond eight.
And then you can look at next latent and and
even the next latent at d equals one
k, it's not quite as strong in in
the position in the next next token. But
it stays strong and it keeps being relatively
strong even out to 20.
And you can go, to the next lady
eight out, and and it is it's strong all the way
out there.
So this is a what we're
observing here is that we're paying nothing
in terms of our prediction
accuracy. Next token prediction accuracy.
To have quite a bit of information about the
future embedded into our
belief state.
Why is it possible Well, you only need about 18
dimensions to
be able to decode to the typical vocabulary size
of a decoder.
So there's a thousand more dimensions
that are just sitting there
relatively unused in these transformers.
So there's there's lots of capacity to do much
more.
Okay. So I'm about
finished. The paper's
here, if you wanna take a look at it. I'm
excited Oh, not the paper.
Interesting. I'll
Paper is on archive.
Somehow it got changed.
The compact world of model, I think, is a key
to having better extrapolation.
And to nailing this look ahead problem.
It may also be quite key to the exploration.
We haven't done experiments for that but
I think it
it could be critical or the active learning.
Next latent is a very easy way
comparatively, to get a compact world model. So this
is a generic approach
to convert a POMDP
into an MDP. That's that's that's
very powerful.
There's lots to do here. People are interested,
And if if you want to play with these
things, come and talk to me.
There's for every one of these question
marks, I can tell you a story about why
this is an important problem to work
I I think
there's fantastic things that we will discover here
the next year or two.
And then there's a related work
there's this Sikh Jeppa which is at this neurops,
I believe. They're
not using action as a target, but they
are doing something
pretty similar.
Approximate information,
states. Is where
we are actually learning
an approximate information state. And I was
seeing that the ASTRA paper, poster over
there He's also talking about
information states. So if you
know the literature for approximate information states,
that is indeed the kind of thing that we
are aiming at.
Alright. Let me stop here.
Are there other
questions
If you have question, you can come to
the center.
Use the microphone.
Hi. Thanks for the amazing talk. I
had a question. So
there's like some work in
adjacent fields where people
try to come up with vocabularies
in the latent space So in
stuff like as you mentioned, like,
things being done in pixel spaces, like, the dimensionality
is too high. People have tried to come up
with vocabularies in the latent space.
You think that next
latent prediction is essentially trying to learn
that vocabulary itself While Yeah.
So I mean,
there is a sense in which you're getting a vocabulary,
but it it's it's a continuous thing
rather than a discrete thing.
There's a strong reason to think about discrete
things, which is discrete things have a built in error correction
mechanism. It may or
may not be necessary. To
do good long range prediction.
There's also the diffusion approaches for doing
error correction. Which potentially could
be used. But,
I do think that it's an interesting
direction to be looking at.
Right Is
there particular I guess finding
discreet a discrete vocabulary could
be a good multiplier on how far out you
can do faithful simulations. So like for example,
even while you train the sort of
next like, you have some regression function
in your paper which predicts the next latent. Right
Yeah. But right now, it
doesn't enforce, sort of
discreteness. So the whole thing is still
like, continuous. I also was, like, curious
if you have any thoughts on because this is, like, opposite
of what VQA used to do, right,
where just wondering if you
have any thoughts on that. I think it could be powerful.
Honestly, we haven't pushed the limits of
how far out you can get the simulation to go.
And it it I could easily imagine that,
you come up to me, in a month
and you tell me, hey. We
can push out the simulation a 100 times
further if we use a BQBA
in the process. Thank
you.
Due to the time limitation, we will
have just one more question. One more.
Okay. K.
Hi. Hi, doctor,
Langford. Really nice talk. I have a
question which you, I guess, you initially
pointed out in the first half of the talk.
That transformers are already world models
but they're world is not persistent.
So the states are not persistent, So
they move around a lot because the
key and value thing. So
transform yourself quite a bit already
solved this long context
of attention. So this key and values
have become more persistent because now you can
have everything in it like, a
million token size or something.
The tea has been grown a lot.
So given that we have achieved
transformers with really long context, which are
kind of persistent belief state
Why where do you think
that additional relief state which is work that you've
shown, a persistent memory can play a
Thank you. Yeah.
So, think the observation here is that
people have managed to create
longer and longer context.
And reading between the lines of various
public announcements, they have some
compactification process.
Seems like a good idea. I guess the same
it's it's it's it's aiming at a similar thing.
Right So you're you're creating a more concise representation
I I don't know, of course, what's going on inside of
OpenAI or or these other companies.
But, what
I do know is that
if you get the right objective,
it makes a huge difference in how easy
it is to do these things. So,
this is a relatively easy way
to do to to create a belief state. Think it's probably gonna be
widely useful. And I
could imagine that it composes with whatever they're
doing internally at these companies.
But in the public world,
this seems like it's a great thing.
Thank you.
Thank you very much again for your fantastic
talk. And let's welcome
the next speaker. Yilung,
Yilung Du from Harvard University.
He's currently an assistant professor
at Harvard and
Camp Kemptor Institute and the CS.
He's research interest about building
models for word using generative
AI And his,
top title will be
building intelligent robots with warped
models. Let's welcome Yiren
Du.
Okay. Okay.
Great. Thanks so much for the introduction, and thanks
everyone for coming. So today, I'll
share some thoughts about how we can to
build more intelligent robots, using world models.
So there's a lot of interesting language modeling
And when we look at language models, they're able to do
these really fascinating things. Such
as if you asked a language model how to build
really powerful AI system, it can give you these step
by step instructions. And while
there's a ton of work, on VLA
VLA models, or models which, use
language models as a way to build intelligent
robots, One big issue with
language modeling that these models actually aren't that
good at physics. So even when you give it
a pretty simple scene here where you have two objects,
want to ask it where should I place a ball so
that the screen the screen green circle
can drop in this cup It will just generate
some nonsense. So I think that one
big issue is that language ends up not having
much information about how to act in the
physical world. This talk will be about
how we can really use video models as
a way to really get a lot more physical information
about the world and how they these can be a basis
for building more intelligent
So in this talk, I'll talk about three different
ways in which we can really use video models for more
intelligent robot systems. So first
I'll talk about how these video models can be seen
as planners. For decision making.
Then I'll talk about how they can be seen as simulators
or dynamics models and finally, I'll
talk about how we can really, use these video
models in combination with other models
to do more complex tasks.
So first, let me talk a bit about how these video models
can be planners. And the overall
idea is that, given the starting
image, and a text instruction, can
imagine that a video generation model
essentially synthesize a visual plan of what you want
to do in a task. So if I
if I want to, take, pick up this
get gas, then a video model can
generate, something like actually picking up
this nozzle. Or if I want to really pick
up this tissue paper, the video generation
model can generate how what it means to pick up this
tissue. And similarly,
this type of video generation works across different
robot embodiments. So that if you want to
generate something where you want to put the screwdriver
on top of this tissue box, again, the video
model can be used to generate a
plan of this. So video gives you a
nice way to encode how to solve different
tasks across a wide variety of different
embodiments in robots.
And one important thing about video
models is that it really allows us to directly
learn a lot of the physical videos on
the Internet. There are many videos on YouTube
about what it means to open a door, what
it means to clean your room, or to cook,
cook dinner, And really by training a video
model, using it as some type of visual planner,
which describes what you want to do in the future,
allows you to really explain and use all the information
on the Internet. And once
you have this type of video generation model,
you can just have a separate process that actually takes
the generated videos and then converts
it into actions. Whether it's some type of learned
neural network or a couple other different approaches
I'll talk about in a bit. And here's
just a brief example of this. So we're,
we're about to release a large
video planning model in the next week. But
here, we just took an image
image of this table with a piece of tape,
And this is a generated video plan. So you can see
it's very realistic. And then we can estimate
maybe some hand pulls. We can do some type of
three d reconstruction. And this allows us
then to take this motion and really execute it
on the physical robot. So you can imagine
that, like, video generation can be directly
converted into real robot execution.
So in terms of, how do you
actually, get actions from video
one straightforward way is to just have some type of inverse
dynamics model. Where what you learn
from your data, another thing you can do oftentimes
is that you can actually just directly
extract or determine what you want to how you want to act
from the video itself. So once you have a set
of image states, what you can do is you
can, for instance, estimate the optical
flow of how, the objects in the
video are moving and once you have this
type of optical flow, as well as some depth, and
maybe you can even have some type of three d flow,
you can just directly determine what are the states
of the road that you want to execute to
accomplish this task.
And as an example, let's
say here we want to pick up a run and
we want to put it on top, like,
so here we want to pick up this
We we can simply just take our
generated video estimate it estimate
the appropriate optical flow of the robot,
arm, and then from there, we can solve for transforms allow us to instantiate
this in the physical world. And this
works not just for manipulation, but you could also
imagine applying this for navigation. So
here you have a generated video
over here. From the generated video, you can estimate the optical
flow of how the scene is changing
from the chain from the predicted optical flow, as
well as depth map, can infer
the corresponding robot action that you want to take,
to instantiate this task.
And here's just a brief example. So here, this
is a zero shot approach where we estimate the
optical flow and we're able to pick up an
apple and place it down.
And and another and another and another thing you
can also do, when you have some type of video
generated model, you can actually learn a
policy to actually convert the video
videos to concrete actions to execute in
a completely online online learning manner.
And the idea here is you have your video
model that generates these visual goals,
of where you want to reach, and you can
actually use these visual goals as a way to
explore it in your online environment
to gather data, to train your policy,
to be able to reach those
goals states. And the idea is, you
essentially take your video model. You have it generate
some visual goal. And then you just simply
have initially a random policy that
tries to reach those goals. And whatever goals it
actually reaches, just relabel
that data a goal of the policy.
You essentially gather these
free goal condition trajectories
in the real robot, And as you repeat
this procedure over and over again, eventually, you learn
to reach your final goal.
Here's an illustration of this. So here we have
this, goal where we want to pick up this cup and we wanna
place it on a plate. And
what we can do is without any rollouts, so
here you just have a random model While the
random model isn't able to ever, like, ever
solve this task, But by interacting
with the environment for a while, you can learn this
model that can somewhat shakily put this cup
on this plate. And by doing
this for a very long time period, you can gradually get
a very accurate policy that can
exactly put the mug top of the plate.
And then, and in general, we find
that, this this type of video planning,
works pretty well in open world settings.
So here are these, so the next couple videos I'm
gonna show are just in the wild images.
We asked, some graduate students to just take
some images, of their dorm. Again, this is a model
that we're gonna release in probably in the next week.
But we we find that if you want to insert a straw into
the cup, we can find that it's able to accurately
do this. We also find that these
models are able to generate these longer horizon
tasks. So this is again generated, but you can
put the straw in the cup but now you can actually
put push the cup to a mouse.
You can also have this instruction here where you
can pick up a blue book.
And actually place it on top of a red book.
And and in general, you can you can
actually
the they tend to be, follow language
instructions much more accurately than
VLA models. But we find this, for instance, if you
have a set of objects here, we can actually
so this is generated video of putting a cookie can
on top of green cube. But again, we
can have we can also take the same instruction
and, you can put a green cube on the Coke
can. There's some intuition for this. So
when you look at VLA models, really
like you're often just predicting the next short chunk of
actions, so it's hard for the models to learn as
Or with your actions. But here with this more video
based planning approach, actually learn a correlation between
the entire generated video plan and
language actions. So so oftentimes when you try these
things on like the existing VLA models, they simply
ignore the language instruction and will always do the
same action. But here's
another thing you can do. So you can also say, we want to
put that cocaine on a men's or
so forth. And and
just as another couple examples,
of how you can actually take these video plans
and actually convert them
into corresponding robot actions So again,
this is generated video. So if we say spread coffee
beans, can do this. And then again,
we can just retarget on a robot. And have
it do the corresponding actions.
With with some lack of precision. That's
some models are a bit slow right now.
And we can here's I get another one. So here we can
this is generated. So we generate close the laptop,
we can extract some hand poses, do some reconstruction.
And then correspondingly execute it on the same setting.
And,
as as it can be for some of those videos, there
is still a part that's missing where, like, how do you
actually convert these videos to, like, the concrete
robot actions So I think that's an interesting
direction for future work. But in general, the idea is
you can kinda use these models
as a way to imagine what are the what are
the next stage you want to reach then use
those as visual goals then to to actually
to actually execute your actions in the environment.
That's the first part on how you can really use these models
as planners. Another thing you can do with these
models is use them more directly as some type of
dynamics model.
And essentially, one thing you can do is you can kinda have
these models be something similar to
like a kind of interactive simulator, just given some starting
images, you can just simulate all types of different
actions on the world. So you can simulate, picking
up a bowl, you can simulate putting carrots in the bowl,
and so forth. So let me show a concrete
example on the robotics setting. So here
we have the starting image. And we can just roughly
simulate this sequence of instructions.
So
one thing that's interesting about these models when you
treat them as these dynamics models that you
can actually simulate very long horizon rollouts.
In the world. And this can allow you to
simulate try to simulate and solve
longer horizon tasks. And I'll talk a bit more
later how we can really get these models to really do
very long horizon simulation.
But one thing but once you have this simulator,
one nice thing that doesn't lock is it allows
you to directly, train your policies in
the in the space of the generative simulator.
So by drag by once you have
trained this, interactive dynamics model,
you can just directly simulate these
actions. So the, simulate these actions, block
pushes, inside entirely inside
the space of the generative model. And
you can just directly train a policy directly on
the simulated data. And once you have
this policy trained on simulated data, you
can just directly zero shot deploy them in the physical
world. So this allows so this type of simulation
then allows you to really build
systems that that can do reinforcement learning in
a very safe manner.
Another thing you can do with these dynamics models
is that you can do some type of modeled
predictive control at prediction time
to actually infer the actions you want to take. Where
here what you do is you simply have your policy
predict different possible sequences of actions
use a role model to simulate what
happened in the road in response to each of these actions,
then you just select the action trajectory
that has the maximum reward. So here's an illustration of this.
So here we have a policy or some action proposal
model that proposes a sequence of actions
The world model simulates all of them
You you estimate some reward for the
for each of the simulations, choose the one
that has the highest reward, and just repeat this process,
allowing you to, to refine
actions until you reach your final goal.
And one really nice and and you can see that here's
at the end is when it reaches the final goal.
And one really nice thing about this is when your
environment has pretty complicated dynamics,
so when there are several objects obstructing your
goal, then this procedure allows you to more
accurately do these like
like consider these more collisions between objects.
So it allows you to, accurately,
move your move this object to the c,
even when there are a lot of collisions, where eventually it
reaches this final goal. We find that in these settings
where there's a lot of contacts, we find that
these behavioral cloning methods tend to struggle
a bit more. So one of these models can be both used
during training time to,
as a way to train your policies, but also at test time
directly through model predictive control,
to directly control your robot.
And these models don't have to just be
in the two d image space. So
the road is the physical road is in
three d, you can actually also have these models
operate in the space as three d. Where in
this setting what we do is instead of just directly
predicting the RGB pixels, we the
both the depth map as well as a normal map.
And this allows you to essentially get a three d
surface, which allows you to much more
accurately, extract the actions necessary
to solve a task, as well as simulate the three d
surface.
And, yeah, and and like and this is roughly
what does that accounts do. Here
you just, the model generates
both this, video. The depth is raw as
the corresponding normal. And by combining
all three of these things together, you can get
a more complete three d
model of the world.
And you can also use this not only to get
a surface, but you can also imagine like training
some type type of three d latent model. So
here, so instead of instead of just directly predicting
the surface, of the future, you
can actually have the model predict
an overall three d latent. This allows
you to get a much more three d, consistent
reconstruction of the entire scene, so
allows you to accurately do more precise tasks.
And and and here's an example of this. So we find
is, so now now you predict the
latent states, the latent state actually captures entire
three d scene. So here is the first, first
latent that's predicted. So you can see that it's actually
full fully three d complete. For maybe a for
a set of partial images. But you can
simulate, so here the task is to
pick up the bow and put it in the plate. But
now it's here it's able to more accurately simulate
picking up the ball. So now this
is the third step of placing it down.
The fourth step of placing it actually in the plate. So you're
gonna see that by actually directly predicting in
a three d latin, you can actually also get
a full
full three d reconstruction of the scene.
So so one thing about all of what I've talked
about so far is all of these dynamics models
are chunk based in the sense that, I'm just directly
predicting a sequence of future actions
or a sequence of future states at once. There,
in some sense, it's actually nicer if we can have more
of an autoregressive model. Something similar
to LLMs, where you just simply
predict the next image. Given the next image, predict
the next action, and auto aggressively roll
out the dynamics model over time.
And and as as rehashed so far with most
of the dynamics models we talked about are much
more of these chunk based models
where you get this entire Gaussian noise
trajectory. Just directly generate the entire
trajectory all at once.
And in a sense, when we thought when we think about,
these dynamics models, there are
some, there are actually quite a few
advantages towards having the dynamics model be
a bit more autoregressive in nature.
And the the nice thing about having more of an autoregressive
model is that, you can
actually, do some type of tree search with the model.
So you can imagine that roll out simulate different
possible future states and then determine,
for some type of monoclonal search, which are
the right trajectories to do, by,
you can also, by changing the attention
mask, you can roughly enforce that,
dynamics model is Markovian or satisfies,
or, like, or in general,
is less causally dependent on the other parts of
the trajectory, You can also make the dynamics
model have some type of causal uncertainty.
So that states that are more that you're more uncertain
about in the future.
Are are are predicted later. And you can finally, you
can have these dynamics models have much more flexible
horizon, predictions. And in contrast,
all of these dynamics models we've talked about earlier,
they you they they generate
everything all at once, so it makes all of these four
properties a little difficult. Although
one advantage of directly predicting the dynamics
all at once is that it allows you to
directly directly steer your
entire trajectory if you want to do some type of planning.
So so you can also try to
blend a bit more of these autoregressive
models into your dynamics models.
And the idea here is we will view
the diffusion process along two axes.
So instead of, so now
we will have two axes in which we do denoising.
So one is along the time dimension,
Another one is along the noise dimension.
And the overall idea here is when you have
some type of autoregressive model, what you
do is the auto is this that the autoregressive
model just simply would just simply
given noisy image, it
would just directly generate the next image to
predict. But,
similar to LLM, while when
you have some type of diffusion model, the diffusion model
will directly predict
all the future frames that you want to reach.
But you can, where where here you start from full
wiggas and noise on everything, and you
simultaneously denoise everything to generate your
final clean output.
And so in this paper, we we try to
really this type of diffusion road model
where you're doing things at the chunk level with
this autoregressive model we do is
we simply add different levels of noise
to every single, token that you want
to predict. So here what happens here
is is that, you the model predicts
this increasing increasing
levels of noise, trajectory.
So the model takes in a trajectory where the
next immediate state has very low noise, the
state afterwards has higher noise, and
an increasingly higher noise. And then the model
then learns to predict what will be
the less noisy version of this entire
trajectory. But what this
enables you to do at sampling time is it allows
you to sample in a more autoregressive
manner where states are earlier in time,
denoise them first, into these, into
these clean images, And then later on,
you then denoise or simulate
these later frames. With higher
levels of noise. So by so by training your diffusion
process with two axes where you have
different levels of noise at every single time step,
you can now denoise or
simulate the model in an autoregressive
manner. And
what we find with this type of autoregressive simulation
is that it allows you to do pretty long horizon rollouts.
So at this so here it allows you to move
around in this, in this environment,
and and and over time,
like, be able to, simulate
a very long time period.
And and, when you do these very long horizon
dynamics simulations, what you'll find is that there's
still a bit of a blow up So what
you find is as you run these simulations,
as as time goes on for a very long time period,
eventually you do get a lot of noise
So additional thing you can do to try to get much
more long horizon
dynamic simulations to do some type of
more low temperature like sampling based off your history,
And and this is typically done in diffusion
models through classifier free guidance. Where
you combine these unconditional and conditional
scores. We do,
so so we, to to be able to do these
long horizon simulations,
where low temperature sampling was a history,
we can actually, use this type of denoising
procedure to be able to do this more effectively also.
And the idea is, idea here
is that, our diffusion model
when trying to predict the future, which is
shown here, with this
idea with this diffusion forcing technique,
is trained on the history corrupted with
different noise levels. So
to so in order to predict
predict the future, given
given a pretty long horizon set
of history, you do is you simply just corrupt
the history with different levels of noise, whether you
corrupt it with full noise, a small amount
of noise, or like, or a short amount
of history, or over low frequency data.
And this allows you to essentially decompose
your generation into these different noise corrupted versions
of your history. Allowing you to
avoid going out of distribution as much
your history is very long. Because now you've decomposed
it into things that are a bit more familiar.
And what we find is that by,
by by adding different levels of noise to history and then composing
them, we can now do something similar to Classroom
Fire Free Guidance, along the history
dimension. And what this allows us to do is it just
allows us to build these models that can very
smoothly run over a very long time period
of time. Allowing you to run very long
horizon simulations.
And one thing you'll note is,
for a lot of these simulations,
if you look at, if you look at some trajectories and you
look back, suddenly the entire scene changes. So there's
still some issues on trying to really build
a bit more, consistent three d memory into these
And and we have some preliminary work at at the
NeurIPS Conference on trying to build this. But I
think in general this is a very interesting direction.
Of work. You can imagine trying to build
something more like a three d three d and
persistent to represent the world state
over time. Then there's the question of how do you
actually simulate the world state world state
when the world itself is also dynamically changing.
Okay. So that that was about how like
these video models can be used more as a
dynamic simulator. Finally, I'll talk a bit about
how we can also in a hybrid
hybrid way, actually combine these dynamics
models, with other, vision language models
to do more, long horizon tasks or
do tasks that require a bit of both high
level reasoning as well as low level visual
reasoning.
And and one idea here is that
let's say we want to generate some very long horizon
video generation. We want to generate
a very long horizon video task for
something that we, that for for some goal that
we want to reach. Where we don't really know
exactly what we should do, reach that goal.
What we can do is we can really try to combine
both the vision language model as well as the video
model together solve these long horizon
tasks. So as a
hypothetical example, let's say that I
want to solve this task of putting all the fruits
in the top drawer. What I
can do is I can try to ask a vision language
model, predict what I should do in this setting.
So given this image on the left, the model
can predict to either place the banana in the top
drawer, or it can predict how,
to open the top drawer.
Now the issue is the vision language model
isn't really able to fully understand the physics of the scene, so
it doesn't know which of these instructions
should be the one that should be chosen.
But you can what you can do is you can simply
take each of these language instructions and
have the video model simulate how
the road will execute, will change if you do this
task. So if you try to put the banana in the
top drawer, because the top drawer is not open,
it will simply drop the banana.
And in contrast, if you try to
open the drawer, it will actually correctly open
the drawer.
And now you can use the vision language model
to take each of these video plans and estimate
how much progress they're making. And
estimating whether this goal is
the goal you want to reach is a simpler
task for the VLM than actually generating
the plan, it's just simply trying to verify
whether the final state reaches
a state that it it wants to reach. This
allows you to essentially, help the VLM
understand or estimate what the progress in
the physical world will be.
So once you have all three of these components,
now you can do an overall tree search to
get a very long horizon task
that depicts solving this task. So what
you can do is in the first time steps, you can
do what I, what we showed earlier, where the vision language
model predicts two actions. Either opening
the top drawer placing the banana in the top drawer.
Would estimate some progress on each
of these states, choose the one that has the most progress. And
then from that last state, repeat
the process having the video, vision, language model predict
an action to execute. Select the most
promising one, and repeat it over the
entire entire time period until you get an overall
long horizon video plan for solving this task.
And then after you have this video plan, you can also
convert it to corresponding real robot execution.
I'll skip the rest.
And, and here's a here's an actual illustration
of this, in more detail.
So here here we have, the the two, two fruits.
And we can see that this approach is able
to get this video plan. And from the video plan, it's
actually able to infer a set of actions
to execute in the environment
so that you can, place both of these fruits in
the drawer, also finally close
the drawer.
And here's another example of this. So here we
want to we have all these blocks. And we really
want to move all these blocks to form a line.
And this procedure is able to dynamically generate
the sequence of plans for moving the blocks
allowing us to get this,
overall synthetic, video
sequence plan of moving these blocks precisely to
form this overall line.
And you can also convert it to a corresponding
real robot execution.
And what we find with these pretty long horizon
tasks is this type of planning you
can do by using a combination of the video
model as well as the vision language model,
works a lot better than these end to end
approaches, where, like, where you
just have a VLA model or even just
you have a semi single video model that just directly
predicts what to do, because,
this these before these long horizon tasks,
this type of planning allows you to more carefully
reason about the actions to execute so
that you can directly and finally solve your overall task.
And the final thing I'll talk a bit about is how we can
also use a combination of these video models
and DLMs to really, like,
improve the spatial reasoning or physical reasoning
of these vision language models. And as an
example here is, we have this question
I sit on the couch on my right,
and I face the chairs, the kitchen be on my right or
left And this is a question that is pretty important
if you want a physical robot to navigate in your environment.
But tends to be something that is
very challenging existing vision language models that can't really
reason for effectively spatially.
But here, you can again use the video model as
a way to really help you solve this
task. Where the idea is you can use a
video model as some type of mental
imagination that can imagine how
how the environment would change if you were
to navigate inside of it. And by taking
these, imaginations of how the environment would
change, this imagination
in context
can then be used as data to allow the VLM
to accurately answer what to do in the environment.
So So here's the graphic illustrating this.
So here I have the question, if I sit on the couch and face
the chairs, will the kitchen be on my right or
left And now, essentially,
this what this procedure does is a v
is it uses a VLM to
estimate what would be useful actions in this
environment so, like, so estimates these
trajectories here. That that might be
useful for solving the task. Then we
have this video model be its imagination
it takes each of these trajectories,
imagined visuals
these environments. And now once you have these
imagined visuals, the
VLM can take, take this additional context
and use it to reliably answer this final
question. So you're using this,
video generation model as a source of spatial common
sense that you can then augment the language
model with. To accurately answer this question.
And then this is just a more detailed,
GIF showing this. Where
essentially the the model predicts the sequence of
trajectories to execute. Also have a bit
of a beam search procedure to find the
right set of actions to execute in the environment to solve
this task. But we just choose the most relevant
ones and use them to to solve the
final trajectory in this task.
And overall, what we find is that when you have this
type of spatial imagination, you get a pretty
large boost in performance. Compared to
when you don't. Across a wide variety of
models from GPT, o one,
these open source vision language models. So essentially,
these video models provide a very useful
source of common sense that's orthogonal
to the high level of reasoning abilities of
the language model.
So overall in this talk, I've talked about a
couple of different ways in which
how in which video models can really be seen
to help robotics. At first, they can
be seen as these high level planners that allow you
to imagine what it would mean to
accomplish a task. Second, you can
use these models, as a type of simulator
to simulate or to or to
find the right set of actions to execute something
in the environment And finally, you can
really combine them with with a high level
reasoning from vision language models to jointly
solve these complex tasks that require both a
bit of high level reasoning as well as a bit of
low level or spatial or visual
reasoning. Thanks everyone for listening and happy to take
any questions.
Thank you, professor, for the
for the fantastic talk. I really learned a lot from it.
You mentioned that using video models for
robotic control and you said that it's
better at instructive following. But
I really don't understand if it's necessary
to predict the whole future, the video frames
actions that could be spontaneous for human
What's your take on that Thank you. Yeah. Thanks for the
question. So I think for a lot of tasks, it can
be helpful to imagine it would
mean to accomplish the whole task. But then, of course, there
are many actions where maybe you would not want to
imagine the whole thing. Right So certain actions, maybe
it's a bit more reactive and you just have
policy for it. But I think for a for a certain class of tasks,
it is very helpful to imagine what it would mean to
do the entire scene.
Yeah. Hi. Hi. Thank you for the talk.
I have question about using video
word model for planning. So
do you think that it's gonna be
break if I
use this planning for
highly sarcastic or
very dynamic environment. So
because I suspect that first
of all, it takes a lot of time to do or
search into to planning using
video word model, especially when you generate
frame by frame. And I suspect that it's,
this gonna break in, like, real
live environment when you have a lot of, changing.
Element
And yeah. Yeah. That's
all. Yeah. Yeah. Thanks for the question. No. I I think you're
right. I think for a very dynamic environments,
it can be hard for the video models to operate
at a quick enough frequency that you can
actually respond to the dynamic changes. Like, it
could be the case that the time the video
model generates these frames, already too late for you
to accomplish the dynamic task. I think in
these settings, might want to use
a more compact representation of the state, like
maybe some lower dimensional latent that
allows you to be a bit more reactive. I mean,
another thing issue with dynamic tasks is all the video models I've
talked about so far just in the image space. So they
don't they don't have any information about contact
or forces. I think it would be interesting also
to think about how you can integrate these information
into the models to more accurately solve these more
dexterous or dynamic tasks. Thank you.
Professor. Thank you for the talk.
Earlier you mentioned that some
of these they may not have,
like, object permits. I was wondering
if these are dynamics models, if
through different type of training would be able to eventually
accomplish this Or if you would a
three d belief or memory
Yeah. I think it's, it's, not really
at the moment. I think both are very interesting to explore.
I think that the current models are still
quite small, and maybe they they might be more
similar to like what GPT one was like, many
years ago. I think it's possible
that with these much larger models, as well as much better,
like, post training, they could already have
very persistent objects. But then it's also
the the case that let's say you're in a room and you really
want to model to remember objects for a period of
like two or three hours. It seems like having
some type of spatial memory might be very helpful.
Thank you.
Thanks for the talk. How much compute does
these video models add to the entire
pie Like, if you didn't have the video module
at all and try to do it with,
I don't know, BLM or whatever, How
much does this add to it and what video
models are you using here Yeah. So
for a lot of the examples I showed, especially
in planner section, we're basing it off
the one two point one model. We are
using that 14,000,000,000 parameter model.
So to train it, it did take around
16 or 32 GPUs, so it is quite
a bit more expensive. I think that,
in the future, maybe when we have more powerful models,
could imagine that maybe you just have to learn a very
small, like, adapter on top of them, and then maybe
you only need, like, a couple GPUs. But at
the moment, it is a little more expensive.
Was it a division model It was a
diffusion model. Yes.
Hi. Thanks for the great talk.
Video generation involves going from
low dimensional latence to high dimensional pixel
space. And the perception is the opposite
going compressing to some latent I'm wondering do
you think it's necessary to go through pixel
space Or can we just cut through the
latence Yeah. I think that's a great question. And I think
in a sense, yeah. When you try
to predict all the pixels, you're wasting a lot of
your time because maybe, like, for, like, robotics,
you don't care exactly what the precise textures
are. Think the reason why videos are nice and
pixels are nice is at least it does, when
you have some type of handcrafted latent space,
you don't know what you're missing, I guess. So, like, maybe
you trained your latent space on a couple of tasks, and
something and these things, and, and maybe the
texture didn't matter, then there might be one task
where the texture really does matter. So I feel
that images are a complete space where you don't
lose information per se. So in that sense, they're better.
But like for many specific tasks, it may be better
to have a more compact latent space.
Thank you. Thank you.
What you have just shown are mostly
What When it comes to, like, deformable object,
what do you think about the word model Can
do Yeah. That's a great question. So I think,
I I would imagine that you could also do things
are somewhat deformable. I guess on the planner setting,
I did show a couple of things like picking up that coffee meeting
and so forth where there was it was a bit
deformable. I think that the control problem gets a
little harder. When you have these deformable things, and I
think you really have to have a good learned policy.
I think, as long as you have a good learned policy, I
think it should be okay also.
Okay. Thank you. Thank you.
Thank you very much for your fantastic
talk. And let's give
pause to our speaker, Yilun Du.
Well, let me introduce
our next speaker from Google DeepMind,
Jenny team. And,
Philip and Stephen
Spencer from Google DeepMind.
Oh, you are here. We are looking for we
are looking for you for a long time. Okay.
Oh, yeah.
So just plugging this in.
Oh,
I'm sorry. There.
It's not
and I'm like,
Can you
Can you connect the conference
Yes. Should
The second I come on stage, this
I think so. Yeah, we the presenter
view.
I mean,
so bad.
Subscriptions on point
But
Yeah. I don't know. It's just not showing
Yeah. Because you're using the
the extension screen.
Now I can set
because you want this speaker note Notes.
Okay.
Awesome. Okay.
I mean, how are we gonna do the speaking
notes We should just do it live.
Yeah. I don't really know where they
Okay.
It's just honestly,
should we just use your laptop Yeah. Yes.
With this.
Got it.
Can you help
I can try.
Alright. Sorry about that long delay.
We figured it out.
So, yeah, I, I'm here today to talk
about, Genie with Phil. So
I'm Steven, this is Phil. We
both worked on G and E three and also on
G and E two. But,
of course, we weren't alone.
This is the Genie team. I'd
just like to acknowledge, it was quite a large
team that together to generate Genie three.
And without the contributions of all these
people. None of it would have been possible.
So just to give you an idea of what I'm going to talk about,
I'm going to try and put Genie in context,
tell you why DeepMind
cares about world models and why we're working on them,
talk to you about the beginnings of
the Genie three project and then move on
to the capabilities of the model
that we discovered, after we trained the model,
And then I'll hand over to Phil, and he will talk more
about the possibilities of
using world models for training agents.
I'm sure you're all aware of DeepMind's
success in training agents in
simulated environments from Atari
to AlphaGo to
Starcraft And these
allow agents to kind of train on their own experience
and learn interesting things about the
world the famous move 37
in, the Lisa doll game
as prime example of that.
But we also, train
agents in simulated
environments like these physical
simulations of the world. So this is some fun
early work from 2017
training, kind of human locomotion
in Mujoko, which is a
a kind of a physics simulator.
I love these. I love these videos.
They're crazy, but they they get the job done.
And lastly, this is a piece of
work, this environment is called X
land. And this is a
a very kind of configurable
environment for training agents. You can see all
of this, all of the, the
setting of the world can be configured as well as
the tasks. That you can convey
to the agents. And so this was
a piece of work called adaptive agents
and you can train models in
these kind of more diverse settings with
more diverse tasks. And
show that you can kind of solve these tasks
more efficiently than humans in this in this
setting. Just to kind of
summarize here, we
train agents in in real games. We
train agents in kind of physics simulators.
And we make our own environments to try and
broaden out the set of environment
But
in recent years, these
these foundation models, these large LLM models
seem to be having a huge impact
in many different kind of real world domains.
And so it led us to start thinking, well,
can we train foundation models to
generate a kind of unlimited number
of environments with
rich, agent experience as a
way of kind of massively expanding
the range and diversity of environments
that we can use to train embodied
AI agents.
I'd just like to say,
just to sort of be more specific about what a world
model is because I think,
people use the term to mean different things in
different contexts. So at
least for us and the kind
of world models team, at GDM,
this is the the definition that we prefer.
So and specifically,
this is an action conditioned model. So
this is quite important, we think, for
creating models that allow agents
to interact with and
have effects on on the world
that they're they're operating in.
Also pointing out that this is sort of like the original
definition from 1990
Schmidt Huber and then carried on
in further work.
Yeah, just to give you this is a picture of,
what happened in in 2024.
And
I think he's trying to paint the picture of
the the idea of world models
kind of slowly gaining pace throughout
2024. So
Genie one was published at the 2024.
Then there were other
world models that were released and announced
throughout period from World Labs,
Oasis, NVIDIA Cosmos, as
well as kind of various
hype tweets from around the timeline
from kind of the larger players. So it
definitely felt at the end of last year
that this was a active
research direction, And,
yeah, we released Genie
two in December year.
So Genie two was a model we trained
and announced towards the 2024.
It's a interactive world model.
You can interact with it
through kind of navigation actions.
Point out that it's not real time because Genie three
is real time, but it was it's a sort of a turn based
and it can simulate a broad range of three
d environments. This is just one example, but
you go to the blog, you can see quite a broad
range of different environments that it can
generate. Conditioned on an an
initial image.
But they are all kind of
not real world examples. So you can see that
they're kind of game like environment.
I'd also bring attention to, a
paper diffusion models of real
time game engines. So this was published by
these these authors, but I'm drawing attention to
Shlomi Fruchta who is the lead of
Genie three. So this,
this is not somebody playing doom. This
is somebody playing a diffusion model that has
been trained on trajectories from Doom.
So this I think, is very impressive because I
can't tell the difference between this and doom.
And this is fully interactive and also
fully real time. So
it's a very impressive result in a very
narrow domain. So if you train these models,
on a large amount of a very restricted
domain, It's kind of a proof that
we can generate very long
consistent trajectories that don't degrade.
So this was kind of a cool result and something
we wanted to build on.
DeepMind also has a video model called
VO. Which is
not really a world model. You can't
interact with it except to prompt it. And
it's not real time. It takes a long time to generate
these videos.
It would be nice if I could replay that video for
you. But if you can if you can remember
it, there's a sort of interesting smoke dynamics
Two people smoking cigars and the smoke kind of drifts
off in quite a realistic way.
And I just as another example,
we like to show for
VO as a world model, So, I mean,
if you look at this, it just feels like
the model has understood something about the real
world, about the dynamics of snow
hitting rigid bodies It's not intended
to be a world model, but it feels like
it has some understanding
of the real world in it.
So we felt like all these three things were
kind of proofs that we could bring them
all together into a single model. So
the goal for Genie three was to make
a photo realistic by which we meant
0720 p at the time, but also
just high quality imagery.
Real time, action controllable
world model. And so we were specifically targeting
being able to generate long
trajectories that were consistent with themselves,
And they didn't kind of degrade over time.
Grow blurry, do sudden
scene jumps to other parts of the world
that stayed consistent throughout that time.
And I guess,
these guys okay. This is This is an example
of, skipping forward to
some of the results of Genie three. So
I is a very broad range of
the kinds of things that this model can generate.
So we have kind of fantastic worlds down
in the bottom right. But also
quite convincing weather physics
if middle video could
load. I'll give it a minute.
But you can see the wind acting
on the trees, the the
wind blowing the the water into the into
the path of the road.
As well as a bunch of other different environments.
This is to give you an idea of the kind of
breadth of
generation that Genie can do.
Struggling.
Yeah. I like these videos. So this is a side by side
of Genie two and Genie three. So on the left
is, Genie two. And
so we saved the image prompts we did for Genie
two and
played them through in Genie three just to see
how far we've come. So as you can
see, Genie two had a length generation
of about sixteen seconds. So Genie
three can certainly go much, much further than
that. But also, doesn't
suffer from the kind of blurring effects that we
saw for longer generations in Genie two.
And also the
kind of persistence of the world
is much increased. So if I think
in a few minutes, we'll turn back around and that kind
of tower that we started out at the beginning
is still there and still looks pretty
much like did at the beginning.
This is another nice example. In Genie two, we were
really happy that we could open doors. We thought this was
a a fun way that
to show that we'd kind of learned some
interactions with the world. But I mean,
if you notice, the new
room that it generates is kinda janky and weird.
With Genie three, we can use the,
the the text prompting to describe,
okay, you open this door, you go out into a kind of
futuristic space ship
bridge. So you can kind
of wander around here. So the off screen generation
is been significantly improved by being able
prompt these models. And also the kind of reliability
of object interactions been improved because
you can, not only open a door once, but you
can open the door twice, and I think even three
times. There
we go.
Yeah. So I just point out some of
the properties that we have
kind of observed in Genie three.
One is the generality. So these are just some fun
generation. So this this
is, I think the prompt is something like walking
around a glacier lake.
You plug that into Genie and you get this kind of very
beautiful and almost hyper real
kind of landscape. It's very
aesthetic. And this
is this is a member of the team just playing
this in real time. This is just footage of
somebody playing with the interactive
demo walking around, just kind
of looking around this kind of beautiful scenery.
This is I feel
very realistic. If you've ever walked around
the countryside in The UK, you
you will recognize this scene, including the
puddles.
Yeah. This is, very dear to my heart.
And the really nice thing about this generation
is that the model has some kind of
understanding of who you are in the world.
So look down and see our legs. We
can also see some kind of,
kind of cool modeling
of the ripples and the puddles.
But we've also retained
an ability to generate, like,
less realistic, more kind of game like world.
So this is kind of pixel art
style world where you're a
knight and complete with a big
red dragon walking along the scene.
So this is just a couple of examples to show
that, there's a very
broad range of of of environments
that we can generate.
To touch on long horizon and memory. So
first question anybody asks you, if you train one of
these models is if I look away from something, and
then look back, is it still the same I
think people have a have an intuition
that, memory is hard in these problems.
So this is a very
clear example of that. So try and remember
red apple cup tree.
So we kind of walk away
into this little corner of this classroom
and just look out the window for a bit.
So this kind of slightly blank
scene outside the window.
And I think we sort of linger here for about
forty or fifty seconds.
Just to try and prove point.
Alright. I think we're gonna go back to the
black. Board now.
Slightly grim little classroom.
And there we go. Red apple cup. Tree.
So it's a
nice example that memory
is, maintained across
at least a minute. And I
mean, I was pretty surprised at how high quality
the regeneration. That black.
Board was. And indeed, even the scene outside is still
still there. So,
that's addressing memory. Another
example of memory is this cool video
of kind of painting inside.
So not only
will we generate these paint marks and turn away
and remember the shape of them, But it's also
a really nice indication of being
able to perform actions in
the world have consequences
in the world, and then have those consequences
be persistent. And I think that's kind
of like pointing at the utility
of these world models for agent training.
Where okay. So at this point, I'm gonna
hand over. Phil to talk to you about Crumple World
Event. Yeah.
Thank you, Steven. Yeah.
So the third we wanted to talk about
or highlight was this kind of new thing for Genie
three, which was prompt to world events.
So
concretely, obviously, you've seen
like, standard generations where you can prompt it,
and you can kinda get this scene, which is actually
quite close to the office, which is walking along Regents
Canal. And, yes, know, it's nice.
It's a bit boring, but that's actually kinda how it is.
There's not much that really happens along this canal.
Of course, for agent training, we wanna generate counter factuals.
We wanna rerun scenes that maybe also
prepare for, like, long tail events. After all, these are, like,
foundation models. They should have good understanding
of things that aren't just within this context, but
things from other, you know,
areas that it's been trained on. So,
in this case, we kinda wanna prepare agents
for like random events that might happen. For instance,
someone running by in a chicken suit.
So there's exactly the same scene, but then halfway
in, we kind of inject this prompt and
very strange man going for a jog.
Maybe it's for charity.
You can also interact with or
get drenched by really annoying commuters on
the way. To work.
So, yeah. We kinda again, yeah. Just injected
this prompt halfway through and then boom. You kinda
have this jet ski
And, yeah. I mean, Game of Thrones might become reality,
so we gotta prepare for that. So
again, injecting this
And kind of as Steven pointed out, there's this you
know, not a real scene,
but like you get this kind of like element of like
realistic physics, particularly with the water, which we quite
liked. We
Some more examples would be like here,
the skiing scene. So, of course,
you can just interact
like ski down the slope, as
a standard, but of course, like many
things can happen on the ski slope, for instance,
another skier might appear wearing some merch
He's gonna encourage us to go. Yeah.
You very much.
There we go. It's what we needed.
But well, like, more sort of
exciting things might be happening as well.
And, yeah, we might wanna pick up a little care package
as we down the slopes as well.
And you notice the, the shadow cast there.
Pretty neat even though it is a bit unrealistic.
We missed out. Alright. Better let me start.
So, yeah, I guess, why are we all here
This workshop I think we truly think Genie
three can help with agentic use cases.
So, like, let's speak to that a bit.
Of course, we need an agent to, like, roll
out. In Genie, so this is where we
turn to our amazing colleagues in the CIMR
team you might be aware of some of their work.
But effectively, they've trained
and can say there's now Qasimma two is out, but
effectively a Gemini backbone
to specifically execute user
commands within
various different games that
we've illustrated on the left there. What's
particularly exciting is because we've moved over
to this Gemini backbone,
we're able to do this kind of multi turn and just
like much more like dynamic interaction with
the agent. So instance, you can see
on the left, it's just like a single instruction
on the right, you know, the
SIMR agent starts talking to us. We can start querying it as
well during the rollout.
So what does this all
mean for Genie Well, it'd be really
nice, you know, as
researchers we ought to ensure
our model is generalized. So of
course, like never seen these
environments. This is not at all
the, training set. But, we
can kinda test out how
adaptable the similar agents
are to these brand new never seen before environments.
This is a nice kind of I guess,
awesome scene in New England. And, yeah,
we can kind of query it as well.
This is the kind of power of, like, training these, like, large
foundation models. And it can kind of answer
questions about the rollout and then perform tasks
So that already
in in and of itself, even if you don't think about,
like, Genie creates a
kind of this open ended, self
improving loop whereby have these
like agents
that are Gemini and then you can get Gemini
to also set tasks within these automatic automatically
just based on the context like, oh, if there's a bench, maybe
we should walk towards it. Right
And then once a similar agent tries to execute
that, we can then use Gemini to know,
ask, like, well, was this a useful trajectory Did it actually
perform the task that we said it should
But we can actually expand the scope of this
significantly greater by great basically going beyond, like,
the training set, by having effectively
Genie three being the environment rather than the
training environments. And and again,
we can introduce,
Gemini as basically this environment setter.
So not only kinda saying what tasks
should you complete, but actually what should the environment
be in the first place.
And I think this we didn't really
say this when we released it, but actually this
was all done basically
in, like, Gemini and
CIMR and Genie Land. So we basically asked
Gemini, like, what do you think would be useful,
like, real world situations where
maybe we'd need some, like, kind of robotic training
data and it basically just proposed a load
of environments. We got some first frames,
out of, the Genie model. And then
we said, okay, well, cool. Here's, like, the environment
you've kind of asked for. Like, what do you think
we should do in this environment And yet proposed
a bunch of tasks, which the similar agent was then
able to, to
solve. And, yeah, there's kind of like
I guess,
purely agentic in sim interaction.
And, yeah, here probably are aware of reinforcement
learning. This kind of feels like reinforcement learning.
You're kinda doing your rollouts. You're gathering your datasets,
and maybe you can then do some sort
of step of policy improvement on those generated
trajectories of course, you can
update certain trajectories because you can use Gemini
to determine whether or not those trajectories were
successful or not.
So of course, there are limitations.
You could kind of as you can see with the
UI, the action space is quite limited.
We're particularly excited about much more finer
grained control schema. Particularly
things around like dexterous manipulation.
We also firmly believe that multi
agent is very important. And what we mean by that
is of course, we can simulate, like, other agents
within Genie, but we can't really, like, necessarily
control them apart from prompting. So getting the same level
of control for multiple agents and having them all
sort of exist within the same world model
is, like, particularly exciting. And,
of course, as kind of Steven was
talking about, the interaction duration is great,
but you know, we really want, like,
multi hours of like, interaction.
Like great persistence, but of course, that's
both, like, pretty hard to serve something
like that and also it's just like a general like memory
like research topic. So, yeah, these are these
are things we're deaf definitely like looking to, you
know, work on and, think about
a lot.
So in summary, g three
is a general purpose foundation
world model that generates interactive three d worlds
from simple text prompts in real time.
We believe world models are on the path to
AGI because they enable unlimited
rich simulation environments for training
embodied AI agents And,
well, what are the implications of this Well, infinite
training environments for agents. Really. Enabling
open ended self improvement.
Thank you very much.
Thank you very much for your talk.
Due to the time limitation, we only have one
question for Jenny team.
Thanks. Great work. You share a
little about little bit about insight on
how to prevent a compounded error when you're generating
very long form video.
Yeah. Look, very difficult
problem.
I will say you kind of
when you scale these things up, right, you
get things that are more sensitive in the output. But
sometimes they become more sensitive in the input.
So I think what's crucial
is and, like, finding ways
that you can ensure that
models become more stable respect to the
outputs that they generate So
I don't think I can say much more than that, but, yeah,
there are ways we can control the outputs
while still remaining sensitive in the inputs.
Thanks. I'm trying to understand
that. Yeah. We can probably talk
offline a bit.
Okay. Thank you so much again for your
amazing talk. And,
for the next session, we will have
our panel discussion about our workshop.
So,
please the panelist, can you
come to the stage
Hi,
Josh,
Hello, everyone. We Welcome to
our panel discussion of our
World Model workshop
I'm Mung Yui Yang, the lecturer from
University of Bristol. And I
will be moderating this panel discussion.
We have amazing lineup of,
our panelists and I will just
briefly introduce each
one of this. So first
one, Yi Leung Du from Harvard
University and Jia Joon Woo from Stanford
University. John Lefford from,
Microsoft Research New York based.
And, Lin Chao is our
online member of our panelist.
And also,
Gianluca Corrado
from Weave, our sponsor.
So, maybe I can ask
you to introduce
give a more detail introduce
about yourself.
Just one by one.
Okay. Hi, everyone. I'm Chiang Chiang, our
assistant professor at Stanford. I work on computer
vision robotics. And
I was like, how you can help systems understand
scenes reason about them, make predictions,
interact with them.
Okay. Hey everybody. Glenn Bresseth.
So I do a lot of I'm a professor at
Mila, in Montreal. And
I do a lot of re reinforcement learning and robotics.
As main research areas.
Hi. I'm Elan. I'm an assistant professor at Harvard.
And a lot of my research is on generative models
and decision making. Making.
Hey, everyone. I'm Gianluca,
Workaway, one of the scientists.
Spent a lot of time working on word models,
building models like Aya one, Aya two.
And, yeah, glad to be here.
I'm John Langford. I've
worked on many things over time, but I guess
of late, I'm very interested in
how exactly you can
use generative models and
what capabilities they lack and how you can actually create
those capabilities they lack.
Okay. Fine. Today,
we're going to explore how close
war models are
close to real world application
like robotic and autonomous driving.
We have a a a jump panelist
Can you briefly introduce
yourself
Yes. Hello
Ideas.
Thanks. Thank you
very much. So briefly,
I just directly start the
panel discussion. So for the first
question, so
I think most of you are wondering
that how close are our models
to real world application like
and autonomous driving So beyond the
robots there are other domains,
where world models have been
applied
Yeah. I guess I can start.
They are very close and I would actually say they're
probably because, I mean, even today, we
saw in a few sessions like this morning Sonya
showed us some examples at NVIDIA. We
had Wei. We showed some examples on how
world models that we developed today are actually
used to help us build
safer and better, autonomous agents.
They're not fully there, meaning probably
we would we'll be done with it once
we can fully train a policy in a simulated
world and that policy ends up being much
better than training on real data. But we're seeing
this progress
every month now. There is a new world
model. It's getting better. It's better
simulating critical scenarios.
So I guess they are basically
they're they're improving fast.
Sure. I'm happy to at least,
be a good contradiction, I guess, for all we're doing
this too. I guess I was
thinking even during some of the last talk, like,
what is a good definition for a world model
Like, one that I also want would probably be faster
than the real world or something that's, like, less
resources than actually having to use the real world.
And that's a bit more challenging to get
there for maybe some of the world models we're thinking about
using. And I I
suppose to wonder if there's
a bit of a difference here between how well this is working
for self driving which
maybe has a few other specific
conditions, It seems to be a bit more tricky
to be using world models specifically
for, like, manipulation tasks and things
like that with a lot of contacts.
I don't have access to the today's
best world model at the exact moment, but
at least generating some videos, which are versions
of world models for these tasks, did not go well
for last week.
Yeah. I I think that, for manipulation,
at least the current open source model, I think a lot of the
models online like Vio or
or Sora, I think they're very, targeted towards
entertainment, so they tend to not be very physical
world. So you need to really, try to
fine tune them specifically for manipulation for them
to work well.
I guess I have kind of a broader definition of a
world model than just the pixel based things. And so
if you use the broader definition, I think that you could say
that there are already world models people routinely
use there, like, Ted GPT.
And, there's improvements in these world models
which are underway. And I expect that many of
the techniques that people are developing for world models
will be more broadly adopted over the next
year or two. So it's it's
yes. It's there, and, yes, it's it's it's,
growing, and, the domains are growing as well.
Well, actually don't let you just said John, you just
said what I want to say. So, yeah. So don't
think I have anything to add. Okay. Do we
have some opinion from online member,
Linxiang
I guess one of the the the key differences
about the warmong involves contact
for our documentation is really high very
precise. Modeling modeling.
But for noncontact noncontact
contact, which task, like, autonomous driving,
there's already some of the potential location.
Yeah. Okay. Thank you
very much for sharing your opinions.
And the second question,
so today, we have a lot of speakers
mentioned about VLA models.
So how could the VLA models and
warm models be effectively
combined to have some, thinking
on that
So one thing you can do is, you can actually
have that VLA essentially be
your initial, like, proposal distribution. So you can imagine
the VLA proposes a series of actions
a model can simulate the results of each each of
these actions, and then you can do some type of search
where you just choose the sequence of actions
that maximizes your reward or your goal based
off the world model simulations.
You can also learn a
latent world model inside the VLA
itself.
So if you have enough data, from
an external world model or experience, you can
just learn that latent world model internally
and then use it effectively.
There might also be, like, a nice blend between
these two, because I'm also thinking about where some
world models are still not quite the best.
Vision manipulation tasks. How to use this
combination to basically figure out how to fill
in and collect the data that still isn't quite
there yet. Some of the world models that we'd
use. So even if you do a nice combination,
like, using a VLA to help
kinda like warm start to provide the prior
for some type of world model. Then either
look at where these things are not doing well
where their prediction is bad or something like that to start
to do, like, a bit of
like, active control and figure what data is missing
to go collect in the real world. Feed that
back into the world model where you're missing.
Information.
There is a way where, again, if we start expanding
a little bit the definition of what's a world model
and we think about it's a lot of
knowledge of the world as, you know, going back
to, John's point before,
ChachiPT may be a world model. It just has knowledge
about the world. And these VLAs are actually
connecting a lot of this knowledge. Right Some
means some comes from videos, some comes
from text, and this is still knowledge and common sense
about the world. So it helps the help
learn a lot of these things. And
then the action part is also really important because
that's part of your world model mental
world model. You wanna know how
the environment will react to your actions. So actually,
I think that could be the unifying
interface to actually add
information from video, language, and action put it
all in one place.
Well, you can also take the output
of VRA model and then you can
similar to even what you just said, you see it as
a proposal, but you can also even train
it in conjunction. With the war model. Right
It just take us the VA take the action, but then
you also same time, you can even join
train or fine tune a word model. And
see maybe how help each other.
Okay. Any opinion from online member
No. I think I've paid all. And all the
Okay. I have noticed that,
some of you agree with that we can,
we can train a VoIP and war
model simultaneously. At the same time.
So do you think a follow-up question.
Do you think that when we
train it as integrated
into the same single system
will be very computation cost.
At least when you're playing around with a latent world
model, your computational cost
may decline. Because you
don't need to fill in a bunch of details which turn out to
be unnecessary
Yes, I kind of agree probably Learning
a good latent space to
learn your word model is
the best way to get the efficiency. So, yeah, I agree,
complete.
Yeah. For most of that makes sense. I just also
worry about one corner case where I guess
the assumption in the question is you're also training the
world model from new data. That's
being collected in some case. So just be
cautious that your VLA policy doesn't start
really poor and you only explore some really bad
area or really frequent visiting
of the same space and then basically you
converge to nothing,
the same action, the same state the whole time.
So it also can be helpful with
some theory that also shows I got at least some random
exploration things like that that aren't just the BLA
model in these cases. I think mostly we don't have to
deal about too much because you start out with a good
world model. But if you're training from scratch
or tackling a task that's fairly new
and out of distribution, just be cautious
in that case.
Oh, there Oh, sorry. So
this is a question for John, I guess.
For the latent representation, for
human we learn with word
modeling. We are dreaming.
So what space are we dreaming in
Is it, like, very latent or very
like, concrete, like, with perceptions and
everything
So that's little hard to answer because
it's an introspection here, but
we can take an extreme stance and say,
humans don't have any way to,
render things in full fidelity.
And so it has to be has to be while while dreaming.
So it has to be latent in some sense.
Right But you
if you an agent if
you think about an agent that has a a latent
representation, if it's a good latent representation,
it'll be the same as if it's getting whatever
external stimulus. Creates that latent
representation. And so
you can't introspect or really understand the
difference necessarily.
Because on one hand, you generate
a latent On the other hand, you you have
a a latent projected from an external stimulus.
And they're they're the same if you have a good latent.
Okay. Thank you.
Yeah. The decision is based on the
latent, then you'll just work in latent variables.
That makes sense
Okay. Thank you very much.
For the jump question. And,
so the following question would be, like,
as mentioned several times today,
that we might learn a kind of latent
level or models. So do you think
latent led latent level word model is a
kind of a compression and it will loss
a lot of details of the real world.
So it be a kind of
a thing that we should treat
it as a kind of trade off between the real
world generation and also
the latent space understanding.
Oh, this is a fun one. So you certainly
if you have a latent space, you're certainly losing
a lot of information about the world. Yes.
But if you lose the right information about the world,
you win. You don't not a trade off.
It's it's a win. And
the the extrapolation that I was showing is an
example of this. Right So you
get an improved ability to extrapolate if you
throw away the right detail.
No. I was just sort of like, how do we figure out
the right thing To leave out is Because, like, the real
latent space is the location and
velocity of every atom in the universe. Signal.
We really don't need to track that, like, in any way. We all
seem to do pretty good without that information.
So is there any
like, rules of thumb or something to understand
what this right version is. Usually
for me, it's just like whatever makes my reward
better, but that still assumes I know a reward
function for starting.
So I think the
general notion of what the right, information is
is whatever you need to drive your policy.
It's related to the rewards, but,
whatever basis you need to to take
for taking action. Now that's that's kind
of an implicit definition. But you
can imagine that if you have an updated data collected,
you could excite that definition
in a learning system and actually, get that
representation out.
And, yeah, I think I think that comes from
the actions, right These world models are trained
taking into consideration actions that agents
are supposed to take in that world. And
so once you start modeling the latent space,
for example,
trying to model the sky and the clouds,
probably not gonna help you much deciding what's
your next next action. That's just in
in the scene. I'm I'm thinking about the
the self driving case here, but there
are always things in the scene that are probably
not very relevant to your decision making.
And if you can close the loop between the
world modeling and the decision making, while
learning the latent space, you are
more likely to obstruct out things that you
don't really need. But, yeah, there is
there is always like It's always the danger.
Right Compress too much, you lose too much. You're missing
out on something.
I'm just trying to close that loop in my head
as well. So, does this mean it's some
sort of information metric
like mutual information metric over like
what you can control in the latent space with
respect to the actions that will be used inside
of this Over time, and I guess how that relates
to the real worlds Ish
Yeah.
Okay. Cool. I'll do that tomorrow.
So do you have, some opinion from
online member
Link Yeah. Yeah. I think
it is a very good, interaction, very
good topic. It's really challenging
part is how to define and to be, like,
a good lab space. For example, you know, I know we
we kind of need for the information of
for the model. But, you know,
like, like, for different
task, probably there will be
different, important information,
like, and how to how to know about
the theory exists and how to, you know, have, like,
relatively universal representation
that can, adapt you to, like,
a like a general set of of tasks,
that will be the challenge, I will say.
And, whether we are some kind
of a transformer that allows us
to allow the model to
shift or focus on different, or identify
some of the most relevant information or different
tasks, that would be a very interesting direction
to too. Explore to explore.
Okay. Thank you very much. Have
some okay. For the
next question, I just want
to ask, so nowadays,
there are several ways to build up the word
model. One is building
up the physical, physical interaction
with the environment. Building up a kind
of transition model like that. And the one way
is just to reconstruct real
world, like, three d reconstructing, like, a
spatial intelligence, this kind of thing.
So, how do you think about
this two line of the works
about, how to represent the world
So my thought is, you should, like, represent the
world at many different levels of information.
So for instance, I think a language model like what was
talked about earlier gives you a very high level semantic
understanding of the word. World. But then I feel that
some type of maybe like video modeling
with dynamics a video model a video modeling would
really tell you some more of the dynamics of the
world. But then you probably also need something that's
much more tactile or force based to really get
the low level of physics physical details. So
I guess you would need many different objectives to get different parts
of the world knowledge.
I like the latent approach because, it feels
like it's a little bit unexplored
rather than speaking. And there's huge potential
to be much more efficient.
I guess, I mean, the two approaches can be complemented
especially right now where we're not
done with latent word models or
this word model. So three d reconstruction
is quite powerful. In
and has very, very good use
case for embodied AI applications,
which is you can three d reconstruct
a failure case, you can basically build
what used to be unit tests. Of
when your software used to fail. Right And then you can
just build this user array
of unit tests that you can run all your policies
in just to check. Is my new policy
failing at things that an old policy used to
fail It still has the challenge
there. Right Because once you reconstructed
the the three d world perfectly and you want
to resimulate something, you still have the
issue of having to resimulate or to simulate
the correct physics and
doesn't come for free in three d reconstructed worlds.
Although that's the kind of promise of
latent word models and learned word models, they will pick
up on physics and
you will get them for free. And, you know,
in the previous talk, we kind of saw some of these
examples where the physics were quite well
represented already.
Oh, okay.
Please.
What actions a lot So I wonder
we have different definitions of action
for different applications. For example, for video
game, it's like simple and discreet
mostly. And then for robotics, it could
be more complicated. And one can imagine we can
have different world models for this
different deaf But intuitively,
we also shouldn't build a specific
world model tailored to specific
action definition. So what do you what do
the dependent list think about it
So actually, I personally think that you do
role models at different levels of abstraction. So
if you say something like, like, maybe if you
abstractly are playing a video game, maybe walk forward
is just your action. But maybe if you're controlling a
robot, you really do want a much more low level, like,
like maybe the way points you want to reach, right, or force or
torques So it does seem like there there
should at least be a couple of different road models.
For levels of different levels of obstruction.
I guess like, even the the latent space
way of figuring this out too is also like a nice
way to preserve some resources.
But maybe we kind of want like a a latent representation
where you can generate lots world. Models.
Like in this case, you can just
figure out from that latent space which components of
it actually make the function that's good enough for
discrete space. Because you don't necessarily
need a big giant latent space to figure out
how like a pendulum swings in the world things
like that. Maybe there's ways to
factor it in a nice way that's learned especially
you can get a more efficient
somehow, like, component version of that latent
space model even for each task you've might use.
This is I guess robotics has doing
this for a while for every task specifically.
Depending on what the design and constraints are.
I I guess if we have
latent action, you can also potentially
Potentially, yeah. Mhmm. You.
Thank you.
So do you have any opinion about
the previous question on
the three d reconstruction
Also, like, the physical dynamics modeling.
From our online member.
I think, I think it's it's
important to, look at the problem from
different perspective. For the photo, I
think the the major difference will be the values
should be
mean, for the city representation is is
this kind of provides like a like a like a
LEGO structured understanding.
And, for, let's actually,
it's a challenging part of this, how to do this,
like, a incremental learning. And how to
do this command and tutorial.
Like, things you know, how to, add
more different different knowledge and a combineings.
I guess that will be the one to all the challenging,
things. And
and and also I want to congratulate you about the water
model is also like different mortality from,
I mean, I'm doing I'm doing robotic manipulation. We know that,
you know, really require different
type of sensing, what's the, like, the tactile, the
force, to be incorporate,
to make the to have the robot to make
the right decision. So,
how to combine this kind of, like, different modalities
or look at the I mean, build the water bottle
from the
So any opinion from,
our
I I thought there you know, there's a distinction between
what is the goal. What's the problem you're trying
to solve, as well as the method or the
algorithms that are that are solving, and and then
there's implementation. So at a
goal level, you know, it's like I I guess,
eventually, people want more models to have all of these.
Right So you wanna be able to do
plausible physical simulation, release the visual
appearance, help decision making, stuff like that
And then in terms of approach
or in terms of, you know, the
the representations, then,
no then the two things you mentioned,
you know, let's say, three d reconstruction
or versus more like focusing
on dynamic prediction. I think they're just, you know, sometimes tackling
different aspects of the problem. But I'm
pretty sure that, you know, whenever people say spatial
intelligence, eventually, they also want have predictions
as well. But you just like how you're prioritizing
these different sub goals. But in terms
of method, is a separate question, right, whether
it should be implicit or or
latent or more explicit because
the visual realism or three d consistency,
you can also imagine a totally latent approach.
It doesn't it's not a case that if you focus
on three d reconstruction, you have to use three d Gaussians.
So so there's a kind of I think, there's a
separate or disentanglement of a problem versus
approach that we should be aware
Okay. Thank you very much for sharing your
opinions. And another
question. Just following the questions
we asked before, so
so I think some of the participant
might want to some something
like if, the current world
model can capture
the physical laws, physical rules
of the real world. For example, in the
in generated videos or
generated three d things,
is this something that we feel to
capture, like, some physical concept like
gravity, like, this kind of thing.
So how could this limit
limitations be addressed that will have
some suggestions for that
I can start just for a second, but maybe part
of the well,
I think even for the way some people operate
is there some latent space that's deemed
average existence we've been doing for a while, but then when
well, winter come along comes along for a little
bit, everything on average gets a little bit more
slippery. So there's some hopefully, like, at
least context or something. That's fed into this that's
from recent history.
To either help help update the latent model
or about what would be used
to better predict or figure out
gravity on a different planet given some recent information
So I feel that the middles, the video
models have some sense of physics, but for a lot
of especially for rare events, like, for instance, let's
say you drop a cup and it falls on the ground,
It seems like these events that are very rarely depicted
in data, they don't seem to be accurately able
to learn. So I
think that I think one way to really get better
physics knowledge in these models is really to
maybe try to execute them on physical robots and
see use, like, real world data as a way to
gather this physics information. Because
the data on the Internet is pretty biased towards
certain physical interactions and not other ones.
So I guess to me,
failure to for gravity to work properly is,
an extrapolation error. Presumably, there's
some instances of gravity doing something useful
in the data somewhere. And there's just
a failure to extrapolate to, other situations.
And so the solution that we found
experimentally so far is if you
have a much more compact world model, then you
get much better extrapolation. And
so that'd be my first instinct.
Yeah. I I I tend to agree because
it's unclear what like I think for
me, it always goes back to latent spaces. Right
If it is possible in your latent
space, you do you are understanding the physics
up to the extent that is needed. To
solve a task. And in pixel
space, that is hard.
To then simulate and So it's it's
unclear whether these models
actually understand the physics a lot better than
we think And what we're seeing in image
space and we can understand that image space
is what we think it's for physics,
but they're kind of there, and they're getting
much better, faster. And that
that for me is one way of mitigating
the issue. And then the second is always
data. Right These models are
trained from large scale datasets
and we're still not there
at the point where can just take all the videos
in the world and train one word model
with it. And so data selection becomes
much more important. So if you want physics
to be well understood by your model, you need to be
careful and choosing videos that
tend to teach the physics a lot better.
If I remember correctly, I think in your question, you mentioned
some kind of physical laws. Stuff like that.
So it's I guess it's probably
the case that, you know, there are there are methods that we
can try to incorporate and including
data collection strategies that Elon and Foresimplify
suggesting. Can help us to get these models
better at physics And I also feel like, you know,
for any sort of applications including robotics, there's
probably an extent that these models would just be
good enough and and that's all we
need because you know, they'd never they'll
probably never be perfect, but it's okay because, you know,
know, I understand very little physics. But
you can still live if it's it's alive in the world.
Right So so the models don't have to understand
perfect physics. And if you hear about
having the models understand theoretical
laws and, you know, symbolic equations and help you
do science discovery, a different types of model,
and then we require something else. But that's sounds
a bit different from the war models that has
been discussed in this context for this particular workshop.
So any ideas from online
member
I think, one of the things
challenge would be how do you, extract a
representative the the phasing constraints. Your phasing
constraints are kind of solved in like like part of constraints.
For example, you cannot like, a
cup cannot look like it's dropped. To the
ground, but it cannot go through the ground. Right I mean, it's
already is is is this sound like like a
hard constraint on how to represent
and know, loans this, like, an hard like,
this hard constraint would be,
very interesting to think about. I think he's
also very important
direction. It's associated
with some of the interpolation. And and
then if there's, like, the car constraints,
what kind of interpolation
could be, be, learned and represented,
and yeah, that
would be, I I I guess
that would be my of the potential
ways to to to to mitigate this.
Like, issues
K. Thank you very much. So
the second last question,
so in the context of embodied
AI, so what do you say as the
most promising direction
for war models over the next two or three
years
I think we are seeing a lot of progress
these days in building these world models
and they become more accurate.
But think in the next two or three years, the most promising
thing to figure out is how you actually use them
to generate
data for embodied AI.
There are so many things that you want to be able
to cover in the space of possibilities,
and a lot of these models still
use interfaces like text prompts
and such that are quite hard
to automate. In a way
that guarantees you kind of cover the space
of possibilities. So figuring
out the best way to, like, create
interfaces for these models to then automate
the testing and making sure it's
safe enough, which means it cover the space of
possibilities exhaustively this is gonna
be probably one of the most impactful things that we
can see for Embodied AI because it will
bring embodied agents from
being, good to to things in the real
world to being safe to do these things in the real
world and therefore deployable and actually
we can bring them to the world.
I think that there's a real chance that in two
years, we'll be using
latent world models at inference
time. And that will
be a quantum jump terms of
how reliable systems can be Because
they can roll out the consequences of decisions
and choose which action to take.
In useful ways.
Yeah. I very much agree with that. I think the most
exciting use case of road models is actually to use
So because now you can much more consistently select
which actions to execute in comparison to a
policy. I think that models are much more
likely to generalize to new tasks than
policies are.
Yeah. The only thing I'd add onto that, but kinda one
of the comments you're making earlier is maybe also
having a bit of a mix
like a short hierarchy of models that are trained.
Because there'll be one that's really fast that you can run-in
real time, and that will keep to do some sampling.
Hopefully have one that's gonna be more expensive, more
resource constrained. But now it's only if you're like task
is more critical. Or it's finally passed
the test through all the other simulators that were cheaper
and less resource. You know, expensive
to run including being able to test them as well.
Well, I'm I'm quite excited about
having even just video models that
take real actions as input. It's
like not just genius grape, but not just
forward backward, like, real actions. I think there's
related to of the questions that was asked before, like,
if you have a different robot actuators and you have
all the dexterous hands and stuff like that, how can you specify
actions As input,
as conditions to these new generation methods And
I think there's also hope that these methods can be
made much faster even to real time, so that they can
also be used at inference time as well.
Any ideas from link
From the robotics and spatial modulation
perspective, we are still interested in,
like, how to incorporate different modalities like the
tech
the the decision making for a bot communication.
I think that will be also very important.
Okay. Thank you so much for your sharing.
So the last question
maybe some of our attendees really
curious about So could you please
recommend one of your favorite
papers on world models or related topics
such as embodied AI and
the video generation or model based reinforcement
learning or this kind of thing.
Sure. I mean, I guess I'll suggest
is like the second last paper that I was including
a part of my presentation. Was just a
mix of figuring out how to combine a good vision,
language, action model, and
some pipe some kind of model based search.
But I think the fun part in there, which
we haven't really talked about is kinda
make good use of a model. Because the VLA action
policy is already learned at a whole
bunch of stuff. And we don't really need
at that point the world model to predict really
good for the stuff the vision language action model already
knows. It should instead, you know, figure
out how to predict better for the stuff that's on
the edge of what this VLA policy
knows. Because that's what it's gonna use to actually
help correct those actions anyway. So if we
have to contribute capacity somewhere,
maybe think about this part of the search and
expansion.
No one else has any other papers,
just me
Alright. You can also share
your your paper.
I'm excited about the paper I was talking about,
the next latent one. But at the same time,
I'm even more excited about the next paper.
I don't have one specific paper I would
suggest, but I think we're at a stage
with world modeling where
there's probably a lot of noise
going on. Right So it becomes really, really hard
to even figure it out. What's
important and what's not. So in this case, you know,
usually my suggestion is keep an
eye on all the review papers that come out because
once you start combining all the
review papers together, they help a lot
trying to marginalize out lot of
the noise and really understand what's going on in the
field where the field is headed,
and maybe spot the next
big opportunity.
Yeah, I also I
guess in general, I'm very I don't know if I have a favorite
paper, but I'm very excited about these papers where you're
actually using these models at inference time to get more robust. Execution.
Okay.
We have to recommend one
Okay. I'm gonna shamelessly say advertise our
work that it's not about war model,
but it's about evaluating war model. Because,
you know, you have all these war models, and you want to evaluate
how good they are. So we have a
paper called ORR score, which just try to
benchmark how these different word models
they behaved. But but it was but
it doesn't apply to your latent word models, so so it
doesn't require these word models to have a pixel
output eventually at the end. The end.
Thank you. And online suggestions
Okay. Last but not least, I would
say maybe random management
some of your paper. I'm just to find, like,
a recent one, that's, like, called, like,
understanding word or predict the future, a comfort
and safe survey for the one about.
I guess that if, like, find a company and see
somewhere you're down to the the the program so far.
So this would be the recommendation on
my
k. Thank you very much, and, thank
you for all the panel
discussion today. And, I just give a
brief summarization. So
today, I think our topic would
be, like, a way talk a lot about
latent war model. So maybe in the
future, latent war model
is a kind of promising direction about
word about word model. So thank
you for all the
panelists. For your efforts
today, and also
thank you for all the attendees
for, for listening to this
panel discussion. So our discussion,
it's, just end.
So, maybe we can
move to the next session.
Okay. I'm sorry.
Yeah. Oh, is if you come out
Okay. Let me just give a brief introduction
about our next speaker.
Jian Lan Luo from
AGI Bolt. And
he is a chief scientist of AGI
Bolt. And his research interest
about enabling robust long
term autonomy for complex real
world system. And today,
he's topic is a
world model powered, robotics
manipulation.
Thanks for the introduction and thanks everyone
for being here.
I'm Janan. Maybe I should give a quick introduction
of the companies. Fifteen seconds.
The company is pretty new. It's founded in 2023.
It's mainly operated in Shanghai. And
it's covers basically on the
full stack of robotics.
We have lots of humanoid robots.
I did a research org there. Which
was founded just this year. Mostly
focusing on the fundamental research
in robot learning. We're just we're
very new, and we just get started.
From the left, we actually release
today the largest this is one of Vietnam,
the largest open source robotic
dataset is called AEGI award.
It's online.
The right is our open source video foundation
model. It's called Genie Visional
which we also cover in this talk.
So I think there are a lot
of progress on WIDO foundation models in the
past two years, This
Cosmos, Weo, and Genie three,
soar as well. But most of them
are primarily focusing on
generating photorealistic videos,
but what we are really interested in
at least, is that, we
want to understand, we want to
understand the question, how could we leverage
those models for for robotic
manipulation problems we're, we're interested
in.
So since last year,
we took our journey on this topic,
And in this talk, I present two
ways, which we explored,
as promising directions leveraging water
models for robotic manipulation,
is what we call generative policy learning.
Basically use trend
with the foundation models to guide policy learning
process and the other one is,
use it as a interactive generative simulator.
So
Well, I mean, if you look at the picture,
he was often in margin first, and and then
ask if you actually want to shoot a basketball,
you probably have a have a image
of a series of image of how you shoot a basketball,
and then you, and you act
So in the first walk,
we
the the the we basically take a
first step basically took a first step
in this direction. So the idea here
is actually pretty simple. We want we we basically
ask, can we use the latent
features Basically, the learn latent
embeddings from a generative
water model, essentially, By WER model,
I really mean video prediction models. To guide
the policy. And
what it does is use robot data
to train a language conditioned video
models and then use a learn,
features from the with our models.
To fine tune to robotic actions.
In this work, Annivers this
is actually machine learning center is pretty old.
Because it's already one year.
What we have is that we
at that time, we took some open source
data and some simulation data,
and because at the time, we we
don't have the
HIBOR WAN dataset yet, we we took
a lot of real world data that's already open source
and trained it with a foundation model.
And then use this WILDL foundation model features
from the WILDL prediction models
to basically learn a policy on top of
it. So, in this
particular work, we actually
used the autogenerative diffusion model
to generate chunk of actions, window chunks,
not the not the whole videos.
And it it does contain history,
but it's it's sparse sparse memory. You
look at some of the it don't look at
the whole thing. It picks some key frames
in the past. Once these video models
are trained in the second step,
we fine tune the learned WIDO models
to to to robotic
policies, basically to find into robotic action
data. Because at that time,
again, we we don't have a lot of real world data, and
we did actually use a small part of the
simulation data use Gaussian splattering to augment
it. To multiple
views. We here, we're actually using a we're
actually in a multi view setting.
So here is this is actually if you if I
could play this yeah. Great. This
is actually the generated video from
two views. From the both the
head camera and and one side camera.
For the from the robot.
Once the the the window
is trained, we actually have
additional policy
the policy head It's a diffusion
transformer block. And then fine tune this
block to robotic actions.
At that time, this is actually, I believe,
end of last year, the left
is a figure, the table in the left.
Is we compare to
the best method Of course,
this is actually probably not an
upper to upper comparison because comparing
to those VLA models,
that's fine tune on the same amount of real world
robotic data.
In Libro and Calvin, that's a simulation
robotic benchmark. We actually
our method actually attained the highest performance
on those same, on those same benchmark.
But this is a pretty old result from last
year. And this is actually
some of the real
video, the real experiment we do with the
actual robot. The the top the
video I'm playing right now. Is
a generated
result from the from the model.
And on our on
the bottom, this is actually precise pick and place
task You need your object sorting. This is as
a policy route. That I
can do this precise
sorting task. Okay.
So we I talked
about previously the first step, we're
actually trying to essentially a
language and condition of water model. We
prompt the the model to generate
realistic videos, and then we fine tune these
video models to robotic
policies, The
second step
as I mentioned, is that we're also interested into
using it as a simulator, basically a generative
simulator. We are asking,
can we could we actually directly
condition the continuous robotic actions
to the water motor. So it could be actually
used as a simulator.
So in in the ANIMALS AC,
I believe one of our colleague will be
presenting this paper in detail in this workshop
just just after right after me. As
well. What I would do is
we take the foundation model,
and and we represent
directly represent the n effective motion, the gripper
motions, it's we're using
somewhat interesting representation,
as you see in the in the images,
it's the the the the red
circle and and the green circles,
the arrows on top of it. That's actually
directly in the pixel space that
directly one to one maps to the gripper
actions in the in the real world. So
this thing will actually gets bigger when it gets closer,
and it gets smaller when, when it's
the gripper is far away. In the
image. We we we found it actually particular
effective represent
an action this way, then you directly contain,
condition on the continuous
real numbers of the gripper actions.
Also actually roll out the
policies so that we have
autonomous data. Then using negative data as
a counterfactual. So the the
wall model is actually learning the whole support
of the data distribution, not just on the
narrow successful
human demonstrations, rather a mix of
raw raw data a tons of data, and and
a part of the demonstrations
and we're using this simulation simulator
mainly for policy evaluation.
In this work, we're actually taking five
views. So the the model is asked to
generate the five views, where the robot actually has a
lot of cameras. To generate five views
simultaneously. From one
initial observation image.
So, here is one video that we actually
roll the same policy, and then the
the real world and
both in the real world and the
simulator. The in a generative
sim simulator, So you you could see the
because this is actually the same policy, you could
see the the motion
roughly actually it's actually you
could not tell the difference. It it matches with
the same actually much the
real. And right
is the table. You evaluate a
bunch of policies, then
we compare the success rate basically, the
performance of the policy, both in
the model, in the window model,
and also in the the real world.
We could see actually the performance
also actually matches very closely. That
means our model
is is relatively,
effective in capturing the neurons on this precise
robotic manipulation tasks.
Here are just more videos. I think this is
a plane. Again,
the the circles are actually getting bigger and and smaller
depending on the distances. These are
actually two cases of wine is doing
the, I think, object sorting in a grocery
store. The other one is trying to make an,
is trying to make a sandwich.
From one it's just from one initial image.
And then we roll out a policy.
Continuously. Okay.
We talked about we could use this WILDF
foundation model or wall model
as generative policy
learning, also as a simulator.
But then the next idea we have is that could
we actually develop because they
are separate, we need two models for them, could we
actually develop one model
integrated framework encompass all of them
So in Genie Vision,
we actually did this. So the
idea is that we
have one, what we call, world foundation model.
That has a base model, but then we could
use base video foundation model, video
prediction model to adapt it to
action models to adapt it to also
as a simulator and
then this also serves as a benchmark
for one model itself. It's a
benchmark for one model
rather focusing on the metrics, relevant
to robotics to embody embody agent.
Not necessarily means that it has highest
fidelity image resolution or things like that.
And by the way, this is all to open
source. I I believe we are the one of
the very few works that are actually open
source arrays and and codes. And everything.
About the in in a invalid,
foundation model domain.
Here is
what it does. Actually, eventually,
This is one policy that you fine tune now, a small
amount of real world data, but by
by fine tuning the pretrained, with the foundation
model, the task, is
to ask a robot doing a
pretty hard box folding task
we also ask a robot to
follow language instructions,
It needs to place appropriate
candy with appropriate
color, in the
in the box. And also you need to stamp it.
With, I believe, also
with we have two two type of
different colors of the stump. It needs to pick
the right one and you stump it
eventually. And it's confusing enough
for me, Yuan. But I think
there's also, by the way, one x is is not
the window is not sped up. This
is just real time.
I will just fast forward a little bit.
So, eventually, it does fold the box
and, it does rise
stamp.
Here, actually more of it,
this is waterproofing Also,
because this is actually a cross environment model, it's
trained on multiple robot
types, It does further cause
with different type of robots.
Which is actually with a Franco robot
that that that ties
that folds folds and and and tied it up.
Okay. So
for the for the base model, the base video
prediction model, we're this
time, we're adopting a pretty lightweight base
model. It's called LTX. It's it's
it's open source, but we adopt it because
you can possibly runs out
of real time. You use a sparse
memory as before, it does have a history context,
but not content. Not throwing off the histories.
We also adopt a
multi view consistency by, by using
coarse attention between the tokens
of the different views. So that,
the view generated between other the multiple
view generated considers a
cross.
The action models, we have a separate
action head. We have a group of diffusion
transformer blocks that's connected
properly to the generation diffusion
block. One by one. And
then once
the foundation model was trained, we
trained the action the
action block action expert
as before. And this was
trend on types of robots.
When Franco our own robot, and
the other one is some
some some platform.
And for the result, I believe,
if you look at the table, in general,
all method was able to actually achieve higher
performance. Than just using
the VA models, that are trend that was fine
tuned on the same amount of real world data.
Probably because during the window pre training, you
see small history context.
And and it captures more neurons of dynamics
between in the task, so it
actually attends higher performance.
So For specific
training, what we do is,
this is actually getting into the details. What
we found actually pretty effective is
once we have the the video
WIDO deficient model trend, We
actually freeze it have an action pretraining
step. So we just in this step, we we
just take basically all the robot data we
have, This is actually generic robot data,
not particularly one specific task.
We we fine tune we pretrained the action
diffusion block, and the second step,
freeze the action diffusion block, and then
we fine tune the video diffusion block to
this one specific target we are interested in.
And then in the first step, we actually open everything and
fine tune this end to end, to
the specific task. Like closed folding
or or box folding. That I just showed
before.
Here is, some more video
could see, this is actually the video generation.
By conditioning on the continuous action. This is actually the
simulator. It
does actually generate pretty good pretty
good realistic videos and also physically
consistent videos. The
bottom is the actions the gripper
right now is taking, it's representing
where to go.
And here is the
widow here is showing more
more cases. When more different,
challenging task.
I believe there should be one handling one deformable
object. It's not
showing here, but it's the pouring water
and pick up object. Etcetera.
Also, for the
the last last component of this work,
is the
embodied wall wall model benchmark.
So
rather than evaluating
the FID of the generated videos,
In robotics, we really
care about is
the generated trajectory, if they are consistent
or not. And the the
scene semantics, if it's consistent or
not. And also, does it actually
perform consistent across a
variety of diverse a variety
of of the of the scenarios.
So compared
to the wall models, that's
available right now,
we we we we didn't found a
pretty convincing metric. Like,
basically, other models are primarily focused on
generating photoresist videos,
and this benchmark, what we call it,
EWMBench, essentially is
a benchmark evaluating the the model model
itself. In a robotic setting in a robotic
robotic context. We have a
set of metrics that, we propose
we found actually pretty along well with the
real world setting we're talking about, what I just talked
about. And
I think the the good
thing is that everything was
open source. It's it's on our GitHub,
So if you're interested in we're
also maintaining this repo
we'll keep updated. We're come
to download it and and try it out. It's
it's it's it's very easy to to
to try it out. Okay.
So I talk about, how we
how could we use it
for action condition, for generating,
for permit, for to
generate windows,
and also Genie Visionary that actually combines
two of those, to have one unified
framework that does evaluation,
video generation, and policy learning, in
one loop. But
we start to ask, basically,
for the wall model, if it could
generate a video, then
you generate the first frame, you generate the last
frame. And you also,
of course, generate anything in between. Then
it actually naturally well suited
for constructing goal condition policies.
A goal condition policy
is you take a
current observation, and you take a goal image.
Basically, you specify your intention by taking a
goal image, you ask the robot
what to do. Basically, you ask the model
what to do from my current observation. How do
I do from my current observation
So that I can go to my goal.
Go image. This actually this method
is particularly useful
when other means of
specifying specifying intentions
are hard. Let's say, you
wanna draw something, like, draw letter a,
and that letter a has a specific shape.
It's probably much easier if you just draw
that letter a in that particular shape you
want, and then present it to a robot, and you
let a robot to do it.
So in actual goal, this is a work
we are releasing pretty soon, I believe one
week from now, We
really actually take
a step towards this direction.
What we have is that
instead of generating the whole
video, we basically
we additionally making
the warm model become goal
condition, is condition on the current observation,
also condition on the goal image.
You you put you take a image of the
the thing you want the robot to do, and it was
asked to generate a
set of set of frames, basically, a
set of key frames, between the
current observation and and the goal
goal image. The current observation can be can be
anything.
So once we train this
this was towards this step, everything
is was action free. We don't use
label action for this step, Essentially,
this could be amenable to using
any sort of, videos
videos on the on the Internet, maybe
human videos, Eagle Ford, and things like that. And
in fact, we did use a small portion of it.
After the pretraining was done, we
actually additionally attached an action
expert to the goal condition
one model we just trained.
And then this one was actually fine tuned to robotic actions.
So here is some, experiment
we did what we can achieve
this method. It's pretty simple, but
pretty, pretty use pretty effective.
So, the right corner, the right bottom
corner, is a gold condition
would basically take an image the goal condition,
and then we present you a robot present
to your to your model. And and the video
was the robot executing
the policy autonomously
by conditioning on the
on the go. So the left is
object sorting task. We want it to
be a into a particularly
order and into that particularly position.
The middle is a writing
task. We basically present
a robot with a set of images that
with a specific
English word, we want a robot to
write.
And and it just write
a write a ink swab in a in a whiteboard.
The rightmost is
we wanted that this task is
actually pretty precise. It's also object
is pretty heavy. We want you to grab it,
and insert the the the
peg precise peg into a specific
blocks specific holes
into the blocks. Actually
perform pretty well. So
For
that's the second step. For, basically,
if we want to further improve the performance
of the policy,
or if the policy is
actually in a totally new situation,
we actually have a
online autonomous improvement procedure,
in this regard. Because it's a goal
condition policy then what we could do is
we if it's in a totally new situation,
we could take one,
go image in that situation. Of course, the robots
will actually initially, if you haven't seen
that situation, most likely
will fail a lot. But
we we leverage a technique called
HER, or hand side experience replay,
Essentially, it's practicing
all the data in the
replay buffer, treating all the from
the episode every single observation
as they go. As a goal as a
goal image, and then it tries to
use that as a practice as
a to to practice on that
generating the new goal.
So here's actually one
video
that we actually ask a robot to
write to draw a
a picture that's actually was never seen in the
training set. Initially pull a
web polling, It doesn't know how to draw this
two square layout in this way.
But with her,
and we take
some and also some, online autonomous
improvement steps, robot was
able to draw it. You could say it's actually
improving on the fly.
About ten to twenty minutes.
This window is explaining
fiber aspect, About twenty to twenty
minutes, is joined to
basically to the
the to the desired shape of
of the going into that. That. We're
asking the robot to do.
If I could go to the end,
in the end, it's actually a pretty
good shape. So
the right is the right table
of is
something, is the right the right table
is, we would test this method
based on five different situations.
Drawing and all or five different tasks.
Robot actually performed decently well. All
of these tasks. It does actually improve
with practical range of
within practical range of real world interaction
time. And, also, the RIMALS is
that we also did ablations
for the replay buffer for the HER2
practice. If we need
to include successful
failure trajectories. I think the conclusion is
that we just we don't need to
really care about it. We could just let
robot practice all the
data it has on the repair buffer. And
it does actually improve, steady autonomously.
Over time.
Great. That's everything we have. I'd be happy to
take questions.
Okay. Let's send to
the speaker again. And, I
will, introduce the next
speaker. Pablo
Samuel Castro
from DeepMind and Mila,
He's a senior staff research scientist
in Google DeepMind in Montreal.
He's also an adjunct professor in
department of computer science
and operations research at
the University of Montreal. His
research interest about fundamental reinforcement
learning research. And today,
he will give us an introduction
about automated reward,
machines with via
foundation models for
conversational reinforcement learning.
Let's welcome the speaker.
Okay. Hello, everyone.
Thank you for being here. Thank you to the organizers
for inviting me.
Although when they invited me, my first question was
like, why me I don't really
work on world models.
And that's why the first question is like, why
am I here And
as I thought about it more,
listening to the the previous talks, I was thinking
more about what is a world model.
So when we talk about world models, what do we
mean by world models Maybe I do
fit in this world. Maybe I am actually
research in world models.
So of course, when the natural
things that come up come to mind when we think of world
models are things like Genie, which we saw, presented
earlier. We're simulating
worlds and and we're able to interact with worlds.
We can think of world models also as,
and this is I think traditionally
prior to Genie and things like that, what people would think
of with world models, something like Dreamer, where
where it's learning to reconstruct
inputs, pixel inputs, but it actually
does planning in latent space. Right So it's
no longer trying to reconstruct pixel to pixel
reproductions. It's using this
information to be able to then do, this
imagination planning or the dreaming
where the name comes from, in order to be able to plan
more effectively. But
we can go back further into 2021
and something like SPR,
where there is reconstruction loss
that's happening in
latent space. Again, we're not SVR wasn't
trying to reconstruct pixel the pixels. It was trying
to reconstruct these latent states,
and comparing things in the future.
So this is in a sense also a form of world
model, but not in the traditional way we've
we tend to think about it nowadays where we're not
sort of generating new trajectories,
new new videos, new pixels.
To be able to plan with that. But we are constructing
some form,
of structure or or some form of
reconstruction in in a sense of
of the world that the agent is interacting in.
We can go back even further to 2002.
So this is the EQ algorithm from
Kurz and Singh. And here, this this paper
was basically proving theoretically
how you can, get optimal exploration
or near exploration in in polynomial
time. And they did this by constructing this
sort of met MDP where
you have known and unknown states and so the agent transitions
in this met MDP, from known and
unknown states and this allowed them to
to have their theoretical guarantees, which at the time
that's all we could really do with R l.
Aside from from toy environments. So all of
these, I consider them kind of like world models.
Because we're constructing,
we're we're taking the the environment and
and you know, trying to to
to con have some internal representation
of it that allows us to do
learning faster or planning faster.
Another type of world model that,
thinking about this are simulations.
So these are three Nature papers
that, were deployed
in real world systems.
On the left, we have, this is a project I worked
on, strategic, strategy balloons to deliver
Internet. Sophie would
was deployed in in real, Gran Turismo
games. The tok Tokamak,
simulator that that's being used in
in fusion reactors to try to get, fusion
energy. All of these relied on
really high fidelity simulators, and that's how they
were able train in simulation. And
that's the simulators themselves are world models.
So what came to mind
to me is that for me at least, world models
taking taking a more of a generous
or or wider net in terms
of how I define world models in reinforcement
learning in particular, I view them
as kind of modifications or applications,
simulations, simulations. Of the
environment that you care in. And
the purpose of of these is to help learning efficiency.
Right So why do we care about reconstructing
decisions. Pretty videos, but ultimately,
for us as oral researchers, what we wanna do
is train agents quickly and and
effectively. So under that lens
of, of what world models mean,
I I do I guess, consider myself
a world model, so it's it's okay that I'm here,
I hope it's okay that I'm here speaking to you about this.
Okay. So this is the standard thing that you've seen in
many RL talks. We have
the MDP, or an agent interacting with an environment, etcetera.
And one of the key aspects for I mean,
the key component for why we call the
the our field reinforcement learning is the reward.
This is the reinforcement that the agent is receiving,
and it's incorporating that signal
in order to adjust its behavior or its policy
so that it can, the tests that
we wanted to do. And how do we specify
this reward This is extremely
important and it's how we
get agents to to perform well or fail.
Like, a very simple example is original
DQN paper that sort of spearheaded
that or started the field of DeepRRL that that
most of us are working in nowadays.
They had all these, you know, 57 Atari
games, and they all have very different rewards.
Scales. So training a single network to deal with all of
them proved. Quite difficult, and
so the the approach they took is they clip rewards between
negative one and one. And
again, this is taking very generous view of
world models, but I kind of view this as a world model in
itself. Because, again, it's taking this modification
of the environment that they cared of at the
time and adjusting it in order to
achieve, learning efficiency. And in this case,
it was a unified approach
to to learning, good strategies for all of these
games.
Rewards are very critical. This is a a famous
example from a few years ago from OpenAI.
Where they had this
boat racing game, and, the boat the the agent
found figured out that it doesn't So
humans play this game, they wanna to finish the race quickly.
But the the agent, because the number
of points it got is part of the reward
function, it figured out that if if it just does this
these loops over and over again, it gets a whole bunch of
points, but it's not actually doing the thing we
wanted it to do, which is race the boat quickly. It's just
accumulating points because that's what the reward
function, said it should do. And this is often
referred to as reward hacking and, of course,
it's something that, we care about deeply
especially nowadays where
many of, the peep of of us and and others in
the field are are thinking about post
training or LHF or LVR, etcetera,
where maybe not LVR, but RHLHF,
where you're training a reward function, and this is what
going to adjust the behavior of your of your foundation
models to them to do the thing you want them to do.
Coming back to this paper that we worked on,
with some colleagues a few years ago, we were trying to
deploy Deep RL in,
stratospheric balloons to bring, Internet,
across the world. And
we had a a good simulator. We had a really, really
high fidelity simulator for this, but
the people working on on on there was a company,
Loon, that were working on this weren't thinking
of reinforcement learning, and they had handcrafted
controllers for for the balloon. And
so what they they asked us to do is
to see if we can use RL
to improve on the controls because what they wanted,
what they needed is this balloon to come over
a certain area to deliver Internet
to that area. And the way
the balloon, navigates was just
basically by surfing winds. It could only go up and down,
and then the stratosphere winds push you
in different directions. And so the balloon has to sort of
do these fancy figure eight things
to stay one place. Otherwise, it just gets pushed,
across the world.
And this is one trajectory of the balloons
sort of navigating over
to this this
area where we wanted to deliver Internet, and this was
controlled by our our Deep RL agent.
So when we started this project, we came up
with a reward function which we felt was
was pretty natural, where you want to
have high reward if,
you're in the area of interest,
and lower reward the further away you
get. So you can see that we want to
be within 50 kilometers
of the the center of that circle,
of that target circle. What
ended up happening when we use this reward function
is that the balloon figured it out figured out
how to navigate to the to this,
area, but then it just kind of hugged
the circle. It just stood stayed
in the perimeter around the circle,
didn't really go in because there was no real incentive to
go in. It actually it seemed like,
it was getting more signal by just kind of
stepping outside of the circle and coming back in, and
that's where what the reward function
led it to produce, which is not what we wanted. And
again, this is the difficulty and the importance of
coming up with good reward functions.
And when we tackle problems that aren't
given to us in in benchmark or existing
simulators, this is one of the key issues. How do we
define an environment is and in
particular do we define what the reward function is so that
we get the behavior that
we actually expect, which isn't always specified
cleanly in as a reward function.
What we decided what we're what I'll be talking to you
about is, work that we've been doing
with, with, some students and and
colleagues at Mila on seeing if we can use
some of these modern foundation models
to help us in this task of designing reward
functions that can help speed up the
learning and make us make our agents learn,
the tasks effectively.
So what I'll be presenting is a paper that we,
put out not long ago led
by, my student, Roger, who might
be around here. Glenn was also giving a talk earlier.
I don't know if he's still around, so you can also go bug
them. If I don't answer your questions
to satisfaction.
And so what we're doing, what we wanted
to leverage in this work is the use of reward
machines. Which is a a notion that the,
an idea that was introduced a few years ago
and it's to try to specify these reward
functions. We typically think of reward functions
as a mapping from states, to to
real numbers or states and actions to real numbers.
This was trying to give more structure
in in a way that's easier to specify
than these, state action, reward
functions. So as an example that they
use in this paper, they have this offer grid
world where the agent is supposed to bring coffee to
the office and also, maybe get
the mail and avoid these, decorations
because if it steps on the decorations, it breaks them or
something like that. And so the
reward function you know, we could specify
it manually where you get a reward of one if you
get the coffee and a reward of 10 if you deliver
the coffee to the office. But it
starts becoming combinatorial because you have,
these prepositions of whether you have the cough
or not, whether you're in the office, whether you expect
stepped on a decoration, etcetera.
So even without that, you have already, you know,
a 108 states with four actions, which isn't
that bad, but if imagine scaling
this arbitrarily, and then it becomes quite difficult to
specify these rewards manually.
So their proposal is to use reward machines, which
looks something like this. They're finite state automata,
which have these prepositions like you
have a coffee or not Do you did you
step on a decoration And it has these
terminal states, and you have rewards.
So if if you're in the office, you haven't
stepped on the on the
decoration and and you and you delivered the coffee,
then then you get a reward of one.
And formally, this is the the the formalism
for the reward machine. I won't go into,
too much of the details. The only thing I'll mention
is, the the states u.
So these are states of the automata
that define the the reward machine.
This will come back later, so just wanted to highlight
that.
So one of the tasks we started looking
at, and this will be sort of the running example,
throughout the talk, is
a mini grid. Mini grid, we can define,
these pretty challenging tasks where
the agent has to pick up the yellow key,
open this yellow door, pick then get the
red key. Then open the red door and finally
get to the the green, the
green dot. So designing
a reward function to produce
this, this behavior is
quite challenging, and you can do it again just
by defining, numerical rewards when
it picks up the different objects, etcetera.
It becomes a very difficult, sparse reward,
long horizon problem, which we all know as RL
researchers, is is quite challenging. And
also to specify the reward becomes difficult. It's a
lot easier to do it how I just did explaining
it with words. And
that's exactly what what we're after here. So
what we'd like to do is be able specify things
in words and get some some type of reward
machine like this. And this is what what our approach
is is, aiming to do.
So we first introduced this notion of language
a language aligned reward machines.
So I mentioned the states in this, finite
state automata that define a reward machine.
We are, assuming that we have
a textual description of each of
these states. And we're we're not this is not assumption
that somebody's handed it to us actually
going to be using, these foundation models
to come up with these textual descriptions.
And additionally, also so for instance, here's
an example, rather than specifying these logical
prepositions, we can, convert them into text
just like this. And and language models are pretty good
at doing things like this. And additionally,
we assume we have an embedding function, which we can get
from our language model itself. That takes
that textual description and gives us,
some some vector in RD. The
reason we want this is because
this gives us a sort of a a a latent
skill space that's semantically grounded. So
so nearby points will have semantic
coherence and this will allow us to have
some form of generalizability, which which I'll demonstrate
later.
Okay. So we start with reinforcement learning. This is the
standard reinforcement
RL agent that gives actions to an
environment. The environment returns the reward, and
the agent learns from that. And the environment
is typically defined as an MDP, where you have
states actions, transitions, functions, and
reward. And the agent's behaviors is formalized
as a policy that maps states to
distribution over actions. We
With with, these alarms that we have,
we're extending this. So the environment
now the state goes through
a labeling function. I'll explain what
this is. In in a bit. This
labeling function is what sort
of is is is making us,
transition within the reward machine that's also
going to be automatically generated. The reward machine
would will produce its own reward,
and a new reward machine state,
we can pass through our embedding
and then the RL agent will will use this. So
now the policy of
of our RL agent depends
not just on the environment state, but also on this
embedding of the textual
description of what reward machine state we're in
at the moment. And the MDP were
a new MDP that, the state space
is enhanced is expanded with
the reward machine states. This
delta is a transition function for the reward
machine. And this is the labeling function, which
I'll explain in a little bit. So transitions
in here now, you know, we start with some state
and some embedding of of our reward
machine, state. And the
policy, as I said, is is a function of
that. So we get a new action, we're
going to transition within our MDP just as
we do normally with our transition function.
And we're gonna take the labeling function
which tells us basically, do you have things like, do you have
the coffee Did step on a decoration That
type of thing. Which of the prepositions have become true
with this new transition And
this, we we sent to our our
transition function for the reward machine, and we
get a new reward machine state,
which we can then embed again and,
continue this process. So this is our enhanced,
MDP m prime. That
combines traditional reinforcement learning
with these, language aligned reward
machines. Now
the reward, is also, enhanced.
So rather instead of just taking the reward
from the environment, we're going to add it with the
reward reward
machine, and this is going to be quite important
because as I as I said before, it
can be difficult to specify a reward
that gives you the objective, the behavior you
want, but also in an effective way.
You know, you you end up with sparse rewards or
or long horizon problems. So our
hope was that these reward machines can can give us a
denser signal that guides the agent
in a way that's more effective for learning.
So let's come back to this this, run
prompt where the agent has to get this green ball
and has to open up these two doors before it does this.
The way our method works, which is
our RMFM method, is we send
this, the specification.
So we're using a VLM that can understand
these these images, and it can understand
text. We send it to our foundation model,
and we ask it to generate three things.
The first thing is the reward machine's language
specification. So it's going to try to convert
this, specification we gave about
what the objective is into,
this formal language of, for reward
machines. So it has the states, it
has the transition functions, and it has
the rewards. It receives in each of the
reward machine, states.
It's also going to produce labeling functions. So this
is the labeling function I mentioned before. Which
basically tells us, you know, did you
open the key Did you get the, sorry, did
you get the key Did you open the door
Are you next to the green, circle, etcetera
And finally, it's going to give us the,
the language description
of these, reward machine states. Along
with their embeddings. And the embeddings we kinda get for free
because we're using neural nets, with
these foundation models.
In addition to that, so we have our reward machine
generation. But as we all know, these
language models don't always get it right the first
time, and you have to sort of probe them and and
try to get them to improve. So we have this,
self improvement loop where we
have two we have
the generator foundation model that gives us the reward
machine, but we also have a a
critic foundation model that's going to take that reward machine
and give automated feedback
to the generator on whether it
it it actually produced something useful or
not. We have an optional
step, which is final human verification
because everything is specified in
It's quite easy for humans to look at this
and say, this is not what I wanted.
This we we believe is is actually quite useful
because especially when we're dealing with real
world problems, where as humans, we can,
you know, we know it when we see it. If
we if we look at a specification of,
this formalism of reward machines,
it's pretty easy for us to say, no, that's wrong.
And so we can give that feedback pretty pretty
easily with an up down,
but it's a lot harder for us to you know, manually
come up with these reward machines. So
we feel like this this optional human
step is is quite useful
in in our setup. For some of the experiments
we did, give some of
this this human feedback where we basically said, no,
try again, language model that wasn't
wasn't good enough. And and it was able to improve.
Okay. So let's go into the of the
empirical evaluations. We're going to look
at four different types of environments.
Mini grid, craftium, meta world, and,
excellent mini grid. And these are
we chose these environments to evaluate different,
challenges capabilities of of,
of our system. Mini
grid, we're going to be we chose because as
I said, it has this this long horizon sparse reward
problem that makes it difficult for RL agents to learn
there. Craftium is a
is a reduced form
of of of Minecraft,
that is three d. It's pretty challenging because
you have to, somewhat similar to
mini grid, you have to achieve certain tasks
in order, like to mine, the ore or the
the metal And
if you don't do it properly, then you you just
don't get to do it. MetaWorld is a continuous control,
environment. And finally, XLAN
mini grid, this allows us to test, for multitask
generation because we can these are procedurally generated
environments, and so we can, test how
well our system generalizes.
So as an example for some of the things that,
our our method produces, these
are this is the for this particular task.
This is the reward machine that it produced in
the in the language that we specify
so in the prompt, we we kind of tell
it this is the the form that reward
that we want reward machines take. And so
if it it's pretty capable at producing things
like this. It produces labeling
functions, which as you can see, they're just Python functions
that assume we have access to the
environment state, and so then it can check
whether it has the coffee or whether it
has the the key or whatever. And finally,
it produces these natural
language specifications of the
reward machine states. Along with your embeddings.
So So we compared for this mini
grid with DQN
and, we also compared with an LMM
agent basically, just seeing whether an LLM
we can prompt it and ask it to
to, give us actions.
And, you know, this is not
learning, so that's why the we have a flat line, but it gives us
a a reasonable baseline to see how well
we compare against just doing anything
with LMS. As you can see, our our blue line is
is able to pretty cleanly
surpass all of the the baselines.
Most of these environments. And in fact in some,
like in this environment, it's the only one that's able
to make progress, the other one's completely flat lined.
And we tried it with even harder environments
that have really long horizons
and very sparse rewards, so none of the other methods
are able to make any progress because they basically get
no signal just exploring around. And we know
this already to be an issue with
horizon sparse reward problems, you can
make square mini grid, worlds
that are very large and just Epsilon
greedy type approaches won't work.
But using this idea of reward machines, really
get some more dense reward signals that,
enable us to to achieve this
properly. So craftium,
as I mentioned, is is, this reduced form
of of Minecraft where the agent has
to mine don't remember what these represent. This is like wood.
This is I don't
know. Steel. Gold, and diamond
or something like that. And it has
to do, mind them in that order. Otherwise,
it won't be able to to get the
the diamond.
Also, obviously, it has to, like, learn how to move
and and use its ax to to to
break these things, etcetera. We
use PPO as a baseline, and you can see it
does quite poorly. It's, like, barely
able to to mine wood.
And it after 10,000,000 steps, it's it's
unable to to do much. But when we combine
PPO with our reward
machines approach, we can see that it is able
to to pretty cleanly, find the diamond,
and so this is quite a dramatic
improvement over what we could do before. And
it really came from this automated approach
of specifying the objective in
words and have, leveraging the
the natural language understanding capabilities
of these reward models of these
language models, sorry, and their capabilities to write
code to come up with, these reward
machines, automatically and help
us in in this learning task.
In meta world, this one's perhaps a
bit less exciting, but, we're still able to to
improve things, you know, just
But Exon mini grid,
as I said, this is these are procedurally generated
environments. And so
we generated simultaneous environments
to test the multitask capabilities of
of of of our
system. So basically, can you come up with a reward
machine that useful for multiple tasks
at a time And you can see that as we increase the number
of simultaneous tasks,
using our our full system is really what gives us the best
performance. And it's able to achieve
really high success rates. These also
include ablation, so what happens if we remove
the embeddings What happens if we don't
use the the, reward machine reward
and simply use the the the rest
of the the the pipeline of
the reward machines, just not use the additive reward.
And we can see that really all of them
are important to achieve the the best performance.
We Another thing we we
it investigated was the zero
sought generalization of our
system. As I mentioned, we
part of the the motivation for using these embeddings
of the natural language specification
this latent skill
space that we feel
allows us to to have these generated
generalization, capabilities. So we
trained the agent with these two tasks.
That know, they're kinda similar,
but but they're different in the sense
that, this one has to pick up a blue key, this
one has to position itself to the right
of the of the blue pyramid. And
after doing that, without any extra
training, we specify this new
task where it has to pick up a blue key
and, move to the to the right of the blue pyramid.
So it's sort of combining objective
from the two separate tasks.
And seeing if if it's able to generalize
well, then it should be able to leverage the
latent skill space that it that it learned,
in order to generalize to this new task without
extra training. And and,
we were, very happy to see that it that it was
able to do that, quite successfully.
So it this is just sort of showing the trajectories
of the of the agent. But the important
thing is that these embeddings
they they have a semantic
interpretation or they're they're they're meaningfully
placed within this latent space that,
in such a way that we can combine
different skills and come up with new tasks and
have this this type of zero
shot generalization.
Most of the experiments that that we showed
before were run with, GPT four o.
At one point we lost access to GPT
four o, so we started experimenting with other
LLMs that that we still had access to.
And this, led us to ask the question, well,
does it matter what LM you use And in fact,
it does. One thing that's neat to see
is that as we use more
and more capable LLMs, able to
get more and more useful,
reward machines. So here we use an, separate
LLM none of the ones that we used here.
As a verifier. So we asked the you know, how we had the
critic that in self improvement loop, we can use
an LLM to ask it,
is the reward machine correct Is the
the the labeling function correct
And whether both are correct. And
we can see that the more capable models
are able to to
perform better. So bigger models, better models,
better reward machines, better RL training.
I already sort of hinted
at this the advantages of having this well structured
latent skill space. And here,
the colors represent, different points
in the task at hand.
So, you know, it could be picking up the red key,
that's the first thing you have to do. Or at the end,
you're already moving towards the the
green circle or something like that. And we can see
that, when we do this PCA
projection, we do see
grouping of of the of of these tasks
in a semantically meaningful way
which is, I mean, a qualitative
assurance that that the the method is
is doing things properly, and
know, it adds further evidence to
the claims that we have that generalizes well and can
potentially do, zero shot,
have zero shot performance.
So okay. So that's I'll I'll
end there. I know we're we're a bit
over time. But
this for me has been quite eye opening.
I I for the past few years, I've been pretty
skeptical to step into this LLM
world, but Roger was
was quite enthused by this and he managed to convince me
that this is actually quite powerful. And so I'm
very excited for, I know there's other works
that are doing similar things. I've of leveraging the power
of these, foundation
models to be able to, improve the
the learning capabilities of our our
l agents without necessarily having to
just have the LLM do everything. So we
can leverage them for those of us that
still work on on classic
RL. As I said, Roger and Glenn are
still around, so if you have
more questions, you can ask them. And if there's time,
I can answer a couple questions. Thank
you.
Okay. Thank you for the presentation and
sharing. And
does anybody have any questions You can stand
up to the microphone.
Oh,
Hi. I I work in robotics. There's been
a lot of similar work over the last few years,
like Nvidia's eureka, Text to Reward and such that
leverages very similar methods. I was
curious of the specific
paper why you chose to go specifically
for a reward machine as opposed to just the
Python program like the rest of the labeling functions What does
what additional benefits does the structure of the
MVP give you out of curiosity
So the part of the the advantage of going with reward
machines is that, they're a concise,
mechanism for specifying,
the the the evolution or the
the trajectory of the reward functions.
And this allows us to to do have this human oversight for
instance. In a way that
Python functions, although they are legible, they're
arguably a bit more complex to to just look
at and and be able to to assert whether they're
they're doing the the correct thing or not.
Alright. Thanks.
Hi. My question is
specifically regarding the training of the
framework you have. It seemed very similar
to GANs basically.
Whether you have generator and
a discriminator
The offering GANS is a problem of initial
training
There's problems in training of
both things because you're training both them together, right
This framework Were there any tricks or things
you used to stabilize the training
Framework here So it's not quite like
GANs. So the the critic that we're using,
I'll go back to that slide.
We're not training that critic. So we're we're using,
Here. We're
This this critic here, we're using a foundation
model. We specify
a prompt but the language
model is pretrained. So we we wouldn't have the
same type of pathologies that that GANs
would have. Even the the generator foundation
model, we're not retraining any foundation. We're using
pre trained foundation models. It's just
the the prompt that we use, to generate
the the reward machine, and then the critic gives
a feedback that is also included into the
prompt to to then refine the reward machine.
And in this loop, did you initially
use auto training, right For this loop and then at the
end you use some human No. No. So
we don't train the the language
models. Nice talk.
Seems like your I'd like you to talk about
how this relates to reward shaping. Because in
these examples you gave with Mini
Grid, it seems like there is a
goal state, but then the other things were
like procedural hints. Like oh,
you should first pick up the blue box. And
and maybe that could be misleading, right
Like, you might get reward hacking kind of like
so that was first question. And secondly, your baselines
when you just have DQN, without
the reward machine, does it just get the
reward for the goal or does it also get
pseudo points for the intermediate steps
Great points. So
on the reward hacking front,
that's that's absolutely correct. So if you have
a misspecified
objective because we
can make errors as well. I would say
it it's still probably going to be prone to
to reward hacking.
You can under specify the objectives.
And and
that for something like mini grid, I imagine
it wouldn't work well. Like, if you just say you need to
go to the the green dot,
I would actually be surprised.
Mean, I've been surprised by LMS, but
before. But it, having it figure out
the intermediate steps to do that might
be challenging. Maybe with
something with like the self improvement loop
you could get something
that automatically is able to figure out
the intermediate steps. I'm not sure.
One thing, for the
for the baselines, so,
the DQN is with with just the environment
reward. We didn't give it the the intermediate
rewards. This is coming
from the actually,
should verify that because this is basically using
whatever reward specification is in mini grid.
And Mini Grid, the default setting
is actually reward is not Markovian,
strangely enough. So, it might be a bit
less sparse than what we would traditionally imagine
when you just get to the goal. I believe
in the paper we put some ablations where we added
the the reward
machine reward to DQN
without any of the other stuff. And that
helped quite a lot, which is not terribly surprising because
it's making the the space more denser.
Yeah.
There still a 10
Okay. Hi.
Have you considered using this method for
more complex environments like computer
use And what challenges might you end
up I mean, we've thought about it. It'd
be really nice, There's
this is so I at DeepMind, but this
work was done with with, at Mila with with,
with the students there. So there's a limiting
compute in in what we can and can't do.
It would be interesting. I mean, this
is a more general comment,
but, it's
a standard frustration or tension that we
have when we're working in academic research
and we wanna get our stuff published
where you often you know,
you have an incentive to go with standard benchmarks because this is
what reviewers understand. Ideally, you do wanna
test on on more complex environments like what you're
suggesting. So just that the incentives
aren't always there. That being said, it is something
that that we're quite interested in. So if
you want to more, we be happy to chat more. Maybe
there's something we can do there. There.
Alright. You.
Okay. That's that is all
for our today's
keynote sharing, and we'll now
proceed to our oral representation.
Session. And
and we have a total of five papers got,
selected as the oral
paper, and, the first one is
why pry video predictions for robot
actions Let's now welcome
Sandeep for
for the presentation.
Okay. Just a moment.
I'll just use the HTML, you think.
Happening
I'll keep the changes.
Oh, we should duplicate. Right
Let's do show only
on Are you on point. It
probably won't show on the booking.
You just
Maybe
I need to see. Right
Extend
Karen, there is no way to to do
it without extent.
We would have to change the resolution
Yep. So
just
If you hit present, it will show up there.
Hello, everyone. I'm Sandeep. And,
I'll be presenting my work
WIPRA, also called Video
Prediction for Robot Actions. We
Okay. So let's start with the motivation.
So this this
work has two access motivation. The
fast the first access being
robot learning these days needs
large amount of, action label datasets
to really scale
learning. But they are expensive to collect and,
time consuming as well. But
there are plenty of human plenty of videos
on Internet of human doing
tasks or robots or, like,
simply, like, where videos
the wild, they're
abundantly available. But they don't have any consistent
action levels as such.
So on interesting question
to ask here would be, like, can we use
this, actionless
videos to facilitate robot
learning. And the second axis,
of motivation for this work is,
have had significant progress in pre trained video
models. Like OpenSolar,
CogVideo, Cosmos, and HumanVideo.
So these these video models
have been trained on, like, large amount of
video data, and they already capture some sort
of physical representations of
the environment. So a natural
question to ask here would be like, we turn this pretrained
video models into robot policies
So Bipra our
work, BIPRA tries to tackle both this motivation
and bring them together. So we prioritize
a framework, this
pre trained video models and turn them into
robot policies while
simultaneously using unlabeled human
plus robot videos. That it'd
be easier to scale. So
I'll present, the
framework on a high level here, then we will dive deeper
into each component. So first,
we had, like, videos coming actionless videos coming
from both human and robot. And
then we learn
unified latent action representation
which we call z t here. So what z t
here represents is like
a motion represent motion aware representation.
And, represents the transition happening
from t to t plus one.
So, we train this completely in a
self supervised manner, just from videos
learn these representations. So once
we have these abstract motion representations,
we we use, underlying video
language model a large video language model,
and, develop a pretraining scheme
that jointly predicts the future visual
step and the latent action that we learn.
So once this pretraining is done,
we have a third stage.
Where we fine tune this
setup. On any downs
downstream task with, like, minimal amount of action
level demonstration So this can be deployed in
both real so we have deployed this
in real world in our own
setup. As well as in various,
simulated benchmarks. And,
as you can, as you can see, the cross
there is cross embodiment generation baked in
into this because it naturally exploits
both human and robot videos.
Okay. So let's move on to the latent
action model. So the latent
action model for the latent action model, first,
we take, like, a sequence of frames that
are sampled at, like, three to six hertz.
We ensure the sampling, temporal
coarseness in the in the in the videos.
Then we pass it through a latent action
tokenizer, which is essentially an inverse
model. Which we call I beta here.
And it learns to infer,
representation z t, which is
representing transition from o
o t to o t plus one. So
so this I beta here is,
like, which is, like,
user design for inverse models. We have, like,
non causal bidirectional attention within
this. And we also have an information bottleneck,
ensuring that the latencies are capacity
limited, and we will use we will see
shortly why this is So then
we have a forward decoder. The forward decoder
takes in like, the history
o o zero to t. O zero
to t and zed zero to t, and tries to reconstruct
the future frame t plus one.
And this this is a causal, causal decoder.
And we train this using reconstruction loss
some per perceptual loss,
which is the LPS loss here, and a optical
flow consistency loss
so this optical flow consistency
loss ensures that the reconstructed
the optical flow of reconstructed frames
and the optical flow of the ground truth frames
match. So the reason for having this
optical flow consistency loss is like it is like an
explicit loss ensuring that, our model
learns to encode motions from d
to t plus one, t plus one to t plus two, and so
on. And the reason for having this information
bottleneck here is to ensure that,
since if if the
latents are here here are, like,
have high capacity, they could simply learn
to copy the future directly from
the model since it already has access to the future.
So ensuring that it's capacity limited,
it is necessary to
make sure that the loan only motion and
not, like, irrelevant background noise.
So so we learned this end to
end in a sub self supervised manner from
actionless videos. And once we have this,
we we we then
also also before before moving on there, we have
some, illustrations here.
Of the latent action
model actually encoding motion centric dynamics.
So here we see, like,
ground truth video. The video
is ex exhibiting in motion or moving
up, we pass it through the
I beta, and we infer the latent,
for upward motion.
Then we have another ground truth video.
This is a ground truth video where
the person is person hand is moving
down, what we do is we take
the first frame of this video, and we use
the upward latitudes
that we extracted. And,
autoregressively decode audio video.
And we will see that this new video
has upward motion in this. So
what this, implies that our latency
that we learned actually had
the up up the
notion of up encoded in them. So similar
example here. Showing left
and left to right.
Yeah.
So Okay. So let's move on,
to the pretraining and fine tuning framework.
So we have a video
language model. So with the video language
model takes in a history
of, visual frames.
And, it it it predicts
a frame like after k steps.
And then, we take the related action model that
we learned I beta, from the
earlier stage, and we predict the sequence
of latent action latent action,
like encoding the transitions happening from t
to t plus k. So here it is. So
this is this is the pretraining
part. And note that, here, up
until now, we have been using actionless
videos. There is no action required in
any of this till now. So once
this, pretraining is done, can deploy
this model, on any downstream
environment of our choice by just simply, like,
fine tuning this model And here,
we used, like, a flow matching decoder with
action chunking to
do the fine tuning.
So we we, here are some simulation results.
On the simpler environment,
and, we'll go through this
slowly. First, we have,
like, first key takeaway here is, like,
we have two variations of our model. One is a
discrete action model, well, which predict,
like, like, discretized
action. Through binning. Autoregressive decoder.
And we also have a continuous version of our model
which predicts continuous actions. A
flow matching decoder. And in both variance,
we outperform other baselines
like existing in the field.
So in particular, the scratch
baseline here is like we simply
take the video language model and, fine
tune it on the domain instead of doing the
pretraining. So the improvement over the
scratch based lines indicate that,
our pretraining is an
effective procedure in learning good
representations. So next, we have,
LaPanda and Univule. These are some
like, state of the art latent
action works existing in the field, and we
also managed to, like, improve upon them.
And then finally, we have
comparison with OpenVLA and Pi Zero.
These are, like, state of the art VLA
models trained using large amount
of action label data. And
we also, like, improve
upon them as well.
So these are, real world results,
and we test the real world results in
our own back end world setup. And we,
again, have, like, the scratch model
and the pie zero, and our model, like, trained
here. And,
we obtained, like, better performance than
all the baselines. So we observed
some emergent behavior like generalization
to unseen objects
tasks, and visual variations, and, robustness
like some retry behavior and,
adaptation under perturbations.
And, high like, we we also
do kvCashing to ensure high frequency control
here.
So I have some videos, like, for example,
like, the upper
video here is, like,
like, a task which was seen in the training set,
like, covering Apple, but
it can also do covering baseball, which was never
seen in the trading set. Similar
examples here.
And here as well. So
And we we we did did
absorb some emergent robust
behavior. Like, for example, if
initial attempt for grasping
fails, it goes again.
And attempts to grasp the cloth here.
And the cup. In the other
example.
And, it is also like
if if there are some external perturbation here,
it can adapt itself to the perturbation.
And handle it. Finally, also
test this in, challenging bimanual setup.
Which are significantly more challenging
than single answer. Answer tops.
And, finally, this is the final example,
we show demonstrate that. It can
also do high frequency report control.
And we use kvCashing to attempt this. So these are,
like, side by side, rule outs. One
is at 3.5 hertz. One is at seven
hertz. And you can see that, there is
significant delay, slag and delay. In
the right example.
So yeah. So yeah. Thank
you. Any questions
I'm okay. Thanks. For the,
sharing. Of the speaker.
And our next paper
is
and let's welcome
Yuxin Zhang from Ichabod to
share his paper.
Okay.
Because it's
But that would be the same.
Sorry for the technical problem.
Hi, everyone. I'm Lee Liang Chen
from AGI boat. And
today, I'm so glad to hear to
present our work
and inverse AC it's a
generative
simulator, recently always be called
the war simulator.
So And first, let's, briefly talk about
why we need a wireless simulator.
A key application is about,
policy evaluation. You know,
evaluate a policy robot in
real world always consume a lot of
time, and always cost a lot of
materials. And sometimes
may fear meet some,
safety problem, make it a little bit
dangerous. And also a very important
problem is about reproducibility.
And it is almost impossible to reproduce,
exactly the same value you
last time meet. So that that's maybe
a big problem for
evolve your policy.
And common use of ways to evaluate
your policy in a physical based
simulator like ISAC C.
But so far, this kind of simulator
still struggle for some
complicated simulation like
deformable objects non rigid objects,
and, fluid,
sometimes complicated lightning
and shadow. This kind of
domain gap make it
always the conclusion from
the simulator is
not the same as a real world.
And the other problem is about
scalability. The simulator
whenever it is created, it cannot
be better or it cannot get better.
But for a wall simulator, we can
make it, better and smarter
if we feed more data into it.
So
So what kind of model we need to
build such a world simulator
First, it should, generate a
subsequent vicious state. So it
makes sense to be a a video generation
model. Sorry.
So And so far, a lot of
modern policies requires
a multi view observations to do
some press precise
manipulation task. So we think
our simulation cover this kind of
features. So we extend it to
be, multi view video generation
model. And
the most important part, the generation model
should be conditioned on the robot action.
So Okay. The
problem setup is clear and then
the most important problem
is about how to represent the continuous action
condition. Because for robots, action
space is a virus, and, a
lot of robot type, a lot of,
expression for action space, like,
joint and effector, velocity,
a lot of kind of, representation.
We want to represent the
condition in a universe way and
the more important thing is to
make it easy to learn for our generation model.
Find some inspiration from
such, anime generation work,
You might have been you might have seen such,
amazing results for
like, animation.
The secret for such, strong controllability,
it comes from the pixel
aligned control. Like the post
map on the left.
So And we think, we can
try try to convert the action
condition to be the pixel aligned
condition. So
Specifically, we use the camera
intrinsic and the extrinsic to render
the robot's continuous
action. Especially for the
60 and any factor pose,
like the position and the rotation.
Together, sir, with the openness of
the gripper. Render them into
the two d image plan. So we call
that action map.
So Then we can train a video
generation model We use
diffusion model manner, and we
simply concatenate the action
map with the noisy
hidden state. It's and it works
where Here,
I have to mention about the training
data. Not only use
the expert demonstrations to train
such, video diffusion
model, we also add some
imperfect value data, and that's very important.
Because you cannot expect a generation
model which never seen
value data to generate
visually plausible value interaction.
But that is very important for
a simulator.
Here I show some multi
view video generation results.
You can see for each view,
it presses link follow the action
condition.
Okay.
When we have this, video generation
model as a simulator, how can
we evaluate if it's
a reliable simulator for policy
evaluation Think the
most convincing way is to
evaluate the policy in our simulator
compare it with the sim
the evaluation result in the real world
under the almost the same visual
initialization. Here, I
show some evaluation process
procedure. For
a policy model in our
simulator.
Our experiments demonstrates
that the the result
in the real world and the result in our
simulator is very consistent. You can
see the left. The left left is
the generation video
compared with the real world in
the in the real world. And the right
side shows the success rate
for the evaluation in the real
world and the success rate in the
simulator. I mean, our war simulator.
We try different
policy, and we evaluate different tasks.
And the results are very
consistent.
Okay. In conclusion, there are some
So first, we prove our
action map is effective by pixels
based alignment. And second, the data recipe
is very important, especially for
the negative samples that mean the value data.
And the last one, we demonstrate
the evaluation with our
model can be consistent with the real world.
Yeah. That's all. Thank you.
Okay. Thank you for the sharing
And our next paper is
BLAOS, and
Yes. Welcome.
As welcome from AUS to represent
the paper.
Hi. I think I will present to you,
Aman.
Or if you if you would like to, you can
open the camera and or
and the PPT share it.
Okay. Okay.
Yeah. Yeah. Yeah. Please begin.
Okay. Thank you.
Hello, everyone. My name is Tong Kai Gao from the
National University of Singapore. Today,
I'm very happy to present a BOE OS,
Structuring and Detecting
language engine model.
And they are pretty good because they can
solve non horizon and complex
and delicate tasks. And show
superior spatial and object generalization
ability. And it does have some
data scalability. And, recently,
some papers are incorporating the
task planning models into
various through either language
planning, video planning, or word models.
And they are making way becomes better by
improving the performances and
the data efficiency. Enable
enabling conversational generation
and making where it becomes explainable, safe,
and controllable. However,
these task planning methods in VAE
exhibit a significant divergence across various
aspects these methods vary in
planning representations they use.
The paradigms, the bill
and bank phones. The
training data sets. The network architectures,
and the training strategies.
This all makes it very hard for us
researchers to determine which
component is more important and which one needs
to be further improved. So
in this work, we present a VLAOS
unified framework for evaluating VLE
OS task planning. We provide
a standard and a unified implementations
across VLAN backbones.
Data sets, network architectures, and training
strategies. And explore the
effects of different VOA paradigms and
planning representations.
This makes VAOA OS a strictly
controllable experimental investigation.
We first divide the VAOA
into four categories of paradigms.
The first category is planning only
VAE, which only performs task
planning. Using a per train VOM.
We do not compare this paradigm in this work
since the no trainable low
level action networks.
The second category is action only VAE.
Which is trained in an end to end fashion.
And, directly generate a robot actions
from visual and the language inputs.
The third paradigm is integrated BLE,
which use a monolithic network to simultaneously
generate a task planning output and
the robot actions.
And the last one is hierarchical bill,
which employs a hierarchical structure
First, generate a task of planning results.
From some high level planner. And then
use them to generate actions
with no gradients between these two levels.
Then we build a series of composable network
modules with unified structure.
We first, retrain
a series of bank phones with
the same network structure and only different
model sizes.
Then we design the action heads
for for for the action generation that
can extract the VOM keys
and values from the VOM
with blockwise cultural attention, just
like
So And finally, we designed
three different, planning heads for
different representations, including the language planning
head, Visual planning head includes
the bunny box,
planning, any factor
flow planning, and affordance area planning. And
the go image generation planning head.
Which is used for, in many world
models.
Then we can use this modular network modules
to implement different VOA paradigms
just like building in with LEGO bricks.
And all of them supports both l one house
training and the flowmetrics training.
And finally, we manually label
large scale of multimodal manipulation
planning datasets. With the help of
some foundation models. And, this dataset
is built across different variable dimensions.
Object types. Robots,
fingers, and, platforms.
With all of these preparations, we
perform comprehensive evaluations to investigate
a different paradigm and
representations of very we first do a
sanity check on the labor benchmark for our model.
And we can see that the VAE
OS performs better or comparable to other
exist
Note that our model is trained from scratch.
And, utilize only,
0.5 b
backbone. This result
shows that a larger video is trained on larger
scale data size. Do not
necessarily outperform smaller models than
are trained from scratch. Model
architectures and algorithm designs are still
important. The time have
not yet come to scale real world model status.
Secondly, we compare different
planning representations. On the labor benchmark,
we show that a visually grounded planning representation.
That is a virally planning. And the
image foresight planning. Work better
than language planning representations. And
also have faster inference speed and
smaller training cost.
Then we also designed experiments to show that
a virally grounded planning representations
are easier for the low level action
to follow. Than the language planning
representations.
Then we perform continued learning experiments on the
liberal law with sequential learning
algorithm. We can see that a fairly grounded
planning representations deliver superior forward
transfer. And, exploit a slower
forgetting. Relative to the language
planning representations.
Then to compare different variant,
paradigms. We train all three kinds
of paradigms on six different benchmarks and
show that the integrative VOE
and the hierarchical VOE outperform
action only AOE across a broader
spectrum of tasks.
Then we want to know which paradigm can generalize
better and can benefit from the
planning head of pretraining. We use a
cornerstone benchmark and a liberal benchmark and show
that the hierarchical value generalize
the best And both the integral
VAE and hierarchical VAE
benefited similarly from the
planning ahead of pretraining.
Then we show that when employing
multiple planning repetitions simultaneously.
As a hierarchy, we outperforms others.
Paradigm.
We also show that on the liberal
benchmark, the Heroku VAE
performs the best in the high level
task planning alone.
And finally, for the continued learning,
show that the integrated Bayou and the
Arakawa BOE achieves the higher forward transfer.
But incur faster forgetting.
Compared to the action only value.
And lastly, we also perform data and model
scalability experiments which show that
all VOA paradigms have a data
scalability. However, we show that the four
tasks that are trained from scratch with
no more than five k
demonstrations. There is no model
scalability. On it.
So here are the summary of our conclusions.
Thanks very much for the listening.
Okay. Thank you for the sharing of
Yixin. For
thank you for the sharing of Chung Tongkah.
And our next
paper is Latent Weight Fusion.
And now let's welcome
Shahshank from USC. To
representative
Awesome. I can do HDMI.
Okay.
Okay. Surprisingly,
Linux was the easy Ubuntu was the
easiest to set up a display
from ironically. Right Alright.
So I'm I promise you I'm gonna keep this short. I get
it. It's the end of a very, very long.
So I'm not gonna take a lot of time. But having
said that, I hope at the
end of this six minute conversation, you
guys are as excited about the implications of this
work as I am. Alright.
Cool. So this paper originally was called latent
weight diffusion. But
through a few iterations, we have
a more, up to date version of
this, which we call world
model assisted reactive policy diffusion.
You can find it in this archive link and this
QR code, which I will share show it
again at the end. Alright.
So people who are familiar with robotics
are familiar with this massive
dataset, the cross embodiment dataset.
It's a massive dataset that
has a lot of people contributing teleoperation
data from different
locations. And so a very big aspect
of this is because they're multiple
people and because each person never behaves
the same way twice, we have a lot of multi
modality in the action space.
And also, we have, like, a distribution of
experts because we don't have just all this data coming
from one person. But we have it coming from different
people in different labs in different countries.
But we ask ourselves,
a lot of people who try to train
models on this train diffusion models
where your action
distribution is modeled by a diffusion model.
So they do diffusion policies. So
they sample actions using a
diffusion model But we say, hang on.
A lot of people have contributed to this data
set. So you in fact have a distribution of policies
not just a distribution of actions.
And so in this work, we actually try to see if we
can retrieve that distribution
of policies rather than distribution
of actions.
Alright. So very briefly, during inference,
this is what our model is capable of.
So if you have can I see my
mouse Okay. Yeah. If you have this
initial state, you have a
our model that takes this initial state as input,
and then takes Gaussian noise
and denoises it to give you
a neural network, a policy which is a neural network
which then will interact the environment for the
next foreseeable future.
At the same time, we can also condition
our diffusion model on a task,
and we can actually generate a policy
based on a task. So we are diffusing in
parameter space. Here in a sense.
Okay. So we derive
in this paper but
since we are still training on a dataset of
trajectories of state action data,
like in like we do in robotics,
we derive that you can have this
elbow function, elbow objective function.
Which has an encoder here which
maps your trajectories to a z.
You have a decoder here which maps your z
to a tilde. And then you have
a behavior cloning loss here which says, if you have a teter,
which is what you parameterize your policy with,
and a state, what should the action be
You have a word model here which says, suppose
you choose an action given
a tata. Then like,
are you making sure that this generated
policy tracks the, desired state space
And then we have a Kale regularizer here.
Alright. So what does our model look like
We take a simple trajectory encoder.
Which takes state action pairs, over
a trajectory and maps it variationally
to a latent. And then
we have a hyper network decoder
which takes this latent and maps
to parameter space. So, these are hyper
networks which are networks that are capable
of estimating weights of another network.
And then, in, when we
have this teacher forcing, part
of our algorithm, we we
take these, this original
state information, pass it through our estimated
policy, and see these estimated
actions, and then we have BC loss
over these actions.
At the same time, we also
train a word model in parallel
to make sure that we can track the dynamics,
that are defined by these state action pairs.
And then mid training, sometimes
we also pass an additional gradient
to our policy generator by freezing this
world model and making sure that our generated
policy tracks the desired
action, desired state trajectory.
So this is inspired by model based imitation
learning. It's a paper by Wave.
Which, where they actually show that you can make
sure that you can use a world model to
kind of mitigate covariate shift.
Alright. So,
this entire thing can be trained end to end.
This is the VA section of our
of our model. And then once you have this
latent action as access
to this latent space, you can train a
latent diffusion model to kind of
sample in this latent space. So during
inference, what you're actually doing is sampling
in this latent space based on
the previous states and a task identifier and
then decoding it to the policy space.
Alright. So what are the benefits of
doing something like this The first
is because you're actually generating
a policy, you're generating a function
You're not actually generating a sequence of
actions which you usually do
in robotics. Because you are generating
a function, you are more robust to stochastic
environments, you are more closed loop
If you were to generate an action trajectory
like most of our baselines do,
then during the trajectory tracking phase,
your policy is pretty much open loop.
Whereas here, since we generate functions, we are closed
and we see when we have adversarial perturbations,
we are more robust to it. So this
is on the push t task where you have to align the
gray t over the green t. And
the DP is the diffusion policy benchmark
here. And because we
are generating in function space and we are
more robust to these perturbations, we can actually reduce
the number of diffusion calls that we need
so we can actually run this with
for a far longer action horizon.
Now, similarly, we can show also
condition our policy generation
on on a task. And we
can we actually see that since we are generating
policies for just the task you
care about at hand, the generated policies
tend to be really, really small, like
orders of magnitude smaller, and so
there's a lot of computational benefits for doing
something like this.
This is the result that I'm most excited about.
Which is we have a
latent space. Right And if we take a
TSNU of that latent space, we
see clustering in that latent space, and we believe that
these clusters actually map behavior
space so this was
trained on the Robomimic dataset. Which
is we have six operators.
Trying to
manipulate a can from one place to another
on the table. And these six
operators are of diff different,
degrees of
proficiency in completing this task. And
when we apply our method and we when we visualize
this latent space, we see very clear clustering.
Based on, which point
person. And like our method
was never given explicit information about this.
So during test time, for example, we could
actually just sample a few latents from
this red region over here,
which is Yeah. So just this red
region because maybe we like the behavior of
operator one more than any other operator
when we want to see our robot control something.
So we have actual access to what kind
of behavior we want. When
we generate our policies.
So in conclusion, we
can now generate parameters rather than actions,
and these parameters are modeled by their
behavior distribution.
One kind of limitation that we saw is
if you can run diffusion at a high enough
frequency, where you're just generating
actions at every time instant, that's
really hard to beat because you you have a really
good probabilistic model that
works with a very easy task here.
But if you have a compute constraint platform
and if latency is an issue to you,
then our method can actually alleviate a lot of
those issues. Now the open
directions, are
we want to see how we can expand this
to vision based policies. So all the results
I've shown so far are on low
dimensional state spaces. So we assume that we have
some kind of image encoder that kind of
gives us these embedding information.
Or we have proprioception, which gives us
a lot of state information. But we
want to see if we can somehow estimate generate
entire end to end vision policies.
We wanna see if we can use a richer hyper network.
So there's a lot of lot of work on
hyper networks lately where you can actually
scale it up to even vision transformers.
You can estimate weights for even vision transformers.
And now since we have access to this behavior
distribution, we wanna see if we can do some cool things
with it. Say, we want this kind of behavior.
We don't want that kind of behavior. In
our generated policies.
And also,
also remember Glenn saying this, earlier
during the discussion panel. Wherein
he wants to see where we have a world model
that can plan but then
but then the output of that plan is a
small locally optimal policy that we can then
fine tune This is a method that
actually can give you that because we can plan
with our pretrained world model, and then
we can generate a policy based on the outcome
of that world model's plan. And then because
it's a neural network that we generate, can
even fine tune it if we want to.
Yeah. So those are the open directions.
And if if you're kind of interested in this or
if you wanna get in touch yep.
You feel free to Any questions
Thank you. Thank you. Thank you. Thank you.
The next speaker is Chunghao
Ni. She will get up to the
final paper robotic model.
So
Awesome. Yeah. Thank you, Urs.
Thank you all for staying this late,
for my paper. That's what
I told myself. And I like to stand very close
to the screen so I can be more engaging.
Great. So this workshop this workshop
is about Embodied Work Model for Decision
Making. And a lot of people
like present very nice ideas of like
video upward models. But we
wanna show this actually
works in real robot. So we're very
proud of, this work. It's kind of one of the
first to really
you can't distill a policy as innovates
by previous speakers from the work
model owner. Great. So we
presenting a robotic work model, a neural network
black box end to end simulator.
Robust policy optimization
robotics. Great.
So if you come from the background
of robotics, especially for, like,
legged locomotion or quadratic humanoids,
What people do nowadays is, well,
have a robot, just throw that in as high fidelity simulator,
you identify all the parameters, and
train a PPO policy on it. And you
deploy the policy on the robot.
And hope it transfers. This
works relatively well, but you
know, after deploying a policy on the real hardware,
well, our hardware is running in the real world. Right
So since real world data, we
have state action transitions, we're not
just used to use these data, to,
you know, keep improving our policy because, well,
the simulators are never good enough, and
we can always keep improving.
So Yeah. This is like
the typical other training pipeline,
and there's no online adaptation anymore for
the current pipeline.
So what we're thinking is, oh, and
since we're doing RL in real world,
and in our real world of RL, we have very limited
data. Then a very simple
strategy is train a work model or a dynamic
model, and that can directly
give us infinite amount of data. And with that data,
synthetic data or people say imagination,
we can improve our policy. It's a very
simple idea.
Yeah, we after we have the model, we
train a policy with purely imagination,
and we can deploy the policy back into real world.
So you can see, if the model can be bad, it's
fine. The policy will that that's
be bad, but it's fine after we deploy on
real world. Can collect more data to
update the model update the policies. So
both the policy and model are trained
online like, this kind of continual learning
setup. Yeah.
But why are people not doing this This is
so simple. Right Like typical
model based RL, you see a lot of great results
in simulation at least. It's
kind of hard because no matter
well, I wanna motivate why do we need
to train a policy and planning doesn't really
work. Because we're dealing
with, lack of locomotion, and we have
control frequency of at least 50 Hertz.
And planning, if you have a lot of, you know, zero
short planning like Sam, you have a
lot of particles this doesn't
give you 50 hertz. So we do need to
learn a policy from this
work model. But
matter whether you use policy optimization, like,
no matter whether you use policy gradient methods,
or the value function based method,
To get the unbiased value
estimation, you have to roll out your dynamics
kind of long enough, you know, to do the TDE bootstrapping.
Right But this is especially challenging
on the learn dynamics. Because, you know, errors
compounds. When you do it, when you imagine autoregressionally,
like, in the end you will deviate, you'll
fly. So that's kind of challenging.
And the second thing is for real
hardware, you know, when people do model
free RL with a similar layer, a bunch
of domain randomization, like
like dragging the robot, like pushing the robot,
and you can change it to terrain.
But this is also a little bit hard to do on the learn
dynamics, Again, like, once you perturb
the dynamics, dynamics will
fly away, right, will deviate.
So the model have very limited robustness
of noise, which in return
have very limited robustness
to the policy that transferred to real.
And finally, for lack of locomotion,
especially, the dynamics itself, as we are
we is
inherently discreet. Right So
when we wanna learn an end to end neural
network policy, it's kind of smooth. This is really
not really there. So the main
contribution of this work is
to stabilize long horizon auto
regressive model rollouts.
And to make it robust, against
noise. Also,
like, you know, there are a lot of very advanced
or, you know, fancy architect
like, well,
Sure.
Sorry guys. Workshops were over at 5PM.
So this session has gone overtime.
And we'll have to ask you to pull your posters
off the wall and exit the
room please. Thank you.