{
  "extraction_date": "2026-01-27 16:20:05",
  "source_url": "https://www.nvidia.com/gtc/session-catalog/",
  "session_type": "All",
  "total_sessions": 529,
  "sessions": [
    {
      "description": "In this keynote, NVIDIA founder and CEO Jensen Huang looks ahead to the future of accelerated computing and AI, and how they will shape the next era of computing across every industry.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [],
      "nvidia_technology": "",
      "session_id": "S81595",
      "session_type": "Keynote",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Jensen Huang",
          "title": "Founder and CEO"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "GTC 2026 Keynote",
      "topic": "AI Platforms",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81595/"
    },
    {
      "description": "NVIDIA Cosmos world foundation models are transforming the future of physical AI, enabling reasoning and generating photoreal data for machines and AI agents to learn to perceive, understand, and adapt to real-world complexity. Learn how developers can use Cosmos open models and tools to accelerate training, customize pipelines, and deploy faster across robotics, autonomous vehicles, and AI agents. This session will also explore how the latest updates for Cosmos advance physics-aware learning, unlock powerful simulation-to-real capabilities, and empower developers to build the next generation of intelligent, adaptive AI systems.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how video foundation models are evolving as Ming-Yu, VP of generative AI research, provides an in-depth look at cutting-edge visual multi-modal research.",
        "Learn how robotics, autonomous vehicle, and humanoid downstream model training can benefit from world foundation models."
      ],
      "nvidia_technology": "Cosmos",
      "session_id": "S81667",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Ming-Yu Liu",
          "title": "VP Generative AI Research"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "How Open World Models are Powering the Next Breakthroughs in Physical AI",
      "topic": "Video Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81667/"
    },
    {
      "description": "Open-source AI is redefining how we build and scale intelligence. In this session, we explore how AI factories unlock a new generation of digital workers who learn continuously, collaborate with humans, and drive breakthroughs across every industry.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "See how open-source AI accelerates innovation and enables a new class of intelligent digital workers.",
        "Understand how AI factories transform raw data and models into continuously improving agents.",
        "Learn how memory, reasoning, planning, and multi-modal intelligence are reshaping real-world workflows."
      ],
      "nvidia_technology": "NeMo, NVIDIA NIM",
      "session_id": "S81789",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Justin Boitano",
          "title": "VP of Enterprise and Edge Computing"
        },
        {
          "company": "NVIDIA",
          "name": "Kari Briski",
          "title": "VP AI Software Product Management"
        }
      ],
      "technical_level": "General Interest",
      "title": "Open-Source AI Shaping the Next Era of Intelligent Digital Workers",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81789/"
    },
    {
      "description": "Join Rev Lebaredian, NVIDIA’s VP of Omniverse and simulation technology, as he shares how digital twins and real-time simulation—powered by OpenUSD and NVIDIA’s accelerated computing platform, libraries, and ecosystem—are accelerating breakthroughs across industries in the era of physical AI. Discover their impact on everything from product design and manufacturing to robotics, autonomous vehicles, smart cities, climate science, and space exploration.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Discover how digital twins and real-time simulation are transforming product development and manufacturing, enabling faster iteration and more efficient production.",
        "Learn how digital twins and real-time simulation are advancing critical fields like smart cities, climate science, and space exploration, benefiting society and the planet."
      ],
      "nvidia_technology": "Jetson, RTX GPU, Isaac, Omniverse, Metropolis, Modulus, OVX, cuOPT, PhysX",
      "session_id": "S81598",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Rev Lebaredian",
          "title": "VP of Omniverse and Simulation Technology"
        }
      ],
      "technical_level": "General Interest",
      "title": "Accelerate the Physical AI Era With Digital Twins and Real-Time Simulation",
      "topic": "Industrial Digitalization / Digital Twin",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81598/"
    },
    {
      "description": "A revolution is transforming healthcare as continuous, intelligent systems link scientific discovery to everyday care. Advances in open models, simulation, and agent orchestration are creating AI-native research environments where ideas are explored, tested, and scaled across digital and physical labs with unprecedented speed. This next generation of R&D is shifting from slow iteration to rapid, data-driven innovation. AI agents, clinical copilots, and robotics are redefining healthcare delivery, becoming integral to hospital operations, diagnosis, and treatment. Hospitals and clinics now function as adaptive learning systems that anticipate needs, extend clinical research, and elevate patient experience through intelligent, connected technologies. Discover how these breakthroughs are building the foundation for an AI-native health system in this new era.",
      "format": "In-Person",
      "industry": "Healthcare & Life Sciences",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Gain a practical framework for building and deploying AI-native research and clinical systems.",
        "Bridge scientific discovery and real-world impact with rapid, data-driven innovation.",
        "Transform clinical workflows and patient experiences through connected devices and robotics."
      ],
      "nvidia_technology": "CUDA, BioNeMo, Clara Guardian, Clara Holoscan, Clara Parabricks, MONAI, NVIDIA NIM, CUDA-Q",
      "session_id": "S81806",
      "session_type": "Talks or Panels",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Kimberly Powell",
          "title": "VP, Healthcare and Life Sciences"
        }
      ],
      "technical_level": "General Interest",
      "title": "AI Revolution in Healthcare: Redefining Research, Care, and the Patient Experience",
      "topic": "Biology - Generative AI",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81806/"
    },
    {
      "description": "Robots learn faster and safer in simulation, but complex joints, balance, and contact-rich motions challenge current physics engines. Developers need high-fidelity simulation so virtual training transfers safely to real robots. This session with NVIDIA, Google DeepMind, Skild AI, and Disney Research introduces Newton, an open-source, GPU-accelerated physics engine built on NVIDIA Warp and OpenUSD to advance robot learning. Newton’s flexible design and multiple solvers enable realistic simulations—like walking through snow, or handling cups and fruit—and reliable deployment in the physical world. Explore Newton’s architecture, integration into robot-learning pipelines, and ways to develop new solvers for safer, more capable robots across diverse environments and tasks.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "The importance of differentiable physics for robot simulation",
        "How developers can couple multiple solves for training robot policies",
        "How they can extend Newton to develop their own customer solver"
      ],
      "nvidia_technology": "Isaac, Omniverse",
      "session_id": "S81613",
      "session_type": "Tutorial",
      "speakers": [
        {
          "company": "Disney Research Imagineering",
          "name": "Moritz Baecher",
          "title": "Director, Research Lab Zurich"
        },
        {
          "company": "NVIDIA",
          "name": "Miles Macklin",
          "title": "Director, Simulation Technology"
        },
        {
          "company": "Google DeepMind",
          "name": "Erik Frey",
          "title": "Simulation Lead"
        },
        {
          "company": "Skild AI",
          "name": "Deepak Pathak",
          "title": "CEO and Co-Founder"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "An Introduction to Newton Physics Engine for Robotics",
      "topic": "Robotics Simulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81613/"
    },
    {
      "description": "Autonomous vehicles (AVs) are emerging as the first mass-deployed physical AI agents, reshaping mobility and driving new demands in AI, safety, and scalable development. This special address explores the industry’s path to Level 4 autonomy and how NVIDIA is helping accelerate it. With the DRIVE Hyperion architecture, manufacturers (OEMs) and AV developers can build scalable, safety-certified autonomy solutions. NVIDIA will also highlight its cloud-to-car platform—spanning training, simulation, and in-vehicle compute—to support developing and validating next-generation drive-policy models. Gain insights into L4 software architectures, the DRIVE Hyperion platform, and ecosystem collaboration across OEMs, AV developers, mobility providers, and robotaxi companies.",
      "format": "In-Person",
      "industry": "Automotive / Transportation",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Understand how foundation models and visual-language-action models are shaping the next generation of Level 4 perception, prediction, and planning systems.",
        "Learn about the three computers for autonomous vehicles used for training large-scale models to validating their behavior in simulation to real-world performance.",
        "Learn about the NVIDIAHyperion Platform architecture for L4-ready vehicles, and how NVIDIA is working with a broad ecosystem of OEMs, AV developers, and robotaxi companies to accelerate the development and deployment of safe autonomous vehicles."
      ],
      "nvidia_technology": "DRIVE, DRIVE SDK, Omniverse, DRIVE AV",
      "session_id": "S81713",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Xinzhou Wu",
          "title": "VP of Automotive"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Automotive Special Address: Advancing Level 4 Autonomy, the Path to Scalable, Safe AVs and Robotaxis",
      "topic": "Vision Language Models (VLMs)",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81713/"
    },
    {
      "description": "Aman Sangar, a co-founder and CTO at Cursor, will talk about Cursor's integrated development environment (IDE).",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "An AI assistant’s power scales with context; whole-repository indexing beats single-file awareness.",
        "For intelligent dev tools, build AI in as a core architectural primitive, not a plugin.",
        "Balance AI tool performance with a hybrid architecture: local models for speed, cloud models for power."
      ],
      "nvidia_technology": "Hopper, Blackwell, DGX Cloud",
      "session_id": "S81528",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Cursor",
          "name": "Aman Sanger",
          "title": "CTO/Co-Founder"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Code with Context: Build an Agentic IDE that Truly Understands Your Codebase",
      "topic": "Code / Software Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81528/"
    },
    {
      "description": "The CUDA platform is the foundation of the GPU computing ecosystem. Every application and framework that uses the GPU does so through CUDA's libraries, compilers, runtimes and language—which means CUDA is growing as fast as its ecosystem is evolving. Presented by one of the architects of CUDA, at this engineering-focused talk you will learn about all that's new and what's coming next for both CUDA and GPU computing as a whole.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "CUDA's roadmap: what's coming in 2026",
        "A deep dive into the technical details of CUDA's latest features",
        "A look into the future of GPU computing",
        "Pointers to many other CUDA talks at GTC 2026"
      ],
      "nvidia_technology": "CUDA",
      "session_id": "S81859",
      "session_type": "Talks or Panels",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Stephen Jones",
          "title": "CUDA Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "CUDA: New Features and Beyond",
      "topic": "Programming Languages / Compilers",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81859/"
    },
    {
      "description": "Be among the first to hear how software-defined intelligence is aiding enterprises to scale AI operations. Hear directly from an AI innovator as they reveal key learnings from deploying NVIDIA DGX, sharing firsthand insights into architecting and deploying high-performance AI infrastructure and orchestrating complex inference and training workflows. In this session, we’ll also discuss the critical role of software for NVIDIA Rubin and how to simplify complex liquid-cooled deployments at scale, boost site reliability engineering (SRE) productivity, and enable enhanced AI-powered anomaly detection and remediation across the entire AI factory.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Learn how AI innovators optimize their infrastructure management and boost SRE productivity in their AI factory with software-defined intelligence.",
        "Discover practical insights from deploying NVIDIA’s latest architectures with NVIDIA Mission Control, including architecting high-performance AI infrastructure, orchestrating diverse inference and training workflows."
      ],
      "nvidia_technology": "DGX Platform, Base Command Manager, NVIDIA Run:ai, Mission Control",
      "session_id": "S81796",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Premal Savla",
          "title": "Sr. Director of Product Management, DGX Systems and Solutions"
        },
        {
          "company": "Lilly USA, LLC",
          "name": "Jon Klinginsmith",
          "title": "Executive Director of Cloud Engineering and High Performance Computing"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "From Cluster to Factory: Scale AI Infrastructure Operations With Software-Defined Intelligence",
      "topic": "Software-Defined Data Center",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81796/"
    },
    {
      "description": "Humanoid robots are evolving from prototypes to production-ready systems, powered by breakthroughs in reasoning AI, large-scale simulation, and real-time edge computing. But to get there, the industry must bridge the gap between digital intelligence and physical reality. Hear from luminaries at the world’s leading robotics companies on the future of physical AI, and what it will take to unlock truly capable humanoids.",
      "format": "In-Person",
      "industry": "Manufacturing",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "How leading teams are training and deploying humanoid robots today, from data collection and simulation to real-world rollout",
        "What technical stack is emerging for physical AI—covering foundation models, world models, and control systems—and how it generalizes across different robot platforms",
        "Where the most promising real-world use cases, business opportunities, and open research problems lie for humanoids over the next three to five years"
      ],
      "nvidia_technology": "Jetson, Isaac, Omniverse, Cosmos",
      "session_id": "S81645",
      "session_type": "Talks or Panels",
      "speakers": [
        {
          "company": "Skild AI",
          "name": "Deepak Pathak",
          "title": "CEO and Co-Founder"
        },
        {
          "company": "NVIDIA",
          "name": "Amit Goel",
          "title": "Head of Robotics and Edge Computing Ecosystem"
        },
        {
          "company": "Tesla",
          "name": "Ashok Elluswamy",
          "title": "VP of AI"
        },
        {
          "company": "Hexagon",
          "name": "Arnaud Robert",
          "title": "CEO"
        },
        {
          "company": "Agility Robotics",
          "name": "Pras Velagapudi",
          "title": "CTO"
        }
      ],
      "technical_level": "General Interest",
      "title": "From Concept to Production: Humanoid Robotics at Scale",
      "topic": "Humanoid Robots",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81645/"
    },
    {
      "description": "Simulation has evolved from a niche research tool to a cornerstone of modern healthcare innovation. Early breakthroughs focused on GPUs to accelerate physical interactions for device design and testing. Today, the scope is much broader—incorporating physiological processes such as respiration, cardiac motion, and electrophysiology. These comprehensive simulations are already being used to conduct virtual clinical trials and train autonomous surgical robots. We're helping to build the future in which AI is used to design, accelerate, and interpret these simulations, enabling digital twins that guide the care of each and every patient. Throughout the day, as your smart watch records your activities or during surgical procedures when unexpected events occur, your AI-enabled digital twin will be there to detect, diagnose, and suggest the treatment that provides the best outcome for you.",
      "format": "In-Person",
      "industry": "Healthcare & Life Sciences",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "AI is critical to the complex analyses needed to convert medical data into a digital twin of a patient.",
        "AI can accelerate complex, multi-system simulations from multi-day computations to real-time insights.",
        "AI-enabled, patient-specific digital twins are being used to plan the treatment of some of the most challenging surgical procedures.",
        "NVIDIA has the accelerated hardware and the software platforms needed for personalized digital twin creation and operation, e.g., MONAI, Omniverse, and PhysicsNeMo"
      ],
      "nvidia_technology": "Grace CPU, Hopper, Modulus, OVX, MONAI, Cosmos",
      "session_id": "S81643",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Medtronic",
          "name": "Jim Peichel",
          "title": "CTO"
        },
        {
          "company": "Ansys, part of Synopsys",
          "name": "Mark Palmer",
          "title": "Field CTO and Lead Chief Technologist, Healthcare"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "How Simulation and AI are Giving Life to Digital Twins for Patient Care",
      "topic": "Physics",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81643/"
    },
    {
      "description": "AI-assisted coding has sped up code generation, but human-only code reviews are now the bottleneck. Too many pull requests (PRs), not enough reviewer bandwidth. This session shows how you can build the right context-engineering architecture that helps LLMs deliver high-signal AI code reviews. Catch logical or functional bugs, memory leaks, security vulnerabilities, code refactors, edge cases, and more. You'll see how bringing in the right context from external datasets (such as code graphs, PR history, issue tickets, MCP servers, and linters) yields better-quality reviews of PR diffs. You will leave with a practical playbook to pilot and scale AI code reviews that increase your release velocity with fewer bugs.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Code reviews are a worsening bottleneck to release velocity, and AI coding agents need an independent auditor.",
        "Proper context engineering helps LLMs deliver higher signal review comments that catch hidden and critical bugs.",
        "Fix 50% more bugs (e.g., logical bugs, refactors, security vulnerabilities) and ship 50% faster with independent AI code reviews."
      ],
      "nvidia_technology": "NVIDIA NIM",
      "session_id": "S81612",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "CodeRabbit",
          "name": "David Loker",
          "title": "Director of AI"
        },
        {
          "company": "CodeRabbit, Inc.",
          "name": "Harjot Gill",
          "title": "CEO"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Practical Context Engineering: Eliminate Bugs With High-Signal AI Code Review",
      "topic": "Code / Software Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81612/"
    },
    {
      "description": "Explore how we scaled Kimi K2 by pioneering the Muon optimizer to double token learning efficiency and maximize training throughput through Day 0 infrastructure co-design. Gain insights into AI-native training and linear attention architectures to help you unlock the potential of longer-running agents.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "How Kimi pioneered the use of the Muon optimizer in massive-scale pre-training to replace Adam and double token learning efficiency",
        "Strategies for achieving training stability at scale through the Day 0 co-design of model architecture and high-performance infrastructure",
        "A paradigm shift toward AI-native training, where the model actively participates in its own data synthesis, evaluation, and evolution",
        "Why Kimi is betting on next-generation linear attention architectures to unlock the possibility of longer-running agents"
      ],
      "nvidia_technology": "Hopper, Blackwell",
      "session_id": "S81695",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Moonshot AI",
          "name": "Zhilin Yang",
          "title": "Founder and CEO"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Racing to the Top: How We Scaled Kimi K2",
      "topic": "Large Language Models (LLMs)",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81695/"
    },
    {
      "description": "Scaling and deploying AI into restaurant operations requires unique technology and partners. Learn how Yum! Brands, the world’s largest restaurant company, is using NVIDIA software to scale intelligence into their restaurant technology, expanding use cases, applications, and partners.",
      "format": "In-Person",
      "industry": "Service",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Understand how a multinational, cross-brand company like Yum! Brands is setting its strategic AI and agentic strategies with NVIDIA software.",
        "Learn tips on how to integrate AI into restaurant operations.",
        "Hear about the importance of strong partner ecosystems and other lessons learned."
      ],
      "nvidia_technology": "NeMo, NVIDIA NIM, NVIDIA AI Enterprise, Riva, Nemotron",
      "session_id": "S81755",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Yum! Brands",
          "name": "Cameron Davies",
          "title": "Chief Data Officer"
        },
        {
          "company": "NVIDIA",
          "name": "Joey Conway",
          "title": "Senior Director, Enterprise Software"
        },
        {
          "company": "NVIDIA",
          "name": "Andrew Sun",
          "title": "Senior Director, Retail Business Development"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Scaling AI Agents Globally Across Brands, Use Cases, and Restaurants",
      "topic": "",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81755/"
    },
    {
      "description": "As AI transforms every layer of modern computing, security must evolve in parallel—shifting from a separate function to an embedded capability woven into every stage of innovation. Today’s CISOs and security leaders are moving beyond reactive defense, redefining their roles as proactive business enablers who accelerate safe adoption of AI, cloud, and emerging technologies. This talk explores how that shift is reshaping security architectures, operating models, and organizational influence. Finally, it highlights why meaningful collaboration across industry, academia, and government is no longer optional. Addressing global cyber threats at AI scale demands shared intelligence, coordinated standards, and partnerships that blend cutting-edge research with real-world deployment. Together, these forces define the future of secure innovation.",
      "format": "In-Person",
      "industry": "Public Sector and Sovereign AI",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Discover how integrating security into every stage of AI innovation drives smarter, safer breakthroughs.",
        "Learn how forward-thinking CISOs are transforming security from a defensive function into a catalyst for business growth."
      ],
      "nvidia_technology": "NVIDIA NIM, NVIDIA AI Enterprise",
      "session_id": "S81491",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "David Reber",
          "title": "CSO"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Security for the AI-First Enterprise",
      "topic": "Security for AI",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81491/"
    },
    {
      "description": "The unsatiable demand for AI computing is creating the need for many new kinds of AI infrastructure providers. As AI inference traffic scales, compute must move beyond a few centralized data centers to a vast, distributed AI grid. Telcos, with their nationwide networks and dense footprint of data centers, are uniquely positioned to transform existing infrastructure into this scaled AI grid. This talk explores how telcos can build an AI grid to power new AI services and applications, gives a first look at NVIDIA’s reference architecture, highlights edge AI applications that run best on this grid, and shares announcements from partners already on this journey. It also offers a forward-looking view of how innovations in AI-RAN, autonomous networks, and 6G come together as workloads on the AI grid—creating a once-in-a-decade opportunity for telcos to redefine their role in the AI era.",
      "format": "In-Person",
      "industry": "Telecommunications",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Understand the new AI grid opportunity and how it can become telcos’ most important new business platform this decade.",
        "See concrete examples of how applications providers, telcos, and partners are collaborating to realize the AI grid.",
        "Learn how AI-RAN, autonomous networks, and 6G converge as strategic workloads on a shared AI grid."
      ],
      "nvidia_technology": "BlueField DPU, Cloud / Data Center GPU, RTX GPU, CUDA, TensorRT, Aerial, Infiniband Networking, Ethernet Networking, Interconnect Networking, NeMo, NVIDIA NIM, DGX Spark",
      "session_id": "S82003",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Ronnie Vasishta",
          "title": "Sr. Vice President, Telecommunications"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Telecom Special Address: The AI Grid—Intelligently Connecting AI Infrastructure",
      "topic": "AI Networking, 5G/6G",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82003/"
    },
    {
      "description": "What if we could dramatically shorten the path from discovery to lifesaving therapy? Discover how Stanford Medicine’s LabOS, an AI extended reality (XR) operating system, makes this a reality. Learn how LabOS drives CRISPR GPT, a groundbreaking AI agent to design complex experiments and deliver step-by-step instructions through augmented reality glasses. LabOS unites computational reasoning with physical experimentation, built on NVIDIA Cosmos-powered XR/AI system. By connecting multi-model AI agents, smart glasses, and robots, LabOS allows AI to see what scientists see, understand experimental context, and assist in real-time execution. From immunotherapy to stem cell engineering, LabOS turns any lab into an intelligent environment where human and machine discovery evolve together—not just accelerating research, but delivering personalized, accessible therapies at unprecedented scale.",
      "format": "In-Person",
      "industry": "Healthcare & Life Sciences",
      "intended_audience": "Research: Non-Academic",
      "key_takeaways": [
        "Architecting the AI-XR Lab: Discover how LabOS integrates multi-modal agents, XR, and robotics to bridge the gap between computational design and physical execution.",
        "Deploying Visual Reasoning: Learn to build agentic guidance systems with NVIDIA Cosmos that validate lab workflows and catch errors in real time.",
        "Connecting Gen AI to Reality:"
      ],
      "nvidia_technology": "NVIDIA AI Enterprise, Cosmos",
      "session_id": "S81748",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Stanford University",
          "name": "Le Cong",
          "title": "Professor"
        },
        {
          "company": "Princeton University",
          "name": "Mengdi Wang",
          "title": "Professor"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "The AI-XR Co-Scientist Lab: How Stanford's LabOS is Accelerating Advanced Science With XR",
      "topic": "AR / VR AI",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81748/"
    },
    {
      "description": "Explore how Canva engineered AI systems with foundational models that understand creative context; preserve artistic control; and amplify, rather than replace, imagination. We'll share technical approaches for building AI platforms and products where AI becomes an invisible enabler, letting designers focus on their work, from editable designs to video, from web apps to interactive experiences.",
      "format": "In-Person",
      "industry": "Media & Entertainment",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "How to build AI products that feel accessible to every user",
        "How AI can turbocharge the creative work in a scalable and controllable way",
        "How generative AI is transforming all types of media creation"
      ],
      "nvidia_technology": "Cloud / Data Center GPU, CUDA, HPC SDK, cuBLAS, cuDDN, cuFFT, NCCL, NSight Comute, NSight Systems",
      "session_id": "S81778",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Canva",
          "name": "Stef Corazza",
          "title": "Head of AI Research"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "The Creative OS: AI Built With Human Imagination at the Center",
      "topic": "Image Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81778/"
    },
    {
      "description": "This session delivers a technical state of the union on GPU-accelerated data processing across SQL/DataFrames, vector search, ML, and decision optimization. Learn how GPU-native engines enable interactive analytics on massive lakehouse-scale datasets, real-time semantic and vector search over billions of embeddings, and makes the hardest ML and decision science workloads tractable, cost-efficient, and energy-efficient. The talk highlights the implications for high-impact scientific and enterprise computing, then looks ahead to what’s in flight for 2026 and beyond, outlining concrete architectural patterns and practical guidance for building the next generation of GPU-accelerated data platforms and using them in your day-to-day work.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "Accelerated computing is the cost- and energy-efficient way to process the massive datasets generated by modern enterprises, whether its structured, unstructured, or somewhere in between.",
        "Accelerating data processing is transforming scientific computing, from genomics to materials science and beyond.",
        "Accelerated data processing libraries are designed as building blocks and ready to be integrated into your custom applications.",
        "Interactive data analytics at scale is now possible—and affordable."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, Grace CPU, RTX GPU, DGX Platform, RAPIDS, Infiniband Networking, Ethernet Networking, Interconnect Networking, Blackwell",
      "session_id": "S81769",
      "session_type": "Talks or Panels",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Todd Mostak",
          "title": "Sr. Director of Engineering"
        },
        {
          "company": "NVIDIA",
          "name": "Joshua Patterson",
          "title": "VP, Solutions Architecture - Accelerating Data Processing"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "The Era of GPU Data Processing: From SQL to Search and Back Again",
      "topic": "Data Analytics / Processing",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81769/"
    },
    {
      "description": "This panel brings together industry leaders from Honeywell Industrial, Caterpillar, and Medtronic to explore how advanced AI and edge computing are reshaping the future of industrial and healthcare systems. Together, they will discuss the major trends accelerating edge intelligence—from autonomous operations and multi-modal sensor fusion to real-time decision-making and digital twins—and share high-impact AI use cases driving measurable business growth and societal value. The conversation will highlight challenges in deploying scalable, safety-critical edge AI and reveal how these global innovators are architecting their next-generation solutions using NVIDIA’s accelerated computing and end-to-end edge platform.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Learn industry trends driving advanced AI and edge computing."
      ],
      "nvidia_technology": "Jetson, AGX, TensorRT, Isaac, Metropolis, IGX, JetPack, Blackwell",
      "session_id": "S81844",
      "session_type": "Panel",
      "speakers": [
        {
          "company": "Honeywell",
          "name": "Suresh Venkatarayalu",
          "title": "CTO"
        },
        {
          "company": "Medtronic",
          "name": "Rodolphe Katra",
          "title": "Chief AI Officer"
        },
        {
          "company": "Caterpillar",
          "name": "Charlie Wood",
          "title": "Director of Data and AI"
        },
        {
          "company": "KION Group",
          "name": "Eike Wibrow",
          "title": "VP Global Technology Strategy"
        },
        {
          "company": "NVIDIA",
          "name": "Chen Su",
          "title": "Sr. Technical Product Marketing Manager"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "The Future of Industrial Edge",
      "topic": "Autonomous Machines",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81844/"
    },
    {
      "description": "Step inside Mistral’s journey to industrial-scale AI as we share lessons learned and the principles guiding our approach in creating Mistral Compute. Built on four key pillars—open-source foundations, security by design, sustainability at scale, and digital sovereignty—we’ll share how our approach unlocks bare-metal GPU performance, why that matters in building trusted, European-hosted future-ready AI systems, and how we’re bringing collaboration to the next level by giving innovators access to the same infrastructure powering our models.",
      "format": "In-Person",
      "industry": "Cloud Services",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Bare-metal performance through turnkey orchestration platform (K8S/Slurm) based on cutting-edge GPUs",
        "Open-source software as a foundation",
        "Secure by design, from DC to runtime",
        "Sustainable operation relying on low power-usage effectiveness data centers, direct liquid cooling platforms, and decarbonized energy",
        "Full operation control with an European-hosted solution"
      ],
      "nvidia_technology": "BlueField DPU, Cloud / Data Center GPU, Grace CPU, Infiniband Networking, Ethernet Networking, Interconnect Networking, Blackwell",
      "session_id": "S81715",
      "session_type": "Talks or Panels",
      "speakers": [
        {
          "company": "Mistral AI",
          "name": "Jean-Olivier Gerphagnon",
          "title": "Software Architect"
        },
        {
          "company": "Mistral AI",
          "name": "Gauthier Delerce",
          "title": "HPC Lead"
        }
      ],
      "technical_level": "General Interest",
      "title": "Turning Models Into Engines: The AI Factory Era",
      "topic": "Software-Defined Data Center",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81715/"
    },
    {
      "description": "For two decades, the \"self-healing system\" was just a theory. Today, it's an architectural reality. This session unveils the blueprint for the fully autonomous AI factory—a cognitive system where fleets of specialized agents replace static operations such as issues detection and troubleshooting to deliver true self-governance. We'll move beyond the hype of simple chatbots to deconstruct a multi-agent architecture capable of complex reasoning, predictive failure analysis, and continuous self-improvement. But we won’t just tell you about it; we will show you. This session features a high-stakes LIVE DEMO of an autonomous multi-agent swarm connected to a large-scale production system. Watch in real time as agents detect (and predict) anomalies and use advanced tools for troubleshooting as the first self-healing AI platform for autonomous AI factories.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Witness a live, unscripted demonstration of a self-improving agent swarm as it detects, diagnoses, and resolves production issues in real time.",
        "Deep dive into the agentic stack—spanning generative AI, planning and reasoning, and high performance compute/networking—required to power the next generation of AI factories.",
        "Architect safe, mission-critical agentic systems by implementing advanced guardrails that enable autonomy without compromising security or control.",
        "Redefine the operational model by transitioning from \"human-in-the-loop\" to \"human-on-the-loop,\" allowing humans to focus on strategic governance while agents handle execution."
      ],
      "nvidia_technology": "NCCL, NVLink / NVSwitch, NVIDIA AI Enterprise",
      "session_id": "S81584",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "JP Vasseur",
          "title": "Sr. Distinguished Engineer and Chief Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Autonomous AI Factories: A Technical Preview of Agent-Native Production",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81584/"
    },
    {
      "description": "Join Margaret Mitchell, Hugging Face chief ethics scientist, and Nikki Pope, head of AI and legal ethics at NVIDIA, in a conversation about how their respective companies are delivering trustworthy artificial intelligence products and tools.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Hear how large entities like Hugging Face and NVIDIA are helping develop responsible AI.",
        "See how these tools are underpinning advances in AI.",
        "Hear promising research from the last year in review."
      ],
      "nvidia_technology": "Maxine, BioNeMo, NeMo, NVIDIA NIM, NVIDIA AI Enterprise",
      "session_id": "S81537",
      "session_type": "Fireside Chat",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Nikki Pope",
          "title": "Sr. Director, AI and Legal Ethics"
        },
        {
          "company": "Hugging Face",
          "name": "Margaret Mitchell",
          "title": "Chief Ethics Scientist"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Beyond Guardrails: Best Practices Across the Industry for Trustworthy AI",
      "topic": "Zero-Trust Security",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81537/"
    },
    {
      "description": "Creating truly scalable AI solutions for cities is a massive challenge, one that requires many AI models and platforms (and tons of data) working in harmony to make it possible. Announced at GTC Paris, NVIDIA Blueprint for smart city AI leverages 3-computer to activate physical AI and apply to cities for a unified, real-time intelligence solution for creating a smart city. Learn about the latest workflow updates. Linker Vision will share how they've been able to apply the blueprint with U.S. cities, from training models with synthetic data and NVIDIA Cosmos to deploying video analytics AI agents using NVIDIA Metropolis Blueprint for Video Search and Summarization (VSS) that leverages the fleet of camera sensors deployed in cities—an end-to-end approach to processing tens of thousands of camera streams for proactive automation of city operations, safety, and planning.",
      "format": "In-Person",
      "industry": "Smart Cities / Spaces",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "How to use to deploy video analytics AI agents that leverage existing city camera infrastructure at scale.",
        "How Linker Vision trains and adapts models using synthetic data and NVIDIA Cosmos to handle complex, rare, and evolving urban scenarios.",
        "Latest updates to Smart City AI Blueprint workflow to enable traffic management, incident response, and urban planning."
      ],
      "nvidia_technology": "DeepStream, Omniverse, Metropolis, TAO Toolkit, NVIDIA AI Enterprise, Cosmos, Blueprint",
      "session_id": "S81862",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Carlos Garcia-Sierra",
          "title": "Product Manager - Metropolis AI Workflows"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Bringing Physical AI to Smart Spaces",
      "topic": "Video Analytics",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81862/"
    },
    {
      "description": "Modern commerce platforms must support millions of entrepreneurs while serving billions of buyers with personalized recommendations across massive product catalogs. To bridge this gap, we introduce Shopify’s Commerce Foundation Models: sequence autoregressive models built on the HSTU encoder to represent the journeys of merchants and buyers. Key innovations include generalizing to multiple tasks, large-scale contrastive learning, temporal encoding that fuses multi-scale time signals with commerce-specific features, and the scaling to petabytes volumes of data training and on-demand inference via CUDA kernels developed in collaboration with NVIDIA. While results are promising, challenges remain in handling non-stationary preferences and multi-objective optimization. We discuss our roadmap toward truly foundational commerce models that generalize across domains and market conditions.",
      "format": "In-Person",
      "industry": "Retail / Consumer Packaged Goods",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "Understand HSTU architecture",
        "How to build commerce foundation models for retail",
        "Recsys workload acceleration using CUDA"
      ],
      "nvidia_technology": "CUDA, cuBLAS",
      "session_id": "S81636",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Shopify",
          "name": "Yang Liu",
          "title": "Sr. Staff Machine Learning Engineer"
        },
        {
          "company": "Shopify",
          "name": "Diego Ardila",
          "title": "Sr. Staff Machine Learning Engineer"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Building Shopify’s Commerce Foundation Models",
      "topic": "Recommenders / Personalization",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81636/"
    },
    {
      "description": "The manufacturing industry is entering a new era, driven by AI, robotics, and simulation. Join distinguished leaders from the semiconductor and electronics, industrial manufacturing, pharmaceutical, and robotics industries as they share their pioneering use cases and how these technologies enable them to innovate, reimagine production systems, and transform the design and operation of advanced manufacturing facilities.",
      "format": "In-Person",
      "industry": "Manufacturing",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Discover how manufacturing leaders are applying AI, robotics, and simulation to accelerate innovation and efficiency.",
        "Learn how these technologies are reshaping the design, planning, and operation of next-generation manufacturing facilities."
      ],
      "nvidia_technology": "Jetson, Isaac, Omniverse, Metropolis, OVX, IGX, Blackwell, NVIDIA AI Enterprise, Cosmos, DGX Cloud",
      "session_id": "S81573",
      "session_type": "Panel",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Rev Lebaredian",
          "title": "VP of Omniverse and Simulation Technology"
        },
        {
          "company": "ABB",
          "name": "Craig McDonnell",
          "title": "Managing Director, Business Line Industries"
        },
        {
          "company": "SK hynix",
          "name": "Seungyong Doh",
          "title": "EVP, Head of Digital Transformation"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Building the Future of Manufacturing",
      "topic": "Industrial Digitalization / Digital Twin",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81573/"
    },
    {
      "description": "AI is fundamentally transforming real-time applications, reshaping both content creation and player experiences. Join John Spitzer, VP of Developer Technology and Performance at NVIDIA, for an industry panel featuring leaders from Creative Assembly, Decart, and Krafton. These panelists will explore the current state of the art—from breathing life into digital humans to generating entire worlds instantaneously—and offer their vision for the future of gaming and entertainment.",
      "format": "In-Person",
      "industry": "Gaming",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "See practical applications of generative AI in real-time experiences today.",
        "Discover how cutting-edge world models can transform how real-time experiences are built.",
        "Learn how the biggest challenges of AI in real-time experiences are being tackled today.",
        "Get a glimpse into the mind of industry leaders on how AI will evolve over the next decade."
      ],
      "nvidia_technology": "RTX GPU, Audio2Face, Avatar Cloud Engine (ACE), NeMo",
      "session_id": "S81874",
      "session_type": "Panel",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "John Spitzer",
          "title": "VP of Developer Performance and Technology"
        },
        {
          "company": "Creative Assembly",
          "name": "Duygu Cakmak",
          "title": "R&D Director"
        },
        {
          "company": "Krafton",
          "name": "Kangwook Lee",
          "title": "Head of KRAFTON AI"
        },
        {
          "company": "Decart",
          "name": "Dean Leitersdorf",
          "title": "CEO"
        }
      ],
      "technical_level": "General Interest",
      "title": "Charting a Course for the Next Decade of Gaming with AI",
      "topic": "3D Model Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81874/"
    },
    {
      "description": "Join NVIDIA expert Jiwen Cai for a technical deep dive on architecting scalable teleoperation pipelines that bridge simulation and reality. Learn how to leverage NVIDIA CloudXR to stream high-fidelity, low-latency visuals to wireless headsets, unlocking operator comfort and precise dexterity for industrial robotics deployments and data collection at scale.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Architect a unified \"Sim & Real\" teleoperation stack using NVIDIA CloudXR to stream complex robot data to lightweight headsets without tethering.",
        "Implement low-latency streaming protocols that enable precise dexterity (finger/hand manipulation) and lower-body control for humanoid robots.",
        "Optimize operator efficiency and ergonomics by offloading rendering compute to the cloud, resolving common fatigue issues in large-scale data collection.",
        "Introduce wrist camera reference design and post processing for human data collection."
      ],
      "nvidia_technology": "CloudXR, Isaac, Omniverse",
      "session_id": "S82001",
      "session_type": "Tutorial",
      "speakers": [
        {
          "company": "Manus",
          "name": "Maarten Witteveen",
          "title": "President and Co-Founder"
        },
        {
          "company": "NVIDIA",
          "name": "Jiwen Cai",
          "title": "Distinguished Engineer"
        },
        {
          "company": "Lightwheel",
          "name": "Jay Yang",
          "title": "Founding Engineer and Chief Architect"
        },
        {
          "company": "PICO",
          "name": "Ke Jing",
          "title": "Lead Product Manager"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Deep Dive: Streaming Immersive Robotics Teleoperations",
      "topic": "AR / VR Development Tools",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82001/"
    },
    {
      "description": "Experience the next generation of 3D simulation with Newton, an open-source, GPU-accelerated physics engine co-developed by NVIDIA, Disney Research, and DeepMind. Built on NVIDIA Warp with native OpenUSD integration, Newton delivers real-time simulation of complex physical interactions—from rigid and soft bodies to contact, friction, and actuators—enabling creators to push the limits of modern 3D workflows.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Understand how Newton’s open-source, GPU-accelerated physics engine delivers real-time, high-fidelity simulation for complex 3D scenes, including rigid and soft bodies.",
        "Learn how integration with OpenUSD and NVIDIA Warp streamlines modern 3D pipelines, from authoring to simulation and iteration.",
        "Learn how developers can use Newton to simulate rich physical interactions—contact, friction, actuators, and more—to build more realistic robotics, visual effects, and interactive experiences."
      ],
      "nvidia_technology": "CUDA, Isaac",
      "session_id": "S81492",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Disney Research Imagineering",
          "name": "Moritz Baecher",
          "title": "Director, Research Lab Zurich"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "How Disney Droids Come to Life With Physics Simulation",
      "topic": "Robotics Simulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81492/"
    },
    {
      "description": "As AI adoption accelerates across every industry, teams are discovering that model quality alone is not quite enough to deliver real value. Scalable and reliable AI systems depend on robust MLOps that streamline the entire life cycle, from data to deployment. At the same time, the field is undergoing a major transformation driven by agentic AI, advanced reasoning, multi-modality, and new patterns for large-scale, disaggregated inference. This session introduces the foundational patterns of MLOps while also exploring how these new capabilities reshape AI design and operational strategy. We'll examine several use cases to break down the process and ideal platform for MLOps in this new era. Whether you're launching your first AI initiative or modernizing your infrastructure, this session provides the technical blueprint and best practices you need to operationalize AI at scale.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how to build a comprehensive MLOps stack, the layers involved, and which concrete solutions work at each layer.",
        "Understand the MLOps pipeline, including feature engineering workflows, experiment tracking, pipelines, inference, and packaging and deploying AI at scale.",
        "Learn best practices you need to operationalize AI at scale."
      ],
      "nvidia_technology": "NVIDIA NIM, NVIDIA AI Enterprise, NVIDIA Run:ai",
      "session_id": "S81628",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Michael Balint",
          "title": "Director, Product Architecture"
        },
        {
          "company": "NVIDIA",
          "name": "William Benton",
          "title": "Principal Product Architect"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "MLOps 101: Platforms and Processes for Building AI",
      "topic": "InfraOps",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81628/"
    },
    {
      "description": "Hudson River Trading (HRT) is at the forefront of automated trading, processing vast amounts of data at lightning speeds. But their most impressive metric might be their efficiency. In this session, HRT engineers reveal how they are leveraging NVIDIA infrastructure to build the ultimate \"AI factory.\" Discover how to break the trade-off between speed and sustainability. We will cover the end-to-end journey of deploying AI-driven trading models that are faster, smarter, and greener. Learn how innovative data center design, renewable power strategies, and NVIDIA-optimized workloads combine to create a trading infrastructure that is resilient, autonomous, and environmentally conscious.",
      "format": "In-Person",
      "industry": "Financial Services",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Learn how to build and scale AI factory environments that achieve world‑leading energy efficiency through innovative data‑center design and NVIDIA‑powered systems.",
        "Optimize AI workloads for performance and sustainability by balancing low‑latency performance with renewable power and thermal efficiency in large‑scale trading operations.",
        "Discover how HRT architects and deploys AI‑driven trading models on NVIDIA infrastructure to improve autonomy, speed, and resiliency while minimizing environmental footprint."
      ],
      "nvidia_technology": "HGX, Ethernet Networking",
      "session_id": "S82132",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Hudson River Trading",
          "name": "Gerard Bernabeu Altayo",
          "title": "Compute Systems Lead"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "The Blueprint for a Modern Resource-Responsible AI Factory",
      "topic": "Sustainable Computing",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82132/"
    },
    {
      "description": "The path to a fault-tolerant quantum computer (FTQC) is not solely a physics challenge; it is an AI and supercomputing challenge. This talk outlines the singular mission required to deliver scientifically useful accelerated quantum supercomputers by 2028: the complete convergence of AI with quantum hardware, systems, and algorithms. We will also present the blueprint for a collaborative and open quantum ecosystem. By leveraging open standards in software, CUDA-Q, interconnects, and NVQLink, we are creating a unified, hybrid infrastructure where national labs, startups, and industry partners can collaborate.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Press / Analyst",
      "key_takeaways": [
        "AI for quantum is the key to unlocking the potential of quantum computing.",
        "Future useful quantum is an accelerator to supercomputers—a new type of supercomputer.",
        "Open systems and platforms are essential to enable ecosystem success."
      ],
      "nvidia_technology": "Grace CPU, DGX Platform, CUDA, cuQuantum, CUDA-X, Blackwell, CUDA-Q",
      "session_id": "S81804",
      "session_type": "Talks or Panels",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Sam Stanwyck",
          "title": "Group Product Manager"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "The Genesis of Accelerated Quantum Supercomputing: Unifying AI and Quantum",
      "topic": "Quantum Computing",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81804/"
    },
    {
      "description": "Building generalist robots is a complex AI challenge—developers must ensure these machines can safely adapt across industries, from manufacturing floors to hospital corridors. Join this lightning session featuring founders and technologists from robotics startups in the NVIDIA Inception program as they share real-world lessons from tackling these problems at scale. Hear how they're leveraging NVIDIA CUDA-accelerated libraries, foundation AI models, and simulation frameworks built with Omniverse to build and deploy AI-driven robotics products—what worked, what didn't, and why. Gain insights on architectures, deployment patterns, and measurable customer impact across manufacturing, healthcare, retail, and food service. Join this lightning session featuring founders and technologists from robotics startups in the NVIDIA Inception program, sharing real-world lessons from building and scaling AI-driven robotics products. Hear how they are solving tough industry challenges using NVIDIA CUDA-accelerated libraries, foundation AI models, and simulation frameworks built with Omniverse—what worked, what didn’t, and why. Plus, gain insights on architectures, deployment patterns, and customer impact across multiple industries.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Practical examples of how robotics startups in the NVIDIA Inception program build and scale AI-powered robots using NVIDIA platforms",
        "Real-world lessons on overcoming technical challenges when deploying robotics solutions in industries like manufacturing, healthcare, retail, and food service",
        "Concrete examples of customer impact and how NVIDIA Inception support, technical resources, and community help startups move faster from prototype to production"
      ],
      "nvidia_technology": "Jetson, CUDA, Isaac, Omniverse, Metropolis, PhysX, Cosmos",
      "session_id": "S81490",
      "session_type": "Lightning Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Sumay Parikh",
          "title": "Inception Partner Lead - Robotics"
        },
        {
          "company": "LightWheel",
          "name": "Steve Xie",
          "title": "Founder and CEO"
        },
        {
          "company": "Workr",
          "name": "Ken Macken",
          "title": "CEO and Co-Founder"
        },
        {
          "company": "Serve Robotics",
          "name": "Rajesh Radhakrishnan",
          "title": "VP of Autonomy"
        },
        {
          "company": "Bedrock Robotics",
          "name": "Boris Sofman",
          "title": "CEO & Co-founder"
        }
      ],
      "technical_level": "General Interest",
      "title": "The Next Generation Of Robot Developers",
      "topic": "Robotics Simulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81490/"
    },
    {
      "description": "As AI reshapes digital experiences, content delivery needs a major transformation. Traditional networks built for static caching can’t meet real-time, personalized expectations. Modern applications require generative AI inference at the edge, close to users and data, to meet strict performance, privacy, and efficiency needs. Learn how edge AI redefines how content is created, delivered, and monetized. By combining generative, agentic, and future physical AI with NVIDIA accelerated computing and software, you can synthesize content in real time and tailor experiences to users and markets. Learn how advanced edge compute and intelligent data systems reduce data movement, maintain sovereignty, and deliver adaptive, low-latency experiences across augmented/virtual reality, internet of things, and autonomous systems. Join us to see how Gen AI at the edge unlocks new value and innovation.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Understand how moving generative AI inference to the edge transforms content delivery and enables real-time, personalized digital experiences.",
        "Learn how to leverage edge AI, intelligent data systems, and NVIDIA accelerated computing to improve responsiveness, reduce data movement, and enhance privacy.",
        "Discover how advances in AI data processing at the edge are enabling faster insights, richer experiences, and new forms of real-time intelligence."
      ],
      "nvidia_technology": "BlueField DPU, RTX GPU, DOCA, Ethernet Networking, NVIDIA NIM, Blackwell, NVIDIA AI Enterprise",
      "session_id": "S81690",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Kevin Deierling",
          "title": "SVP, Networking"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Transform Content Delivery With Generative AI at the Edge",
      "topic": "Storage Networking",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81690/"
    },
    {
      "description": "The next generation of visual AI agents starts with NVIDIA Blueprint for video search and summarization (VSS), which enables the creation of interactive analytical agents that reason through massive amounts of video, assess physical processes, and provide decision-making insights to streamline operations. The blueprint combines powerful vision language models (VLMs), advanced search capabilities with large language models, and Cosmos Reason VLM into a full suite of software for building, We've made it easier to customize and deploy at the edge, on-premises, or in the cloud. Be the first to hear about the new features of VSS to accelerate your AI agent development.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how VLMs, LLM-powered search, and Cosmos Reason are integrated into a cohesive agent pipeline.",
        "Be the first to learn about the latest capabilities to NVIDIA Blueprint for video search and summarization.",
        "Discover strategies for customizing, deploying, and iterating on VSS-based AI agents across edge, on-premises, and cloud environments."
      ],
      "nvidia_technology": "DeepStream, TensorRT, Metropolis, CV-CUDA, TAO Toolkit, NVIDIA AI Enterprise",
      "session_id": "S81836",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Roopa Prabhu",
          "title": "Sr. Director of Engineering"
        },
        {
          "company": "NIVIDIA",
          "name": "Adam Ryason",
          "title": "Product Manager, NVIDIA AI Blueprint for Video Search and Summarization (VSS)"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Visual AI Agents for Real-Time Video Understanding",
      "topic": "Video Analytics",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81836/"
    },
    {
      "description": "For AI and data center growth, time-to-power is most critical bottleneck. In the United States, average time for interconnection is between three to five years. This panel with Southwest Power Pool, GE Vernova and Southern California Edison will discuss how they are using agentic AI and digital twins to better understand existing capacity, peak loads, and accelerating the interconnection process.",
      "format": "In-Person",
      "industry": "Energy",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Generative AI to increase workflow efficiency between real-time optimization, utility, and developer",
        "Accelerating 3-5 years average time for new gigawatt connection to the grid under 1 year.",
        "AI factories for enterprises to adopt AI at scale",
        "Securing billions of dollars investments for utilities and states and help win the AI race"
      ],
      "nvidia_technology": "BlueField DPU, Omniverse, RAPIDS, Modulus, CUDA-X, cuOPT, NeMo, Omniverse Replicator, NVIDIA NIM, Blackwell, NVIDIA AI Enterprise",
      "session_id": "S81629",
      "session_type": "Panel",
      "speakers": [
        {
          "company": "Southwest Power Pool",
          "name": "Felek Abbas",
          "title": "SVP and CTSO"
        },
        {
          "company": "NVIDIA",
          "name": "Ahsan Yousufzai",
          "title": "Global Head Energy Surface"
        },
        {
          "company": "Southern California Edison",
          "name": "Shinjini Menon",
          "title": "SVP, System Planning & Engineering"
        },
        {
          "company": "GE Vernova",
          "name": "Vera Silva",
          "title": "Chief Strategy and Technology Officer"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Accelerate Power Grid Interconnection for Load and Generation Using Agentic AI and Digital Twins",
      "topic": "Retrieval-Augmented Generation (RAG)",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81629/"
    },
    {
      "description": "Discover how NVIDIA’s Halos safety tools, methods, and principles, from chip to cloud, accelerate safe physical AI development. Understand how safety innovation and standards compliance converge to enable the next generation of autonomous vehicles and robotics. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "Automotive / Transportation",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "AV Safety Blueprint: See the comprehensive Halos stack for autonomous vehicles, from the algorithmic safety dataset and scalable simulation to certified hardware.",
        "Robot Functional Safety: Get an expert look at how Halos integrates into robotics platforms (like IGX), enabling robust, embedded functional safety features and sophisticated outside-in functional safety AI agents.",
        "Deciphering Certification: Learn how the ANAB-accredited Halos AI Systems Inspection Lab is the fast track for customers seeking safety, cybersecurity, and AI safety certification, saving you time and cost.",
        "Future-Proofing Your Fleet: Understand the principles driving safety and standards compliance that will prepare your physical AI systems for future regulatory and deployment challenges."
      ],
      "nvidia_technology": "DRIVE, CUDA, DRIVE SDK, Isaac, Omniverse, Metropolis, OVX, DRIVE AV, IGX, Blackwell, Cosmos",
      "session_id": "CWES81819",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Khoa Ho",
          "title": "Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Suhas Hariharapura Sheshadri",
          "title": "Sr. Software Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Riccardo Mariani",
          "title": "VP Safety"
        },
        {
          "company": "NVIDIA",
          "name": "Viola Wu",
          "title": "Sr. Product Marketing Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Apoorva Sharma",
          "title": "Research Scientist"
        },
        {
          "company": "NVIDIA",
          "name": "Ekram Mukbil",
          "title": "Director Global Automotive Developer Relations"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Advance Physical AI Safety With NVIDIA Halos",
      "topic": "Edge Functional Safety",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81819/"
    },
    {
      "description": "The Cascadia subduction zone is capable of unleashing a magnitude 9 earthquake with tsunamis as high as 30 meters, endangering millions in the Pacific Northwest. While it has been quiet since 1700, scientists place a 37% probability of an 8.2+ earthquake in the next 50 years. In response, the 2025 Gordon Bell Prize was awarded to UT-Austin, UC San Diego, and Lawrence Livermore National Laboratory for the world’s first digital twin for real-time, full-physics-based tsunami early warning. Leveraging exascale-class HPC systems, it delivers unprecedented accuracy and speed, achieving a ten-billion-fold performance improvement over existing state-of-the-art methods. This enables rapid, data-driven hazard assessment and actionable insights during critical windows. The work not only redefines the boundaries of real-time tsunami forecasting, but also sets a new standard for disaster resilience.",
      "format": "In-Person",
      "industry": "Academia / Higher Education",
      "intended_audience": "Research: Non-Academic",
      "key_takeaways": [
        "With advanced algorithms and exascale-class computing, digital twins can achieve both full physics accuracy and real-time response.",
        "Billion-parameter inverse problems (inferring large-scale model parameters from observational data) can be solved in a fraction of a second.",
        "For the first time, 3D wave propagation-based tsunami inference and forecasting was achieved in real time."
      ],
      "nvidia_technology": "Grace CPU, Hopper",
      "session_id": "S82161",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "The University of Texas at Austin",
          "name": "Stefan Henneking",
          "title": "Research Associate"
        },
        {
          "company": "The University of Texas at Austin",
          "name": "Omar Ghattas",
          "title": "Professor and Cockrell Endowed Chair in Engineering"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "2025 Gordon Bell Winner: Forecasting Tsunamis in Real Time With Digital Twins",
      "topic": "Climate / Weather / Ocean Modeling",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82161/"
    },
    {
      "description": "Join this interactive Q&A session to get direct, actionable advice from NVIDIA's core engineering teams on overcoming the critical developer friction points in building large-scale, physically accurate digital twins for robotics, autonomous vehicles, and manufacturing. This is your unique opportunity to discuss your specific use cases and technical challenges directly with the experts who develop and maintain the core Omniverse libraries and technologies for world simulation, including OpenUSD, PhysX, RTX, and more. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Engage in deep-dive Q&A with the primary engineers behind OpenUSD, PhysX, and RTX to resolve specific implementation bottlenecks.",
        "Obtain actionable technical solutions for performance and fidelity issues common in large-scale, physically accurate digital twin simulations.",
        "Learn low-level strategies for integrating Omniverse libraries to better scale workloads for robotics, audio-visual, and industrial manufacturing."
      ],
      "nvidia_technology": "Omniverse",
      "session_id": "CWES81740",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Sebastian Misiurek",
          "title": "Sr. Product Manager, Omniverse"
        },
        {
          "company": "NVIDIA",
          "name": "Neelakantan Mani",
          "title": "Sr. Product Manager, Omniverse Physics"
        },
        {
          "company": "NVIDIA",
          "name": "Stephanie Rubenstein",
          "title": "Sr. Product Marketing Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Richard Yarlett",
          "title": "Technical Marketing Engineer, Omniverse"
        },
        {
          "company": "NVIDIA",
          "name": "Brian Harrison",
          "title": "Sr. Director of Product Management Omniverse"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "A Deep Dive Into Omniverse Libraries for OpenUSD and Physical AI Development",
      "topic": "Industrial Digitalization / Digital Twin",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81740/"
    },
    {
      "description": "More than 20 telcos worldwide have already built sovereign AI infrastructure powered by NVIDIA, with many more underway. These telco-operated national AI clouds are becoming strategic assets, combining local control and regulatory compliance with high-performance AI capabilities to serve priorities ranging from secure government workloads and public services to industrial clouds for sectors like manufacturing. This session brings together two leading operators who will dive deep into their vision and journey for sovereign AI in their markets: how they designed and built their infrastructure, what they are optimized for, how adoption is ramping, and where their roadmaps are headed. Gain a front-row view into how telcos are turning sovereign AI into real platforms for innovation, growth, and national resilience.",
      "format": "In-Person",
      "industry": "Telecommunications",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Understand how telco-built sovereign AI clouds, powered by NVIDIA’s full stack platforms, enhance data security and compliance, while enabling AI workloads.",
        "Gain insight into real-world use cases, adoption patterns, and roadmaps for leading sovereign AI clouds, and the lessons other telcos can apply in their own markets.",
        "Learn how industries and governments can collaborate with telco sovereign AI clouds to accelerate innovation in a trusted, locally operated environment."
      ],
      "nvidia_technology": "BlueField DPU, CUDA, Infiniband Networking, Ethernet Networking, Hopper, Interconnect Networking, NeMo, NVLink / NVSwitch, NVIDIA NIM, Blackwell, NVIDIA AI Enterprise, Blueprint, Dynamo",
      "session_id": "S82020",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "SingTel",
          "name": "Bill Chang",
          "title": "CEO Digital InfraCo"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "A Deep Dive Into Two Telco-Built Sovereign AI Clouds",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82020/"
    },
    {
      "description": "Explore how the MGX architecture is reshaping AI server design with its modular, building-block approach engineered to maximize performance for GPU-driven AI workloads, while also simplifying hardware reuse across multiple processor platforms to provide a scalable, future-ready foundation for evolving enterprise AI infrastructure. This session also showcases how optimized resource allocation, integrated internal cabling, and MGX-powered MSI AI platforms accelerate AI deployment, enhance efficiency, and support both on-premises and cloud environments.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "AI-Centric Modular Design: Gain insight into how MGX removes unnecessary components and prioritizes GPU and AI optimization to elevate server performance.",
        "Streamlined Hardware Reuse: Understand how MGX’s building-block methodology enables chassis reuse across diverse processor platforms, improving cost efficiency and long-term compatibility.",
        "Future-Ready Architecture: See how MGX provides a scalable, proven foundation that supports the rapidly evolving demands of AI server technologies, fostering innovation and adaptability.",
        "Use Case Highlights: Discover how MSI’s NVIDIA MGX-powered AI platforms deliver fast-to-deploy solutions that help solution providers accelerate deployment and scale AI infrastructure for both on-premises and cloud services."
      ],
      "nvidia_technology": "MGX",
      "session_id": "EX82040",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "Micro-Star International Co., Ltd.",
          "name": "Philip Maher",
          "title": "Director of Marketing, MSI Enterprise Platform Solutions"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "A Modular Blueprint for Next-Generation AI Servers: Inside the MGX Architecture (Presented by MSI)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82040/"
    },
    {
      "description": "This session is designed for companies already deploying vision AI applications with NVIDIA Metropolis (DeepStream, TAO). It will outline the next steps – how to integrate Vision Language Models (VLMs) into existing pipelines to unlock new reasoning and contextual understanding capabilities. Milestone Systems, a leader in video management software, will share how they have fine-tuned NVIDIA Cosmos Reason to create a specialized and highly accurate VLM using 150,000 hours of curated video. Attendees will learn how they can use Milestone's VLM directly within their own applications and walk through their experience integrating VLMs into their video management system, XProtect. Through these use cases, we’ll examine how the next generation of VLM technology can transform video analytics into reasoning-driven applications.",
      "format": "In-Person",
      "industry": "Smart Cities / Spaces",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "How to extend an existing computer vision pipeline with a domain-specialized VLM so it can understand context, conditions, and events.",
        "Model customization fine tuning workflow",
        "Challenges and solutions to adding reasoning agents into existing applications."
      ],
      "nvidia_technology": "Jetson, DeepStream, Metropolis, TAO Toolkit, NVIDIA NIM, NVIDIA AI Enterprise, Cosmos",
      "session_id": "S81863",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Milestone Systems",
          "name": "Edward Mauser",
          "title": "Director and Product Lead"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "A New Frontier for Vision AI with Expert Reasoning Agents",
      "topic": "Vision Language Models (VLMs)",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81863/"
    },
    {
      "description": "Autonomous AI requires a new security model. From AI agents to robots, we need to rethink how we should trust these systems to deliver the full promise of their automation. Can we verify their operations, security, and alignment as we grant them access to our most coveted data and delegate them critical tasks?. As multi-agent AI systems span boundaries from cloud to edge, the challenge of maintaining trust and security across every layer of the stack—digital and physical—has never been greater. This session introduces EQTY’s verifiable AI runtime, a new approach that continuously ensures verified integrity from model training and inference to deployment in real-world, physical AI environments, including the verification of AI sovereignty.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Validate the full AI stack—from data ingestion to inference—using hardware-based attestation for zero-trust execution environments.",
        "Enforce entitlement boundaries and safeguards that define and restrict what AI agents can do on machines or physical platforms.",
        "Go beyond traditional secure boot and authentication methods with new runtime security that provides continuous verified AI.",
        "Bind agent identify with tamperproof certifications of governance and execution to verify that only authorized agents are actually operating.",
        "Integrate with BlueField-3 and NVIDIA DOCA to enable network-level trust enforcement, allowing only verified traffic while blocking unauthorized access at the hardware layer."
      ],
      "nvidia_technology": "DOCA",
      "session_id": "S81489",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "AI Pathfinder",
          "name": "Brad Lewis",
          "title": "Chief Revenue and Transformation Officer"
        },
        {
          "company": "NVIDIA",
          "name": "Ofir Arkin",
          "title": "Sr. Distinguished Engineer"
        },
        {
          "company": "EQTY Lab",
          "name": "Jonathan Dotan",
          "title": "Founder"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "A New Paradigm: Verifiable AI",
      "topic": "Security for AI",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81489/"
    },
    {
      "description": "Discover how to build and operate AI factories that scale with your ambitions. In this session, NVIDIA shares hard-earned lessons from working alongside Cloud Partners and ecosystem ISVs to power massive AI infrastructure around the world. You’ll gain a practical framework—built on open, composable infrastructure software—for managing the full lifecycle of large AI systems, from reliability and performance to multi-tenant orchestration. Walk away ready to accelerate your time-to-production and deliver sustainable scale for your own AI operations.",
      "format": "In-Person",
      "industry": "Cloud Services",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how open, composable infrastructure software empowers AI platform builders.",
        "Gain field-tested insights from NVIDIA’s real-world AI deployments."
      ],
      "nvidia_technology": "Hopper, Blackwell, NVIDIA AI Enterprise, Mission Control",
      "session_id": "S81847",
      "session_type": "Talks or Panels",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Vishal Ganeriwala",
          "title": "Sr. Director, Product Marketing"
        },
        {
          "company": "NVIDIA",
          "name": "Warren Barkley",
          "title": "VP of Product Management, DGX Cloud"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "A Playbook – Operating Cloud AI Factories at Scale",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81847/"
    },
    {
      "description": "As enterprises move beyond AI experimentation and into production, they face the growing complexity of modern AI across capabilities, technologies, and use cases. This session presents a practical, outcome-driven playbook for making AI real by translating that complexity into a clear, actionable framework. By leveraging physical AI and agentic AI, organizations can deliver step-change improvements in efficiency, safety, and profitability. We’ll explore how enterprises connect real-time simulation, robotics, intelligent collaborative agents, and computer vision into scalable architectures that transform the end-to-end value chain. Through real-world business use cases, you'll gain a clear framework to simplify AI adoption, articulate value to stakeholders, and accelerate the deployment of physical and agentic AI in production environments.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Learn how to simplify AI complexity and apply an outcome-driven approach to deploying physical AI and agentic AI at scale.",
        "Understand how real-time simulation, robotics, computer vision, and intelligent agents work together to transform the end-to-end enterprise value chain.",
        "See real-world industry use cases demonstrating step-change improvements in efficiency, safety, and profitability, and how these outcomes translate into measurable business value."
      ],
      "nvidia_technology": "Isaac, Omniverse, Metropolis, NeMo, Omniverse Replicator, NVIDIA NIM, Cosmos, Blueprint",
      "session_id": "EX82287",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "SoftServe, Inc.",
          "name": "Taras Bachynskyi",
          "title": "VP of Technology"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "A Practical Playbook for Making AI Real: Bring Physical and Agentic AI to Life (Presented by SoftServe, Inc.)",
      "topic": "Industrial Digitalization / Digital Twin",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82287/"
    },
    {
      "description": "Real-time AI inference at scale requires high-performance GPUs combined with efficient data movement, preprocessing, and data access from edge to cloud. Developers often struggle to eliminate bottlenecks that slow down inference pipelines and waste valuable compute cycles. The NVIDIA DOCA software platform enables high-performance AI inference processing by accelerating data movement and preprocessing tasks using RDMA, NIXL, and GPUNetIO. DOCA-powered storage solutions like the NVIDIA AI Data Platform design and S3-compatible storage enable seamless, high-performance data access for AI workloads. This session demonstrates how to design inference services that deliver responsive and reliable results for latency-sensitive applications in both AI factory and edge environments. It also shows how to apply DOCA-accelerated networking and storage for high-performance data access.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Discover techniques to optimize AI inference pipelines across environments and from edge devices to cloud data centers.",
        "Learn to identify data-path bottlenecks and scale latency-sensitive inference services with DOCA and BlueField-based acceleration.",
        "Apply best practices for building real-time, latency-sensitive AI services using DOCA-accelerated storage and networking."
      ],
      "nvidia_technology": "BlueField DPU, DOCA",
      "session_id": "S81773",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Oren Duer",
          "title": "Principal Software Architect DOCA"
        },
        {
          "company": "NVIDIA",
          "name": "Einav Zilberstein",
          "title": "Sr. Product Lead DOCA"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Accelerate AI Inference Using DOCA for Storage",
      "topic": "Storage Networking",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81773/"
    },
    {
      "description": "In this training lab, we'll introduce Aether — a set of automation and AI tools that speed up and reduce the cost of accelerated extract, transform, load and analytics workloads on the RAPIDS Accelerator for Apache Spark. Aether combines automation along with advanced AI tools to automatically test and optimize your Apache Spark workloads with GPU. Some experience with Linux and the command line needed. An understanding of large-scale analytics and Apache Spark is not required, but would be helpful.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Introduce the RAPIDS Accelerator for Apache Spark — leveraging GPUs for dramatically speeding up the distributed big data processing framework of Apache Spark while reducing total cost.",
        "Introduce Aether with hands-on experience to automate migrating a Spark job from the CPU to the GPU with the RAPIDS Accelerator.",
        "Use AI-based config tuning models to predict optimized Spark configurations for your workloads.",
        "Use the Aether Assistant to rewrite user-defined functions, which are typically slow and hard to manually optimize into GPU accelerated versions to ensure maximum speed-up and cost savings."
      ],
      "nvidia_technology": "CUDA, cuDF, nvCOMP",
      "session_id": "DLIT81642",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Hirakendu Das",
          "title": "Principal Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Navin Kumar",
          "title": "Sr. System Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Rishi Chandra",
          "title": "Systems Software Engineer"
        }
      ],
      "technical_level": "General Interest",
      "title": "Accelerate Apache Spark With GPU and AI: A Hands-On Workshop",
      "topic": "Data Analytics / Processing",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81642/"
    },
    {
      "description": "We'll walk through how we built our graphics pipeline in Omniverse, an upgrade that slashed rendering times by nearly a factor of 100 and cut our production cycles in half. This improvement now lets us deliver photorealistic videos alongside traditional renderings, significantly enhancing what we can offer our customers. We’ll delve into our customer workflow and the custom tooling we’ve developed around Omniverse to maximize efficiency.",
      "format": "Virtual",
      "industry": "Aerospace",
      "intended_audience": "Marketing / Sales",
      "key_takeaways": [
        "Discover how we leveraged Omniverse's scripting system for task automation to support a business process.",
        "Learn how we standardized our deliverables while maintaining flexibility."
      ],
      "nvidia_technology": "RTX GPU, Omniverse",
      "session_id": "S81985",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Dassault Aviation",
          "name": "Axel Cocat",
          "title": "Computer Engineer"
        },
        {
          "company": "Dassault Aviation",
          "name": "Jean-Charles Fedini",
          "title": "Falcon Graphics Lab Team Leader"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Accelerate Business Jet Customization With Photorealistic Rendering",
      "topic": "Rendering Engines / Pipelines / Tools",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81985/"
    },
    {
      "description": "Explore how to scale causal inference using GPU-parallel spatio-temporal models. We show how to implement spiking neural networks for event detection, compute Rényi transfer entropy with custom CUDA kernels, and build lag-aware causal graphs using cuGraph, NCCL, and NVLink memory patterns. You’ll learn how to run large counterfactual simulations with CUDA streams and graph execution, and how to optimize each stage for multi-GPU NVL72 SuperPOD systems.",
      "format": "In-Person",
      "industry": "Media & Entertainment",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how spiking neural networks and entropy-based estimators identify rare events and directional influence in high-dimensional time-series data."
      ],
      "nvidia_technology": "DGX Platform, Blackwell",
      "session_id": "S81594",
      "session_type": "Talk",
      "speakers": [],
      "technical_level": "Technical - Advanced",
      "title": "Accelerate Causal Discovery in Complex Systems Using Modern GPU Techniques",
      "topic": "Industrial Digitalization / Digital Twin",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81594/"
    },
    {
      "description": "AI advancement depends on cloud infrastructure to deliver massive compute scale, high-performance, high resilience, and robust security. Oracle Cloud Infrastructure (OCI) is addressing these challenges through an innovative architectural approach featuring the Oracle Acceleron fabric integrated with NVIDIA BlueField technology. This session explores how OCI’s accelerated networking, intelligent isolation mechanisms, and high-speed data movement capabilities collectively form a robust and secure platform for giga-scale. Speakers will share details of the Oracle Acceleron architecture and show how it enables the world’s most demanding AI workloads. Gain a deeper understanding of the infrastructure patterns shaping modern AI platforms and how organizations can leverage them as they accelerate their AI journeys, securely and efficiently.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Explore how accelerated, software-defined networking and strong workload isolation enable secure, predictable AI cloud performance.",
        "Understand how high-speed data access and movement, powered by Oracle Acceleron and NVIDIA BlueField, support gigascale AI workloads.",
        "Learn practical insights from OCI’s architectural journey and how these patterns can support next-generation AI applications.",
        "Connect With the Experts",
        "Workshops, Training Labs & Certification",
        "Sponsors & Exhibitors",
        "Privacy Policy Your Privacy Choices Terms of Service Accessibility Corporate Policies Product Security Contact",
        "Copyright © 2026 NVIDIA Corporation",
        "NVIDIA uses cookies to improve your experience on our web site. We and our third-party partners also use cookies"
      ],
      "nvidia_technology": "BlueField DPU, DOCA",
      "session_id": "S81788",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Oracle Corp.",
          "name": "Pradeep Vincent",
          "title": "Executive Vice President & Chief Technical Architect, OCI"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Accelerate Cloud Platforms for the Next Era of AI",
      "topic": "Cloud Networking",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81788/"
    },
    {
      "description": "Join NVIDIA experts to discuss GPU-accelerated compression and decompression with nvCOMP and dedicated hardware decompress engines for data analytics, genomics, deep learning, and more. We’ll walk through best practices for offloading decompression to Blackwell’s dedicated engine, show how the latest nvCOMP APIs (including Python APIs) simplify integration into existing CPU and GPU pipelines, and talk through real-world performance tuning patterns. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Understand which nvCOMP algorithms and configurations to use for real-world analytics, genomics, and deep learning workloads.",
        "See how the latest nvCOMP APIs, including Python APIs, integrate into existing CPU- and GPU-based data pipelines with minimal code changes.",
        "Learn how to offload decompression to NVIDIA hardware decompress engines to free GPU compute and increase throughput.",
        "Get practical tuning tips (chunk sizing, batching, concurrency) to reduce I/O bottlenecks and improve end-to-end pipeline performance.",
        "Share your workloads and requirements to help influence the future roadmap for nvCOMP and GPU decompression features."
      ],
      "nvidia_technology": "CUDA, cuDF",
      "session_id": "CWES81905",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Naveen Himthani",
          "title": "Sr. Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Eric Schmidt",
          "title": "Sr. DevTech Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Makan Taghavi",
          "title": "Sr. Product Manager for Image Processing and Data Compression"
        },
        {
          "company": "NVIDIA",
          "name": "Balazs Nagy",
          "title": "Sr. CUDA Math Libraries Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Accelerate Data Compression and Decompression on GPUs with nvCOMP",
      "topic": "Data Storage",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81905/"
    },
    {
      "description": "The field of drug discovery is increasingly benefiting from AI, helping researchers build efficient predictive and generative workflows that drastically reduce time-to-clinic. NVIDIA accelerates this process with open-source solutions like BioNeMo Recipes, NIM, and Clara Open Models for Digital Biology, providing easy-to-adopt solutions for training and deploying foundation models and agentic AI systems for drug discovery. In this session, we'll demonstrate how the latest NVIDIA accelerated libraries and models streamline AI workflows, accelerate drug discovery, and enable more efficient biomolecular design and analysis. Basic understanding of Python, Machine Learning, and Docker",
      "format": "In-Person",
      "industry": "Healthcare & Life Sciences",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn about the latest advances in NVIDIA accelerated libraries and platforms for drug discovery.",
        "Understand how NVIDIA BioNeMo streamlines the development and optimization of cutting-edge models.",
        "Gain hands-on experience using NVIDIA Accelerated Computing Platforms and Libraries for Drug Discovery to train and deploy AI models at scale."
      ],
      "nvidia_technology": "CUDA, Clara, BioNeMo, NVIDIA NIM",
      "session_id": "DLIT81682",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Kristopher Kersten",
          "title": "Technical Marketing Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Neel Patel (WWFO - Clara Healthcare)",
          "title": "Technical Marketing Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Accelerate Drug Discovery With NVIDIA Libraries and Platforms",
      "topic": "Biology - Generative AI",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81682/"
    },
    {
      "description": "Learn about how Lightwheel's SimReady assets and scenes, egocentric data, and evaluation platform advance physical AI development.",
      "format": "In-Person",
      "industry": "Manufacturing",
      "intended_audience": "Research: Non-Academic",
      "key_takeaways": [
        "Physical AI needs three ingredients: World, Behavior, and Eval. World corresponds to the physical and simulation world. Behavior corresponds to robot action data. Eval corresponds to evaluation for the physical AI model.",
        "How Lightwheel builds and defines the industry's SimReady standard for simulation assets and scenes in NVIDIA Isaac Sim",
        "How Lightwheel empowers physical AI model training through large-scale synthetic data and egocentric data collection"
      ],
      "nvidia_technology": "",
      "session_id": "EX82219",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "LightWheel",
          "name": "Myles Liu",
          "title": "Head of Business Development"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Accelerate Embodied AI With Simulation-Centric Data Engine (Presented by Lightwheel)",
      "topic": "Robot Manipulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82219/"
    },
    {
      "description": "Enterprise AI adoption is accelerating through AI agents embedded in applications and workflows. While these agents drive innovation, they present unique security challenges as organizations move to production. Traditional security approaches weren't built for agent architectures, creating vulnerabilities in agent logic, prompts, model interactions, and infrastructure. Learn how CrowdStrike is collaborating with NVIDIA to drive secure enterprise AI adoption across the entire AI life cycle, from governing agent behavior to protecting models, and delivering built-in protection for AI factories. By integrating security into agent design and runtime, organizations can confidently scale AI systems while managing risk.",
      "format": "In-Person",
      "industry": "Cloud Services",
      "intended_audience": "Security Operations",
      "key_takeaways": [
        "See how to extend protection to agents built with NVIDIA NeMo Agent Toolkit, from agent creation and orchestration to runtime behavior and inference.",
        "Learn ways to secure agent prompts, tools, and model interactions with inference-time controls and guardrails to reduce prompt injection, data leakage, and agent misuse.",
        "Discover how to scale secure, agentic workloads on the NVIDIA Enterprise AI Factory validated design, with integrated protection across agents, models, data access, and runtime infrastructure."
      ],
      "nvidia_technology": "Morpheus, NVIDIA NIM",
      "session_id": "S82266",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "CrowdStrike",
          "name": "Mike Petronaci",
          "title": "CTO of Proactive Security"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Accelerate Enterprise AI Adoption: Secure Your Agentic Foundations from Day 1 (Presented by CrowdStrike)",
      "topic": "Security for AI",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82266/"
    },
    {
      "description": "Learn how the integration of NVIDIA's accelerated computing and Planet Labs' cloud-scale imagery platform is driving a revolutionary shift in geospatial workflows at scale and in real time. This integration of Earth observation innovation pioneered by Planet, NVIDIA’s platform, and AI unlocks the full potential of petabytes of Earth observation data, providing detailed, actionable insights that address some of the world’s most pressing environmental and humanitarian challenges.",
      "format": "In-Person",
      "industry": "Public Sector and Sovereign AI",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how the shift from a traditional, batch pre-compute, and human-centric analytical workflow to an on-demand processing architecture accelerates analysis and reduces costs across massive geospatial image datasets.",
        "Understand how integrating NVIDIA’s accelerated computing and Planet Labs’ cloud-scale imagery platforms allows high-throughput, real-time inference—rapidly transforming raw satellite data into actionable insights with increased efficiency and automation."
      ],
      "nvidia_technology": "Grace CPU, TensorRT, OptiX, Hopper, Clara Holoscan, cuBLAS, CUDA-X, Interconnect Networking, nvCOMP, Blackwell",
      "session_id": "S81732",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "May Casterline",
          "title": "Director, Solutions Architecture"
        },
        {
          "company": "Planet",
          "name": "Kiruthika Devaraj",
          "title": "VP of Spacecraft"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Accelerate Geospatial Workflows for Planetary Insight",
      "topic": "Data Analytics / Processing",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81732/"
    },
    {
      "description": "In 2024, NVIDIA unveiled nvmath-python, a library designed to bridge the gap between Python scientific community and NVIDIA CUDA-X math libraries, and it’s now generally available. You’ll learn what makes nvmath-python a useful addition for GPU-accelerated computing with Python, along with how to use the library’s unique capabilities to reach extreme levels of performance to solve cutting-edge scientific problems previously unimaginable in Python.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how nvmath-python's just-in-time kernel fusion allows achieving high computational efficiency.",
        "Get familiar how to leverage Python just-in-time compilers, along with nvmath-python, for writing sophisticated device kernels, from cooperative general matrix multiplication algorithms to fast Fourier kernels, floating-point emulation, and more.",
        "Scale a single-GPU problem to very large ones with minimum code changes using nvmath-python's multi-GPU/multi-node capabilities.",
        "Learn how the library interoperates with the rest of the Python scientific computing ecosystem for getting richest experience."
      ],
      "nvidia_technology": "CUDA, cuBLAS, CUDA-X, cuFFT",
      "session_id": "S81581",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Sergey Maydanov",
          "title": "Sr. Software Engineering Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Aart Bik",
          "title": "Distinguished Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Accelerate GPU Scientific Computing With nvmath-python",
      "topic": "Accelerated Computing Libraries",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81581/"
    },
    {
      "description": "Building physically accurate digital twins for electrical grid infrastructure is becoming critical to be able simulate and predict outages during extreme weather events. We'll discuss Southern California Edison's journey of adopting Omniverse as a platform to bring all its ecosystem partners to build digital twins of the grid. Presenters will showcase how SoftServe built the solution, demonstrating Southern California Edison's vision, and in parallel helped Siemens Energy build scalable product integrating physics and AI-based models from substation.",
      "format": "In-Person",
      "industry": "Energy",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Building an open ecosystem for technology providers to integrate and build solutions faster",
        "Physically accurate digital twin to simulate accurately for extreme weather events",
        "AI- and physics-based models working together for faster and better predictions"
      ],
      "nvidia_technology": "RTX GPU, Omniverse, Metropolis, OVX",
      "session_id": "S81683",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Southern California Edison",
          "name": "Brenden Russell",
          "title": "Director Grid Innovation"
        },
        {
          "company": "SoftServe",
          "name": "Mike Wisniewski",
          "title": "NVIDIA Partner Executive"
        },
        {
          "company": "Siemens Energy",
          "name": "Adnan Chaudhry",
          "title": "SVP Digital Grid"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Accelerate Grid Modernization for AI Growth Using NVIDIA Omniverse",
      "topic": "Industrial Digitalization / Digital Twin",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81683/"
    },
    {
      "description": "This talk explains why Hon Hai Research Institute’s integrated strategy for advancing quantum science across the full stack requires an integrated strategy, developing both better hardware and applications algorithms together. We'll outline Hon Hai Research Institute’s approach to this—including Foxconn’s ion-trap roadmap—from first-generation blade-type traps to the establishment of Taiwan’s first industry-based ion-trap quantum computing laboratory and the creation of multi-layer and chip-based architectures. We'll also highlight advancements from Hon Hai Research Institute in software and theory for areas including quantum error correction, efficient tomography and noise benchmarking, quantum machine learning, and quantum networking.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Research: Academic",
      "key_takeaways": [
        "Introduce the research focus at HHRI",
        "Introduce the quantum research outcomes by HHRI",
        "Highlight our achievement in quantum error-correcting code, quantum algorithms, and quantum simulation areas"
      ],
      "nvidia_technology": "CUDA Quantum, CUDA-Q",
      "session_id": "S82043",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Hon Hai Precision Industry",
          "name": "Min Hsiu Hsieh",
          "title": "Director"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Accelerate Hybrid Quantum-Classical Research at Scale With NVIDIA CUDA-Q (Presented by Foxconn)",
      "topic": "Quantum Computing",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82043/"
    },
    {
      "description": "Discover how building and scaling virtual twins will reshape decision-making and accelerate value creation across all industries.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Virtual twin of physics behavior",
        "Virtual twin of production systems",
        "Virtual twin of materials"
      ],
      "nvidia_technology": "BioNeMo, CUDA-X, cuDDN, NeMo, PhysX, NVIDIA NIM",
      "session_id": "S81501",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Dassault Systèmes",
          "name": "Florence Hu-Aubigny",
          "title": "Executive VP R&D"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Accelerate Industrial Digital Twins With Dassault Systèmes",
      "topic": "Industrial Digitalization / Digital Twin",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81501/"
    },
    {
      "description": "Deploying autonomous robots and smart manufacturing systems requires a computing architecture that extends far beyond the edge. This session explores how to build a converged \"AI factory\" infrastructure that prioritizes the low-latency demands of advanced robotics and intelligent production, while simultaneously supporting the massive compute requirements of upstream physical AI workflows.​Learn architectural strategies to manage these mixed workloads on a single platform, ensuring real-time responsiveness for machines without sacrificing simulation performance. Finally, we will demonstrate how the GAIFA (GIGABYTE AI Factory Accelerator) ecosystem integrates specialized partners to power this end-to-end industrial autonomy.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Prioritize Real-Time Autonomy: Learn how to optimize infrastructure specifically for robotics and embodied AI, focusing on minimizing latency for inference and control loops in dynamic environments.",
        "Operationalize Physical AI: Understand the compute requirements for physical AI workflows, including how to efficiently bridge physics-based simulation (digital twins) with actual smart manufacturing operations.",
        "Leverage a Unified Ecosystem: See how the GAIFA ecosystem combines hardware and software to eliminate silos, providing a turnkey foundation for the entire spectrum of industrial workloads."
      ],
      "nvidia_technology": "Isaac, Base Command Manager, NeMo, NVIDIA NIM, Mission Control, Dynamo",
      "session_id": "S82229",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Giga Computing",
          "name": "Weiru Li",
          "title": "Deputy Director at the Software and Solutions Center"
        },
        {
          "company": "Information and Communication Lab., Industrial Technology Research Institute",
          "name": "Huei-Ru Tseng",
          "title": "Department Manager"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Accelerate Industrial Intelligence: Practice Across Design and Production (Presented by GIGABYTE)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82229/"
    },
    {
      "description": "We dive deep into new techniques for accelerating native sparse attention (NSA) kernels on Blackwell using CuTeDSL, showing how precise control over data movement and sparsity scheduling unlocks dramatic performance gains. Together, we'll explore practical strategies you can apply to push your model training efficiency to the next level.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn the NSA execution structure: three attention kernels with tight inter‑kernel data dependencies that make naive PyTorch custom ops or Triton implementations underperform.",
        "Discover how CuTeDSL enables precise control of layout, data movement, sparsity scheduling, and on‑chip reuse to implement high‑performance NSA training kernels on Blackwell.",
        "Walk through the key optimization strategies we used to unlock performance."
      ],
      "nvidia_technology": "CUDA",
      "session_id": "S81470",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Akash Mehra",
          "title": "Sr. Gen AI Algorithms Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Vincent Zhang",
          "title": "Sr. Software Engineer"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Accelerate Native Sparse Attention Kernels on Blackwell by CuTeDSL",
      "topic": "Programming Languages / Compilers",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81470/"
    },
    {
      "description": "As GPUs become central to scientific Python workloads, SciPy-ecosystem projects—including NumPy, SciPy, and scikit-learn—face the challenge of adopting CUDA without sacrificing usability, portability, or community values. This panel brings together open-source maintainers and ecosystem partners to discuss how tools such as CuPy, Numba, and CUDA Python are being used to accelerate core numerical routines while preserving Pythonic APIs. Panelists will share lessons from real-world integrations, including managing build and packaging complexity, handling optional GPU dependencies, and supporting diverse hardware environments. The discussion highlights how collaboration across open-source communities, hardware vendors, and scientific users enables sustainable GPU acceleration aligned with the long-term goals of the SciPy ecosystem.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Practical Pathways to GPU Acceleration: Learn how major SciPy ecosystem projects are adopting CUDA to accelerate core numerical routines while maintaining Pythonic APIs and cross-platform support.",
        "Collaboration Across Communities: Understand how open-source contributors, hardware vendors, and scientific users are working together to align GPU integration with community values and long-term sustainability.",
        "Lessons from Real-World Integrations: Hear concrete insights from maintainers who have navigated build system complexity, dependency management, and user adoption when introducing CUDA into established codebases."
      ],
      "nvidia_technology": "CUDA",
      "session_id": "S82113",
      "session_type": "Panel",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Leo Fang",
          "title": "Python CUDA Tech Lead"
        },
        {
          "company": "Princeton University",
          "name": "Ianna Osborne",
          "title": "Research Software Engineer"
        },
        {
          "company": "Open Teams",
          "name": "Travis Oliphant",
          "title": "Chief AI Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Katrina Riehl",
          "title": "Principal Technical Product Manager"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Accelerate Open Science: Incorporating CUDA Into the SciPy Ecosystem",
      "topic": "Programming Languages / Compilers",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82113/"
    },
    {
      "description": "Explore the NVIDIA AIOps resiliency tools that keep AI systems reliable at scale. Learn how to match these solutions to your operational challenges to build a practical adoption roadmap.",
      "format": "Virtual",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Understand the full range of AIOps resiliency solutions available in our portfolio.",
        "Identify which tools address specific operational challenges and use cases.",
        "Understand integration possibilities across our product suite.",
        "Determine the right starting point for their AIOps journey.",
        "Plan a phased adoption strategy aligned with organizational maturity and use cases."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, Multi-Instance GPU (MIG), NCCL, NVLink / NVSwitch, NVIDIA Run:ai, Mission Control",
      "session_id": "S81552",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Ava Arnaz",
          "title": "Sr. AI Solutions Architect"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Accelerate Operational Excellence Through AIOps Resiliency Solutions",
      "topic": "InfraOps",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81552/"
    },
    {
      "description": "Explore how to accelerate physical AI deployments using the QCT Application-Ready Solution concept. By deep-diving into the QCT Dev Kit architecture, a toolkit that integrates high-performance QCT GPU servers with NVIDIA Cosmos, NVIDIA Isaac GR00T, and NVIDIA Omniverse, customers can deliver end-to-end physical AI solutions faster than ever. Learn how QCT works with TM Robot to overcome humanoid era challenges and achieve industrial workload validation. Through the TM Robot \"Xplore I\" collaboration, the companies manage to improve humanoid performance via specialized GUI designs and enable five key features (system, skills, datasets, models, and USD file management).",
      "format": "In-Person",
      "industry": "Academia / Higher Education",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Explore how the QCT Application-Ready Solution concept integrates GPU servers, a custom front-end GUI, and partner solution to deliver end-to-end physical AI solutions.",
        "Discover the QCT Dev Kit architecture integrated with NVIDIA Cosmos, NVIDIA Isaac GR00T, and NVIDIA Omniverse to enable five key features including system, skills, datasets, models, and USD file management within a unified workflow.",
        "Learn to how overcome humanoid era challenges by examining real-world industrial workload validation through the TM Robot \"Xplore I\" collaboration.",
        "Connect With the Experts",
        "Workshops, Training Labs & Certification",
        "Sponsors & Exhibitors",
        "Privacy Policy Your Privacy Choices Terms of Service Accessibility Corporate Policies Product Security"
      ],
      "nvidia_technology": "RTX GPU, HGX, Isaac, Omniverse, Cosmos",
      "session_id": "S82218",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Techman Robot",
          "name": "Kevin Wu",
          "title": "Engineer Manager"
        },
        {
          "company": "QCT",
          "name": "Pei-Wen Wang",
          "title": "5G/AI Solution Manager"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Accelerate Physical AI: An Integrated Solution for Rapid Development (Presented by QCT)",
      "topic": "Synthetic Data Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82218/"
    },
    {
      "description": "This hands-on lab introduces Newton, a GPU-accelerated physics engine. Then we'll integrate Newton with Isaac Lab, showing how they can work together in robot-learning workflows. You'll configure tasks to run on Newton, and train and evaluate policies in Isaac Lab with Newton providing high-fidelity, high-throughput physics. You'll leave knowing how to use Newton for more realistic robotics simulation. Intermediate experience with Python, including working with packages, virtual environments, and basic scripting. Familiarity with 3D content concepts such as meshes, materials, transforms, and scene hierarchies from tools like Blender, Maya, or similar DCC applications. Understand command-line usage for running scripts and managing project files. Have familiarity with robotics or simulation workflows (for example, using simulation tools or working with physics-enabled assets).",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Explain Newton’s role and benefits for modern robotics and robot learning workflows.",
        "Design Newton-based simulations, highlighting its architecture and rich import pipeline.",
        "Optimize Newton solver, contact, and friction settings for robust robot behaviors.",
        "Implement Isaac Lab training that uses Newton as the physics engine."
      ],
      "nvidia_technology": "Isaac",
      "session_id": "DLIT81700",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Akul Santhosh",
          "title": "Solution Architect, Robotics"
        },
        {
          "company": "NVIDIA",
          "name": "Eric Heiden",
          "title": "Sr. Research Scientist"
        },
        {
          "company": "NVIDIA",
          "name": "Mohammad Mohajerani",
          "title": "Sr. Product Manager"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Accelerate Robot Learning With NVIDIA Isaac Lab and Newton",
      "topic": "Robotics Simulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81700/"
    },
    {
      "description": "Learn how to reduce the cost associated with frame extraction for computer vision post-processing using free and publicly-accessible American Sign Language data. Use a novel, open-source framework and contribute to the world's largest dataset. Have AWS Command Line Interface Configured (for S3 Operations)",
      "format": "In-Person",
      "industry": "Media & Entertainment",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "Access and use a novel and open-source pipeline for extracting and processing key frames and landmarks.",
        "Build using Super Annotate's integrated environment to visualize, manage, and label data.",
        "Contribute to the world's largest dataset for American Sign language."
      ],
      "nvidia_technology": "",
      "session_id": "DLIT82150",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Khanh Nguyen",
          "title": "Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Suseella Panguluri",
          "title": "Sr. Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Radha Sri-Tharan",
          "title": "Data Engineer, HitL"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Accelerate Video Frame Extraction and Labeling",
      "topic": "Data Analytics / Processing",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit82150/"
    },
    {
      "description": "This session will be an extension to the session led by NVIDIA senior solutions architects Kristof De Brouwer and Nicola Bianchi titled \"Mastering AI Infrastructure At Scale — a DevOps Playbook.\" You can ask questions about your scaled AI deployments of our technical experts, explore deployment strategies in depth, and engage in discussions on optimizing NVIDIA AI infrastructure for your specific environments. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Best practices for initializing HGX/DGX clusters and configuring storage, networking, and compute fabrics",
        "Techniques to reduce mean time between failures and accelerate time-to-first-token",
        "Deep dives into GB200/GB300 NVLink architecture, system management, and tooling advancements"
      ],
      "nvidia_technology": "BlueField DPU, Cloud / Data Center GPU, Grace CPU, HGX, Infiniband Networking, Ethernet Networking, Hopper, Interconnect Networking, Blackwell",
      "session_id": "CWES81471",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Nicola Bianchi",
          "title": "Sr. Solution Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Kristof De Brouwer",
          "title": "Sr. Solution Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Pramod Kumbhar",
          "title": "Sr. Solutions Architect - HPC & AI"
        },
        {
          "company": "NVIDIA",
          "name": "Giovanni Mascari",
          "title": "Sr. Solutions Architect - HPC & AI"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Accelerate Your AI Infrastructure at Scale",
      "topic": "Software-Defined Data Center",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81471/"
    },
    {
      "description": "As AI models scale, data center networking must provide ultra-low latency, high throughput, and programmable data paths so training and inference workloads remain efficient. With the NVIDIA DOCA software platform, developers can move critical data-path functions onto NVIDIA BlueField data processing units (DPUs) using DOCA libraries and microservices to offload, accelerate, and isolate cloud infrastructure while enhancing observability. We’ll cover how DOCA enables centralized orchestration of BlueField‑accelerated services, dynamic traffic steering, and fine‑grained network and security controls that work together to deliver secure, high‑performance fabrics with predictable performance and efficient resource utilization for multi‑tenant AI environments at scale.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Understand how industry-leading partners create DOCA-accelerated applications with wire speed performance data paths.",
        "Learn how to use DOCA’s programmable networking APIs and data-path acceleration capabilities to customize and offload packet processing for large-scale AI clusters.​",
        "Learn practical strategies for tuning and operating secure multi-tenant, high-bandwidth AI networking environments using DOCA-enabled DPUs and SuperNICs."
      ],
      "nvidia_technology": "BlueField DPU, DOCA",
      "session_id": "S81857",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Alexander Petrovskiy",
          "title": "Sr. Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Majd Dibbiny",
          "title": "Sr. Director DOCA engineering"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Accelerated AI Networking with DOCA",
      "topic": "AI Networking, 5G/6G",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81857/"
    },
    {
      "description": "Embracing next-generation technology like AI-infused scientific workflows involves blending familiar building blocks with innovation, timeliness, and purpose. Whether it's models pre-trained on petabyte-scale data for common forecasting tasks, or real-time digital twins, developers need building blocks to enable their vision. In this talk, we walk through developer software used to enable AI and HPC innovations, including recent enhancements that maximize the use of NVIDIA hardware and improve the developer experience.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Discover the latest HPC SDK updates for increased flexibility and application performance examples.",
        "Learn how HPC partners with AI to accelerate traditional simulations in computer-aided engineering, electronic design automation, and earth science."
      ],
      "nvidia_technology": "CUDA, OpenACC, cuBLAS, CUDA-X",
      "session_id": "S81792",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Heidi Poxon",
          "title": "Director of Product, HPC Software"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Accelerated Building Blocks for Next-Generation AI+HPC Workloads",
      "topic": "Performance Optimization",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81792/"
    },
    {
      "description": "Join this interactive session with NVIDIA experts to learn best practices and latest techniques for deploying AI applications and accelerated computing to banking use cases. We'll answer your questions, share implementation insights, and discuss architectural patterns for deploying AI reliably across on-premises, cloud, and hybrid environments. Gain practical guidance on navigating regulatory, data, and governance challenges, and learn how to design robust, efficient, and scalable solutions that unlock new products, services, and operational efficiencies for banks and financial institutions. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "Financial Services",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Explore how AI and high performance computing (HPC) are currently reshaping financial institutions",
        "Navigate deployment challenges toward building robust, efficient solutions that meet the rigorous demands of the industry",
        "Discover what state-of-the-art AI applications in banking can enable for your customers",
        "Gain insight into real-world banking use cases across risk, fraud, and customer experience",
        "Understand how to scale AI securely and responsibly across your organization",
        "Connect With the Experts",
        "Workshops, Training Labs & Certification",
        "Sponsors & Exhibitors",
        "Privacy Policy Your Privacy Choices Terms of Service Accessibility Corporate Policies Product Security Contact",
        "Copyright © 2026 NVIDIA Corporation",
        "NVIDIA uses cookies to impr"
      ],
      "nvidia_technology": "Grace CPU, CUDA, RAPIDS, Hopper, LaunchPad, CUDA-X, cuDF, cuOPT, Triton, Blackwell",
      "session_id": "CWES81506",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Benika Hall",
          "title": "Sr. Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "James Sutton",
          "title": "Sr. Solutions Architect, Gen AI"
        },
        {
          "company": "NVIDIA",
          "name": "Benjamin Wu",
          "title": "Sr. Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Ellie Arbab",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Jessica Clark",
          "title": "Sr. Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Bogdan Vioreanu",
          "title": "Sr. Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Roman Yokunda Enzmann",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "David Williams",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Alex Stephens",
          "title": "Sr. Solutions Architect, Financial Services"
        },
        {
          "company": "NVIDIA",
          "name": "Yongming Shi",
          "title": "Solution Architect Manager"
        }
      ],
      "technical_level": "General Interest",
      "title": "Accelerated Computing in Financial Services for Banking",
      "topic": "Retrieval-Augmented Generation (RAG)",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81506/"
    },
    {
      "description": "Bring your own laptop to experience advanced networking in action with this hands-on workshop. You'll learn how to design a resilient Ethernet management network with BGP, VXLAN, EVPN, and MLAG for modern AI clusters. Dive into monitoring and validating InfiniBand communications using performance tests and subnet manager tools. And finally, learn the most unique networking technology for high-speed GPU-to-GPU workloads as you configure NVIDIA NVLink™ with NMX-C. Don’t miss this opportunity to gain practical expertise in building and optimizing scalable, high-performance AI infrastructure. Configure a multi-tenant Ethernet network using Cumulus Linux NOS. Validate, test and operate an E-W InfiniBand fabric for multi-node communication. Setup, manage and troubleshoot high performance NVLink fabric, using NMX control services for GB200 and GB300 deployments. Certificate: Upon successful completion of the assessment, you’ll receive an NVIDIA certificate to recognize your subject matter competency and support your professional career growth. Basic Linux administration skills, including directory navigation, file management, and text editing with nano or vim are required. Intermediate TCP/IP networking experience is expected, with familiarity in L2/L3 protocols, IP networking concepts, and proficiency using tools such as iperf, wireshark, and the command line.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [],
      "nvidia_technology": "",
      "session_id": "DLIW82209",
      "session_type": "Full-Day Workshop",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Nawar Nawar",
          "title": ""
        }
      ],
      "technical_level": "",
      "title": "Accelerated Networking for AI Infrastructure",
      "topic": "AI Networking, 5G/6G",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dliw82209/"
    },
    {
      "description": "The explosion of biological data generation and innovations in AI have opened new frontiers in scientific discovery and biomedicine. With the ability to process vast amounts of biological data, accelerated analysis and foundation model approaches are transforming digital biology. This workshop provides hands-on experience for how accelerated libraries and deep learning models can be leveraged to process and analyze complex biological data. Gain an understanding of the end-to-end process, from how NVIDIA libraries support data generation, pre-processing, and analysis of single-cell data, to how foundation models are being used and evaluated in the life sciences. Get an overview of relevant solutions in digital biology, such as Parabricks, RAPIDS-singlecell (developed by scverse), NVIDIA CUDA-X Data Science Libraries, and Clara Open Models. Basic familiarity with Python, Jupyter, and Bioinformatics workflows.",
      "format": "In-Person",
      "industry": "Healthcare & Life Sciences",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "Understand how NVIDIA is powering genomics innovation through data generation, accelerated data science, and foundation model development.",
        "Gain practical experience in preparing and analyzing omics data, including single cell, to drive downstream workflows.",
        "Learn to leverage NVIDIA's accelerated software platforms, including RAPIDS, Parabricks, and Clara Open Models to apply and evaluate scalable, high-performance models and uncover biological insights.",
        "Learn about how NVIDIA accelerated computing tools are being used in agentic workflows."
      ],
      "nvidia_technology": "Clara Parabricks, CUDA-X",
      "session_id": "DLIT81587",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Gary Burnett",
          "title": "Technical Marketing Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Severin Dicks",
          "title": ""
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Accelerated Omics and Single-cell Analysis Using NVIDIA GPUs",
      "topic": "Bioinformatics / Genomics",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81587/"
    },
    {
      "description": "Accelerated computing and AI supercomputing are redefining industrial engineering, spanning computational engineering (CAE) to semiconductor design and manufacturing. Together, these fields drive how the world conceives, simulates, and builds everything from advanced machinery to next-generation chips increasingly in the digital world before physical production begins. In this address, NVIDIA’s Tim Costa, general manager for Industrial and Computational Engineering, will show how CUDA-accelerated platforms, AI physics, DGX and cloud-scale AI infrastructure, and NVIDIA Omniverse are reshaping workflows across computer-aided engineering and semiconductor design and manufacturing. He will demonstrate how customers use GPU-accelerated solvers, AI physics models, and agentic AI to cut turnaround, broaden design exploration, and boost yield and reliability, revealing a unified accelerated-computing blueprint spanning classical, AI, and emerging quantum acceleration.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Accelerated computing and AI supercomputing are turning industrial engineering workflows into end‑to‑end digital, simulation‑first pipelines, where products are designed, validated, and optimized before they reach the factory.​​",
        "NVIDIA’s CUDA platforms, DGX and cloud-scale infrastructure, and Omniverse enable GPU-accelerated solvers, physics-ML models, and agentic AI to dramatically cut turnaround time, expand design-space exploration, and improve yield and reliability across industrial engineering.​​"
      ],
      "nvidia_technology": "CUDA, Omniverse, Modulus, cuBLAS, CUDA-X, cuLitho, NeMo, Blackwell, Blueprint",
      "session_id": "S82017",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Timothy Costa",
          "title": "General Manager (GM) for Industrial and Computational Engineering"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Accelerating Industrial Engineering: From Product Design to Manufacturing in the AI Supercomputing Era",
      "topic": "CAE",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82017/"
    },
    {
      "description": "Discover how organizations are accelerating their AI journeys with WWT’s AI Studio and Foundry—leveraging NVIDIA NIM, NVIDIA NeMo, and NVIDIA AI Blueprints to move quickly from ideas to validated prototypes and scalable, production-ready solutions. We will share real-world examples of how this approach is delivering meaningful business outcomes in industries such as global service providers and energy production.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "How to rapidly identify and validate high-impact AI use cases with AI Studio",
        "Techniques for accelerating AI development by connecting prototypes and NVIDIA NIM, NVIDIA NeMo, and NVIDIA AI Blueprints technology to enterprise-scale engineering via AI Foundry",
        "How AI-native engineering, such as the use of agentic AI coding tools, can further escalate the speed of both planning and deployment",
        "Real-world examples of how organizations are applying AI to transform operational performance and customer outcomes",
        "Actionable guidance for launching an accelerated AI adoption pathway with measurable business impact"
      ],
      "nvidia_technology": "Blueprint",
      "session_id": "S81998",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "World Wide Technology",
          "name": "Nathan McKie, Jr.",
          "title": "Sr. Executive AI Advisor"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Accelerating Innovation With AI Studio and Foundry: Cases Driving Customer Success (Presented by World Wide Technology)",
      "topic": "Code / Software Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81998/"
    },
    {
      "description": "Large-scale LLMs require enormous computational resources, making efficiency crucial for training. Learn how cutting-edge reduced-precision formats such as FP8, MXFP4 and NVFP4 can dramatically accelerate pre-training, post-training, and inference while maintaining accuracy close to full-precision baseline models. You’ll learn how NVIDIA's innovative NVFP4 format better captures dynamic range and reduces quantization error compared to MXFP4. We'll cover techniques including Random Hadamard Transforms, two-dimensional block quantization, stochastic rounding, and strategic placement of higher-precision layers to stabilize training. Gain practical skills with the software tools necessary for quantization-aware training and efficient inference deployment, helping your applications retain accuracy while benefiting from reduced precision.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Large-scale LLMs require tens to hundreds of petaflops for training, making computational efficiency critical.",
        "Reduced-precision formats like FP8 and NVFP4 enable faster pre-training, post-training, and inference while maintaining full-precision accuracy.",
        "Learn training techniques that enable stable low-bit training.",
        "Gain hands-on experience with quantization-aware training and deploying efficient inference pipelines that preserve model accuracy.",
        "Understand the principle of reduced precision formats and their necessity."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, CUDA, TensorRT, Hopper, CUDA-X, NeMo, NSight Systems, Blackwell, NVIDIA AI Enterprise",
      "session_id": "DLIT81567",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Leo Du",
          "title": "Sr. Solutions Architect, Generative AI"
        },
        {
          "company": "NVIDIA",
          "name": "Oleg Rybakov",
          "title": "Principal Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Sergio Perez",
          "title": "Solutions Architect"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Accelerating LLM Training and Inferencing With Reduced Precision Format",
      "topic": "Large Language Models (LLMs)",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81567/"
    },
    {
      "description": "Gaussian-basis quantum chemistry on NVIDIA GPUs is primed to change the world. Join this session to learn about new technologies and offerings from NVIDIA that help accelerate this paradigm shift.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [],
      "nvidia_technology": "CUDA, Hopper, cuBLAS",
      "session_id": "S81770",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Robert Parrish",
          "title": "Sr. Engineering Manager"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Accelerating Quantum Chemistry on GPUs—Latest Advances",
      "topic": "Accelerated Computing Libraries",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81770/"
    },
    {
      "description": "Learn how advanced rack-scale connectivity solutions are revolutionizing data center infrastructure for next-generation AI workloads. We'll explore the technical innovations enabling seamless multi-GPU scaling and the performance breakthroughs achieved when integrating Astera Labs' purpose-built connectivity solutions with NVIDIA's Blackwell and upcoming Rubin architectures.",
      "format": "In-Person",
      "industry": "Hardware / Semiconductor",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Discover how Astera Labs' PCIe and CXL connectivity solutions eliminate bottlenecks in multi-GPU NVIDIA Blackwell deployments.",
        "Understand the technical architecture behind seamless GPU-to-GPU communication across rack boundaries, including real-world performance metrics from production AI training clusters.",
        "Learn how open standards accelerate ecosystem innovation, allowing third-party connectivity solutions to unlock new performance and scalability capabilities across AI infrastructure."
      ],
      "nvidia_technology": "Blackwell",
      "session_id": "EX82087",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "Astera Labs",
          "name": "Mike Hendricks",
          "title": "Associate VP, Solutions and Ecosystem"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Accelerating Rack-Scale Connectivity for NVIDIA Blackwell and Rubin Platforms (Presented by Astera Labs)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82087/"
    },
    {
      "description": "Learn about the practicalities and challenges of developing a flow-matching sequence-to-ensemble model for intrinsically disordered proteins, and an evaluation framework for ensemble prediction. You’ll see how augmented training data, experimental biophysical constraints, and dedicated benchmarks enable robust conformational ensembles where conventional predictors fail, and how these ensembles plug into GPU-accelerated protein structure generation workflows for hard-to-target oncology, central nervous system, and immunology programs.",
      "format": "Virtual",
      "industry": "Healthcare & Life Sciences",
      "intended_audience": "Research: Non-Academic",
      "key_takeaways": [
        "Learn how to implement and train a flow-matching, sequence-to-ensemble model for disordered proteins using augmented data and experimental constraints.",
        "Hear how to use a dedicated benchmark suite to evaluate ensemble predictors and diagnose where standard single structure tools break down.",
        "See how GPU-accelerated ensemble prediction pipelines plug into protein structure generation workflows for challenging oncology, CNS, and immunology targets."
      ],
      "nvidia_technology": "DGX Platform, Infiniband Networking, Hopper, BioNeMo, cuGraph, cuML, NCCL, Triton, DGX Cloud",
      "session_id": "S82028",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Peptone, Inc.",
          "name": "Carlo Fisicaro",
          "title": "CTO"
        },
        {
          "company": "Peptone, Ltd.",
          "name": "Michele Invernizzi",
          "title": "Sr. Scientist"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Accelerating Small Molecule Drug Discovery for Hard-to-Target Proteins",
      "topic": "BioPharma",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82028/"
    },
    {
      "description": "As dense frontier and mixture-of-experts models move toward multi-trillion parameters, the gap between theoretical GPU performance and actual goodput has become a bottleneck. While the industry average is 35–45% model FLOPS utilization (MFU), CoreWeave will show how to achieve 20% higher MFU with 96% goodput across clusters of hundreds of thousands of GPUs to complete training workloads efficiently. We will examine the architectural breakthroughs that made these gains possible, and provide a deep dive on how to engineer rack-scale compute from training to leveraging serverless reinforcement learning and ultra-low latency inference for the next generation of agentic AI.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Gain technical insights into CoreWeave’s full stack optimizations that run your models reliably and efficiently from bare metal, CKS, to Slurm on Kubernetes."
      ],
      "nvidia_technology": "",
      "session_id": "S82200",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "CoreWeave",
          "name": "Corey Sanders",
          "title": "SVP of Product"
        },
        {
          "company": "CoreWeave",
          "name": "Chen Goldberg",
          "title": "Sr. VP, Engineering"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Achieve 20% Higher Model Utilization: A Deep Dive on Training and Inference Infrastructure (Presented by CoreWeave)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82200/"
    },
    {
      "description": "We'll walk you, step by step, through how to achieve peak Tensor Core performance for GEMM. We'll provide tutorial examples to help you get started with the CuTe DSL, and also demonstrate how to further optimize performance using the latest Blackwell hardware and software features.",
      "format": "In-Person",
      "industry": "Hardware / Semiconductor",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "An entire GEMM kernel can be complex for beginners to get started. We'll demonstrate how to implement GEMM on Blackwell step by step, adding only a few features at a time, along with the corresponding performance gains.",
        "This work showcases the power of the CuTe DSL in low-level programming control, including precise management of data movement, tile scheduling, and pipelining. It is designed to keep both productivity and peak performance, enabling fast iterations when implementing GPU kernels.",
        "CuTe DSL can achieve peak Tensor Core performance for GEMM on Blackwell, and the performance of the Python DSL is comparable to that of C++ implementations.",
        "We will also highlight the key considerations for GEMM problems of different sizes."
      ],
      "nvidia_technology": "CUDA",
      "session_id": "S81463",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Linfeng Zheng",
          "title": "Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Albert Di",
          "title": "Sr. Developer Technology Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Achieve Peak Tensor Core Performance for GEMM on Blackwell via CuTe DSL",
      "topic": "Programming Languages / Compilers",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81463/"
    },
    {
      "description": "Unpredictable workloads make inference a natural fit for serverless computing (a.k.a. \"functions-as-a-service\"). But serverless execution only works if new function replicas spin up quickly. Learn the techniques we've developed at Modal to speed up the boots of GPU containers and achieve truly serverless execution for workloads running on thousands of GPUs.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Running GPU inference serverlessly can cut costs while improving quality of service.",
        "Starting up quickly is the key to success in serverless architectures.",
        "Clever file-system tricks and CPU memory snapshotting achieve fast spin-up for serverless functions that don't use GPUs.",
        "NVIDIA's CUDA-checkpoint tool brings the final critical piece for GPU inference: snapshot and restore of GPU memory.",
        "Modal has deployed CUDA-checkpoint to support production inference workloads and can report major savings on latency and cost."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, CUDA, Hopper, Blackwell",
      "session_id": "S81424",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Modal",
          "name": "Charles Frye",
          "title": "Member of Technical Staff"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Achieve Truly Serverless GPUs With libfuse, CRIU, and CUDA-Checkpoint",
      "topic": "Performance Optimization",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81424/"
    },
    {
      "description": "AI is rapidly transforming the cybersecurity landscape, with security operations centers (SOCs) facing growing AI-powered threats that overwhelm traditional tools and workflows. AI-driven security is therefore no longer optional—it is a necessity. Organizations are now re-architecting their SOCs around specialized AI agents that can reason over massive telemetry, take action in real time, and continuously learn from adversary tactics. This session explores how organizations can move from experimentation to production by embedding specialized, agentic AI directly into SOCs to boost detection and response, elevate analyst expertise, and orchestrate defenses across dynamic environments. Learn how the CrowdStrike Charlotte AI platform integrates NVIDIA’s AI stack to power always-on security agents, building a resilient foundation for AI-driven cyber defense at enterprise scale.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Security Operations",
      "key_takeaways": [
        "Explore how to streamline SOC workflows by embedding specialized AI agents capable of real-time reasoning, action, and continuous learning.",
        "Learn strategies for bringing secure, resilient AI agents from prototype to production in dynamic environments.",
        "Gain practical insights to securing AI—from large language models to complex, multi-agent systems.",
        "Connect With the Experts",
        "Workshops, Training Labs & Certification",
        "Sponsors & Exhibitors",
        "Privacy Policy Your Privacy Choices Terms of Service Accessibility Corporate Policies Product Security Contact",
        "Copyright © 2026 NVIDIA Corporation",
        "NVIDIA uses cookies to improve your experience on our web site. We and our third-party partners also use cookies and other tools to collect and record in"
      ],
      "nvidia_technology": "CUDA-X, NeMo, NVIDIA NIM, Blackwell, NVIDIA AI Enterprise",
      "session_id": "S81765",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Bartley Richardson",
          "title": "Sr. Director of Engineering"
        },
        {
          "company": "CrowdStrike",
          "name": "Mike Petronaci",
          "title": "CTO of Proactive Security"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Activate Always-On AI Security, From Cloud to Edge",
      "topic": "Security for AI",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81765/"
    },
    {
      "description": "In today's AI landscape, even powerful Large Language Models (LLMs) face limitations when confronted with specialized business knowledge, technical domains, or cultural contexts absent from their training data. While retrieval-augmented generation can mitigate some gaps, true domain mastery requires deeper model adaptation. This comprehensive workshop equips developers with hands-on skills to transform open-source LLMs into domain-specialized AI assets. Through five interconnected modules, you'll master the complete lifecycle of model customization. By workshop completion, you'll possess the complete technical skillset to develop, deploy, and operate sovereign AI systems tailored to your specific requirements - from data preparation to production scaling. Bring your laptop; we'll provide the GPUs. Certificate: Upon successful completion of the assessment, you’ll receive an NVIDIA certificate to recognize your subject matter competency and support your professional career growth. Previous work with LLM-based applications and understanding of prompt engineering principles. Experience with data processing pipelines and text preprocessing techniques. Understanding of fine-tuning, training/validation splits, and basic ML metrics. Basic knowledge of GPU acceleration for ML workloads (CUDA experience helpful but not required). Familiarity with containerization and basic Kubernetes concepts (Optional but helpful).",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Build custom evaluation benchmarks using NeMo Evaluator to identify model limitations and track engineering progress. Learn metrics that matter for your specific use case.",
        "Implement state-of-the-art data cleaning pipelines with NeMo Curator to assemble high-quality domain-specific datasets that address your business or cultural requirements.",
        "Master multiple adaptation techniques including in-context learning, Parameter-Efficient Fine-Tuning (PEFT), Continued Pre-Training (CPT), Supervised Fine-Tuning (SFT), and Reinforcement Learning from Human Feedback (RLHF).",
        "Apply distillation, quantization, and pruning techniques with NeMo Model Optimizer and TensorRT-LLM to dramatically reduce inference costs without sacrificing performance.",
        "Learn to deploy, monitor, and scale your custom models within Kubernetes environments using NVIDIA Inference Microservices (NIMs)."
      ],
      "nvidia_technology": "NeMo",
      "session_id": "DLIW82267",
      "session_type": "Full-Day Workshop",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "John Jahanipour",
          "title": "Senior Solutions Architect"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Adding New Knowledge to LLMs",
      "topic": "Large Language Models (LLMs)",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dliw82267/"
    },
    {
      "description": "Learn to implement functional safety for robots using an \"outside-in\" approach. You will gain deep insight into building collaborative, AI-powered factory environments where vision systems and robotics converge to deliver operational intelligence and safety. We will connect industrial robots to factory-wide sensor infrastructure using NVIDIA Metropolis to build intelligent systems dedicated to robot safety. Through practical exercises, you will create a video analytics AI agent that extends a robot's real-time perception. This agent will enable critical safety features, such as automatic speed adjustments (slowing down or speeding up) and risk mitigation, essential for safe co-bot environments. To enable the outside-in approach for functional safety, this lab spotlights NVIDIA Halos for Outside-In Safety Agents powered by IGX Thor. Intermediate experience with Python, including packages, virtual environments, and scripting Familiarity with computer vision concepts, object detection, and AI inference pipelines Experience with robotics workflows and sensor integration Knowledge of data streaming, real-time analytics, and message-passing architectures",
      "format": "In-Person",
      "industry": "Manufacturing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Build an outside-in safety application for robots using NVIDIA Metropolis vision AI-based workflows to integrate sensors, AI inference, and real-time dashboards to optimize robot perception and responsiveness.",
        "Implement a safety AI agent with NVIDIA Halos that enables factory sensors and robot sensors to safeguard workers, robots, and warehouse assets.",
        "Deploy and scale robot safety applications across factory environments for comprehensive proactive responses for factory robots leveraging existing camera infrastructure."
      ],
      "nvidia_technology": "Metropolis, IGX",
      "session_id": "DLIT81776",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Edward Chan",
          "title": "Senior Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Sammy Ochoa",
          "title": "Technical Marketing Engineer"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Adding Safety and Perception to Industrial Robots",
      "topic": "Robot Perception",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81776/"
    },
    {
      "description": "Today’s data center industry is evolving faster than ever, with record demand for gigawatt (GW)-scale designs now paired with the accelerated delivery intervals needed to meet today’s ever-changing technology demands. This race to build large-scale data center infrastructure has fundamentally shifted the industry’s approach, from traditional design and construction methods to ones that are highly optimized for accelerated delivery at GW scale. In this session, explore the drivers and overall impacts of designing and building large-scale GW data center infrastructure that serves as the building blocks for AI factories of the future. Better understand the new challenges faced by the industry and the general path forward for designing and constructing large-scale data center architectures at accelerated delivery intervals to accommodate the technology innovations of tomorrow.",
      "format": "In-Person",
      "industry": "Cloud Services",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Understand global data center demand growth trends and key North American expansion markets poised for new capacity growth.",
        "Compare overall design architectures of traditional AI data centers with the new GW-scale AI factory campuses.",
        "Examine the key modifications of primary design elements of the AI factory design enabling faster speed to market.",
        "Review how modular and prefabricated design elements are helping drive efficiencies to accelerate overall delivery for next-generation data center deployments."
      ],
      "nvidia_technology": "",
      "session_id": "EX82085",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "EdgeConneX",
          "name": "Aron Smith",
          "title": "VP Product Management"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Advance AI Data Center Infrastructure Design and Construction to Accommodate GW Scale (Presented by EdgeConnex)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82085/"
    },
    {
      "description": "Take the next step in ML performance engineering with a deep dive into advanced orchestration on the NVIDIA Run:AI platform. This lab focuses on maximizing infrastructure efficiency through intelligent scheduling and automation. Explore capabilities such as GPU fractioning, GPU memory swap, and topology-aware multi-node placement—and learn how to apply them to complex distributed training and inference workloads. With hands-on guidance, you’ll build flexible, high-performance pipelines that dramatically improve speed, utilization, and throughput at scale. Completion of “How to Accelerate AI Workflows with NVIDIA Run:AI” or equivalent hands-on experience. Working knowledge of Kubernetes, GPU resources, and distributed training architectures. Familiarity with AI/ML pipeline design and containerized workloads.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Confidently apply GPU fractioning to securely run multiple workloads on a single GPU.",
        "Leverage GPU memory swap to reduce latency, improve utilization, and prevent GPU contention.",
        "Optimize distributed training and inference through topology-aware multi-node placement."
      ],
      "nvidia_technology": "Mission Control",
      "session_id": "DLIT82183",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Juan Delgado",
          "title": "Senior Technical Instructor"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Advanced Workload Orchestration With NVIDIA Run:AI",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit82183/"
    },
    {
      "description": "NVIDIA’s developer training programs support faculty teaching students and researchers through downloadable teaching kits, a cloud-based GPU-accelerated platform, and technical certification exams. Instructional content spans foundational topics—deep learning, accelerated computing, and data science—and advanced areas including agentic AI, natural language processing, and generative AI. Online courses and instructor-led workshops with project-based assessments help faculty align coursework with certifications and support students in earning recognized qualifications. We highlight ECPI integrating GPU-accelerated AI into student projects, and Lebanese American University using hands-on, faculty-led workshops to advance applied learning and prepare students for real-world AI applications and industry pathways.",
      "format": "In-Person",
      "industry": "Academia / Higher Education",
      "intended_audience": "Educator",
      "key_takeaways": [
        "Explore approaches for using GPU-accelerated cloud platforms to teach students and researchers in AI, deep learning, accelerated computing, and data science.",
        "Examine practical strategies for integrating NVIDIA developer training into university curricula to modernize courses and enhance research capabilities.",
        "See examples of successful academic adoption demonstrating measurable impact on student and researcher skill development across multiple AI and HPC domains.",
        "Connect With the Experts",
        "Workshops, Training Labs & Certification",
        "Sponsors & Exhibitors",
        "Privacy Policy Your Privacy Choices Terms of Service Accessibility Corporate Policies Product Security Contact",
        "Copyright © 2026 NVIDIA Corporation",
        "NVIDIA uses cookies to improve your"
      ],
      "nvidia_technology": "Cloud / Data Center GPU, Jetson, CUDA, OpenACC, TensorRT, Isaac, RAPIDS, Modulus, cuQuantum, cuBLAS, cuDDN, cuDF, cuFFT, cuGraph, cuML, JetPack, NeMo, NSight Systems, TAO Toolkit, Triton",
      "session_id": "S82156",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "ECPI",
          "name": "Paul Nussbaum",
          "title": "Professor"
        },
        {
          "company": "Lebanese American University",
          "name": "Manal Jalloul",
          "title": "Lecturer"
        },
        {
          "company": "NVIDIA",
          "name": "Joe Bungo",
          "title": "DLI Program Manager"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Advancing AI and HPC Competency in Higher Education Through Faculty Instructional Enablement",
      "topic": "AI Platforms",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82156/"
    },
    {
      "description": "This lab explores key principles for advancing multi-modal systems, focusing on multi-modal information typology, alignment, multi-step reasoning, and cross-modal flow. We analyze redundant, unique, and synergistic information, and examine multi-modal alignment through coarse-grained versus fine-grained behavior. The lab then investigates multi-step multi-modality reasoning with external knowledge, where agents integrate images, text, tables, and diagrams to solve complex tasks. Finally, we study multi-modal information flow through cross-modal translation, cross-modal editing, and cross-modal querying, supported by NIM examples for text-to-image generation, semantic image editing, and visual question-answering. Basic understanding of machine learning and deep learning concepts, including embeddings and neural network architectures. Familiarity with multimodal AI concepts, such as combining text, image, and other data modalities. Experience with Python programming and standard ML libraries (e.g., PyTorch, TensorFlow). Basic knowledge of data visualization techniques (e.g., PCA, t-SNE) for interpreting embeddings.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "Understand the different types of multi-modal information (redundant, unique, and synergistic) and how they influence model representations.",
        "Learn the distinction between coarse-grained and fine-grained multi-modal alignment and how it affects model understanding and generation.",
        "Gain hands-on experience with multi-step multi-modal reasoning using external knowledge sources, integrating text, images, and structured data."
      ],
      "nvidia_technology": "",
      "session_id": "DLIT81592",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Mireille Fares",
          "title": "Sr. AI Solution Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Andrea Pilzer",
          "title": "HER Solutions Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Advancing Multimodal Systems: Alignment, Information Typology, Reasoning, and Cross-Modal Flow",
      "topic": "Image Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81592/"
    },
    {
      "description": "The California Department of Transportation (Caltrans) is responsible for 50,000 miles of roadways across California. Years in the making, Caltrans has worked with companies like Accenture, Deloitte, and Microsoft, and independent software vendors such as Centific, to deliver a custom solution to solve two major concerns for California citizens: roadway efficiency, and the safety of vulnerable road users. We'll dive deep into the specific capabilities of NVIDIA solutions including NVIDIA Blueprint for video search and summarization, generative AI vision language models, Cosmos world foundations, and analytics ingestion to solve these statewide challenges.",
      "format": "In-Person",
      "industry": "Smart Cities / Spaces",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Gain insights into the cutting-edge methodologies that are unlocking new horizons within transportation.",
        "Hear Caltrans experiences with testing and integrating generative AI solutions.",
        "Gain perspective on the cost-benefit economics for the use cases deployed."
      ],
      "nvidia_technology": "DeepStream, Metropolis, TAO Toolkit",
      "session_id": "S81864",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Department of Transportation, California State",
          "name": "Marcie Kahbody",
          "title": "CIO"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Advancing Road Safety for Roadway Users With Caltrans​",
      "topic": "Video Analytics",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81864/"
    },
    {
      "description": "In this session, we'll dive into designing an agentic system to support financial analysis and advisory. Inspired by NVIDIA AI-Q Research Assistant blueprint, and leveraging NVIDIA's multi-modal retrieval-augmented generation (RAG) pipeline, our system not only provides a tool that can take financial analysis—one of the core tasks in finance—to the next level by supporting internal experts, but also makes an important step toward building a sovereign agentic architecture.",
      "format": "Virtual",
      "industry": "Financial Services",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "Hear how to use AI factory and NVIDIA NIMs to scale AI in finance, moving beyond exploration.",
        "Learn how agentic systems can help experts in complex analytical workflows in finance.",
        "See pilot results that reveal how to replace proprietary API calls via on‑premises AI factory deployment.",
        "Harness practical experience on how to successfully and safely migrate AI workloads to empower your organization’s next move.",
        "Understand how to leverage strategic alignment of sovereign AI building blocks to gain ownership of generative AI components."
      ],
      "nvidia_technology": "TensorRT, NeMo, Triton, NVIDIA NIM, NVIDIA AI Enterprise, NVIDIA Run:ai",
      "session_id": "S81718",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "SEB",
          "name": "Anastasia Varava",
          "title": "Head of Research, SEBx"
        },
        {
          "company": "SEB",
          "name": "Nicolas Moch",
          "title": "Head of SEBx"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Agentic AI for the Next Generation of Financial Advisors",
      "topic": "Retrieval-Augmented Generation (RAG)",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81718/"
    },
    {
      "description": "Open models have evolved from experimental alternatives to production-grade building blocks for developers. But choosing and customizing them requires a clear framework. In this session, we'll walk through a practical decision guide for deploying open models at scale—covering everything from picking the right sized models based on the agentic tasks to software for customizing, evaluating, and deploying the models. Whether you're building agentic AI, internal copilots, or cost-sensitive services, this talk will give you the framework and tools to ship with confidence.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Strategic Selection: How to navigate the open model landscape and select the right model size and architecture for specific agentic tasks, versus general-purpose needs.",
        "The Full Life Cycle: A step-by-step framework for the entire pipeline—customization (fine-tuning), rigorous evaluation, and efficient deployment at scale.",
        "Cost and Performance Balance: Strategies for building cost-sensitive services without sacrificing production-grade quality.",
        "Tooling Toolkit: Which software tools and best practices are essential for shipping internal copilots and agentic AI with confidence."
      ],
      "nvidia_technology": "NeMo, NVIDIA NIM",
      "session_id": "S81702",
      "session_type": "Tutorial",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Chris Alexiuk",
          "title": "Deep Learning Developer Advocate"
        },
        {
          "company": "CodeRabbit",
          "name": "David Loker",
          "title": "Director of AI"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Agentic AI with Open Source Models: Architecture, Fine-Tuning, and Deployment Tips",
      "topic": "Code / Software Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81702/"
    },
    {
      "description": "Discover how NVIDIA developed the Nemotron family of edge-ready models, setting new benchmarks for accuracy and performance. This session explores standard pre-training to advanced pruning and distillation for edge-scale efficiency. We will also detail the alignment techniques used to create state-of-the-art multimodal reasoning models. Finally, learn best practices for domain adaptation and the deployment of customized Nemotron models for gaming and AI applications on GeForce RTX platforms. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "Gaming",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "How can edge-ready models be developed?",
        "What are key techniques for making Nemotron state-of-the-art agentic AI models game-ready?",
        "What are key techniques for making Nemotron state-of-the-art agentic AI models more efficient for deployment on the gaming and PC GeForce RTX platforms?",
        "How can you evaluate the quality and efficiency boost provided by these techniques for the Nemotron edge-ready models?"
      ],
      "nvidia_technology": "NeMo, Blueprint",
      "session_id": "CWES81997",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Vinay Raman",
          "title": "Sr. Deep Learning Scientist"
        },
        {
          "company": "NVIDIA",
          "name": "Negar Habibi",
          "title": "Sr. Manager of Technical Project Management"
        },
        {
          "company": "NVIDIA",
          "name": "Aditya Uday Malte",
          "title": "Deep Learning Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Bilal Kartal",
          "title": "Sr. Deep Learning Scientist"
        },
        {
          "company": "NVIDIA",
          "name": "Ameya Sunil Mahabaleshwarkar",
          "title": "Sr. Deep Learning Scientist"
        },
        {
          "company": "NVIDIA",
          "name": "Hayley Ross",
          "title": "Sr. Deep Learning Scientist"
        },
        {
          "company": "NVIDIA",
          "name": "Zuncheng Qian",
          "title": "Senior DevTech Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Jani Joki",
          "title": "NVIDIA Benchmarking and Technical Marketing Lead"
        },
        {
          "company": "NVIDIA",
          "name": "Yoshihiko Suhara",
          "title": "Applied Research Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Oluwatobi Olabiyi",
          "title": "Director of Engineering"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Agentic Edge AI: Nemotron Training & Alignment Best Practices",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81997/"
    },
    {
      "description": "In this presentation, we'll introduce the Surgical AI Copilot, an agentic assistant developed by NVIDIA and Orsi Academy to support robotic surgery in the operating room. The system combines MCP-based agentic control with NVIDIA Holoscan, enabling low-latency video processing, streaming, and real-time interaction with surgical workflows. This tight integration operated on a IGX optimized for healthcare application allows the Copilot to operate alongside robotic systems with the responsiveness and determinism required in safety-critical environments, while remaining invisible when not needed. Through the lens of robotic surgery, this session highlights how agentic AI and fluent human–machine interaction can transform the operating room into a more intuitive, adaptive environment. One where AI augments surgical expertise without introducing friction, complexity, or distraction.",
      "format": "Virtual",
      "industry": "Healthcare & Life Sciences",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Real-time, low-latency audio operators accelerated by NVIDIA Holoscan and IGX",
        "Model Context Protocol interfaces for Holoscan operators for dynamic application runtime control using agentic tool-calling",
        "Holoscan workflow isolation to ensure 100% functionality of real-time critical pipeline"
      ],
      "nvidia_technology": "IGX",
      "session_id": "S81941",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Orsi",
          "name": "Pieter De Backer",
          "title": "Head of Innovation"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Agentic Systems for End-to-End Surgical Workflows",
      "topic": "Speech Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81941/"
    },
    {
      "description": "Discover the evolution of AI-assisted development for accelerated computing as we move beyond general-purpose coding assistants to specialized agents designed for high-performance CUDA optimization. While frontier models have improved at syntax correctness, they often lack the domain-specific context required to utilize the latest hardware features and achieve \"speed of light\" performance. This session details NVIDIA’s \"CUDA Intelligence\" ecosystem—a transition from simple retrieval-augmented generation (RAG) pipelines to advanced agentic workflows using the model context protocol (MCP) and agentic orchestration. We will demonstrate how we are democratizing state-of-the-art code transformation by integrating cloud profiling, Nsight tools, and expert-level optimization skills into a constellation of AI services that can run locally or in the cloud.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Understand why general-purpose LLMs struggle with high-performance CUDA code, and how NVIDIA bridges this gap by injecting proprietary knowledge of new technologies (e.g., CUDA Tile) and libraries.",
        "Learn how the architecture is evolving from naive RAG to sophisticated agent orchestration using MCP services and reasoning models."
      ],
      "nvidia_technology": "CUDA, cuBLAS, CUDA-X, cuDDN, CV-CUDA, NSight Comute, NSight Systems, CUDA Quantum, NVIDIA NIM, Blackwell, NVIDIA AI Enterprise, CUDA-Q, DGX Cloud, DGX Spark",
      "session_id": "S81831",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Matthew Frazier",
          "name": "Matt Frazier",
          "title": "Director Software Engineering, AI Technologies for Developer Tools"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "AI Coding for the GPU: Build a Coding Agent to Help GPU Developers Write Speed-of-Light Code",
      "topic": "Code / Software Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81831/"
    },
    {
      "description": "Scientists use computer simulations to understand how different parts of the Earth’s system interact, or to explore how they respond to external forcing. These simulations have been carried out using numerical models (global climate models, or GCMs). Recently, modern machine-learning-based emulators have been used to reproduce the same dynamics using data. We will present the first generation of AI digital twin of the climate system, using data from state-of-the-art GCMs. These AI emulators can simulate the climate system 100x faster than typical GCMs with one GPU. We will discuss how we build, evaluate, and leverage these emulators to make discoveries about the climate system. For example, it offers strategies to improve simulations of extreme weather such as hurricanes, and forcing them with past ocean states reveals how and why hurricane counts have varied over the past 1,000 years.",
      "format": "In-Person",
      "industry": "Academia / Higher Education",
      "intended_audience": "Research: Academic",
      "key_takeaways": [
        "AI emulators allow us to simulate trajectories of the climate system 100x than traditional numerical simulations.",
        "AI emulators allow us to query for extreme weather events to simulate trajectories of worst-case scenarios.",
        "AI digital twins of the climate system offer insights into past, present, and future weather and climate events.",
        "AI digital twins of the climate system allows us to ask \"what if\" questions to better understand the dynamics."
      ],
      "nvidia_technology": "Grace CPU, CUDA, Hopper, NeMo",
      "session_id": "S81763",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "New York University",
          "name": "Laure Zanna",
          "title": "Professor in Mathematics and Atmosphere/Ocean Science"
        },
        {
          "company": "Boston University",
          "name": "Elizabeth Barnes",
          "title": "Dalton Family Chair in Environmental Data Science and Sustainability"
        }
      ],
      "technical_level": "General Interest",
      "title": "AI Digital Twin of the Climate System",
      "topic": "Climate / Weather / Ocean Modeling",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81763/"
    },
    {
      "description": "As manufacturing shifts from mass production to high-mix, low-volume demands, flexibility becomes key. Traditional automation faces a critical bottleneck: rigidity. We'll explore the paradigm shift toward the \"AI factory,\" a facility where intelligence is manufactured alongside products. ASUS will present a full-stack architecture with our solution partners that integrates agentic AI—powered by generative AI and vision language models—with digital twins built on NVIDIA Omniverse libraries and OpenUSD. Discover how to build a high-performance, closed-loop system where AI agents perceive changes, reason through complex tasks, and validate strategies within a high-fidelity virtual environment using NVIDIA Isaac Sim before physical execution. By leveraging NVIDIA Omniverse for digital twins, manufacturers can simulate edge cases and optimize robotic workflows in a risk-free virtual space.",
      "format": "In-Person",
      "industry": "Manufacturing",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "How agentic AI and NVIDIA Omniverse enable agile, closed-loop automation through digital twins",
        "The role of ASUS AI servers and edge devices as the backbone of an AI factory infrastructure",
        "How NVIDIA AI Enterprise accelerates secure, scalable deployment of generative AI in manufacturing"
      ],
      "nvidia_technology": "Cloud / Data Center GPU, HGX, Omniverse, OVX, NVLink / NVSwitch, NVIDIA NIM, Blackwell, NVIDIA AI Enterprise",
      "session_id": "S82038",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "ASUS",
          "name": "Joseph Lu",
          "title": "Director of Storage Application Solutions"
        },
        {
          "company": "Spingence Technology",
          "name": "Jesse Chen",
          "title": "CEO"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "AI Factory in Action: Achieving Agile Automation With Gen AI Agents and Digital Twins (Presented by ASUS)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82038/"
    },
    {
      "description": "Learn from the NVIDIA financial services Solution Architecture team how artificial intelligence is being applied in the payments industry. We'll discuss techniques like tabular transformers, graph neural networks, and NVIDIA NIMs for large language models, and how they apply to critical payments workflows like fraud detection, anti-money laundering, and customer experience. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "Financial Services",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "Time-series modeling with transformers",
        "Network effect understanding with graph neural networks",
        "Large language models for internal and external use cases",
        "Designing and deploying scalable AI factory reference architectures"
      ],
      "nvidia_technology": "DGX Platform, HGX, Morpheus, TensorRT, RAPIDS, Hopper, cuDDN, cuDF, cuGraph, cuML, NCCL, Triton, NVIDIA NIM, NVIDIA AI Enterprise, cuVS, Blueprint, DGX Cloud, DGX Spark, NVIDIA Run:ai",
      "session_id": "CWES81583",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Benjamin Wu",
          "title": "Sr. Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Jessica Clark",
          "title": "Sr. Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Roman Yokunda Enzmann",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "David Williams",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Alex Stephens",
          "title": "Sr. Solutions Architect, Financial Services"
        },
        {
          "company": "NVIDIA",
          "name": "Marcus Manos",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Anass Majji",
          "title": "Solution Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Flora Huang",
          "title": "Sr. Solutions Architect"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "AI for Payments",
      "topic": "Fraud Detection",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81583/"
    },
    {
      "description": "AI is revolutionizing the way researchers approach their work and accelerating the pace of discovery. Join this interactive session to discuss the latest methods and models in AI for science, including domain-specific foundation models, research copilots, and autonomous science workflows with NVIDIA engineers and solutions architects. Come chat with us about NVIDIA’s latest research and enterprise products, including collaborations with partners. Some examples include training Nemotron models for data science tasks with NeMo-Gym and NeMo-RL, building agentic research assistants, developing hybrid quantum-classical algorithms with CUDA-Q, accelerating traditional simulations with NVIDIA Apollo and PhysicsNeMo, and AI-driven chemical and materials discovery using NVIDIA ALCHEMI. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Research: Non-Academic",
      "key_takeaways": [
        "Understand why AI is transforming science by accelerating discovery across domains.",
        "Gain hands-on experience with real scientific AI models running on GPUs.",
        "Learn how to build your own models, and once trained close the loop with autonomous science workflows.",
        "Adapt for your own research."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, Modulus, BioNeMo, Clara Holoscan, NeMo, NVIDIA NIM, CUDA-Q, Blueprint",
      "session_id": "CWES81649",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Daniel Burkhardt",
          "title": "Developer Relations Manager, Multiscale Biology"
        },
        {
          "company": "NVIDIA",
          "name": "Justin Smith",
          "title": "Sr. Developer Relations Manager"
        },
        {
          "company": "NVIDIA",
          "name": "John Linford",
          "title": "Principal Technical Product Manager, Data Center CPU Software"
        },
        {
          "company": "NVIDIA",
          "name": "Adam Thompson",
          "title": "Principal Technical Product Manager - Sensor Processing"
        },
        {
          "company": "NVIDIA",
          "name": "Christian Munley",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Savitha Pareek",
          "title": "Sr. Solutions Architect NVIS CBU"
        },
        {
          "company": "NVIDIA",
          "name": "Xin Yu",
          "title": "Sr. Solution Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "AI For Science: Driving Innovation in Research Labs and Academia",
      "topic": "Code / Software Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81649/"
    },
    {
      "description": "As AI adoption accelerates, isolated AI factories must scale across to a connected, distributed and secure AI grid that can run diverse workloads —from edge AI applications and telco network functions to internal AI services, digital twins, and RAN—while delivering the right performance and efficiency. This session defines what an AI grid is, why it matters, and how to build it. The speakers will walk through its reference architecture, from the compute substrate and high-performance networking to an intelligent, workload-aware control plane and orchestration that spans data centers and sites of all sizes. They will show the types of workloads that are ideal for running on an AI grid and share concrete steps telcos can take to implement a secure AI grid, where to start that journey, and how to unlock new revenue and monetization opportunities today.",
      "format": "In-Person",
      "industry": "Telecommunications",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Understand the reference architecture and building blocks of the AI grid —from compute, networking, control, orchestration, and security to workloads—and how these interoperate with AI-RAN, sovereign AI deployments and public cloud.",
        "See key use cases that will benefit from AI grid, and practical examples of the AI grid.",
        "Learn how a homogeneous substrate scales from an AI factory to an AI grid, to deliver new revenue opportunities for telcos and AI application providers.",
        "Discover a practical journey to the secure AI grid with concrete steps to get started, and how to partner with NVIDIA and Cisco to operationalize AI at scale."
      ],
      "nvidia_technology": "BlueField DPU, Cloud / Data Center GPU, DOCA, Ethernet Networking, Hopper, Blackwell",
      "session_id": "S82010",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Sree Sankar",
          "title": "Global Head, AI-Grid"
        },
        {
          "company": "Cisco",
          "name": "Masum Mir",
          "title": "SVP and GM, Provider Mobility"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "AI Grid Explained: From Secure AI Factories to Distributed Intelligence",
      "topic": "AI Networking, 5G/6G",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82010/"
    },
    {
      "description": "Physical AI is moving robots from labs into streets, offices, factories, and public spaces—where they must see, think, and act in milliseconds. Traditional cloud-centric architectures can’t meet the latency, reliability, and resiliency demands of these real-world uses, making a distributed AI grid essential. Two leading telecom operators will showcase physical AI use cases, demonstrating how AI-enhanced radio networks and distributed computing let robots offload heavy perception and decision workloads while maintaining real-time responsiveness and safety. The session will outline key architectural building blocks of an AI-native network, explore its evolution toward 6G, and explain why telecom AI grids are poised to become the default fabric for the next wave of physical AI.",
      "format": "In-Person",
      "industry": "Telecommunications",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Learn what physical AI and robotics apps need from networks, and why an AI grid is essential for meeting strict latency, reliability, and safety requirements.",
        "Understand the core architecture and infrastructure needed to deliver this, and how NVIDIA’s AI-RAN and AI grid components can be used together to enable it.",
        "Hear how telcos can become the platform for physical AI by building AI-native networks that evolve into 6G, powering large-scale autonomous systems."
      ],
      "nvidia_technology": "BlueField DPU, Cloud / Data Center GPU, RTX GPU, CUDA, TensorRT, Aerial, Infiniband Networking, Ethernet Networking, Interconnect Networking, NeMo, NVLink / NVSwitch, NVIDIA NIM, DGX Spark",
      "session_id": "S82014",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Softbank",
          "name": "Ryuji Wakikawa",
          "title": "VP, Research Institute of Advanced Technology"
        },
        {
          "company": "T-Mobile",
          "name": "Grant Ries",
          "title": "Chief Data and AI Officer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "AI Grid for Robots: Applications and Infrastructure for Physical AI Powered by Telcos",
      "topic": "AI Networking, 5G/6G",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82014/"
    },
    {
      "description": "This lightning session showcases four edge-native AI applications that only reach their full potential when deployed on a distributed AI grid powered by telecom networks and NVIDIA’s AI infrastructure. Through back-to-back talks, each presenter will showcase their application, the real-world challenge it solves, and the importance of network-proximate inference in enabling hyper-personalized, real-time use cases at scale—all delivered in a cost-efficient and compliant manner. See these applications in motion, hear how NVIDIA’s software stack helps to build and optimize them, and learn how telcos can unlock new revenue streams and return on investment by hosting and scaling these services on the AI grid.",
      "format": "In-Person",
      "industry": "Telecommunications",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Understand how diverse AI applications—including voice agents, real-time media, intelligent analytics, and physical AI—are architected to perform best on a distributed AI grid, leveraging edge and regional resources for optimal latency, throughput, and scalability.",
        "Learn how NVIDIA’s AI stack enables high-performance, low-latency inferencing at telecom distributed computing sites, improving both user experience and economics.",
        "Discover concrete value propositions and next steps for telcos, including how they can collaborate with application providers to provide new services on the AI grid."
      ],
      "nvidia_technology": "BlueField DPU, Cloud / Data Center GPU, RTX GPU, HGX, CUDA, TensorRT, Aerial, Omniverse, Infiniband Networking, Ethernet Networking, Metropolis, Interconnect Networking, NeMo, NVLink / NVSwitch, NVIDIA NIM",
      "session_id": "S82011",
      "session_type": "Lightning Talk",
      "speakers": [
        {
          "company": "Monks",
          "name": "Anthony Walasik",
          "title": "Lead Product Manager"
        },
        {
          "company": "Personal AI",
          "name": "Sharon Zhang",
          "title": "Co-Founder and CTO"
        },
        {
          "company": "Decart.ai",
          "name": "Kfir Aberman",
          "title": "Co-Founder, CTO"
        },
        {
          "company": "NVIDIA",
          "name": "Sree Sankar",
          "title": "Global Head, AI-Grid"
        },
        {
          "company": "Linker Vision",
          "name": "Willy Kuo",
          "title": "Co-Founder and CTO"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "AI Grid in Action: Killer Applications for Distributed Inferencing That Telcos Can Monetize",
      "topic": "AI Networking, 5G/6G",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82011/"
    },
    {
      "description": "As AI usage explodes, optimizing economics and performance of inference is becoming just as critical as training the models themselves. A distributed AI grid makes tokenomics radically more efficient—improving cost per token, time-to-first-token, and token throughput while delivering deterministic performance for the applications that need it most, including highly interactive and latency-sensitive experiences. In this session, two companies advancing AI grids will share which applications they prioritize for distributed versus centralized deployments and how key workload key performance indicators (KPIs) drive those choices. See concrete examples, benchmark results, and economic models that illustrate how NVIDIA-powered distributed infrastructure can enable “least-cost, best-performance” serving at scale—and what that unlocks for the next wave of AI services and business models.",
      "format": "In-Person",
      "industry": "Telecommunications",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Understand the benefits of the AI grid for infrastructure providers, application developers, and end users.",
        "Learn how workload KPIs drive placement decisions between distributed and centralized inference for better ROI.",
        "See concrete examples of NVIDIA-powered AI grid delivering superior economics and user experience at scale, and what that means for future applications."
      ],
      "nvidia_technology": "BlueField DPU, Cloud / Data Center GPU, RTX GPU, CUDA, TensorRT, Aerial, Infiniband Networking, Ethernet Networking, Interconnect Networking, NeMo, NVIDIA NIM, DGX Cloud, DGX Spark",
      "session_id": "S82012",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Akamai",
          "name": "Andy Champagne",
          "title": "SVP, CTO Office"
        },
        {
          "company": "NVIDIA",
          "name": "Chris Penrose",
          "title": "VP and Head of Business Development Telco"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "AI Grid Tokenomics: Powering Distributed AI Inference at Scale",
      "topic": "AI Networking, 5G/6G",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82012/"
    },
    {
      "description": "Discover how telcos are leveraging NVIDIA’s platforms and partner ecosystem to transform their networks, AI factories, and infrastructure assets into a unified platform for distributed intelligence. Whether you're focused on sovereign AI, distributed inference, autonomous networks, or 6G innovation, connect with experts shaping how telcos are becoming the intelligent backbone of the AI era. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "Telecommunications",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Speak with NVIDIA telecom experts to understand how telco AI grids bring scale to sovereignty, harness AI‑native wireless networks, and extend a unified, GPU‑accelerated architecture that maximizes ROI from existing telco investments.",
        "Ask about the token economics of running generative, agentic, and physical AI workloads at grid scale.",
        "Learn how AI grids enable a new class of AI‑native applications that demand distributed AI infrastructure."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, Aerial, Sionna, NVIDIA NIM, NVIDIA AI Enterprise",
      "session_id": "CWES81906",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Sree Sankar",
          "title": "Global Head, AI-Grid"
        },
        {
          "company": "NVIDIA",
          "name": "Chris Penrose",
          "title": "VP and Head of Business Development Telco"
        },
        {
          "company": "NVIDIA",
          "name": "Joao Kluck Gomes",
          "title": "Director, Business Development – AI Factories and Applications"
        },
        {
          "company": "NVIDIA",
          "name": "Elad Blatt",
          "title": "Global Head Business Development Telco Networking"
        },
        {
          "company": "NVIDIA",
          "name": "Kanika Atri",
          "title": "Sr. Director, Telco Marketing"
        },
        {
          "company": "NVIDIA",
          "name": "Soma Velayutham",
          "title": "VP, AI and Telecoms"
        },
        {
          "company": "NVIDIA",
          "name": "Jean-Francois Lacasse",
          "title": ""
        },
        {
          "company": "NVIDIA",
          "name": "Waleed Badr",
          "title": ""
        }
      ],
      "technical_level": "General Interest",
      "title": "AI Grids: Why Telcos Hold the Keys to Distributed Intelligence",
      "topic": "AI Networking, 5G/6G",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81906/"
    },
    {
      "description": "As global AI regulation accelerates, understanding and aligning with evolving compliance standards has never been more critical. This session brings together AI and legal experts to unpack what the EU AI Act means for organizations developing and deploying AI systems today. The panel explores how companies identify and manage high-risk AI applications and support transparency, accountability, and audit readiness through robust documentation practices. Gain a comprehensive understanding of the EU AI Act framework and the Code of Conduct, as well as the enforcement mechanisms and penalties associated with noncompliance.",
      "format": "Virtual",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Gain a comprehensive understanding of the EU AIA framework and the Code of Conduct.",
        "Understand what constitutes a high-risk AI application and the steps needed to ensure compliance.",
        "Understand the processes for conformity assessment, which are necessary to demonstrate that AI systems meet the requirements of the EU AI Act.",
        "Understand the importance of maintaining comprehensive records and documentation for reporting and audit purposes.",
        "Familiarize yourself with the enforcement mechanisms and penalties for noncompliance with the EU AI Act."
      ],
      "nvidia_technology": "NVIDIA AI Enterprise",
      "session_id": "S81627",
      "session_type": "Panel",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Barnaby Simkin",
          "title": "Director, Trustworthy AI"
        },
        {
          "company": "Simmons Simmons",
          "name": "William Dunning",
          "title": "Managing Associate (AI Regulation)"
        },
        {
          "company": "EQTY Lab",
          "name": "Jonathan Dotan",
          "title": "Founder"
        }
      ],
      "technical_level": "General Interest",
      "title": "AI in the Age of Regulation: Build Trust Through Compliance",
      "topic": "Security for AI",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81627/"
    },
    {
      "description": "This technical deep dive analyzes the cost drivers for AI systems, focusing on the interdependent relationship between total cost of ownership (TCO) and performance. We will show how the influence of extreme co-design, spanning silicon, software, and the rack-scale system, unlocks new efficiency levers. The session will detail how these integrated optimizations fundamentally improve operational metrics like cost per token and token per watt in modern deployments.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Understand rack-scale performance/TCO drivers and influence of networking innovations like NVLink 6.",
        "Learn strategies to minimize cost per token by deploying low-precision silicon and software-stack optimizations like wideEP, Dynamo, Sparsity, and NVFP4.",
        "Understand the influence of long-context, agentic, and deep reasoning on inference economics and possible optimizations.",
        "Discover how the different levers in our co-designed stack bring efficiency and optimize inference."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, Hopper, Blackwell",
      "session_id": "S81996",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Eduardo Alvarez",
          "title": "Technical Marketing Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Farshad Ghodsian",
          "title": "Sr. Technical Marketing Engineer"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "AI Performance and Inference Economics for Engineers",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81996/"
    },
    {
      "description": "Don't miss this exciting live panel discussion, hosted by Károly Zsolnai-Fehér (better known as the creator of Two Minute Papers), featuring top researchers from NVIDIA Research. They'll dive into the latest breakthroughs in AI, spotlight the most promising emerging technical trends, and candidly explore the biggest open challenges facing the field today. Join us for an interactive, high-energy conversation packed with cutting-edge insights!",
      "format": "In-Person",
      "industry": "Academia / Higher Education",
      "intended_audience": "Research: Academic",
      "key_takeaways": [],
      "nvidia_technology": "Cosmos",
      "session_id": "S81810",
      "session_type": "Lightning Talk",
      "speakers": [
        {
          "company": "Associate Professor, University of Toronto",
          "name": "Sanja Fidler",
          "title": "VP, AI Research, NVIDIA"
        },
        {
          "company": "NVIDIA",
          "name": "Yejin Choi",
          "title": "Sr. Research Director"
        },
        {
          "company": "Two Minute Papers",
          "name": "Karoly Zsolnai-Fehér",
          "title": "Researcher and Founder"
        },
        {
          "company": "NVIDIA",
          "name": "Yashraj Narang",
          "title": "Robotics Research Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Marco Pavone",
          "title": "Sr. Research Director"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "AI Research Breakthroughs from NVIDIA Research (Hosted by Karoly of Two Minute Papers)",
      "topic": "Robotics Simulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81810/"
    },
    {
      "description": "我们开发了与英伟达AI生态深度融合的工业具身智能控制平台。该平台基于云边端架构，并采用虚拟化和容器化技术，使得工业控制不再局限于特定硬件和操作系统，进而可根据 AI 算力需求部署到不同的硬件设施上，加速推动了 NVIDIA AI 生态与工业的融合。基于该平台，我们开发了一套工业具身智能机器人拆码垛工作站，并已批量交付给客户，以此验证了上述技术方案的可行性和可落地性。",
      "format": "Virtual",
      "industry": "Manufacturing",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "工业具身智能的产业形态、市场价值、行业痛点",
        "以工业具身智能为目标，构建与 NVIDIA AI 深度融合的 I-Motion 架构",
        "基于 NVIDIA AI 开发工具的工业具身智能研发链路",
        "工业具身智能码垛工作站的数据采集与模型训练经验",
        "工业具身智能码垛工作站的高精度仿真与一体化部署经验"
      ],
      "nvidia_technology": "Jetson, Isaac, Omniverse, TAO Toolkit, NVIDIA NIM",
      "session_id": "S81944",
      "session_type": "Talk",
      "speakers": [],
      "technical_level": "Business / Executive",
      "title": "AI 与工业的融合：探索工业具身智能新路径",
      "topic": "Robot Manipulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81944/"
    },
    {
      "description": "上海电影集团成立于76年前，是中国电影产业的重要先驱者。2023年，我们迈出了关键一步——投入建设AI算力体系。 我们围绕上海电影集团昊浦智慧产业社区，构建并规模化落地一套面向影视内容生产、产业协同与生态共创的 NVIDIA 全栈智能基础设施体系。构建面向 CG、AIGC、虚拟拍摄与大模型服务的统一GPU算力池，实现跨企业、跨团队的资源池化、服务化与精细化调度；基于 NVIDIA Omniverse 与 OpenUSD 打造影视行业级数字资产平台，将场景、角色与内容资产从“项目文件”升级为“可复用、可流转、可进化的产业级生产资料”；依托 NVIDIA AI Enterprise 与 NVIDIA NIM，将AI能力深度嵌入创作、制作、交付与运营全流程，形成可复制、可审计、可规模化推广的影视AI生产体系。",
      "format": "Virtual",
      "industry": "Media & Entertainment",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "上影如何将 GPU、网络与 AI 能力封装为标准化服务，使算力成为可计量、可调度、可运营的产业级生产要素，支撑多企业、多团队并行创作与规模化内容生产",
        "基于 Omniverse 与 OpenUSD 的数字资产平台设计方法，实现IP级资产复用、跨项目协同与生态伙伴共创，推动内容生产从“项目交付”走向“资产运营”",
        "AI 智能体与生成式工作流如何贯穿剧本创作、资产生成、虚拟拍摄、后期制作等环节，推动影视工业从“人工驱动流程”升级为“平台驱动流程”",
        "上影如何依托 NVIDIA 技术体系，将影视基地升级为“算力平台 + 创作平台 + 创新平台”三位一体的产业枢纽，打造开放、共创、可持续发展的影视科技生态"
      ],
      "nvidia_technology": "CUDA, Omniverse",
      "session_id": "S81900",
      "session_type": "Talk",
      "speakers": [],
      "technical_level": "General Interest",
      "title": "AI 重塑影视产业：打造基于 NVIDIA 全栈的 AI 算力池，生成式 AI工作流与 OpenUSD 资产平台",
      "topic": "AI Platforms",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81900/"
    },
    {
      "description": "The pace of AI-driven design innovation has accelerated dramatically since early 2025. With the introduction of Cadence’s AI-optimized Millennium M2000 Supercomputer built on NVIDIA, engineers and scientists now have the unprecedented capability to tackle the hardest problems in semiconductor design, system integration, and life sciences. In this session, discover how Cadence and NVIDIA are shaping a unified AI design ecosystem—integrating agentic AI workflows, physics-based models, and simulation-driven approaches to accelerate breakthroughs in chip design, advanced packaging, physical AI, and molecular discovery. Learn how next-generation infrastructure and tools are driving continuous innovation across every scale—from silicon and systems to data centers and molecules.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "AI-Accelerated Design Across Domains",
        "Unified AI Design Ecosystem",
        "Next-Generation Infrastructure Driving Scale"
      ],
      "nvidia_technology": "Cloud / Data Center GPU, Omniverse, BioNeMo, CUDA-X, NVIDIA NIM, Blackwell, Blueprint, Nemotron",
      "session_id": "EX82195",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "Cadence",
          "name": "Rob Knoth",
          "title": "Group Director"
        }
      ],
      "technical_level": "General Interest",
      "title": "AI-Accelerated Design: From Hyperscale Data Centers to Molecular Discovery (Presented by Cadence)",
      "topic": "CAE",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82195/"
    },
    {
      "description": "The decisions AI-first companies make on Day Zero will radically shift based on initial resources. Does a $5 million seed round change the fundamental AI strategy compared to a bootstrapped $0 start? Absolutely. This session explores four distinct Day Zero scenarios, each representing a different starting capital: $0 (Bootstrapped), $500,000 (Pre-Seed), $5 million (Seed/Series A), and $50 million (Series B+). For each scenario, we'll highlight high-leverage AI-first tactics, illustrating how capital immediately dictates where to spend resources and how fast to build proprietary technology. You'll leave with a clear, resource-aligned playbook for your own AI-first launch.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Four Contrasting Day Zero Playbooks: See how the core AI-first strategy changes immediately upon founding, depending on initial capital: $0, $500K, $5M, or $50M.",
        "The Defining Tactic for Each Path: Discover the single most critical, resource-aligned development tactic (e.g., data acquisition, API utilization, infrastructure build-out) to pursue for maximum competitive advantage at each starting level.",
        "The Build vs. Buy Curve: Understand how initial funding dictates the speed at which a company moves from leveraging existing AI APIs and tools (buy) to building proprietary models and infrastructure (build).",
        "Strategic Investment Decisions: Gain a framework for answering the essential Day Zero questions: where should the first dollar of AI engineering budget go, and what should the first hire's focus be?"
      ],
      "nvidia_technology": "NVIDIA NIM",
      "session_id": "S81564",
      "session_type": "Talks or Panels",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Amit Bleiweiss",
          "title": "Sr. Data Scientist"
        },
        {
          "company": "NVIDIA",
          "name": "Daman Oberoi",
          "title": ""
        }
      ],
      "technical_level": "Business / Executive",
      "title": "AI-First Design: Build Smarter From Day Zero Through Every Funding Stage",
      "topic": "Code / Software Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81564/"
    },
    {
      "description": "Discover how large‑scale AI‑accelerated ensemble forecasting is reshaping the way we assess physical climate risk in financial portfolios. This session dives into practical, data‑driven methods for quantifying asset‑level climate exposure—bridging advanced climate modeling with real‑world financial and energy use cases. You’ll learn how massive ensembles help uncover cross‑dependencies among temperature, wind, and solar patterns that drive volatility in energy commodities, grid stability, and portfolio performance under extreme conditions.",
      "format": "In-Person",
      "industry": "Financial Services",
      "intended_audience": "Consulting",
      "key_takeaways": [
        "Explore how AI‑driven ensemble modeling strengthens portfolio resilience against physical climate risks such as extreme heat, flooding, and storms.",
        "Discover techniques for enhancing global climate model outputs with AI analytics to generate high‑resolution, asset‑level insights.",
        "Gain practical strategies to anticipate and manage energy market volatility by revealing renewable generation risks, tail events, and grid stability challenges through ensemble forecasting.",
        "Connect With the Experts",
        "Workshops, Training Labs & Certification",
        "Sponsors & Exhibitors",
        "Privacy Policy Your Privacy Choices Terms of Service Accessibility Corporate Policies Product Security Contact",
        "Copyright © 2026 NVIDIA Corporation",
        "NVIDIA uses cookies to improve you"
      ],
      "nvidia_technology": "CUDA, Hopper",
      "session_id": "S82302",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "S&P Global Energy",
          "name": "Duncan Anderson",
          "title": "Research Director – Climate and Weather"
        },
        {
          "company": "NVIDIA",
          "name": "Georg Ertl",
          "title": "Solutions Architect"
        },
        {
          "company": "S&P Global Energy",
          "name": "Danielle Montagne",
          "title": "Data and Climate Scientist"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "AI-Powered Huge Ensembles for Portfolio Climate Risk Management",
      "topic": "Climate / Weather / Ocean Modeling",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82302/"
    },
    {
      "description": "We challenge the notion that modern AI-era GPUs are unsuitable for high-precision computations, specifically double-precision (FP64), the cornerstone of scientific computing since 1985. While scalar and vector FP64 throughput in GPUs have continued to improve over generations, Tensor Core throughput for FP64 has not grown at the same rate as reduced-precision operations used by AI. This is alleviated with mixed-precision linear algebra algorithms and Ozaki schemes that utilize reduced-precision Tensor Cores delivering significant performance and efficiency gains while guaranteeing native FP64 accuracy. We will showcase recent advancements in these algorithms, how to use them, and demonstrate their effectiveness in real-world applications. We will also preview an experimental analysis tool designed to improve developer productivity in reasoning about precision choices.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Emulated FP64 delivers strong performance gains, primarily for linear algebra, but native FP64 is still indispensable for many algorithms.",
        "Learn when emulated FP64 is used, what optional features are available, and how to enable or disable it.",
        "Tools and algorithmic options are, or will be, available help you determine the right level of precision for your application to optimize performance.",
        "AI-era GPUs are also great for high-precision numerical computing."
      ],
      "nvidia_technology": "CUDA, Blackwell",
      "session_id": "S81811",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Harun Bayraktar",
          "title": "Sr. Director of Software Engineering, Libraries"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "All the Precision, None of the Transistors",
      "topic": "Accelerated Computing Libraries",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81811/"
    },
    {
      "description": "In this session, we'll explore how an exchange moving hundreds of billions of dollars in trades each day evaluates and governs AI under strict latency, reliability, and transparency requirements. We'll discuss where AI can augment the market stack, and where deterministic behavior must remain non-negotiable.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "A Concrete Deployment Journey: How Nasdaq built and launched Dynamic M-ELO, the world’s first AI-powered order type, without disrupting live market operations",
        "How AI and accelerated compute enhance market surveillance, anomaly detection, and real-time risk monitoring",
        "How AI is influencing the interplay between exchanges and algorithmic trading firms, enabling new forms of intelligence on both sides"
      ],
      "nvidia_technology": "Cloud / Data Center GPU, CUDA, CUDA-X, NVIDIA NIM, NVIDIA AI Enterprise",
      "session_id": "S82153",
      "session_type": "Fireside Chat",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Ioana Boier",
          "title": "Global Head of Capital Markets Strategy"
        },
        {
          "company": "Nasdaq",
          "name": "Tal Cohen",
          "title": "President"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Always-On Innovation: How Nasdaq Deploys AI in Mission-Critical Market",
      "topic": "Performance Optimization",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82153/"
    },
    {
      "description": "Physical AI is transforming how robots interact with the world. Building these advanced systems requires three computers: compute for training, advanced simulation for validation, and efficient edge deployment. With a simulation-first approach, robots can train across millions of scenarios, generalize faster, and adapt to real-world complexity. In this technical deep-dive session, learn core robotics simulation workflows, from building virtual environments to configuring robots, sensors, and physics, using NVIDIA Isaac Sim and Isaac Lab, built with NVIDIA Omniverse libraries. You'll also explore how to generate and leverage synthetic data with NVIDIA Cosmos world foundation models to train and validate robot perception and control policies before deploying them in the real world.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "How to build and configure robotics simulations in NVIDIA Isaac and Isaac Lab",
        "How to integrate NVIDIA Cosmos World Foundation Models with Isaac-based workflows to rapidly create diverse virtual worlds and use them to train and validate robot perception and control policies before real-world deployment",
        "How to generate scalable, labeled synthetic data from simulated environments, using techniques like domain randomization and high-fidelity sensor simulation"
      ],
      "nvidia_technology": "CUDA, Isaac, Omniverse, Cosmos",
      "session_id": "S81488",
      "session_type": "Tutorial",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Rishabh Chadha",
          "title": "Technical Marketing Engineer - Isaac"
        },
        {
          "company": "NVIDIA",
          "name": "Asawaree Bhide",
          "title": "Robotics Technical Marketing Engineer"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "An Introduction to Robot Simulation",
      "topic": "Robotics Simulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81488/"
    },
    {
      "description": "As we enter the era of the NVIDIA Rubin architecture, the global demand for trillion-parameter models and agentic AI is redefining the modern data center. Infrastructure is no longer just about individual servers; it is about building the \"AI factory\"—a seamless, high-density environment capable of processing massive workloads with unprecedented efficiency. In this session, GIGABYTE (Giga Computing) will reveal how we are bridging the gap between cutting-edge silicon and a successful industrial-scale deployment with RIKEN QHPC project in running NVIDIA Blackwell products. We will explore the architectural evolution of our latest server lineup, specifically engineered to harness the power of NVIDIA Vera CPUs and Rubin GPUs.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Consulting",
      "key_takeaways": [
        "Maximizing NVIDIA Rubin Platform’s Potential: See how GIGABYTE’s advanced liquid cooling, intelligent pod management, and co-engineered thermal innovations push NVIDIA Rubin to sustained peak FP4 performance, maximize efficiency, reduce the hassle of cluster management, and deliver reliable, large-scale AI training and inference.",
        "The Blueprint of GIGA POD: Discover how our modular rack-scale solutions utilize NVIDIA NVLink, NVIDIA Quantum InfiniBand, and the full stack of NVIDIA solution installations to deliver the high-density interconnects needed for next-generation generative AI, highlighted by the successful partner deployment at RIKEN.",
        "Time to Market: Learn strategies for rapidly scaling from pilot projects to full-scale AI factories, ensuring that enterprises and cloud service providers can stay ahead in the competitive AI race."
      ],
      "nvidia_technology": "BlueField DPU, RTX GPU, HGX, Infiniband Networking, MGX, NVLink / NVSwitch, NVIDIA NIM, NVIDIA AI Enterprise",
      "session_id": "S82216",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Giga Computing",
          "name": "Leon Chang",
          "title": "Product and Market Lead"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Architect the AI Factory for the NVIDIA Rubin Era: Scaling is Key (Presented by GIGABYTE)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82216/"
    },
    {
      "description": "Learn how LALIGA is transforming its business through the most ambitious AI program in global sport, powered by Globant and its sports division, Sportian. This session explores how the league is using agentic AI to build connected intelligence across operations, competition management, content, marketing, sporting performance, broadcast, and fan engagement. In particular, we'll examine an NVIDIA-enabled use case that allows LALIGA to optimize match scheduling and drive tangible growth in broadcast audiences and stadium attendance. See how AI can be implemented in a secure, practical, and scalable way to accelerate business performance and strengthen a league’s global position.",
      "format": "In-Person",
      "industry": "Media & Entertainment",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "Understand how AI is being deployed in the sports and entertainment industry as a central and strategic growth tool.",
        "Discover the architectural and organizational frameworks required to guarantee 24/7 agentic performance in front of an audience of millions."
      ],
      "nvidia_technology": "NVIDIA AI Enterprise",
      "session_id": "S81578",
      "session_type": "Lightning Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Sepi Motamedi",
          "title": "Head of Sports and Live Media Marketing"
        },
        {
          "company": "Globant",
          "name": "Carolina Dolan Chandler",
          "title": "CTO"
        },
        {
          "company": "Sportian",
          "name": "Gonzalo Zarza",
          "title": "Chief Data Officer"
        },
        {
          "company": "LALIGA",
          "name": "Javier Gil Fernandez",
          "title": "Head of AI Implementation and Development"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Architecting AI Systems That Power Business Growth in Sports",
      "topic": "Data Analytics / Processing",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81578/"
    },
    {
      "description": "As organizations accelerate AI initiatives, infrastructure has become a critical competitive differentiator. This session examines how next-generation data centers are architected for exponential AI workload demands, featuring insights from leading-edge deployments. You'll explore technical foundations of large-scale AI infrastructure, advanced cooling, high-performance networking, and validation protocols ensuring reliability at scale. We’ll address real-world challenges: site preparation, capacity planning, and operational considerations separating successful implementations from costly missteps. Drawing on global projects, learn practical guidance for leaders transitioning to AI-optimized facilities. Whether you're expanding existing infrastructure or designing greenfield sites, gain actionable perspectives on architecture, deployment strategies, and operational best practices.",
      "format": "In-Person",
      "industry": "Manufacturing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "This session examines how next-generation data centers are architected for exponential AI workload demands, featuring insights from leading-edge deployments.",
        "We’ll address real-world challenges: site preparation, capacity planning, and operational considerations separating successful implementations from costly missteps.",
        "Gain actionable perspectives on architecture, deployment strategies, and operational best practices."
      ],
      "nvidia_technology": "HGX, MGX, Blackwell",
      "session_id": "EX82084",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "Hyve Solutions",
          "name": "Rami Khouri",
          "title": "Sr. VP Engineering"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Architecting AI-Ready Data Centers: Lessons From Leading-Edge Deployments (Presented by Hyve Solutions)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82084/"
    },
    {
      "description": "As AI adoption accelerates, organizations face a growing tension between data gravity, regulatory constraints, and the need to scale AI quickly. Data sovereignty requirements, air-gapped environments, and operational security expectations increasingly shape how—and where—AI systems can be built and deployed. This session explores how to build secure, scalable AI platforms in highly regulated environments. Drawing on real-world implementations from regulated industries such as government and finance, we’ll examine how organizations are aligning AI performance with strict data residency and sovereignty requirements. You'll leave with a practical framework for designing AI environments that minimize unnecessary data movement, support rapid experimentation, and maintain strong control over sensitive data across private and hybrid infrastructures.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "How evolving regulations and expectations around data, operational, and technical sovereignty are influencing AI architecture decisions and long-term strategy.",
        "Approaches for accelerating AI development and deployment within controlled environments, including patterns for private and hybrid AI platforms",
        "Design considerations for building large-scale, secure AI infrastructure—from model training to inference—capable of supporting hundreds to thousands of GPUs while meeting stringent security and compliance requirements, with the support of the NVIDIA AI Computing by HPE portfolio"
      ],
      "nvidia_technology": "Grace CPU, Blackwell",
      "session_id": "S82136",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "HPE",
          "name": "Gabriel Broner",
          "title": "Chief Technologist, Private Cloud and Flex Solutions"
        },
        {
          "company": "HPE",
          "name": "Thierry Pienaar",
          "title": "WW CTO, SNR Distinguished Technologist; HPC and AI Go-to-Market"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Architecting Trust: Design Secure and Sovereign AI Systems (Presented by HPE)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82136/"
    },
    {
      "description": "In this session, we’ll show how real-time AI is becoming the new fabric of modern media—connecting artist workflows to studio pipelines to distribution at global scale. You’ll learn the reference architecture behind an AI-native media stack: accelerated compute, low-latency networking, and developer-ready SDKs and APIs that let teams build, integrate, and ship AI into real production systems. We’ll walk through practical examples across creation, live production, enhancement, understanding, and delivery—focusing on what it takes to move from demos to deployed workflows, with measurable gains in speed, quality, and cost.",
      "format": "In-Person",
      "industry": "Media & Entertainment",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Learn how AI-powered tools and generative agents are transforming media production workflows, enabling faster, more creative, and cost-effective content creation across multiple formats.",
        "Discover how AI-driven recommendation systems and dynamic content optimization are enhancing audience engagement and discoverability across platforms.",
        "Gain insights into how AI enables more targeted ads, microtransactions, and tailored subscription services, driving more efficient and sustainable revenue streams."
      ],
      "nvidia_technology": "RTX GPU, DGX Platform, Blackwell, Cosmos, DGX Spark, DGX Station",
      "session_id": "S82146",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Richard Kerris",
          "title": "VP and General Manager"
        }
      ],
      "technical_level": "General Interest",
      "title": "Artist to Audience: The Developer Platform for Real-Time AI in Media",
      "topic": "Video Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82146/"
    },
    {
      "description": "What is Mission Control? What goes into operating a high performance AI factory? How does Mission Control integrate with other tools and processes? This session is the place to get those types of questions answered! Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Understand elements of the AI factory \"operating system\" (NVIDIA Mission Control as the exemplar).",
        "Understand how the layers of an AI factory software stack interoperate.",
        "Understand how Mission Control solves AI factory problems, and relates to other industry solutions (such as independent software vendors and fellow travelers)."
      ],
      "nvidia_technology": "Grace CPU, DGX Platform, HGX, CUDA, DOCA, Infiniband Networking, Hopper, Magnum IO, MGX, Base Command Manager, Interconnect Networking, NCCL, NVIDIA NIM, NVIDIA AI Enterprise, NVIDIA Run:ai, Mission Control",
      "session_id": "CWES81473",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Robert Stober",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Salah Chaou",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Jeff Weiss",
          "title": "Sr. Director"
        },
        {
          "company": "NVIDIA",
          "name": "Chad Chapman",
          "title": "Sr. Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Charu Ramachandran",
          "title": "Sr. Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Scott Ellis",
          "title": "Sr. Director, Solutions Architecture and Engineering"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Ask Us Anything About NVIDIA Mission Control",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81473/"
    },
    {
      "description": "In this hands-on lab, participants will explore how to build industrial digital twins from manufacturing facilities to AI factories, using OpenUSD and NVIDIA Omniverse libraries, drawing on real-world collaboration with industry leaders. The session walks through an end-to-end workflow, from CAD models to full digital twin assembly, demonstrating how manufacturers are transforming design and production through simulation and interoperability. Intermediate experience with 3D content concepts: meshes, materials, transforms, scene hierarchies, and USD fundamentals Familiarity with manufacturing workflows, factory layouts, or industrial automation processes Basic Python scripting experience for automation and integration tasks Knowledge of data integration concepts and structured data formats",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Ingest CAD data into OpenUSD to accelerate scene Assembly for digital twins with Simready assets.",
        "Organize project assets effectively through recommended folder structures and OpenUSD best practices.",
        "Configure asset libraries, assign materials, and set up realistic lighting for accurate visualization.",
        "Capture and render high-quality videos of the digital twin environment for presentation or analysis."
      ],
      "nvidia_technology": "Omniverse",
      "session_id": "DLIT81800",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Jay Axe",
          "title": "Technical Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Ernesto Pacheco",
          "title": "coming soon"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Assembling Industrial Digital Twins with OpenUSD and NVIDIA Omniverse Libraries",
      "topic": "Industrial Digitalization / Digital Twin",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81800/"
    },
    {
      "description": "This session introduces Project Southgate, a sovereign network of AI factories across Australia that rethinks the fundamentals of how intelligence is produced. Each site in the Southgate network is purpose-built for high-density GPU systems, using direct-to-chip liquid cooling and modular architecture to reduce energy and water use while maintaining throughput at scale. With a flagship AI Factory campus in the state of Tasmania, powered by firmed renewables. Through a technical lens, we’ll explore how infrastructure design choices—from thermal systems to site integration with the grid—can shape the next decade of AI development. Join this session to learn how Australia’s green AI factory network is redefining what AI infrastructure can be—and why energy-aligned, token-efficient design is the shift the industry needs.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Discover how sovereign infrastructure in Australia’s green AI zone positions Asia-Pacific to meet rising AI demand.",
        "Learn how Firmus AI tokens redefine production metrics for compute—linking energy use directly to lower costs to train and run models."
      ],
      "nvidia_technology": "DGX Platform, Hopper, Blackwell, DGX Cloud",
      "session_id": "S82277",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Firmus",
          "name": "Daniel Kearney",
          "title": "CTO"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Australia’s Radical Shift in AI Infrastructure: Project Southgate (Presented by Firmus Technologies)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82277/"
    },
    {
      "description": "In this instructor-led lab, you’ll learn how to build robust OpenUSD-based data pipelines using the USD Exchange SDK, enabling you to leverage your existing 3D software and tools for scalable physical AI workflows. You'll start from common 3D source formats and write Python-based code to extract, transform, and load assets into reusable, modular USD representations suitable for large-scale workflows. Through guided, hands-on exercises, you'll structure 3D content into composable USD assets, apply simulation-ready (“SimReady”) standards, and enrich assets with rigid-body physics that support downstream robotics and simulation workloads. By the end of the lab, you'll have automated an asset conversion pipeline capable of processing large 3D datasets, giving you a practical blueprint you can adapt to your own production environment. Intermediate experience with Python, including working with packages, virtual environments, and basic scripting. Familiarity with content concepts such as meshes, materials, transforms, and scene hierarchies common to 3D modeling applications. Understand command-line usage for running scripts and managing project files. Have completed the Learn OpenUSD curriculum (https://docs.nvidia.com/learn-openusd/latest/index.html), covering fundamentals of USD-based scene representation and concepts (stages, layers, prims, and composition). Have basic familiarity with robotics or simulation or general 3D content workflows (for example, using simulation tools or working with physics-enabled assets)",
      "format": "In-Person",
      "industry": "Manufacturing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Write Python code to convert 3D assets from source formats to OpenUSD using proper ETL (extract, transform, load) design patterns.",
        "Structure 3D content into reusable and modular assets using USD Exchange SDK.",
        "Implement and validate SimReady standards and augment assets with rigid body scene description.",
        "Automate asset conversion pipelines for large-scale 3D workflows."
      ],
      "nvidia_technology": "RTX GPU, Isaac, Omniverse, DLSS, PhysX, NVIDIA NIM, Blackwell",
      "session_id": "DLIT81639",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Andrew Kaufman",
          "title": "Principal Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Beau Perschall",
          "title": "Director, Omniverse Sim Data Ops"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Automate 3D Data Pipelines With USD Exchange SDK",
      "topic": "3D Data Interoperability",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81639/"
    },
    {
      "description": "AI factories are hard, but automation can make life easier. Join us to get your questions answered about how to use industry standard and product-specific tools to automate the deployment and operation of your AI factory. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Automate Your AI Factory: Learn the best practices for using automation to simplify the deployment and maintenance of complex enterprise AI factories, making your life easier.",
        "Get your questions answered about using industry-standard tools like Ansible, Terraform, and Git to implement robust infrastructure-as-code (IaC) for your AI factory infrastructure.",
        "Understand how to effectively manage and control infrastructure change using proven IaC methodologies to ensure reliability and stability in your AI factory operations."
      ],
      "nvidia_technology": "DOCA, MGX, Base Command Manager, NVIDIA AI Enterprise, NVIDIA Run:ai, Mission Control",
      "session_id": "CWES81474",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Vallard Benincosa",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Scott Ellis",
          "title": "Sr. Director, Solutions Architecture and Engineering"
        },
        {
          "company": "NVIDIA",
          "name": "Martin Piercy",
          "title": "Sr. Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Jeff Weiss",
          "title": "Sr. Director"
        },
        {
          "company": "NVIDIA",
          "name": "Chad Chapman",
          "title": "Sr. Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "David Dean",
          "title": "Sr. Solutions Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Automating AI Factory Infrastructure (Ansible, GitOps, etc.)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81474/"
    },
    {
      "description": "Automation is not the same as autonomy. Automated networks follow predefined rules; autonomous networks understand context, analyze data, reason, act, and learn continuously. Autonomous networks are the next frontier for telecoms, and progress is accelerating rapidly. This panel brings together leading telecom operators and solution providers to show how agentic AI and large telco models are transforming operations. Speakers will share how they unify data across operations and business support systems and network domains and deploy AI agents that self-configure, self-optimize, and self-heal networks while improving performance and customer experience. Hear real-world examples of capabilities already in production, the NVIDIA software and infrastructure behind them, and what the next 12 months of innovation look like as operators advance toward higher levels of autonomy.",
      "format": "In-Person",
      "industry": "Telecommunications",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Understand why automation is not the same as autonomy, and how agentic AI plus large telco models accelerate the journey toward higher levels of network autonomy.",
        "Learn about the latest AI agents and real-world operations use cases telcos are deploying today, and their measurable impact on network and operational key performance indicators.",
        "Hear how panelists have built their autonomous network initiatives, the role of NVIDIA software and infrastructure, and what innovations they expect in the coming year."
      ],
      "nvidia_technology": "NeMo, NVIDIA NIM, NVIDIA AI Enterprise, Blueprint",
      "session_id": "S82016",
      "session_type": "Panel",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Lilach Ilan",
          "title": "Global Head of Business Development -Telco Operations"
        },
        {
          "company": "Tech Mahindra",
          "name": "Amol Phadke",
          "title": "Chief Transformation Officer"
        },
        {
          "company": "Accenture",
          "name": "Tunc Yorulmaz",
          "title": "Sr. Managing Director"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Autonomous Networks: The New Brain of Telco Operations",
      "topic": "Code / Software Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82016/"
    },
    {
      "description": "A beauty product has billions of possible formulations, rendering exhaustive lab testing impossible. L'Oréal is partnering with NVIDIA to accelerate and improve accuracy of in-silico formulation property prediction.",
      "format": "In-Person",
      "industry": "Retail / Consumer Packaged Goods",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Learn how to build an AI-accelerated simulation workflow for beauty product formulation optimization.",
        "Understand how NVIDIA ALCHEMI's atomistic simulation NIM and active learning workflow accelerate accurate property prediction by 100X"
      ],
      "nvidia_technology": "NVIDIA NIM",
      "session_id": "S81624",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "L'Oréal R&I",
          "name": "Matthieu Cassier",
          "title": "Director of Global Transformation & Digital"
        },
        {
          "company": "L'Oréal",
          "name": "Guive Balooch",
          "title": "Global VP, Tech & Open Innovation"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Beauty Reinvented with L'Oréal: AI-Enabled Formulation Optimization",
      "topic": "Computational Chemistry / Materials Science",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81624/"
    },
    {
      "description": "Learn where quantum computing technologies are today, where they’re going, and how to track progress toward the goal of (someday) outperforming any supercomputer at solving useful problems. We’ll discuss how we assess and improve the performance of quantum computing hardware, the expanding role for HPC/GPUs/AI in this quest, and when it will (and won’t!) be useful to integrate HPC and GPUs with fault-tolerant quantum computers.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Research: Non-Academic",
      "key_takeaways": [
        "Quantum computers are radically different from HPC or GPU machines—especially the fault-tolerant quantum computers coming soon.",
        "Learn what \"benchmarking\" means to quantum computing (QC) experts, and how to use it to track QC's growth toward \"quantum utility.”",
        "Quantum computers will complement HPC and AI, not compete with them, and will work synergistically in the future.",
        "Learn about how GPUs and HPC are contributing now to help us build and benchmark better quantum computers."
      ],
      "nvidia_technology": "CUDA Quantum",
      "session_id": "S82191",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Sandia National Laboratories",
          "name": "Robin Blume-Kohout",
          "title": "Distinguished Member of Technical Staff"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Benchmarking the Path to Useful Quantum Computers",
      "topic": "Quantum Computing",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82191/"
    },
    {
      "description": "Join this interactive session to ask questions and deepen your understanding of serving LLMs (and vision language models, VLMs) with SGLang. Our experts are offering one-on-one guidance on efficiently deploying models with SGLang, optimizing throughput/latency, and leveraging key runtime features. Whether you're exploring best practices for deploying SGLang via its OpenAI-compatible API on Kubernetes, or tuning configs (speculative decoding, kernel integration and fused ops, Tensor/pipeline/expert parallelism, request scheduling, prefix/prompt caching, and quantization) to accelerate inference, this session will cover it all. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "One-on-one expert guidance on efficient model deployment with SGLang",
        "Help with optimizing throughput and latency for inference workloads",
        "Best practices for using SGLang’s OpenAI-compatible API on Kubernetes",
        "Tuning advanced configs: speculative decoding, kernel integration, fused ops",
        "Guidance on Tensor/pipeline/expert parallelism, request scheduling, and prefix/prompt caching"
      ],
      "nvidia_technology": "TensorRT, Dynamo",
      "session_id": "CWES81934",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Trevor Morris",
          "title": "Sr. Deep Learning Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Shu Wang",
          "title": "Sr. Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Best Practices for Accelerating LLM and VLM Inference With SGLang",
      "topic": "AI Inference",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81934/"
    },
    {
      "description": "Meet with the experts at NVIDIA to learn how to accelerate vLLM inference performance on NVIDIA GPUs. Our experts at NVIDIA help make vLLM more performance and scalable. We cover a diversity of workloads running from DGX spark to NVIDIA GB200 NVL72. Join us for a deep dive in various optimization techniques, including speculative decoding, kernel performance, deployment recipes, reinforcement learning, and open-source software collaborations. Connect with us and share your vLLM use case, and together we'll explore how we can help improve the framework! Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "How to best deploy your use case with vLLM on NVIDIA GPUs",
        "How to benchmark, understand, and improve LLM inference performance with vLLM",
        "How to leverage various inference techniques such as PD-disaggregation and Speculative Decoding to further accelerate your workloads with vLLM"
      ],
      "nvidia_technology": "Grace CPU, HGX, CUDA, Infiniband Networking, cuBLAS, CUDA-X, cuDDN, CV-CUDA, Interconnect Networking, NCCL, NSight Comute, NSight Systems, Blackwell, DGX Spark, DGX Station, Dynamo, NIXL",
      "session_id": "CWES82007",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Pavani Majety",
          "title": "Sr. Deep Learning Engineer, Inference"
        },
        {
          "company": "NVIDIA",
          "name": "Benjamin Chislett",
          "title": "Sr. Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Xin Li",
          "title": "Engineering Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Shang Wang",
          "title": "Manager, AI Systems Software"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Best Practices for Accelerating LLM and VLM Inference With vLLM",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes82007/"
    },
    {
      "description": "Efficient hashmaps are among the most critical data structures in modern data analytics, genomics, and recommender systems. In this talk, we'll deep-dive into the amazing world of hashmaps for CPUs. We will discuss types, approaches, and properties of different CPU hashmap implementations with respect to our current data-center CPU platform NVIDIA Grace and the upcoming NVIDIA Vera platform. We'll tackle practical considerations for optimizing hashmap construction and lookups on NVIDIA CPUs, share lessons learned from integrating them into data analytical workloads, and describe theoretical performance limits, tuning knobs, and less well-known techniques to improve hashmap performance.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Understand theoretical performance limits and practical considerations for optimizing hashmap construction and lookups on NVIDIA Grace and Vera CPUs.",
        "Learn how custom memory layout, prefetching, lightweight context switch, and single instruction, multiple data can improve CPU hashmap performance.",
        "Learn how to efficiently apply CPU hashmaps to different workloads."
      ],
      "nvidia_technology": "Grace CPU",
      "session_id": "S81658",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Yuzhong Wen",
          "title": "Sr. DevTech Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Matthias Langer",
          "title": "AI and DevTech Engineer"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Best Practices for Hashmap Optimization on NVIDIA Grace and Vera CPUs",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81658/"
    },
    {
      "description": "As organizations scale toward population-level AI adoption, running inference reliably, efficiently, and cost-effectively becomes essential. This session covers best practices for scaling inference across platform, model, and application layers, using real-world insights and NVIDIA technologies like DGXC-Lepton and NVCF. We’ll address challenges in GPU capacity, autoscaling, model optimization with NVIDIA S/W stack, concurrency, architecture, and observability. Learn how to use proven, field-tested strategies for scaling current AI workloads or launching new ones at scale.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Population-Scale Inference Challenges: Understand the key bottlenecks that limit throughput, latency, scaling, and reliability, and learn how to design systems that overcome these constraints.",
        "Benchmarking Across Platform and Model Layers: Gain a systematic methodology to measure GPU efficiency, latency, throughput, and cost, enabling informed architectural decisions.",
        "Platform Optimization and Observability Best Practices:"
      ],
      "nvidia_technology": "Cloud / Data Center GPU, DGX Platform, TensorRT, Triton, NVIDIA NIM, NVIDIA AI Enterprise, DGX Cloud",
      "session_id": "S81519",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Jalaj Thanaki",
          "title": "Sr. Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Anish Mukherjee",
          "title": "Solutions Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Best Practices for Scaling Inference",
      "topic": "Software-Defined Data Center",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81519/"
    },
    {
      "description": "As organizations shift from traditional data center to AI factories, building a full-stack AI infrastructure can be challenging. Connect with NVIDIA experts on how to quickly deploy your AI factories, and learn best practices for your ML/AI workflows. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Explore best practices in developing and implementing your MLOps strategy.",
        "Learn how NVIDIA IT is deploying AI factories.",
        "Learn how NVIDIA's partner ecosystem is leveraging NVIDIA AI.",
        "Connect With the Experts",
        "Workshops, Training Labs & Certification",
        "Sponsors & Exhibitors",
        "Privacy Policy Your Privacy Choices Terms of Service Accessibility Corporate Policies Product Security Contact",
        "Copyright © 2026 NVIDIA Corporation",
        "NVIDIA uses cookies to improve your experience on our web site. We and our third-party partners also use cookies and other tools to collect and record information you provide as well as information about your interactions with our websites for performance improvement, analytics, and to assist in marketing efforts. By c"
      ],
      "nvidia_technology": "NeMo, NVIDIA NIM, NVIDIA AI Enterprise",
      "session_id": "CWES81817",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "William Benton",
          "title": "Principal Product Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Nik Spirin",
          "title": "Director, Generative AI and LLMOps Platform"
        },
        {
          "company": "NVIDIA",
          "name": "Erik Bohnhorst",
          "title": "Director of Product Management"
        },
        {
          "company": "NVIDIA",
          "name": "Michael Balint",
          "title": "Director, Product Architecture"
        },
        {
          "company": "NVIDIA",
          "name": "Nic Borensztein",
          "title": "Principal Solutions Architect"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Best Practices in Building Your AI Factory: Connect With NVIDIA MLOps and AIOps Experts",
      "topic": "DataOps",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81817/"
    },
    {
      "description": "Learn our practical strategies for post-training multi-modal vision language models (VLMs) and diffusion models, leveraging M-Core to enhance GPU utilization and improve model performance. Explore our effective reinforcement learning workflow, which spans from training to resource scheduling, maximizing GPU resource efficiency.",
      "format": "In-Person",
      "industry": "Consumer Internet",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how we optimize models through post-training methods like reinforcement learning (RL) and make algorithmic adjustments to improve efficiency.",
        "How to use M-Core to build efficient multi-modal visual understanding models",
        "How to use M-Core to set up your own RL system and optimize resource utilization",
        "How to schedule training and inference resources based on business tidal patterns to improve hardware utilization"
      ],
      "nvidia_technology": "CUDA, Hopper, NCCL, NeMo",
      "session_id": "S81515",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Tencent Holdings Ltd.",
          "name": "Junyu Wu",
          "title": "Sr. Developer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Best Practices of Multi-Modal and Vision Generation Training in M-Core",
      "topic": "Reinforcement Learning",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81515/"
    },
    {
      "description": "Universal Scene Description (OpenUSD) is the foundational 3D data framework of NVIDIA’s physical AI technology stack, serving as a standard across industries for building digital twins and AI in 3D worlds. As the underlying technology for NVIDIA Omniverse libraries, Isaac Sim, and SimReady assets, OpenUSD streamlines development pipelines, enhances data compatibility across 3D tools, and establishes standardized processes for complex physical AI workflows. Join this session to learn about the latest developments and next major milestones of OpenUSD.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Discover the Evolution of OpenUSD: Learn about the international standardization efforts through AOUSD, open-source collaboration across industries, and OpenUSD's expansion into new platforms and use cases beyond media & entertainment."
      ],
      "nvidia_technology": "Isaac, Omniverse",
      "session_id": "S81630",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Aaron Luk",
          "title": "Director of Product Management"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Beyond the Basics: OpenUSD for Advanced Physical AI Simulation",
      "topic": "3D Data Interoperability",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81630/"
    },
    {
      "description": "Financial institutions are deploying LLMs for several use cases including trading, advisory, and consumer-facing applications—but opaque model behavior limits trust, especially in high-stakes situations. This session shows how mechanistic interpretability can reveal internal circuits and activations that drive predictions, making LLMs auditable and controllable. We’ll cover theory and practical applications across trading strategies, sentiment analysis, and hallucination reduction, linking interpretability not only to compliance and risk governance, but also to actionable trading signals.",
      "format": "Virtual",
      "industry": "Financial Services",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Learn why transparent LLMs are the foundation for trustworthy financial AI.",
        "See how mechanistic interpretability turns opaque reasoning into auditable evidence.",
        "See demonstrated benefits across trading, sentiment, bias, and hallucination control.",
        "Understand how interpretability enhances signal quality and model reliability in high-stakes workflows."
      ],
      "nvidia_technology": "NVIDIA NIM",
      "session_id": "S82175",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Barclays",
          "name": "ARIYE SHATER",
          "title": "Managing Director, Head of Risk AI, Head of Traded Risk and Treasury Quantitative Analytics"
        },
        {
          "company": "Barclays",
          "name": "Hariom Tatsat",
          "title": "VP – Risk AI"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Beyond the Black Box: Interpretability of LLMs in Finance",
      "topic": "Security for AI",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82175/"
    },
    {
      "description": "How can we accelerate breakthrough drug discovery beyond model performance? This panel unites leaders from Sanofi, Latent Labs, Owkin, and Elsevier to explore how AI-driven platforms, multi-modal data integration, and large-scale simulation are reshaping how we design and develop protein therapeutics. Together, we’ll dive into lessons learned from cutting-edge deployments, share emerging best practices, and examine what’s next for collaborative AI ecosystems driving the next era of precision medicine.",
      "format": "Virtual",
      "industry": "Healthcare & Life Sciences",
      "intended_audience": "Research: Non-Academic",
      "key_takeaways": [
        "Integration of Multi-Modal Data — How combining structural, omics, and clinical datasets enhances protein and small-molecule model performance",
        "Advances in Foundation Models — Emerging architectures enabling cross-domain learning and improved generalization in drug discovery tasks",
        "Scalable AI Platforms — Best practices for deploying and maintaining end-to-end AI pipelines in regulated R&D environments",
        "Model-to-Lab Translation — Lessons from real-world implementations bridging in silico predictions with experimental validation",
        "Future-Ready Ecosystems — How collaboration across pharma, AI startups, and data providers is shaping an interoperable, reproducible AI-driven discovery landscape."
      ],
      "nvidia_technology": "BioNeMo, Clara Parabricks, NVIDIA NIM, NVIDIA AI Enterprise",
      "session_id": "S81992",
      "session_type": "Panel",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "David Ruau",
          "title": "Head of Strategic Alliances, Drug Discovery AI, EMEA"
        },
        {
          "company": "Latent Labs",
          "name": "Simon Kohl",
          "title": "Founder & CEO"
        },
        {
          "company": "Owkin",
          "name": "Eric Durand",
          "title": "Chief Data Science Officer"
        },
        {
          "company": "Apheris",
          "name": "Robin Roehm",
          "title": "CEO and Co-Founder"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Beyond the Model: Driving Cutting-Edge Drug Discovery",
      "topic": "Biology - Generative AI",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81992/"
    },
    {
      "description": "Scaling robot automation beyond isolated workcells remains a critical barrier for modern manufacturers, who struggle to coordinate diverse machines in complex, ever-changing production environments. This session explores how leading robotics innovators—IdealWorks, Kuka, Universal Robots, Vention, and Wandelbots—are breaking through these silos by integrating NVIDIA Omniverse libraries, open frameworks, and CUDA-accelerated tools. Discover how foundation AI models and simulation are accelerating robot development, streamlining digital twin integration, and enabling seamless coordination between heterogeneous robots across both high-mix/low-volume and scaled production settings.",
      "format": "In-Person",
      "industry": "Manufacturing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Integration of Omniverse libraries into the partner software",
        "How each partner is accelerating and transforming industrial robotics workflows",
        "How their solutions are being deployed from digital twins to robotics work cells"
      ],
      "nvidia_technology": "Jetson, Isaac",
      "session_id": "S81611",
      "session_type": "Lightning Talk",
      "speakers": [
        {
          "company": "Idealworks",
          "name": "Jimmy Nassif",
          "title": "CTO and Co-Founder"
        },
        {
          "company": "NVIDIA",
          "name": "Brian Klobucher",
          "title": "Omniverse Enterprise Partnerships Lead"
        },
        {
          "company": "Universal Robots",
          "name": "Anders Billesø Beck",
          "title": "VP, AI Robotics Products"
        },
        {
          "company": "Vention",
          "name": "Etienne Lacroix",
          "title": "Founder and CEO"
        },
        {
          "company": "KUKA",
          "name": "Melonee Wise",
          "title": "Chief Product Officer, Software and AI"
        },
        {
          "company": "Wandelbots",
          "name": "Christian Piechnick",
          "title": "CEO"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Beyond the Workcell: Scaling Robotics Workflows Across the Factory Floor",
      "topic": "Robot Manipulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81611/"
    },
    {
      "description": "The gap between a successful \"proof of concept\" and a production-grade AI system is often defined by infrastructure choices. In this panel, technical leaders from leading companies pull back the curtain on how they built their AI stacks using NVIDIA Blackwell GPUs (A4X Max, A4X, G4) on Google Cloud’s AI Hypercomputer. Our panelists will engage in a technical post mortem of their deployment journeys. We will explore: • The Infrastructure: A look at how Google Cloud customers configured their architecture to manage massive GPU clusters • The Use Cases: Deep dives into high-impact implementations, from training sovereign LLMs and high-fidelity digital twins to deploying real-time agentic workflows • The Results: Key outcomes and metrics on both technical performance and business impact",
      "format": "In-Person",
      "industry": "Cloud Services",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Orchestration Best Practices: Best practices on infrastructure configurations to scale massive NVIDIA Blackwell clusters",
        "Operational Performance: Learning how Google’s environment improves model flops utilization and reduces checkpoint/restore times",
        "Production Velocity: Lessons learned on accelerating the transition from single-node testing to multi-rack production using Google’s control plane"
      ],
      "nvidia_technology": "Cloud / Data Center GPU, RTX GPU, Omniverse, Hopper, BioNeMo, Blackwell, NVIDIA AI Enterprise",
      "session_id": "S82244",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Google Cloud",
          "name": "Ruslan Mursalzade",
          "title": "Product Marketing"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Blueprint for AI Scale: How Industry Architects Success With NVIDIA GPUs on Google Cloud (Presented by Google Cloud)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82244/"
    },
    {
      "description": "Drop into these walk-up office hours to meet the experts building NVIDIA’s CUDA-X data science libraries and talk through your end-to-end pipeline needs. ​Get practical guidance on where cuDF, cuML, cuGraph, and GPU-accelerated Spark can boost productivity in your workflow. ​Discuss how to experiment and scale from DGX Spark to the largest supercomputers, and share feedback that can help shape upcoming releases. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "Financial Services",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "Discuss highest productivity accelerators in your data pipeline (cuDF / cuML / cuGraph / Spark).",
        "Scaling considerations across NVIDIA platforms (small to largest)",
        "Feedback about roadmap and upcoming features"
      ],
      "nvidia_technology": "RAPIDS, CUDA-X, cuDF, cuML",
      "session_id": "CWES82212",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Greg Kimball",
          "title": "Software Engineering Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Alexandria Barghi",
          "title": "Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Divye Paresh Gala",
          "title": ""
        },
        {
          "company": "NVIDIA",
          "name": "Vyas Ramasubramani",
          "title": "Sr. Systems Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Bobby Evans",
          "title": "Distinguished Software Engineer"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Boost Data Science Pipelines With Accelerated Libraries",
      "topic": "Data Analytics / Processing",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes82212/"
    },
    {
      "description": "Advances in 3D Gaussian-based reconstruction are transforming simulation for autonomous vehicles and robotics. This interactive session offers an opportunity to connect directly with experts on cutting-edge techniques for indoor and outdoor physical AI simulation. Learn how Gaussian splatting enables real-time rendering of complex, photorealistic environments with greater computational efficiency than traditional 3D reconstruction methods. Whether you’re a researcher, developer, or industry professional, this session provides the technical grounding and networking opportunities needed to apply 3D Gaussian splatting in simulation pipelines that accelerate autonomy development. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "Automotive / Transportation",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn about the latest advances and technologies in 3D Gaussian-based simulation.",
        "Walk through practical workflows to take real-world sensor data into simulation platforms such as Isaac Sim and CARLA.",
        "Connect directly with experts on any questions related to implementing 3D Gaussian-based rendering in your development pipelines."
      ],
      "nvidia_technology": "Isaac, Omniverse",
      "session_id": "CWES81600",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Harel Omer",
          "title": "Sr. Product Manager, Robotics"
        },
        {
          "company": "NVIDIA",
          "name": "Zoe LaLena",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Brent Bartlett",
          "title": "Sr. Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Matthew Cragun",
          "title": "Director of Product, Autonomous Vehicles"
        },
        {
          "company": "NVIDIA",
          "name": "Itai Zadok",
          "title": "Sr. Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Mohamed Hosny",
          "title": "System SW Engineering Manager"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Bridging the Real-to-Sim Gap with 3D Gaussian Splatting",
      "topic": "Robotics Simulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81600/"
    },
    {
      "description": "Join us for a deep dive into how data-intensive workloads can be accelerated using GPUs. This session explores the inner workings of a GPU-accelerated query pipeline that offers excellent performance by leveraging custom kernels and NVIDIA libraries like Thurst and nvCOMP. Learn how data transfer becomes the primary bottleneck, and how faster interconnects like NVLink and GPU-accelerated decompression help mitigate the issue.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "Understand why CPU-based compression and decompression have become a critical bottleneck in modern database and I/O pipelines, even as storage and GPUs continue to accelerate.",
        "Learn how core compression and data-processing operations naturally map to massively parallel GPU architectures, and why GPUs are well-suited for these workloads.",
        "Gain insight into designing GPU-first data pipelines that reduce unnecessary data movement and improve end-to-end throughput."
      ],
      "nvidia_technology": "CUDA, Interconnect Networking, NVLink / NVSwitch, nvCOMP, Blackwell",
      "session_id": "S82203",
      "session_type": "Talk",
      "speakers": [],
      "technical_level": "Technical - Advanced",
      "title": "Build a GPU-Accelerated Database Engine With CUDA",
      "topic": "Databases",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82203/"
    },
    {
      "description": "Designing a modern research cluster is more than racking servers. In this session, we walk through the end-to-end design of a high-performance research cluster using B200 HGX and Spectrum-X, covering NVLink/NVSwitch topologies, intra- and inter-host multi-GPU communication, and how to validate performance with real workloads. We’ll share hard-won lessons from deployment, including observability patterns, common misconfigurations, troubleshooting techniques, and practical tips to keep your cluster both fast and reliable.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Understand B200 HGX network topologies for research cluster designs and learn best practices for inter-host, multi-GPU communication over Spectrum-X.",
        "Apply a repeatable performance-testing approach to validate bandwidth, latency, and scalability.",
        "Diagnose common NVIDIA Collective Communication Library and networking issues before they impact researchers’ workloads."
      ],
      "nvidia_technology": "BlueField DPU, Cloud / Data Center GPU, HGX, Ethernet Networking, Cumulus, NCCL, NVLink / NVSwitch, Blackwell",
      "session_id": "S81731",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Qube Research & Technologies",
          "name": "Chris Turpin",
          "title": "HPC Network Engineer"
        },
        {
          "company": "Qube Research & Technologies",
          "name": "Jerome Vienne",
          "title": "HPC Performance Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Build a High-Performance Research Cluster",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81731/"
    },
    {
      "description": "AI workloads are scaling into full production for training and inference. Standing up these clusters for R&D is challenging on its own, but production brings complexity: stricter security requirements, operational rigor, and a broader user base demanding stability, flexibility, and mature features. The solution is a single, enterprise-grade, software-defined NVL GPU cluster built to dynamically adapt to changing workloads, support rapid deployment, and simplify life-cycle management. In this talk, we’ll examine considerations across the infrastructure, platform, and software layers. We’ll look at network automation, virtualization, and different orchestration platforms. We’ll focus with a concrete example based on our reference architecture, integrating Run:ai, Base Command Manager, vCluster, and Netris to create a secure, scalable, production-ready multi-tenant cluster.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "What are the different layers of consideration when building a multi-tenant GPU cluster?",
        "What are the different security and feature considerations at these different layers of the stack?",
        "How do I make platforms like Kubernetes or NVIDIA Run:ai multi-tenant?",
        "What is an example stack I can deploy to get a fully multi-tenant deployment in my private cloud?"
      ],
      "nvidia_technology": "DGX Platform, NVIDIA Run:ai, Mission Control",
      "session_id": "S81640",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Adam Tetelman",
          "title": "Sr. Manager, DGX Product Architecture"
        },
        {
          "company": "Netris",
          "name": "Alex Saroyan",
          "title": "CEO and Co-Founder"
        },
        {
          "company": "vCluster Labs",
          "name": "Lukas Gentele",
          "title": "CEO & Co-Founder"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Build a Software-Defined Multi-Tenant NVLinked Cluster",
      "topic": "Software-Defined Data Center",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81640/"
    },
    {
      "description": "New vision language models (VLMs) are transforming computer vision with scalable, reasoning zero-shot solutions. Learn about Cosmos Reason — a new open and fully customizable reasoning VLM for physical AI and robotics — that lets robots and vision AI agents reason like humans, using prior knowledge, physics understanding, and common sense to understand and act in the real world. In this lab, you'll build a video analytics AI agent using the NVIDIA Blueprint for Video Search and Summarization (VSS) with Cosmos Reason VLM.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Understand reasoning VLMs for physical AI",
        "Build a project with a video analytics AI agent using NVIDIA VSS Blueprint",
        "Design zero-shot, scalable video workflows for robotics and vision AI"
      ],
      "nvidia_technology": "HGX, DeepStream, TensorRT, Metropolis, TAO Toolkit, NVIDIA AI Enterprise, Cosmos",
      "session_id": "DLIT81774",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Sammy Ochoa",
          "title": "Technical Marketing Engineer"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Build a Video Analytics AI Agent With Vision Language Models",
      "topic": "Video Analytics",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81774/"
    },
    {
      "description": "We'll walk you through how to turn financial services workflows into end‑to‑end AI agents, covering orchestration, tools, retrieval, and guardrails tailored to domain-specific applications in the financial services industry (FSI). Using NVIDIA’s NeMo Agent Toolkit and Blueprints, you'll get hands on to building multi‑agent systems that support use cases such as investment research and agentic commerce. Participants should be comfortable with basic Python and familiar with core AI/ML or LLM concepts so they can follow the code and agent patterns efficiently. Experience working with financial services data or workflows (e.g., banking, insurance, payments) is recommended but not required; the material is designed for technical practitioners such as data scientists, ML engineers, and developers who want to build or own FSI agentic AI solutions.",
      "format": "In-Person",
      "industry": "Financial Services",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "Gain a comprehensive overview of the NVIDIA NeMo Agent Toolkit to orchestrate LLMs that reason, plan, and execute complex financial tasks.",
        "Build domain-specific FSI agents that can reason over complex financial data and (catalog data) using NVIDIA NeMo and Nemotron.",
        "Gain hands-on experience with NVIDIA NeMo Agent Toolkit, including building ReAct-style agents, connecting tools and data sources, and using observability features to inspect latency, tool calls, and behavior.",
        "Get a practical understanding of agentic AI concepts (intent handling, tool use, retrieval-augmented generation, orchestration) and how they map to common FSI workflows such as product recommendation, research, and decision support."
      ],
      "nvidia_technology": "NeMo, NVIDIA NIM, NVIDIA AI Enterprise",
      "session_id": "DLIT81814",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Benjamin Wu",
          "title": "Sr. Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "David Williams",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Flora Huang",
          "title": "Sr. Solutions Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Build Agentic Workflows for Financial Applications",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81814/"
    },
    {
      "description": "Learn how to architect efficient AI factories by eliminating I/O and storage bottlenecks with sustained throughput across AI training and inference. Dive into how WEKA and NVIDIA enable enterprises to build AI factories with reference architectures, featuring WEKA NeuralMesh and NVIDIA accelerated computing and networking, that deliver AI-ready data with NVIDIA AI Data Platform to keep resources fully utilized, simplify operations at scale, and maximize performance, power, and cost efficiencies. Walk away with a practical plan to evolve your organization’s AI infrastructure from initial pilots to secure, resilient, and scalable enterprise deployments.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "A clear mental model and definition of the core building blocks of an AI factory storage solution",
        "The tangible benefits of keeping GPUs continuously fully utilized, and practical ways to eliminate I/O bottlenecks for both AI training and inference workloads",
        "A strong understanding of how accelerator-native systems better leverage the performance and interoperability of the NVIDIA accelerated computing stack",
        "Key factors to consider when deploying WEKA and NVIDIA AI Factory and NVIDIA AI Data Platform solutions at scale across on-premises, cloud, and hybrid environments, while maintaining security, governance, and predictable performance for production workloads",
        "The latest performance benchmarks (IOps, $/throughput, $/watt/iop, metadata, latency, tokenization) and customer use cases tied to real outcomes, including higher GPU utilization, faster pipelines, reduced cost, and better economics"
      ],
      "nvidia_technology": "BlueField DPU, Cloud / Data Center GPU, Grace CPU, RTX GPU, HGX, TensorRT, Infiniband Networking, Hopper, Magnum IO, CUDA-X, Interconnect Networking, NVIDIA NIM, Blackwell, cuVS, DGX Cloud, Dynamo, NIXL",
      "session_id": "EX82133",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "WEKA",
          "name": "Shimon Ben David",
          "title": "CTO"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Build AI Factories for Always-On Throughput and Maximum Cost Efficiency (Presented by WEKA)",
      "topic": "DataOps",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82133/"
    },
    {
      "description": "This session reveals how YTL partnered with Wiwynn and NVIDIA to build an AI-ready data center. We’ll walk through the NVIS deployment methodology—covering site qualification, high-density GPU rack integration, and power/cooling validation under real-world stress. You’ll also learn how to implement end-to-end observability and manage liquid-cooling risks. Finally, we share benchmarking results using MLPerf and AI workloads to confirm the environment meets AI-ready standards.",
      "format": "In-Person",
      "industry": "Cloud Services",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Step-by-Step Checklist: Build or upgrade your data center with NVIS guideline for infrastructure integration, focusing on stable power and water delivery, optimized pod layout, and successful rack-level L11/L12 acceptance.",
        "Mission Control in Action: Implement cluster-level and workload-level management via Slurm/K8s integration, extensive observability, and Wiwynn UMS-based leak detection directly integrated with DC operations.",
        "Performance Benchmarking: Use NeMo Megatron and MLPerf Training to rigorously test and validate scalability, efficiency, and power consumption of your AI clusters.",
        "Reusable Blueprint: A proven framework for operators and solution providers to build high-performing AI factories."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, NeMo, NVLink / NVSwitch, Mission Control, Nemotron",
      "session_id": "S82034",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Wiwynn Corp.",
          "name": "Ted Pang",
          "title": "Sr. Director"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Build an AI-Ready Data Center: Practical Insights From YTL, Wiwynn, and NVIDIA (Presented by Wistron Corp.)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82034/"
    },
    {
      "description": "This session unveils a groundbreaking initiative to build an open, extensible AI platform specifically for surgery robots. This effort introduces a collaborative, open platform built on NVIDIA’s AI infrastructure, leveraging Isaac for Healthcare for photorealistic surgical simulation. It establishes the first unified framework tailored to surgery robotics, featuring modular robot control, multi-modal perception (vision and kinematics), and vision-language-action (VLA) pipelines—all designed for real-world deployment on accelerated computing platforms. Researchers worldwide can now generate high-fidelity synthetic datasets, train policies via imitation and reinforcement learning, and validate algorithms against shared benchmarks enabling reproducible, scalable research in open surgery automation for the first time.",
      "format": "In-Person",
      "industry": "Healthcare & Life Sciences",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "An open-source AI platform dedicated to surgery robotics—addressing a long-standing void in accessible systems, standardized tools, and research infrastructure for this complex surgical domain.",
        "Enables Data-Driven Innovation: The platform creates a high-fidelity simulation-to-reality pipeline to generate synthetic surgical datasets, overcoming the severe scarcity of real-world data that has hindered AI progress in surgery.",
        "Builds a Collaborative Ecosystem: Designed with modularity and extensibility, the platform invites global researchers to co-develop, benchmark, and validate algorithms—fostering an open, reproducible, and inclusive foundation for the future of autonomous surgical intelligence."
      ],
      "nvidia_technology": "Clara, Isaac, Clara Holoscan, MONAI, Cosmos",
      "session_id": "S81512",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Medbot",
          "name": "Weiping Liu",
          "title": "Sr. Director"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Build an Open Surgery Robot AI Platform With NVIDIA Frameworks",
      "topic": "Robot Manipulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81512/"
    },
    {
      "description": "As artificial intelligence evolves from passive tools to active collaborators, a new paradigm is emerging: agentic AI. These autonomous, goal-driven systems—known as AI agents—are designed not just to answer questions, but to plan, act, and adapt in pursuit of creative objectives. In this session, we’ll demystify what AI agents are, how they work, and why they matter for artists, designers, storytellers, and technologists. From intelligent production assistants that automate repetitive tasks to co-creative agents, this talk explores real-world use cases across film, animation, gaming, and immersive media. We’ll also look under the hood at the frameworks used to build these agents and discuss how to keep humans in the loop—preserving creative intent and artistic control.",
      "format": "In-Person",
      "industry": "Media & Entertainment",
      "intended_audience": "Artist / Designer",
      "key_takeaways": [
        "Learn how agentic systems are being used in real creative pipelines, from intelligent production assistants to co-creative collaborators.",
        "Learn about some of the latest tools for building and deploying AI agents in media & entertainment.",
        "Understand how to structure agent workflows that plan, act, and adapt while remaining aligned with creative goals."
      ],
      "nvidia_technology": "RTX GPU, Blackwell, DGX Spark, DGX Station",
      "session_id": "S82144",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Rick Champagne",
          "title": "Director of Global Media & Entertainment Marketing Strategy"
        },
        {
          "company": "NVIDIA",
          "name": "Ashlee Martino-Tarr",
          "title": "3D Workflow Specialist"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Build and Deploy AI Models and Agents for Media & Entertainment",
      "topic": "Video Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82144/"
    },
    {
      "description": "We'll walk you through the full life cycle of a RAG pipeline: data ingestion, chunking and embedding, retrieval orchestration, and response generation, emphasizing robustness techniques such as hybrid retrieval, fallback strategies, caching, and constraint-aware prompting, along with patterns for monitoring quality and detecting drift over time. You'll also explore scaling concerns, including index sharding, cost-aware architecture choices, and aligning RAG design with product requirements and service-level agreements. By the end, you'll have implemented and evaluated a RAG pipeline that balances answer quality, latency, and reliability, providing a template for deploying similar systems in production.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Understand core RAG building blocks (chunking, embeddings, indexing, retrieval, generation) and how design choices in each stage affect relevance, latency, and cost.",
        "Learn practical robustness patterns — such as hybrid retrieval, fallback logic, and evaluation loops — to make RAG systems more reliable under noisy, incomplete, or evolving data.",
        "Gain hands-on experience implementing and instrumenting a scalable RAG pipeline that can be monitored, tuned, and extended for real-world production use cases."
      ],
      "nvidia_technology": "NVIDIA NIM, NVIDIA AI Enterprise, cuVS",
      "session_id": "DLIT82006",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Ruchika Kharwar",
          "title": "Product Manager"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Build Cost-Effective, Scalable RAG Pipelines: From Ingestion to Response Generation",
      "topic": "Retrieval-Augmented Generation (RAG)",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit82006/"
    },
    {
      "description": "Learn how to design, train, and deploy large-scale custom generative AI models. This session will step you through data preparation, fine-tuning, and infrastructure management, and will highlight best practices for scaling large models efficiently. You’ll also see real-world examples with Adobe Firefly Foundry and NVIDIA NeMo integration that demonstrate how teams are applying these techniques to production-grade AI systems.",
      "format": "In-Person",
      "industry": "Media & Entertainment",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Data Preparation: Discover how customer images, video, and 3D assets are analyzed using vision language models to extract unique world context and specific visual style.",
        "Fine-Tuning: Learn how very large generative AI models are fine-tuned at scale. enabling distributed training across tens of thousands of customer assets, using NVIDIA's technology stack (including CUDA and NCCL).",
        "Infrastructure Management: Understand how AI infrastructure is built to serve large and varied customer bases, each with dozens of personalized models deployed concurrently, while maintaining secure isolation, performance efficiency, and production reliability."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, NeMo, NVIDIA AI Enterprise",
      "session_id": "S82197",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Adobe",
          "name": "Ely Greenfield",
          "title": "CTO"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Build Custom Large-Scale Generative AI Models",
      "topic": "Video Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82197/"
    },
    {
      "description": "Developing effective AI agents requires more than a powerful model — it demands the ability to use the tools, data, and systems that drive real-world workflows. Learn how to create a production-ready agent for a custom use case using NVIDIA NeMo powered by Nemotron models. You'll deploy NeMo modules, customize and evaluate model behavior, connect to data and tools, and apply guardrails for reliable operation. A central focus is enabling the agent to work fluidly with an existing data and tooling ecosystem — integrating APIs, function calls, and the Model Context Protocol (MCP) to accomplish meaningful tasks end to end. We'll also demonstrate a data flywheel that captures interactions to boost accuracy, reliability, latency, and cost over time. You'll build a complete system that you can extend in enterprise or personal environments. A general understanding of LLMs or agent workflows or Generative AI concepts (no deep ML background required) Familiarity with REST APIs or function-calling concepts is helpful but not mandatory Optional but beneficial: Exposure to NVIDIA NeMo, containerized development (Docker), or cloud workflows No prior experience with Nemotron models or NeMo is required.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn the core patterns for architecting effective AI agents: design agent workflows that combine model reasoning with real tool usage.",
        "Build tool-enabled intelligence using multiple integration methods: wire agents into diverse tooling ecosystems including APIs, function calls, and MCP.",
        "Master the NeMo suite of tools across model customization, guardrails, evaluation, deployment, and orchestration services to shape reliable, domain-specific agent behavior.",
        "Get hold of the data flywheel paradigm to build continuously improving AI agents by capturing interaction and system data to iteratively improve accuracy, latency, and cost in a repeatable production loop."
      ],
      "nvidia_technology": "NeMo, NVIDIA NIM",
      "session_id": "DLIT81550",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Shashank Verma",
          "title": "Sr. Technical Marketing Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Yang Yu",
          "title": "Solutions architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Build Domain AI Agents That Seamlessly Tap Into Your Existing Tools and Data With NVIDIA Nemotron and NeMo",
      "topic": "Training AI Models",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81550/"
    },
    {
      "description": "This comprehensive hands-on workshop demonstrates how to build production-ready medical AI workflows using NVIDIA Clara's latest open models and tools. You'll gain practical experience with NV-Segment, NV-Generate, NV-Reason, and MONAI Label to create powerful medical imaging applications.",
      "format": "In-Person",
      "industry": "Healthcare & Life Sciences",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Deploy segmentation models for real-time use.",
        "Implement AI-assisted annotation workflows for efficient dataset creation.",
        "Generate synthetic medical data to augment training datasets.",
        "Build explainable AI systems with chain-of-thought medical reasoning.",
        "Integrate visual-language models for comprehensive image understanding."
      ],
      "nvidia_technology": "MONAI, NVIDIA NIM",
      "session_id": "DLIT81609",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Ahmed Harouni",
          "title": "Technical Marketing Engineer"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Build End-to-End Medical AI Workflows With NVIDIA Clara Open Models",
      "topic": "AI Inference",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81609/"
    },
    {
      "description": "Connect with experts on NVIDIA Clara Open Models enabling end-to-end medical AI workflows — from real-time segmentation and synthetic data generation to explainable clinical reasoning and healthcare robotics. Explore deployment strategies across imaging, surgical simulation, and autonomous systems. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "Healthcare & Life Sciences",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Deploy segmentation models for real-time inference in medical imaging workflows.",
        "Augment training datasets with generative models spanning radiology and surgical video.",
        "Build explainable AI systems using chain-of-thought reasoning for medical insights.",
        "Accelerate healthcare robotics development with pre-trained policies for autonomous systems."
      ],
      "nvidia_technology": "CUDA, TensorRT, Clara, Isaac, Clara Holoscan, IGX, MONAI, Blackwell, Cosmos, DGX Spark",
      "session_id": "CWES81655",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Sean Huver",
          "title": "Sr. Manager and Principal ML Engineer"
        },
        {
          "company": "NVIDIA GmbH",
          "name": "Maximilian Ofir",
          "title": "Technical Marketing Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Daguang Xu",
          "title": "Sr. Research Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Pengfei Guo",
          "title": "Applied Research Scientist"
        },
        {
          "company": "NVIDIA",
          "name": "Andriy Myronenko",
          "title": "Sr. Research Scientist"
        },
        {
          "company": "NVIDIA",
          "name": "Andres Diaz-Pinto",
          "title": "Sr. AI Scientist"
        },
        {
          "company": "NVIDIA",
          "name": "Ahmed Harouni",
          "title": "Technical Marketing Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Nigel Nelson",
          "title": "Machine Learning Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Michael Zephyr",
          "title": "Technical Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Walter Simson",
          "title": "Machine Learning Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Build End-to-End Medical AI Workflows With NVIDIA Clara Open Models",
      "topic": "Pre-Trained / Foundation Models",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81655/"
    },
    {
      "description": "As AI agents evolve from simple task executors to adaptive systems capable of reasoning and decision-making, efficient orchestration of compute and model execution becomes essential. In this session, two NVIDIA engineers will walk through the latest frameworks and techniques for accelerating AI agents—from optimizing memory and compute pipelines to managing multi-stage reasoning workflows. Learn how to design high-performance agent architectures that can dynamically plan, retrieve, and act in real time while scaling seamlessly across GPUs.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how to design high-performance agent architectures that can dynamically plan, retrieve, and act in real time, while scaling seamlessly across GPUs",
        "Discover practical methods to maximize throughput and reduce latency with live demos, and learn from performance insights.",
        "Learn how to bring advanced agentic intelligence closer to production readiness."
      ],
      "nvidia_technology": "NeMo, NVIDIA NIM, NVIDIA AI Enterprise, Nemotron",
      "session_id": "S82237",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Michael Demoret",
          "title": "Engineering Manager, Cybersecurity AI"
        },
        {
          "company": "NVIDIA",
          "name": "Kyle Kranen",
          "title": "Team Lead/Manager - Deep Learning Algorithms"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Build Faster, Smarter AI Agents: Techniques for Scalable Reasoning and Real-Time Efficiency",
      "topic": "Retrieval-Augmented Generation (RAG)",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82237/"
    },
    {
      "description": "Learn how to build resilient distributed training pipelines using NVIDIA's Resiliency Extension (NVRx) on clouds like AWS so your long-running jobs automatically restart from fast, asynchronous checkpoints. We'll dive deep into practical integration patterns with PyTorch-based frameworks and show how you can simplify operations while confidently scaling to multi-node, multi-GPU clusters.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Continuous Training, Zero Downtime: Discover how automated restarts in NVRx keeps distributed jobs running reliably—even through node or process failures.",
        "End-to-end Resiliency Stack: Understand how NVRx pairs with AWS to deliver end-to-end recovery including automatic instance replacement, monitoring, and orchestration for cloud-scale AI training.",
        "Developer-Friendly Integration: Learn how to add resiliency to your existing PyTorch based Deep Learning framework codebase with minimal changes, using open-source APIs.",
        "Operational Simplicity at Scale: See how built-in automation replaces manual recovery scripts and complex DevOps workflows, freeing teams to focus on experimentation and model quality."
      ],
      "nvidia_technology": "NeMo",
      "session_id": "S81747",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Shreya Gupta",
          "title": "Solutions Architect"
        },
        {
          "company": "Amazon Web Services",
          "name": "Aravind Neelakantan",
          "title": "Specialist Solutions Architect"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Build Fault-Tolerant Distributed AI Training at Scale",
      "topic": "Training AI Models",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81747/"
    },
    {
      "description": "To meet the growing demand for power of AI, we have to enable the entire ecosystem, from chip to grid. This lightning talk brings together how NVIDIA is working with its partners from power generation and transmission to distribution to build flexible and reliable infrastructure. This session will focus on use of AI for meeting energy demand, using digital technologies inside data centers to manage peak loads, and applying accelerated computing to simulate the grid and build physical infrastructure.",
      "format": "In-Person",
      "industry": "Energy",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Enabling hundreds of gigawatts of power using grid flexibility",
        "Using AI-based simulations to run millions to scenarios to manage grid peak load scenrios",
        "Applying AI to build the physical infrastructure faster",
        "Applying smart meters at the far edge to better understand the distributed energy resources."
      ],
      "nvidia_technology": "RTX GPU, Morpheus, Omniverse, RAPIDS, Metropolis, Modulus, CUDA-X, cuML, cuOPT, Omniverse Replicator, NVIDIA NIM, NVIDIA AI Enterprise, NVIDIA Run:ai",
      "session_id": "S81685",
      "session_type": "Lightning Talk",
      "speakers": [
        {
          "company": "Emerald AI",
          "name": "Varun Sivaram",
          "title": "CEO and Founder"
        },
        {
          "company": "ThinkLabs.ai",
          "name": "Josh Wong",
          "title": "Founder and CEO"
        },
        {
          "company": "NVIDIA",
          "name": "Ahsan Yousufzai",
          "title": "Global Head Energy Surface"
        },
        {
          "company": "ITRON",
          "name": "Raj Vaswani",
          "title": "Fellow and Strategic advisor"
        }
      ],
      "technical_level": "General Interest",
      "title": "Build Flexible and Secure Power Infrastructure for a Sustainable Future at Scale",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81685/"
    },
    {
      "description": "The next generation of AI models is driving unprecedented compute consumption, exposing the limitations of traditional system architectures. Enterprises are now evolving data center designs to support new rack-scale architectures for modern inference, as well as large-scale training optimized for these rapidly escalating demands. Join this session to learn how NVIDIA is systemizing the new NVIDIA Vera Rubin architecture to help customers rapidly deploy gigascale AI factories.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Move beyond do-it-yourself platforms to pre-engineered, rack-scale systems to overcome performance bottlenecks and de-risk deployment.",
        "Deploy a full-stack, optimized gigascale AI factory designed for the next generation of AI models and inference at scale.",
        "Lower total cost of ownership and boost performance-per-watt with DGX systems powered by the Vera Rubin architecture."
      ],
      "nvidia_technology": "DGX Platform",
      "session_id": "S81793",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Charlie Boyle",
          "title": "VP, DGX Systems"
        },
        {
          "company": "Walmart Global Tech",
          "name": "Aditya Kumarakrishnan",
          "title": "Technical Fellow"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Build Gigascale AI Factories With Next-Generation Rack-Scale Systems",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81793/"
    },
    {
      "description": "Discover how financial institutions can accelerate real-time decision-making with high-performance AI systems built on NVIDIA’s Grace Hopper Superchip, H200 NVL, and RTX 6000 Blackwell GPUs. This session showcases breakthrough performance in time series forecasting, real-time inference, and retrieval-augmented generation for financial applications. Learn how these platforms are setting new standards in latency, throughput, and efficiency, enabling engineers to design scalable AI solutions that power trading, risk modeling, and market analytics at unprecedented speed and scale.",
      "format": "In-Person",
      "industry": "Financial Services",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how to achieve microsecond latency and high throughput for time series forecasting, real-time inference, and retrieval-augmented generation workloads in financial AI.",
        "Understand key engineering practices and optimization strategies for building energy-efficient, scalable AI systems that handle diverse financial data at production scale.",
        "Apply insights from industry-standard STAC-ML and STAC-AI benchmarks to design next-generation AI pipelines for trading, risk modeling, and market intelligence."
      ],
      "nvidia_technology": "Grace CPU, RTX GPU, Hopper, Blackwell",
      "session_id": "S81623",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Martin Marciniszyn Mehringer",
          "title": "Sr. AI Developer Technology Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Nikolai Markovskii",
          "title": "Sr. AI Developer Technology Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Build High-Performance Financial AI: Achieve Microsecond Latency and Scalable LLM Inference",
      "topic": "AI Inference",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81623/"
    },
    {
      "description": "Want to learn how to get started building industrial facility digital twins with NVIDIA Omniverse libraries and OpenUSD? Join this interactive support session with NVIDIA technical product managers and solutions architects to get direct help and surface your questions, issues, and ideas specific to your use case. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "Manufacturing",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Get hands-on help starting industrial digital twins with NVIDIA Omniverse and OpenUSD.",
        "Ask NVIDIA product managers and solutions architects questions specific to your use case.",
        "Learn how to use NVIDIA Omniverse libraries and OpenUSD to develop realistic, operations-focused digital twins for industrial facilities."
      ],
      "nvidia_technology": "Omniverse, OVX",
      "session_id": "CWES81472",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Mike Geyer",
          "title": "Omniverse Manufacturing Product Management Lead"
        },
        {
          "company": "NVIDIA",
          "name": "James McKenna",
          "title": "Product Manager, NVIDIA Omniverse"
        },
        {
          "company": "NVIDIA",
          "name": "Adam Brasic",
          "title": "Sr. Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Teresa Conceicao",
          "title": "Solutions Architect Manager - Omniverse"
        },
        {
          "company": "NVIDIA",
          "name": "Florian Gantert",
          "title": "Global Business Development and Strategy for Omniverse"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Build Industrial Digital Twins for the Era of Physical AI",
      "topic": "Industrial Digitalization / Digital Twin",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81472/"
    },
    {
      "description": "This technical session distills lessons from multiple large-scale deployments—unifying recall and ranking with end-to-end generative recommenders, scaling ads/rec/search with generative retrieval, and pushing inference/training efficiency with NVIDIA-accelerated stacks—to show how to deliver measurable business lift with industrial-grade performance and cost efficiency.",
      "format": "In-Person",
      "industry": "Consumer Internet",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Show how end-to-end generative models for recommendation and search (e.g., ads, feeds, etc.) deliver proven business value at 100 million-plus user scale, breaking through the performance ceiling of traditional ad and feed recommenders with measurable lifts in core engagement metrics.",
        "Explain unified optimization principles—custom cross-attention kernels, GPU memory and communication optimization, and distributed sequence training systems—that unlock multi-x gains in training speed, inference throughput, and latency for industrial workloads.",
        "Share practical lessons on scenario-specific tuning and system design, including dynamic padding removal, constrained decoding, and resource-efficient distributed training to achieve “ultimate performance” in ads, rec, and search."
      ],
      "nvidia_technology": "CUDA, TensorRT, Merlin, cuBLAS, CUDA-X, cuDDN, cuDF, Multi-Instance GPU (MIG), NCCL, NSight Comute, NSight Systems, NVLink / NVSwitch",
      "session_id": "S81689",
      "session_type": "Tutorial",
      "speakers": [
        {
          "company": "Baidu",
          "name": "Xuewu Jiao",
          "title": "Principal Architect"
        },
        {
          "company": "Kuaishou Technology",
          "name": "Xiao Liang",
          "title": "Sr. Director of Recommendation Infrastructure"
        },
        {
          "company": "Meta Platforms",
          "name": "Yongxiong Ren",
          "title": "Software Engineer"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Build Industrial-Scale Generative Recommenders with GPU-Optimized Retrieval and Training",
      "topic": "Recommenders / Personalization",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81689/"
    },
    {
      "description": "Financial transactions share key properties with language: temporal ordering, contextual dependencies, and patterns emerging from surrounding context. This session presents a framework for building transaction foundation models using masked prediction and next-item forecasting to learn rich representations of customer behavior without labeled data. We'll cover our end-to-end approach—from tokenizing transactions into sequences to self-supervised objectives capturing spending rhythms, cross-merchant correlations, and anomaly patterns. Learn how this architecture drives improvements across fraud detection, credit risk, and behavior prediction while drastically reducing labeled data requirements.",
      "format": "In-Person",
      "industry": "Financial Services",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "Understand the end-to-end workflow and design considerations for building transaction foundation models using transformer architectures.",
        "Learn how self-supervised pre-training on transaction sequences enables powerful transfer learning across fraud detection, credit risk modeling, and customer analytics.",
        "Gain practical insights into tokenization strategies for financial data and training objectives like masked attribute reconstruction and next-transaction prediction.",
        "Discover how to fine-tune foundation models for downstream tasks fraud detection, credit scoring, and customer profiling with minimal labeled data."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, CUDA, Blackwell",
      "session_id": "S81618",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Revolut",
          "name": "Pavel Nesterov",
          "title": "Executive Director (Head of Data Science Function and AI Department)"
        },
        {
          "company": "Revolut",
          "name": "Anton Repushko",
          "title": "Sr. Python Engineer (Head of ML Research)"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Build Next-Generation Foundation Models in Finance",
      "topic": "Pre-Trained / Foundation Models",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81618/"
    },
    {
      "description": "As AI factories continue to evolve, cybersecurity must protect infrastructure, applications, and emerging agentic workflows without consuming critical compute resources or impacting tokenomics. The NVIDIA DOCA software platform addresses these challenges with a suite of security microservices, frameworks, and APIs that advance protection without compromising performance. See how DOCA delivers accelerated, programmable runtime security for AI infrastructure. Powered by NVIDIA BlueField, DOCA enforces security at line rate without impacting computing performance, installing host-based software, or integrating directly into protected workloads. Learn how to apply granular network security policies at line rate, detect and block threats to AI workloads in real time, implement prompt-level defenses, and secure agentic workflows, the new frontier of AI security.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "See how DOCA secures AI infrastructure, workloads, prompts, and agentic workflows without drawing on compute resources or affecting token economics.",
        "Learn to enforce granular, distributed security and real-time threat prevention using DOCA’s accelerated capabilities on NVIDIA BlueField."
      ],
      "nvidia_technology": "BlueField DPU, DOCA",
      "session_id": "S81625",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Ariel Kit",
          "title": "Director, Product Management"
        },
        {
          "company": "NVIDIA",
          "name": "Ofir Arkin",
          "title": "Sr. Distinguished Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Build Purpose-Built AI Security With DOCA",
      "topic": "Networking Security",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81625/"
    },
    {
      "description": "In this hands-on lab, Lightwheel, a leading provider of simulation-ready assets, platforms, and synthetic data for embodied AI, guides you through the complete SimReady asset preparation workflow. You’ll learn to generate high-fidelity synthetic data using OpenUSD, configure accurate physics properties (mass, friction, collision geometry) with PhysX and Newton, and add semantic metadata along with manipulation affordances. You’ll validate your work by deploying trained robot agents inside NVIDIA Isaac Sim. By the end of the lab, you’ll know how to build simulation-ready scenes that support real-to-sim calibration and sim-to-real transfer, enabling robots to learn robustly in simulation and perform reliably in the real world. Basic familiarity with 3D content concepts (meshes, materials, transforms) and modeling software Understanding of physics simulation fundamentals (mass, friction, collision) Familiarity with robotics terminology (end-effector, gripper, kinematics) Familiarity with Isaac Sim (recommended course: Getting Started With Isaac Sim).",
      "format": "In-Person",
      "industry": "Manufacturing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Analyze existing 3D assets and identify required physics properties, semantic information, and simulation-critical metadata required for robot training.",
        "Configure physics properties (rigid body dynamics, collision geometry, friction, mass distribution) to ensure physically accurate simulation behavior.",
        "Apply semantic tagging and metadata to assets, enabling robots to understand object categories, manipulation affordances, and contextual constraints.",
        "Validate robot-ready assets in NVIDIA Isaac Sim by deploying trained robot policies and observing successful interactions.",
        "Explain how physics accuracy in simulation directly impacts robot learning performance and real-world transfer success."
      ],
      "nvidia_technology": "Isaac, Omniverse, PhysX",
      "session_id": "DLIT81816",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "Lightwheel",
          "name": "Frank Chen",
          "title": "Director of Engineering"
        },
        {
          "company": "Lightwheel",
          "name": "Siyi Lin",
          "title": "Technical Artist Lead"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Build Robot-Ready Assets for Physically Accurate Simulations With Lightwheel",
      "topic": "Industrial Digitalization / Digital Twin",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81816/"
    },
    {
      "description": "Next-generation robots need the ability to sense, plan, and act with full autonomy. A simulation-first approach is key to building robots that are adaptable and efficient — supporting rapid development of robust policies and smooth transfer to new tasks across robot embodiments. Connect with NVIDIA experts to learn how NVIDIA Isaac Sim and Isaac Lab can accelerate your robotics pipeline. Explore how to generate synthetic data, train advanced robot policies, and run software-in-the-loop testing to validate your entire stack and close the gap between simulation and real-world deployment. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how to accelerate autonomous robotics by adopting a simulation-first approach that enables rapid policy development and smooth task adaptation."
      ],
      "nvidia_technology": "Isaac",
      "session_id": "CWES81568",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Teresa Conceicao",
          "title": "Solutions Architect Manager - Omniverse"
        },
        {
          "company": "NVIDIA",
          "name": "Edith Llontop",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Akul Santhosh",
          "title": "Solution Architect, Robotics"
        },
        {
          "company": "NVIDIA",
          "name": "Aravindh Shanmuganathan",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Kartik Sachdev",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Kelly Guo",
          "title": "Engineering Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Gavriel State",
          "title": "Sr. Director - Simulation and AI"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Build Smarter Robots Using NVIDIA Isaac Sim and Isaac Lab",
      "topic": "Robotics Simulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81568/"
    },
    {
      "description": "Gain hands-on experience in setting up simulation environments, generating and converting synthetic medical data, modifying scenes and assets, collecting and curating datasets via state-machine demonstrations, fine-tuning the GR00T N1 Vision-Language-Action (VLA) model, and testing robotic ultrasound tasks in simulation. We emphasize safe, efficient AI model development and deployment for healthcare applications. General understanding of deep learning, and vision language action models.",
      "format": "In-Person",
      "industry": "Healthcare & Life Sciences",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Modify a robotic arm to hold a medical or surgical tool.",
        "Generate synthetic patient data using MAISI CT.",
        "Fine-tune GR00T N1, a vison-language-action model.",
        "Deploy GR00T N1 to autonomously perform a medical procedure."
      ],
      "nvidia_technology": "RTX GPU, AGX, Clara, Isaac, Clara Holoscan, MONAI, Cosmos",
      "session_id": "DLIT81541",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA GmbH",
          "name": "Maximilian Ofir",
          "title": "Technical Marketing Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Build Surgical and Medical Robotics With NVIDIA Isaac for Healthcare: From Simulation to Deployment",
      "topic": "Robotics Simulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81541/"
    },
    {
      "description": "AI-driven workloads are redefining data center design, challenging infrastructure developers to deliver higher performance, stronger security, and greater networking, storage, and compute efficiency. This session covers how the latest capabilities and announcements in the NVIDIA DOCA software platform power the next generation of AI clouds and factories. It introduces the latest DOCA advancements that power massive-scale AI factories and cloud infrastructure. Gain insight into the DOCA software stack, APIs, microservices, and developer tools that enable accelerated, programmable infrastructure. Learn how NVIDIA DOCA’s unified capabilities deliver breakthrough networking performance while maintaining backward compatibility, and explore how to apply new capabilities and best practices to transform AI networking, storage, and security architectures.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Understand the core concepts, components, and architecture of the DOCA software platform for accelerated infrastructure development.",
        "Identify practical DOCA use cases across AI networking, storage, and security.",
        "Gain actionable guidance to access and use DOCA microservices, reference applications, and frameworks."
      ],
      "nvidia_technology": "BlueField DPU, DOCA",
      "session_id": "S81750",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Ariel Kit",
          "title": "Director, Product Management"
        },
        {
          "company": "NVIDIA",
          "name": "Yuval Degani",
          "title": "Sr. Director of Hyperscale Engineering"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Build the Future of AI Infrastructure With NVIDIA DOCA",
      "topic": "AI Networking, 5G/6G",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81750/"
    },
    {
      "description": "Open-source AI is essential for trust, learning, and discovery in education and research, but only if we can inspect, reproduce, and improve how models are made. In this session, Professor Hanna Hajishirzi will present OLMo 3: a fully open language and “thinking” model, plus complete model flows—the full life cycle of checkpoints, data, and dependencies—enabling customization and scientific inspection beyond final weights. Professor Percy Liang will introduce Marin, an open lab for frontier AI where experiments happen in public, with community review and contributions via GitHub, advancing reproducibility, benchmarking, and data-efficient training. Together, these approaches outline a path to verifiable, improvable AI systems that broaden access and accelerate open scientific progress.",
      "format": "In-Person",
      "industry": "Academia / Higher Education",
      "intended_audience": "Research: Academic",
      "key_takeaways": [
        "Stronger Science via Reproducibility: Sharing data, checkpoints, and experiments enables independent replication, fair comparisons, and cumulative progress rather than one-off results.",
        "Democratized Access and Capability Building: Open models and artifacts let educators, students, and researchers learn from, adapt, and extend frontier systems without relying on closed vendors.",
        "Public Trust Through Verifiability: Openness makes it possible to audit how models are built, identify failure modes, and substantiate claims—critical for responsible use in classrooms, labs, and society.",
        "Safer, Faster Improvement Loops: Community evaluation surfaces biases, security risks, and robustness gaps earlier, accelerating fixes and best practices."
      ],
      "nvidia_technology": "NeMo, Blackwell, NVIDIA AI Enterprise",
      "session_id": "S81835",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Co-Founder, Together AI",
          "name": "Percy Liang",
          "title": "Associate Professor of Computer Science, Stanford University"
        },
        {
          "company": "University of Washington",
          "name": "Hanna Hajishirzi",
          "title": "Professor"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Build Trust and Discovery Through Open‑Source AI in Research",
      "topic": "Large Language Models (LLMs)",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81835/"
    },
    {
      "description": "Effective AI deployment requires more than powerful models—it demands end-to-end governance that spans your entire AI ecosystem. In this session, discover how ServiceNow and NVIDIA collaborate across the full AI stack, from model development and agent deployment to comprehensive governance frameworks. Learn how ServiceNow's AI governance capabilities provide the critical connective tissue connecting your AI investments, enabling you to leverage AI effectively, efficiently, and responsibly. We'll explore industry-specific implementations in financial services, healthcare, technology, and beyond, demonstrating how tailored governance approaches address unique regulatory requirements while accelerating innovation. Built on NVIDIA infrastructure and AI software, this solution ensures your AI initiatives deliver business value while maintaining trust, compliance, and control.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Discover how ServiceNow and NVIDIA collaborate across the full AI stack, from model development and agent deployment to comprehensive governance frameworks.",
        "Learn how ServiceNow's AI governance capabilities provide the critical connective tissue connecting your AI investments, enabling you to leverage AI effectively, efficiently, and responsibly across your organization."
      ],
      "nvidia_technology": "",
      "session_id": "S82114",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "ServiceNow",
          "name": "Dorit Zilbershot",
          "title": "Group VP of AI Experiences and Innovation"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Build Trust at Scale: Governing AI Across Your Enterprise (Presented by ServiceNow)",
      "topic": "Code / Software Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82114/"
    },
    {
      "description": "The AI PC ecosystem is exploding. Developers are now running local, high-performance AI workloads. This technical session dives into how NVIDIA accelerates the top open-source software stack—from the different quantization techniques, training recipes, as well as inference frameworks and tools like Ollama, ComfyUI and custom pipelines. Crucially, we will move beyond simple inference to address the architecture of reliable, local agentic workflows. Join us to learn about the technical intricacies and considerations to develop robust, local AI workflows on NVIDIA RTX.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Overview of different quantization schemes for generative AI models",
        "Maximizing inferencing performance on RTX AI PC with top tools and frameworks",
        "Building generative AI workflows for Agentic AI and Creator Workflows"
      ],
      "nvidia_technology": "RTX GPU, DGX Spark, Nemotron",
      "session_id": "S82128",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Ollama",
          "name": "Parth Sareen",
          "title": "Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Annamalai Chockalingam",
          "title": "Sr. Product Manager - GeForce"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Build, Optimize, Run: The Developer's Guide to Local Gen AI on NVIDIA RTX AI PCs",
      "topic": "Performance Optimization",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82128/"
    },
    {
      "description": "Just like how humans have multiple senses to perceive the world around them, computers have a variety of sensors to help perceive the human world. In the health industry, computed tomography (CT) scans provide a 3D representation used to detect potentially dangerous abnormalities. In the robotics industry, lidars are used to help robots see depth and navigate the complex topology around them. In this course, learners will develop neural network based multimodal models that can understand many different data types by exploring different fusion techniques. Certificate: Upon successful completion of the assessment, you’ll receive an NVIDIA certificate to recognize your subject matter competency and support your professional career growth. Basic understanding of Python (including classes, objects, and decorators) Basic understanding of Neural Networks such image convolution and sequential models (covered in Fundamentals of Deep Learning)",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Different data types and how to make them neural network ready.",
        "Model fusion, and the differences between early, late, and intermediate fusion",
        "Structure loss and how to avoid it",
        "The difference between modality and agent orchestration",
        "Applications of NVIDIA AI Blueprints for Visual AI Agents with VSS"
      ],
      "nvidia_technology": "NVIDIA NIM",
      "session_id": "DLIW82269",
      "session_type": "Full-Day Workshop",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Mark Moyou",
          "title": "Sr. Data Scientist"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Building AI Agents with Multimodal Models",
      "topic": "Image / Video Detection & Recognition",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dliw82269/"
    },
    {
      "description": "As AI systems move from experimentation to operational deployment, organizations in aerospace, energy, manufacturing, and the public sector face a common challenge: how to deploy AI reliably in environments where failure, data leakage, or downtime is unacceptable. This session explores how AI factory architectures are being designed to support high-assurance, mission-critical workloads across on-premises, edge, disconnected, and regulated environments.",
      "format": "In-Person",
      "industry": "Aerospace",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Drawing on lessons from Lockheed Martin’s internal AI factory and its commercialization through Astris AI, we examine how NVIDIA-accelerated infrastructure, AI enterprise software, and hardened inference services come together to enable secure training, inference, and governance at scale.",
        "Learn practical design patterns for operationalizing AI in environments that demand trust, repeatability, and resilience—from industrial systems to national-scale missions.",
        "This session explores how AI factory architectures are being designed to support high-assurance, mission-critical workloads across on-premises, edge, disconnected, and regulated environments."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, NVIDIA NIM",
      "session_id": "EX82257",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "Lockheed Martin",
          "name": "Greg Forrest",
          "title": "VP, AI Foundations and Commercialization"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Building AI Factories for High-Assurance, Mission-Critical Environments (Presented by Astris AI)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82257/"
    },
    {
      "description": "Unlock the power of real-time 3D digital twin applications with NVIDIA Omniverse Kit App Streaming. In this lab, you'll build, containerize, deploy, and extend interactive 3D applications that stream directly to web clients, enabling scalable, remote simulation and control for digital twin solutions. We’re going to create a streaming-ready OpenUSD application and a front-end client, then package and deploy a solution using industry-standard tools and APIs. You’ll explore how to implement custom messaging between your app and client, allowing for rich, bi-directional interaction and integration with live data sources. You'll gain practical experience with the full development and deployment workflow for Omniverse Kit App Streaming and develop a solid foundation for building operational digital twins that deliver real-time insights and collaboration across your organization. Intermediate Python experience, including working with packages and basic scripting Understanding of 3D scene concepts: transforms, hierarchies, and basic visualization Basic knowledge of REST APIs and data streaming concepts",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Deploy cloud-streamed digital twin applications using Omniverse Kit App Streaming for real-time 3D simulation.",
        "Scale and integrate embedded digital twin simulations across cloud applications.",
        "Implement bi-directional messaging between apps and web clients leveraging Omniverse Kit App Streaming to stream live data into interactive monitoring dashboards."
      ],
      "nvidia_technology": "",
      "session_id": "DLIT81798",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Sebastian Misiurek",
          "title": "Sr. Product Manager, Omniverse"
        },
        {
          "company": "NVIDIA",
          "name": "Andrew Wrenn",
          "title": "Cloud Software Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Building and Deploying Digital Twin Applications With Omniverse Kit App Streaming",
      "topic": "Industrial Digitalization / Digital Twin",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81798/"
    },
    {
      "description": "Foxconn is collaborating with NVIDIA to build an AI factory in Texas for the production of AI servers. Foxconn will leverage NVIDIA Omniverse libraries to build a digital twin environment for simulation and real-time monitoring to rapidly scale up production lines, and will introduce humanoid robots and a high degree of automation for the first time. All these will first be simulated in NVIDIA's environment and optimized in a visual setting to help Foxconn create a safer, more efficient, and more flexible manufacturing environment. It's committed to building this factory into a benchmark plant that can serve as a model for others to follow.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Foxconn leverages NVIDIA Omniverse to create digital twins of its factories—enabling simulation-driven design, real-time monitoring, and rapid scaling of global production lines.",
        "Advanced AI-powered robotics and automation, simulated and optimized in virtual environments, help Foxconn achieve safer, more efficient, and adaptive manufacturing.",
        "Integrated AI analytics, including video intelligence and data-driven insights, allow Foxconn managers to transform operations, enhance decision-making, and respond instantly to factory challenges."
      ],
      "nvidia_technology": "Jetson, RTX GPU, AGX, DGX Platform, CUDA, Omniverse, Metropolis, Modulus, cuOPT, Blackwell, Cosmos, Blueprint",
      "session_id": "S81508",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Fii (Foxconn Industrial Internet)",
          "name": "Leo Guo",
          "title": "GM"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Building and Scaling AI Factories With Digital Twins and Robotics",
      "topic": "Humanoid Robots",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81508/"
    },
    {
      "description": "Gain practical experience in orchestrating multi-robot collaboration, automating simulation workflows, and executing autonomous navigation and object manipulation tasks in a test scenario, using ROS 2, Simulation Interfaces, and Isaac Sim. These skills will empower you to deploy and test advanced robotics solutions. Basic understanding of ROS (Robot Operating System). Intermediate experience with Python, including working with packages, virtual environments, and basic scripting. Familiarity with 3D content concepts such as meshes, materials, transforms, and scene hierarchies from tools like Blender, Maya, or similar DCC applications. Understand command-line usage for running scripts and managing project files. Have familiarity with robotics or simulation workflows (for example, using simulation tools or working with physics-enabled assets).",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Set up Isaac Sim to run with simulation interfaces and the ROS 2 bridge.",
        "Configure Isaac Sim ROS 2 Action Graphs and sensor integrations.",
        "Deploy and control multiple robots in a simulated warehouse and orchestrate autonomous navigation.",
        "Learn how to use Simulation Interfaces to control Isaac Sim from ROS."
      ],
      "nvidia_technology": "Isaac",
      "session_id": "DLIT81699",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Ayush Ghosh",
          "title": "Robotics Systems Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Rishabh Chadha",
          "title": "Technical Marketing Engineer - Isaac"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Building and Testing Multi‑Robot Scenarios with Isaac Sim and ROS 2",
      "topic": "Robotics Simulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81699/"
    },
    {
      "description": "With the incredible capabilities of large language models (LLMs), enterprises are eager to integrate them into their products and internal applications for a wide variety of use cases, including (but not limited to) text generation, large-scale document analysis, and chatbot assistants. The fastest way to begin leveraging LLMs for diverse tasks is by using modern prompt engineering techniques. These techniques are also foundational for more advanced LLM-based methods such as Retrieval-Augmented Generation (RAG) and Parameter-Efficient Fine-Tuning (PEFT). In this workshop, learners will work with an NVIDIA language model NIM, powered by the open-source Llama-3.1 large language model, alongside the popular LangChain library. The workshop will provide a foundational skill set for building a range of LLM-based applications using prompt engineering. Certificate: Upon successful completion of the assessment, you’ll receive an NVIDIA certificate to recognize your subject matter competency and support your professional career growth. Familiarity with basic programming fundamentals such as functions and variables.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Understand how to apply iterative prompt engineering best practices to create LLM-based applications for various language-related tasks",
        "Be proficient in using LangChain to organize and compose LLM workflows.",
        "Write application code to harness LLMs for generative tasks, document analysis, chatbot applications, and more."
      ],
      "nvidia_technology": "NVIDIA NIM",
      "session_id": "DLIW82270",
      "session_type": "Full-Day Workshop",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Matt Linder",
          "title": "Sr. Solutions Architect"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Building LLM Applications With Prompt Engineering",
      "topic": "AI Inference",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dliw82270/"
    },
    {
      "description": "Europe has built the foundation for having domestic AI compute positioned within the region, with projects across the U.K., Norway, Portugal, Iceland, and beyond, all using NVIDIA-accelerated infrastructure to keep compute, data, and value on European terms. In this 15-minute session, you’ll get a fast, practical view of how public–private partnerships are structuring these deployments—what’s working, what’s hard, and how to apply the same playbook in your own country or organization.",
      "format": "In-Person",
      "industry": "Cloud Services",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Understand what “sovereign AI” really means in practice, and why it matters for Europe’s competitiveness this decade.",
        "Learn how governments, NVIDIA, and partners like Nscale structure public–private partnerships for sovereign AI, including roles, guardrails, and incentives.",
        "See how large GPU clusters (e.g., Stargate-style projects) are being sited, powered, and governed to balance sovereignty, security, and sustainability."
      ],
      "nvidia_technology": "Hopper, Blackwell, DGX Cloud",
      "session_id": "S82174",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Nscale",
          "name": "Philippe Sachs",
          "title": "President EMEA"
        }
      ],
      "technical_level": "General Interest",
      "title": "Building Local, Sovereign AI in Practice (Presented by Nscale)",
      "topic": "Security for AI",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82174/"
    },
    {
      "description": "Learn to develop multi-agent workflows tailored for Asset Lifecycle Management use cases within industrial process applications. You'll be introduced to the NVIDIA open-source NeMo Agent Toolkit (NAT) and its key features, enabling the creation of a comprehensive and customizable reference solution for this use case. You'll also be guided in effectively utilizing other components of NVIDIA AI stack, including NVIDIA Inference Microservices (NIM), cloud endpoints, and NVIDIA Tesseract time series foundational models. These components are used to design agents capable of executing frequently performed tasks such as data querying and analysis, Forecasting, Remaining useful life prediction, and Anomaly detection. Furthermore, you will gain insight into NAT’s marquee features, such as workflow observability and sizing, prompt optimization, and third-party connections. Certificate: Upon successful completion of the assessment, you’ll receive an NVIDIA certificate to recognize your subject matter competency and support your professional career growth.",
      "format": "In-Person",
      "industry": "Manufacturing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Develop multi-agent workflows from scratch using NAT with the popular NASA Turbofan engine time series dataset.",
        "Understand the internals of mulit-agent solutions through NAT framework features such as agent configurations, tool definitions and eval harness",
        "Enhance the capabilities of the solution such as modifying existing tools definitions, adding new tools, and prompt engineering.",
        "Improve the solution's functionality, focusing on modifying existing tool definitions, creating new tools, and refining prompts through engineering.",
        "Leverage observability and monitoring features to gain fine-grained visibility into the end-to-end workflow to identify latency bottlenecks and accuracy issues."
      ],
      "nvidia_technology": "NeMo",
      "session_id": "DLIW82268",
      "session_type": "Full-Day Workshop",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Vineeth Kalluru",
          "title": "Sr. Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Viraj Modak",
          "title": "AI Solutions Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Building Observable and Scalable Multi-Agent Workflows for Asset Lifecycle Management",
      "topic": "Video Analytics",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dliw82268/"
    },
    {
      "description": "Frontier closed-source models hit performance ceilings, and you can't customize them with your data—they are generic. The alternative is to own your AI by closing the loop between production signals and model improvements. This session presents a practical framework for building self-improving agents that are better, faster, and cheaper than frontier alternatives. We'll explore how to systematically integrate production data into your AI pipeline—whether through automatic prompt optimization, supervised fine-tuning, or reinforcement learning—to build specialized agents that excel at your specific tasks. Using real case studies, we'll show how teams achieved up to 33% quality improvements over frontier closed models, orders of magnitude faster inference, and 50% cost reductions by fine-tuning open models on their own data.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Production data is your competitive moat—systematically integrating it through fine-tuning lets you build specialized agents that are better, faster, and cheaper than generic frontier models.",
        "Teams can start wherever they are—optimizing prompts, fine-tuning on curated examples, or training with reinforcement learning—and progress as their data and use cases mature.",
        "Real production results demonstrate the approach: open models fine-tuned with reinforcement learning achieved 93% error-free code generation, 40X speed improvements, and better than 10% quality gains over state-of-the-art closed models."
      ],
      "nvidia_technology": "",
      "session_id": "EX82179",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "Fireworks AI",
          "name": "Roberto Barroso-Luque",
          "title": "Applied AI"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Building Self-Improving AI Agents: Use Production Data to Beat Frontier Models (Presented by Fireworks AI)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82179/"
    },
    {
      "description": "Dive deep into the real-world challenges of training 100 billion-plus parameter foundation models from scratch on NVIDIA Blackwell GPUs. We'll share hands-on experiences from a national-scale sovereign AI initiative, covering fault-tolerant orchestration, storage I/O optimization, and NVIDIA Collective Communication Library (NCCL) tuning strategies that keep massive training jobs running reliably across more than 500 B200 GPUs. At this scale, GPU errors, NCCL timeouts, and storage slowdowns begin to appear. Learn how we built a resilient stack with automated failure recovery, resolved 10x storage degradation from NFS driver issues, tackled MXFP8 precision stability on Blackwell, and tuned RoCE and NVIDIA InfiniBand networks through cross-vendor collaboration.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Explore the architecture of a national-scale sovereign AI training cluster using 500+ NVIDIA B200 GPUs with KubeVirt-based infrastructure as a service and container orchestration.",
        "Learn fault-tolerant scheduling strategies that automatically recover multi-node training jobs from GPU, network, and storage failures without manual intervention.",
        "Discover how to diagnose and resolve storage I/O bottlenecks caused by NFS driver misconfigurations that can slow data loading by 10x or more.",
        "Gain practical insights into MXFP8 precision training on Blackwell architecture, including driver compatibility and stability considerations.",
        "Apply NCCL tuning techniques and network optimization strategies for RoCE and InfiniBand environments to maximize distributed training throughput.",
        "Connect With the Experts",
        "Workshops, Training Labs & Certification"
      ],
      "nvidia_technology": "Cloud / Data Center GPU, HGX, CUDA, TensorRT, Infiniband Networking, NCCL, NVLink / NVSwitch, Blackwell",
      "session_id": "EX82047",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "Lablup, Inc.",
          "name": "Jeongkyu Shin",
          "title": "CEO"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Building Sovereign AI: Scaling 100B+ Model Training on NVIDIA Blackwell Infrastructure (Presented by Lablup, Inc.)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82047/"
    },
    {
      "description": "Digital advertising now rivals high-frequency trading in both speed and complexity: tens of millions of auctions occur every second, each demanding a decision in under a millisecond. AI is transforming the industry as agents autonomously plan, negotiate, and execute media transactions in real time. This session details how PubMatic re-engineered its global infrastructure using NVIDIA-accelerated computing to power some of the world’s first agentic advertising transactions, offering a blueprint for how GPU-driven intelligence is reshaping not just programmatic advertising, but real-time digital markets of every kind.",
      "format": "In-Person",
      "industry": "Media & Entertainment",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Examine the accelerated-computing architecture that enables PubMatic to process 45 million bids per second with microsecond-level inferencing.",
        "Learn how NVIDIA Triton Inference Servers, RAPIDS Accelerator for Apache Spark, and GPUs were optimized for streaming, low-batch, high-frequency workloads unique to digital advertising.",
        "Understand how this infrastructure enables autonomous AI agents to transact via emerging open protocols, enabling true machine-to-machine negotiation for media buying.",
        "Learn how to apply these architectural principles to any real-time industry as agentic AI begins mediating customer journeys across retail, finance, healthcare, mobility, and more."
      ],
      "nvidia_technology": "BlueField DPU, RTX GPU, CUDA, RAPIDS, Multi-Instance GPU (MIG), Triton, NVIDIA NIM",
      "session_id": "S81782",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "PubMatic",
          "name": "Rajeev Goel",
          "title": "Co-Founder and CEO"
        },
        {
          "company": "PubMatic",
          "name": "Vasu Cherlopalle",
          "title": "VP of Real-Time Bidding and Analytics"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Building the Architecture of Advertising Intelligence: How GPU-Accelerated AI Agents are Reshaping Digital Advertising",
      "topic": "AI Platforms",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81782/"
    },
    {
      "description": "Most enterprise AI initiatives fail to scale—not because of models, but due to fragmented infrastructure, data bottlenecks, and weak governance. GPUs alone are not an AI factory. In this session, ASUS, NVIDIA, and IBM present a validated, converged AI factory architecture that transforms raw data into trusted intelligence for agentic AI and retrieval-augmented generation at scale. This end-to-end solution combines NVIDIA Certified servers, NVIDIA AI Enterprise and NIMs, and IBM intelligent storage to accelerate inferencing while ensuring security and governance. Learn how Red Hat OpenShift, NVIDIA GPU Operator, NVIDIA NIM Operator, and IBM Content Aware Storage enable containerized AI deployments from the data center to the edge—supported by validated blueprints and centralized management for faster, more reliable AI production.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "How ASUS servers and edge devices form the physical backbone for an AI factory.",
        "How IBM storage and Red Hat OpenShift orchestrate, govern, and scale AI models across hybrid environments.",
        "How converged ASUS-NVIDIA-IBM technologies enable resilient, trusted, and real-time deployment of agentic AI."
      ],
      "nvidia_technology": "BlueField DPU, Cloud / Data Center GPU, HGX, Ethernet Networking, NVLink / NVSwitch, NVIDIA NIM, Blackwell, NVIDIA AI Enterprise",
      "session_id": "S82042",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "ASUS",
          "name": "Alber Wu",
          "title": "Division Director of Solutions"
        },
        {
          "company": "IBM",
          "name": "Vincent Hsu",
          "title": "VP, IBM Fellow, CTO for IBM Storage"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Building the Enterprise AI Factory: A Unified Stack With ASUS, IBM, and NVIDIA (Presented by ASUS)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82042/"
    },
    {
      "description": "A shortage of scalable clinical expertise is becoming healthcare’s limiting factor, not access to imaging hardware. In this session, Taha Kass-Hout, Global Chief Science and Technology Officer at GE HealthCare, will share architecture patterns for an intelligent control plane where multimodal models run at the edge to interpret images plus workflow context and guide real-time decisions with human oversight. Using autonomous X-ray as an early proof point, you will learn how accelerated computing enables clinical-grade latency and deployment, and how simulation and digital twins support safe validation at scale. Roland Rott, President and CEO of Imaging at GE HealthCare, will highlight real-world examples and measurable outcomes from AI adoption across ultrasound, mammography, CT, and MRI, and how these results inform the path toward increasingly autonomous imaging workflows.",
      "format": "In-Person",
      "industry": "Healthcare & Life Sciences",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "A practical blueprint for an edge-deployed, multimodal control plane that augments clinicians in real time.",
        "Why accelerated computing is essential for clinical-grade latency and deployment close to the device and patient.",
        "How simulation and digital twins accelerate testing and validation for safe scaling of autonomous imaging.AI systems act as intelligent collaborators, reducing radiologist cognitive load and supporting more precise, data‑driven decisions."
      ],
      "nvidia_technology": "DGX Platform, CUDA, Isaac, Omniverse, Clara Holoscan, IGX, MONAI, DGX Spark",
      "session_id": "S81677",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "GE HealthCare",
          "name": "Taha Kass-Hout",
          "title": "Global Chief Science and Technology Officer"
        },
        {
          "company": "GE HealthCare",
          "name": "Roland Rott",
          "title": "President and CEO"
        }
      ],
      "technical_level": "General Interest",
      "title": "Building the Intelligent Control Plane for Medical Imaging",
      "topic": "Robotics Simulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81677/"
    },
    {
      "description": "India’s AI future will be defined by mass-scale inferencing, not just training—and sovereign AI factories will be the nation’s strategic engine for that shift. This session explores how Yotta, powered by NVIDIA, is building India’s first AI-inferencing-optimized cloud, and what the country must prepare for as demand accelerates.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Inferencing will drive India’s real AI economic impact, far beyond training.",
        "Sovereign AI factories are essential national infrastructure for secure, large-scale deployment.",
        "Yotta is leading with NVIDIA-powered AI factories and India’s first inferencing-optimized cloud.",
        "India must prepare for multilingual inferencing, 100x enterprise adoption, and national-scale AI agents.",
        "A surge in AI demand is coming, requiring massive, scalable, sovereign compute capacity."
      ],
      "nvidia_technology": "Cloud / Data Center GPU",
      "session_id": "EX81989",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "Yotta Data Services",
          "name": "Sunil Gupta",
          "title": "Co-Founder, Managing Director, and CEO"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Building the Next Frontier for the World’s Largest Sovereign AI Factories (Presented by Yotta)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex81989/"
    },
    {
      "description": "Learn how AI scientists can automate generation and testing of hypotheses with experiments in a loop. See how to maintain long-running coherence required for their days runtime, and the nuances of benchmarking the process of discovery. Explore their automated discoveries across domains like molecular biology, materials science, and chemistry.",
      "format": "In-Person",
      "industry": "Healthcare & Life Sciences",
      "intended_audience": "Research: Non-Academic",
      "key_takeaways": [
        "Benchmarking AI scientists has shown they can reduce discovery time from six months to one day.",
        "AI scientists can write tens of thousands of lines of code and consider thousands of papers, making them hard to assess.",
        "See how to train parts of an AI scientist with NemoRL and open-source LLMs."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, Hopper, BioNeMo",
      "session_id": "S81694",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Edison Scientific",
          "name": "Andrew White",
          "title": "CTO"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Building, Measuring, and Using AI Scientists",
      "topic": "Biology - Generative AI",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81694/"
    },
    {
      "description": "当代理式 AI、生成式 AI 重塑产业边界，当数据中心成为 AI 工厂，海量算力、算存协同与能效优化成为新一代 10 亿瓦级（Gigawatt）数据中心的基石，一场定义未来的技术盛宴即将启幕！本场线上活动将围绕下一代 AI 工作负载对超大规模网络、无限互连能力和全数据中心综合能效来展开，从 NVIDIA、业界领先客户、合作伙伴以及科研学者的角度来探讨领先的 NVIDIA® NVLink™ ，NVIDIA® Quantum-X InfiniBand、NVIDIA® Spectrum-X™ 以太网、NVIDIA® BlueField® DPU/SuperNIC™ 、NVIDIA® ConnectX® SuperNIC™ 网络平台及其最佳实践，特别是专为大规模 AI 而设计的 NVIDIA® Spectrum-X/XGS 以太网络及其相应的软、硬件优化参考设计如何助力 AI 工厂实现数百万级 GPU 的无缝扩展，缩短部署时间，并优化整体性能和功耗。",
      "format": "Virtual",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "NVIDIA 新一代 AI 网络（纵向扩展、横向扩展、跨域扩展）",
        "NVIDIA AI 网络在 AI 工厂中的价值主张",
        "NVIDIA AI 网络的最佳实践和成功案例"
      ],
      "nvidia_technology": "BlueField DPU, DOCA, Infiniband Networking, Ethernet Networking",
      "session_id": "S81580",
      "session_type": "Panel",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Qingchun Song",
          "title": "网络亚太区高级总监"
        },
        {
          "company": "NVIDIA",
          "name": "Gaofeng Feng",
          "title": "网络技术市场高级总监"
        },
        {
          "company": "NVIDIA",
          "name": "Jingxun Wang",
          "title": "解决方案架构师"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "China AI Networking Day - 迎接 10 亿瓦级 AI 工厂的时代",
      "topic": "Cloud Networking",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81580/"
    },
    {
      "description": "This training lab explores the art and engineering of compressing large language models to make them cheaper, faster, and easier to deploy while preserving practical capability. Designed for a broad audience that spans beginners to advanced practitioners, the workshop will introduce foundational concepts for newcomers, share implementation patterns and pitfalls for experienced engineers, and highlight cutting-edge research directions for specialists. Familiarity with Python/PyTorch: Proficiency in Python programming and basic hands-on experience using deep learning frameworks. Conceptual Knowledge of Deep Learning: Basic understanding of neural networks, backpropagation, and the model fine-tuning process. Familiarity with LLM Architectures: Conceptual understanding of the Transformer architecture, including components like attention mechanisms and feed-forward networks.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Demystify Compression: Move beyond theory to show how pruning and quantization interact with actual hardware constraints on the NVIDIA stack.",
        "Enable Efficient Deployment: Equip practitioners, researchers, and HPC developers with actionable recipes and design trade-offs.",
        "Skill Building: Through hands-on labs, learn strategies to maximize GPU utilization, improve node-level throughput, and reduce end-to-end time-to-solution."
      ],
      "nvidia_technology": "",
      "session_id": "DLIT81861",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Harshita Seth",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Lavinia Ghita",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Sergio Perez",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Liana Mikaelyan",
          "title": "Sr. Solutions Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Compress, Cut, and Distill: The Latest Gen AI Model Compression Techniques in Practice",
      "topic": "Performance Optimization",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81861/"
    },
    {
      "description": "Learn from NVIDIA experts about OpenUSD best practices and physics tuning of mechanisms, using the example of a robotic hand. Starting from a URDF file, see how to structure, assemble, and optimize USD. Then learn about the physics tuning process, to enable dexterous hands to behave realistically in simulation. We will focus on best practices that make complex hand models performant, controllable, and ready for downstream learning and control tasks. Intermediate experience with Python, including working with packages, virtual environments, and basic scripting. Familiarity with 3D content concepts such as meshes, materials, transforms, and scene hierarchies from tools like Blender, Maya, or similar DCC applications. Understand command-line usage for running scripts and managing project files. Have familiarity with robotics or simulation workflows (for example, using simulation tools or working with physics-enabled assets).",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Explain the end-to-end process for importing, assembling, and preparing robot USD assets for simulation.​",
        "Apply best practices to optimize robot USDs (meshes, transforms, structure) for performance and stability.​",
        "Tune joint parameters and control gains using tools and techniques to achieve desired robot motion in simulation."
      ],
      "nvidia_technology": "Isaac",
      "session_id": "DLIT81697",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Alexandra Kissel",
          "title": "Robotics Simulation Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Steven Feng",
          "title": "Robotics Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Configure and Tune Robot Assets With OpenUSD and PhysX",
      "topic": "Robotics Simulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81697/"
    },
    {
      "description": "Join NVIDIA cybersecurity and agentic AI experts to learn how NVIDIA technology and agentic systems can be applied to strengthen your cybersecurity posture. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Explore best practices in defending LLM agents from attacks.",
        "Ask our GPU Confidential Compute team about new solutions for data privacy and edge security.",
        "Explore more with our Networking DPU team for the latest and greatest in NVIDIA solutions allowing unprecedented scale-out and data integration.",
        "Learn how threat hunters are applying the latest innovations in cybersecurity to bolster detection and response.",
        "Connect With the Experts",
        "Workshops, Training Labs & Certification",
        "Sponsors & Exhibitors",
        "Privacy Policy Your Privacy Choices Terms of Service Accessibility Corporate Policies Product Security Contact",
        "Copyright © 2026 NVIDIA Corporation",
        "NVIDIA uses cookies to improve your experience on our web site. We and our third-party part"
      ],
      "nvidia_technology": "NVIDIA NIM",
      "session_id": "CWES81524",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Neha Hudait",
          "title": "Sr. Security Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Ofir Arkin",
          "title": "Sr. Distinguished Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Hsin Chen",
          "title": "Sr. Data Scientist, Cybersecurity AI"
        },
        {
          "company": "NVIDIA",
          "name": "Marcela Denniston",
          "title": "SDO Executive"
        },
        {
          "company": "NVIDIA",
          "name": "Leon Derczynski",
          "title": "Principal Research Scientist"
        },
        {
          "company": "NVIDIA",
          "name": "Rich Harang",
          "title": "Principal AI Security Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Becca Lynch",
          "title": "Offensive Security Researcher"
        },
        {
          "company": "NVIDIA",
          "name": "Daniel Rohrer",
          "title": "VP of Software Product Security"
        },
        {
          "company": "NVIDIA",
          "name": "Laura Seletos",
          "title": "Principal Cloud Security Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Julien Soriano",
          "title": "Security Executive"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Connect With Cybersecurity, Agentic AI, and Threat Hunter Experts",
      "topic": "Security for AI",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81524/"
    },
    {
      "description": "Join the DGX Cloud team to ask questions and learn about the technical details, design philosophies, common issues and solutions, and challenges we’re trying to solve, across our portfolio of technologies from GPU Health to distributed inference at scale, and more. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "DGX Cloud contains a portfolio of leading-edge technologies and innovations that span across the full stack.",
        "Distributed inference is a challenge, and we are developing technologies and solutions to enable inference at scale.",
        "Managing large GPU fleets globally is difficult, and enabling observability and monitoring is a focus for GPU Health."
      ],
      "nvidia_technology": "TensorRT, Base Command Manager, CUDA-X, NeMo, Triton, NVIDIA NIM, NVIDIA AI Enterprise, Cosmos, NVIDIA Run:ai",
      "session_id": "CWES81586",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA Corporation",
          "name": "Andrew Liu (Enterprise Products)",
          "title": "Technical Marketing Engineering Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Christian Shrauder",
          "title": "Director, Cloud Integration Architecture"
        },
        {
          "company": "NVIDIA",
          "name": "Ed Balduf",
          "title": "Cloud Integration Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Peter Cross",
          "title": "Sr. Technical Marketing Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Pete MacKinnon",
          "title": "Senior Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Yuze Ma",
          "title": "Senior Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Sowmyan Soman",
          "title": "Principal Cloud Integration Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Andrew Thappa",
          "title": "Product Manager, Inference"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Connect With the Experts on the Technology Behind DGX Cloud",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81586/"
    },
    {
      "description": "The new wave of AI helps you manage your enterprise inferencing for control, security, and visibility. Come and listen to how Nutanix Enterprise AI with the AI Gateway helps you govern, secure, and control costs for all your generative AI systems, including NVIDIA AI, simply and easily.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Understand how Nutanix helps you manage all your AI inferencing in a single system.",
        "Discover how Nutanix Enterprise AI with NVIDIA AI helps drive enterprise Gen AI across your business, always in your control.",
        "Learn about the next-generation Nutanix Enterprise AI Gateway that helps you control costs and load balance Gen AI requests.",
        "See a demo of the Nutanix Enterprise AI Gateway in action."
      ],
      "nvidia_technology": "NVIDIA NIM",
      "session_id": "EX82138",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "Nutanix",
          "name": "Mikey Barmonde",
          "title": "Technical Marketing Evangelist - AI"
        }
      ],
      "technical_level": "General Interest",
      "title": "Control the New Wave of Enterprise AI With Nutanix (Presented by Nutanix)",
      "topic": "Retrieval-Augmented Generation (RAG)",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82138/"
    },
    {
      "description": "The latest image and video generation models, combined with LLM, are fantastic tools to build powerful agentic AI workflows, but they're hard to access through too many cloud access points to efficiently iterate on concepts and stay up to date. In this two-hour instructor-led workshop, you'll learn how to use GPU acceleration for ComfyUI on local workstation or servers, pulling the latest models to build complex generative workflows. Workflows will range from relighting pictures/video, 3D workflows including Physically based rendering map generation, or even building storyboard from pictures and text. Whether you're a developer willing to learn what models work best for what tasks, or an industrial designer or digital artist willing to understand how to drive AI with your inputs, this course demonstrates how ComfyUI can be used to power design and visualization workflows with Gen AI.  A desire to learn. Whether you are an expert in models, ComfyUI or a beginner, you should learn something if you are ready to drink the AI from the firehose.",
      "format": "In-Person",
      "industry": "AEC",
      "intended_audience": "Artist / Designer",
      "key_takeaways": [
        "Learn insights on which of the latest vision and large language models can be used for what task.",
        "Master ComfyUI workflow to experiment with open-source models.",
        "How to drive AI with design or artistic inputs"
      ],
      "nvidia_technology": "RTX GPU, RTX Virtual Workstations (vWS)",
      "session_id": "DLIT81948",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Alessandro La Tona",
          "title": "3D Workflow Specialist"
        },
        {
          "company": "NVIDIA",
          "name": "Ashlee Martino-Tarr",
          "title": "3D Workflow Specialist"
        },
        {
          "company": "NVIDIA",
          "name": "Daniela Flamm Jackson",
          "title": "3D Workflow Specialist"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Create Generative AI Workflow for Design and Visualization in ComfyUI",
      "topic": "Image Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81948/"
    },
    {
      "description": "Computer vision is a cornerstone of the generative AI revolution, providing the essential visual metadata required by vision language models (VLMs) to extract actionable insights. In this two-hour instructor-led workshop, you will learn how to GPU-accelerate your video analytics workflow by combining the NVIDIA DeepStream SDK with the power of prompt engineering. Discover how to generate complete, GPU-accelerated DeepStream pipelines using simple natural language prompts. Whether you are a developer new to DeepStream or an advanced user looking to accelerate your prototyping, this course demonstrates how to produce intuitive, readable Python applications without the manual overhead. Join us to bridge the gap between traditional video analytics and modern GenAI development. Understanding of Python and basic knowledge of computer vision models.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Understand how computer vision, video analytics, and VLMs fit together to power generative AI applications that turn visual data into actionable insights.",
        "Learn to use natural language prompts to automatically generate complete, GPU-accelerated NVIDIA DeepStream pipelines, reducing the need for manual boilerplate coding.",
        "Leave with practical experience building readable Python-based Vision AI apps faster, helping you move from traditional video analytics toward modern GenAI-driven development workflows."
      ],
      "nvidia_technology": "DeepStream, Metropolis, Cosmos, Blueprint",
      "session_id": "DLIT81879",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Carlos Garcia-Sierra",
          "title": "Product Manager - Metropolis AI Workflows"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Create Vision AI Applications With Generative AI Coding Agents",
      "topic": "Image / Video Detection & Recognition",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81879/"
    },
    {
      "description": "Join this live Q&A session with some of NVIDIA’s own CUDA developers to demystify the process of building real-world CUDA applications. From utilizing CUDA accelerated libraries, profilers, and debugging tools to setting up CI pipelines, packaging, and builds—this session covers the entire development life cycle. If you’re looking for practical guidance on integrating GPU acceleration into your software, this is the perfect opportunity to get answers and advice from engineers who develop and maintain CUDA software every day. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how to effectively structure, build, and package CUDA applications, with guidance on CI and testing automation.",
        "Get practical advice on using CUDA’s ecosystem of developer tools like profilers and debuggers.",
        "Understand when and how to leverage CUDA accelerated libraries to speed up your development.",
        "Discover best practices for managing dependencies, builds, and scaling CUDA-powered applications.",
        "Walk away with actionable insights to improve the development workflow and life cycle of your CUDA projects."
      ],
      "nvidia_technology": "CUDA",
      "session_id": "CWES81535",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Jake Hemstad",
          "title": "Software Engineering Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Georgii Evtushenko",
          "title": "Sr. Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Trent Nelson",
          "title": "Principal Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Leo Fang",
          "title": "Python CUDA Tech Lead"
        },
        {
          "company": "NVIDIA",
          "name": "Jaydeep Marathe",
          "title": "Principal Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Jonathan Bentz",
          "title": "CUDA Technical Marketing Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Vyas Ramasubramani",
          "title": "Sr. Systems Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Jonathan Dekhtiar",
          "title": "Sr. CUDA Python Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Rafael Campana",
          "title": "Sr. Engineering Director of CUDA Developer Tools"
        },
        {
          "company": "NVIDIA",
          "name": "Ashwin Srinath",
          "title": "Senior Software Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "CUDA Developer Best Practices",
      "topic": "Programming Languages / Compilers",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81535/"
    },
    {
      "description": "Join us for a CWE session with the CUDA Windows and WSL engineering group to ask any questions you might have about performance, best practices, and how to leverage some of our new features and new platforms. Bring your questions/requests for customized answers on any topics: WSL containers, tuning programs for MCDM/WDDM, graphics interoperability (DirectX, Vulkan, OpenGL) and AI in games—even some of the most niche memory allocation and optimizations problems, and much more. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Understand how CUDA on Windows works under the hood to better leverage the GPU.",
        "Understand the differences between CUDA on Linux and CUDA on Windows to better optimize your application.",
        "Get familiar with recently launched CUDA on Windows features like CUDA in Graphics, MCDM, and RDMA.",
        "Understand how CUDA on WSL can be leverage in hybrid development and test workflow."
      ],
      "nvidia_technology": "",
      "session_id": "CWES81886",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Jalpa Patel",
          "title": "Sr. Systems Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Raphael Boissel",
          "title": "Principal Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Mike Delorme",
          "title": "Sr. Manager, CUDA Software Engineering"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "CUDA on Windows and WSL",
      "topic": "Performance Optimization",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81886/"
    },
    {
      "description": "This session showcases how Porsche Engineering fosters innovation in the context of automotive engineering. Learn how NVIDIA technology helps push the frontiers of the automotive V-Model in terms of innovation depth and time-to-market. Learn about methods and components used throughout the vehicle development and validation process to ultimately enhance the customer experience in the car.",
      "format": "Virtual",
      "industry": "Automotive / Transportation",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": ["Learn about the importance of systematic innovation."],
      "nvidia_technology": "Cloud / Data Center GPU, Jetson, DGX Platform, CUDA, TensorRT, Isaac, Omniverse, JetPack, NVIDIA NIM, Blackwell, NVIDIA AI Enterprise",
      "session_id": "S82116",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Porsche Engineering",
          "name": "Benedict Kistner",
          "title": "AI and Future Business Fields"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Customer-Centric Innovation in Automotive",
      "topic": "Industrial Digitalization / Digital Twin",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82116/"
    },
    {
      "description": "Express your algorithm; cuTile and Tile IR will figure out the threads, data movement, and Tensor cores. In this talk, we'll present cuTile, NVIDIA's new block-based programming abstraction, and Tile IR, the compiler stack that it is built with. We'll explore examples in C++ and Python from a variety of domains, including inference of a mixture-of-experts large language model and a sparse conjugate gradient solver. CUDA tile programming divides inputs into local arrays that are processed concurrently by groups of threads. Users write sequential array-centric code, and the framework handles parallelization, synchronization, and data movement behind the scenes. Architecture-specific details are abstracted away—you express your algorithm once in a high-level way, and the system lowers it to the right implementation for any NVIDIA GPU architecture.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn the fundamental concepts of the CUDA tile programming model.",
        "Understand the differences between tile and SIMT programming, and when each paradigm should be used.",
        "Gain insight into the performance of cuTile and Tile IR and how to tune it.",
        "See how tile programming provides portability across NVIDIA GPU architectures."
      ],
      "nvidia_technology": "CUDA",
      "session_id": "S81433",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Bryce Lelbach",
          "title": "Principal Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "cuTile Programming",
      "topic": "Programming Languages / Compilers",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81433/"
    },
    {
      "description": "Explore new techniques for scaling and optimizing GROMACS molecular dynamics on the latest NVIDIA Multi-node NVLink (MNNVL) systems, via the use of cuFFTMp and NVSHMEM, to minimize communication overhead and maximize performance. Together, we'll walk through practical, transferable steps you can apply to boost performance in your own HPC and AI workloads.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how the world-leading GROMACS open-source molecular dynamics software has recently been enhanced to fully exploit the latest Multi-node NVLink NVIDIA technology to allow researchers to run faster simulations than ever before.",
        "This work involves intricate use of GPU-initiated communication via modern NVSHMEM and cuFFTMp software functionality, to minimize overheads and maximize communication/computation overlap.",
        "We will present the steps required to achieve optimal performance in a transferable way, as a showcase for other HPC and AI applications."
      ],
      "nvidia_technology": "cuFFT",
      "session_id": "S81542",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Alan Gray",
          "title": "Principal Developer Technology Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Mahesh Doijade",
          "title": "Sr. Compute Developer Technology Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Cutting-Edge Molecular Dynamics on the Latest Multi-Node NVLink Technology",
      "topic": "Molecular Dynamics",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81542/"
    },
    {
      "description": "In this \"Lightning Talks\" session, industry experts will explore the evolving world of data analytics and streaming, covering AI-driven innovations in sponsorship analytics, data processing needs of streaming services, and real-time audience insights. Speakers will discuss how advanced data tools are driving end-to-end content automation and improving attribution accuracy, helping businesses optimize strategies and increase engagement. Gain actionable insights on the role of AI in refining analytics and enhancing operational efficiency in the streaming industry.",
      "format": "In-Person",
      "industry": "Media & Entertainment",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Learn how AI is transforming sponsorship analytics and optimizing real-time data processing for streaming services.",
        "Discover how advanced data tools are enabling end-to-end content automation and improving attribution accuracy.",
        "Understand how data-driven strategies can help businesses fine-tune content delivery and boost viewer engagement.",
        "Gain actionable insights into the evolving role of AI in refining analytics and streamlining operational efficiency across the industry."
      ],
      "nvidia_technology": "DGX Platform, TensorRT, CUDA-X, cuDF, cuGraph, Blackwell",
      "session_id": "S82148",
      "session_type": "Lightning Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Bianca Pryor",
          "title": "Director of Data Science for Media & Entertainment"
        },
        {
          "company": "Relo Metrics",
          "name": "Jay Prasad",
          "title": "CEO"
        },
        {
          "company": "Advolve",
          "name": "Joao Sobreira",
          "title": "Founder"
        },
        {
          "company": "BET+",
          "name": "Jason Harvey",
          "title": "EVP and General Manager"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Data Analytics and Streaming Media",
      "topic": "Data Analytics / Processing",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82148/"
    },
    {
      "description": "Learn to optimize decision optimization problems with GPU-accelerated workflows using NVIDIA cuOpt, RAPIDS, CUDA-X libraries, and Nemotron models. This hands-on lab dives into advanced LP/MIP solver integration, accelerated primal heuristics, cutting-plane methods, and deployment patterns for large-scale, real-time optimization. Foundational knowledge of optimization: Familiarity with Linear Programming and Mixed Integer Programming concepts (constraints, relaxations, heuristics, cutting planes). Basic Python proficiency: Ability to read and modify Python scripts used in optimization workflows. Some experience with GPUs or CUDA-based frameworks: Prior exposure to GPU computing (e.g., RAPIDS, CUDA-X libraries) is helpful but not mandatory. Comfort with OR tooling: Experience with at least the optimization solver ecosystem",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "Understand and apply GPU-accelerated MIP and LP algorithms, including Feasibility Pump, Gomory cuts, and bound propagation to achieve fast, high-quality solutions.",
        "Learn how to integrate cuOpt with Nemotron models and other AI components to solve complex, real-world optimization problems end-to-end.",
        "Gain practical experience in performance tuning, solver orchestration, and deploying scalable optimization pipelines for enterprise and research workflows."
      ],
      "nvidia_technology": "CUDA, CUDA-X, cuOPT",
      "session_id": "DLIT81660",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Adi Geva",
          "title": "Technical Marketing Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Burcin Bozkaya",
          "title": "Sr. Developer Relations Manager"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Data to Decisions: GPU-Accelerated Decision Optimization",
      "topic": "Logistics / Route Optimization",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81660/"
    },
    {
      "description": "As at-scale AI workloads for training and inference make use of gigawatt-scale multi-GPU systems, massive data transfer, and memory access capabilities of Vera Rubin NVL72 systems in AI factories, we will show how these large systems get architected and deployed in the data center, leading to a robust reference architecture embraced globally for its speed of deployment and performance. This session will showcase internal deployments and the tools and software stack developed and leveraged on these platforms. You'll also have the opportunity to have deep dive views on how multi-node NVLink, system interconnects, and storage, as well as the latest liquid-cooled designs, are shaping the future of AI infrastructure.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Large-scale reference architecture for Vera Rubin NVL72 by NVIDIA engineering",
        "Data center innovations in power, cooling, design for multi-node NVLink, system interconnects, storage, and the latest air- and liquid-cooled designs will also be covered.",
        "Learn about performance outcomes from next-generation AI workloads and gain insights into the underlying software stack."
      ],
      "nvidia_technology": "BlueField DPU, DGX Platform, CUDA, DOCA, Fleet Command, TensorRT, Infiniband Networking, Ethernet Networking, Cumulus, Magnum IO, MGX, Base Command Manager, cuBLAS, cuDDN, cuDF, cuFFT, cuGraph, cuML, cuOPT, Interconnect Networking, NVLink / NVSwitch, Blackwell, NVIDIA Run:ai",
      "session_id": "S81848",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Julie Bernauer",
          "title": "Sr. Director, Applied Systems Engineering"
        },
        {
          "company": "NVIDIA",
          "name": "Adam DeConinck",
          "title": "Director, Applied Systems Engineering"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Deep Dive on Gigawatt AI Factories",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81848/"
    },
    {
      "description": "This hands-on lab will get you down and dirty with customizing an AI factory. Experience how typical administrator tasks are made easier with NVIDIA Mission Control, and explore the interfaces and tools that NVIDIA Mission Control provides for doing \"Day 1 and beyond\" work. To be successful, participants should have an understanding of AI clusters, Linux, and containers. Bring a laptop as this session is very interactive!",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Understand how NVIDIA Base Command Manager is used to create, tweak, and manage AI factory configurations.",
        "Perform common AI factory administration tasks with NVIDIA Base View.",
        "Use the NVIDIA Mission Control Web UI to view and manage AI factory infrasturcture.",
        "Learn how to perform routine tasks, such as replacing a failed Kubernetes worker node."
      ],
      "nvidia_technology": "Grace CPU, DGX Platform, HGX, Infiniband Networking, Hopper, LaunchPad, Base Command Manager, Interconnect Networking, Blackwell, NVIDIA AI Enterprise, Mission Control",
      "session_id": "DLIT82008",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Max Steele",
          "title": "Sr. Technical Instructor in AI and Data Center"
        },
        {
          "company": "NVIDIA",
          "name": "Terrell Bennett",
          "title": "Technical Training Content Developer"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Deploy and Customize Your AI Factory: Make Your AI Factory Feel Like Home!",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit82008/"
    },
    {
      "description": "Learn to deploy, optimize, and accelerate large language models (LLMs) and vision-language models (VLMs) on NVIDIA Jetson Thor through hands-on exercises with real hardware. We'll demonstrate production-ready inference frameworks including vLLM and SGLang, apply NVIDIA ModelOpt for quantization, showcase systematic methods to achieve inference performance improvements, and explore advanced capabilities like tool calling and live VLM inference — all running on edge devices you can touch and interact with. Basic understanding of Large Language Models and transformer architectures Familiarity with Python programming and container technologies (Docker) Experience with command-line interfaces and SSH for remote device access Laptop with network connectivity (Ethernet preferred, WiFi acceptable)",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Deploy and run state-of-the-art LLMs and VLMs on NVIDIA Jetson Thor using production-ready containers (vLLM, SGLang).",
        "Apply NVIDIA ModelOpt to quantize models and measure real-world performance improvements on edge hardware.",
        "Compare inference frameworks hands-on: benchmark speed differences from llama.cpp to vLLM/SGLang on actual Jetson Thor devices.",
        "Experience VLMs running on edge hardware through Live VLM WebUI with real-time visual question-answering and multi-modal interactions.",
        "Implement tool calling and function calling with Nemotron models to enable agentic AI capabilities on edge devices."
      ],
      "nvidia_technology": "Jetson, AGX, CUDA, TensorRT, JetPack, Blackwell, Cosmos",
      "session_id": "DLIT81738",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Chitoku Yato",
          "title": "Sr. Technical Product Marketing Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Aditya Sahu",
          "title": ""
        },
        {
          "company": "NVIDIA",
          "name": "Khalil Ben Khaled",
          "title": ""
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Deploy and Optimize LLMs and VLMs on NVIDIA Jetson Thor",
      "topic": "Embedded Edge",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81738/"
    },
    {
      "description": "Reinforcement learning (RL) is essential for developing reliable and autonomous agents that learn from experience, delivering enhanced accuracy and cost efficiency. In this deep dive session, learn how to use the technical architecture of CoreWeave’s RL infrastructure and inference solutions, highlighting how to define model weights, use Weights & Biases with serverless RL and the deployment of high performance inference stacks. We will cover use cases to show how to leverage these tools on NVIDIA GPUs to build a unified stack that scales seamlessly, optimizes performance, and accelerates your time to production.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how to leverage CoreWeave’s high-performance inference stack to deploy RL trained agents efficiently.",
        "Gain insights on how to build reliable agents that learn continuously with RL.",
        "Streamline operations with serverless RL using Weights & Biases to simplify and scale model post-training."
      ],
      "nvidia_technology": "",
      "session_id": "S82201",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "CoreWeave",
          "name": "Kyle Corbitt",
          "title": "Director, Engineering"
        },
        {
          "company": "CoreWeave",
          "name": "Aaron Batilo",
          "title": "Principal Engineer, AI/ML"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Deploy Reliable Agents at Scale With Reinforcement Learning and High-Performance Inference (Presented by CoreWeave)",
      "topic": "Code / Software Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82201/"
    },
    {
      "description": "This session guides you through the end-to-end deployment of generative image models on NVIDIA RTX PRO workstations by leveraging ONNX Runtime and TensorRT-RTX. We'll demonstrate how to ship a multi-stage model pipeline across a wide set of hardware without sacrificing performance. This workflow will be powered by classic graphics APIs, ONNX Runtime, and the TensorRT-RTX Execution Provider. You'll leave with the skills to convert raw models into high-performance, offline-capable applications that run seamlessly across diverse hardware architectures.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Deploying efficient cross-hardware AI pipelines",
        "AI deployment with minimal packaging",
        "Bring models from research to deployment"
      ],
      "nvidia_technology": "RTX GPU, DGX Platform, RTX Virtual Workstations (vWS), Blackwell, DGX Spark",
      "session_id": "DLIT81958",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Julius Gregor Tischbein",
          "title": "Sr. Developer Technology Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Maximilian Mueller",
          "title": "Sr. Developer Technology Software Engineer"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Deploy State-of-the-Art Gen AI on Your RTX PRO Workstation",
      "topic": "Image Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81958/"
    },
    {
      "description": "As foundation models move toward deeper test-time computation, inference becomes the dominant scaling constraint. Latency, throughput, and cost are governed by a small set of forces: autoregressive decoding, KV-cache growth, memory bandwidth, and scheduling under contention. This workshop frames large-scale inference through these emerging laws of inference, starting from first principles and building toward real systems. Learners begin with monolithic and gateway-based vLLM deployments on Kubernetes to establish baseline behavior, then transition to NVIDIA Dynamo to operate aggregated and disaggregated inference architectures using built-in KV-aware routing and scheduling. A core emphasis is observability: attendees will deploy a full stack (Prometheus, Grafana, Loki, Tempo) to monitor metrics, capture structured logs, and perform distributed tracing. The outcome is a principled understanding of where inference time and money go - and how architectural choices bend those curves in production. Certificate: Upon successful completion of the assessment, you’ll receive an NVIDIA certificate to recognize your subject matter competency and support your professional career growth. Experience working with configuration files (e.g., YAML, JSON). Familiarity with containers and container-based workflows (e.g., Docker) and some basic knowledge of Kubernetes. Basic understanding of large language model (LLM) inference concepts. Familiarity with distributed computing concepts, such as data or model parallelism.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Reason about LLM inference from first principles, including prefill vs. decode, KV-cache scaling, and memory–compute trade-offs.",
        "Deploy vLLM-based inference systems on Kubernetes, starting from monolithic serving.",
        "Use NVIDIA Dynamo for aggregated and disaggregated inference with KV-aware routing and scheduling.",
        "Expose inference services and track token-level usage for monitoring and cost management.",
        "Deploy Prometheus, Grafana, Loki, and Tempo to observe inference behavior across requests, tokens, and system components."
      ],
      "nvidia_technology": "Dynamo",
      "session_id": "DLIW82274",
      "session_type": "Full-Day Workshop",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Anshul Jindal",
          "title": "Sr. Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Mohak Chadha",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Severine Habert",
          "title": "Solutions Architect"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Deploying and Optimizing AI Inference at Scale",
      "topic": "ModelOps",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dliw82274/"
    },
    {
      "description": "In this session we explore EXLerate.ai, EXL’s enterprise-grade platform for developing, orchestrating, and running Gen AI and agentic AI solutions at scale. We will illustrate how EXLerate.ai enables capabilities such as domain-aware copilots, agentic workflow orchestration, intelligent automation, and decision intelligence in enterprise environments. While generative AI presents significant opportunities to transform how work gets done, it also introduces complex challenges related to enterprise data integration, contextual relevance, governance, security, cost efficiency, and operational reliability. EXLerate.ai addresses these challenges through a modular platform architecture, enterprise data and knowledge grounding, flexible model orchestration, and built-in guardrails designed to support responsible, scalable, and production-ready AI systems.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn about EXLerate.ai’s reference architecture for enterprise generative and agentic AI deployments.",
        "Understand how to decide between foundation models, fine-tuning, and domain adaptation for enterprise use cases.",
        "Gain insights into testing, evaluation, monitoring, and governance of AI systems in production.",
        "Develop perspective on cost–performance trade-offs and ROI considerations across real enterprise deployments."
      ],
      "nvidia_technology": "Riva, RAPIDS, Metropolis, Blueprint, DGX Cloud",
      "session_id": "EX82048",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "Exlservice Holdings, Inc.",
          "name": "Aidan McGowran",
          "title": "VP - Global Products and Platforms"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Design an Enterprise-Grade Platform for Generative and Agentic AI at Scale (Presented by EXL Service)",
      "topic": "AI Platforms",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82048/"
    },
    {
      "description": "As NVIDIA Blackwell GPUs power production inference stacks originally built for previous generations, developers may observe unexpected bottlenecks. Higher peak compute and faster interconnects do not automatically translate to lower latency or better cost efficiency at scale. This session examines how inference system behavior changes when transitioning from NVIDIA Hopper to NVIDIA Blackwell GPUs. We’ll focus on concrete architectural shifts and show how the architecture improves. Rather than benchmarking peak performance, we’ll analyze observable patterns that emerge under real inference traffic and discuss how to redesign scheduling, batching, and monitoring strategies to fully benefit from NVIDIA Blackwell architecture.",
      "format": "In-Person",
      "industry": "Cloud Services",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Understand which Hopper-era inference assumptions fail on Blackwell-class GPUs.",
        "Identify batching and concurrency strategies that scale more effectively on Blackwell.",
        "Understand how FP4 changes the inference horizon.",
        "Examine the lagging software optimization stack, and how we see it.",
        "Adopt metrics that reflect inference health better than raw GPU utilization."
      ],
      "nvidia_technology": "Hopper, Blackwell",
      "session_id": "EX82083",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "GMI Cloud",
          "name": "Yujing Qian",
          "title": "Head of Engineering"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Design Inference Systems for NVIDIA Blackwell: Inference on a Rack-Scale System (Presented by GMI Cloud)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82083/"
    },
    {
      "description": "Ensuring the safety of children and young people is a global effort. This panel brings together leaders from industry, law, and civil society engaged in protecting children and teens online to discuss what tools and techniques are available for companies interested in joining the fight.",
      "format": "In-Person",
      "industry": "Consumer Internet",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "How to identify and remove harmful content and output",
        "How to build safeguards that prioritize the rights and well-being of children and teens",
        "Protecting data privacy",
        "The importance of transparency for parents and their children"
      ],
      "nvidia_technology": "NeMo, Omniverse Replicator, NVIDIA NIM, NVIDIA AI Enterprise",
      "session_id": "S81551",
      "session_type": "Panel",
      "speakers": [
        {
          "company": "Gibson Dunn",
          "name": "Frances Waldmann",
          "title": "Partner"
        },
        {
          "company": "NVIDIA",
          "name": "Jessica Butler",
          "title": "Sr. Product Security Engineer"
        },
        {
          "company": "Tech Coalition",
          "name": "Sean Litton",
          "title": "President and CEO"
        }
      ],
      "technical_level": "General Interest",
      "title": "Design, Detection, and Duty of Care in the Age of AI",
      "topic": "AI Platforms",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81551/"
    },
    {
      "description": "As AI factories become mission‑critical infrastructure, organizations must design, build, operate, and maintain electro‑intensive environments that are intelligent and scalable. This session shows how Schneider Electric and NVIDIA, along with open standards including SysML and OpenUSD enable the full AI Factory lifecycle through a unified approach powered by accelerated computing, high‑fidelity simulation, and agentic AI.",
      "format": "In-Person",
      "industry": "Energy",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Learn how accelerated computing and simulation reduce risk and time-to-value in AI Factory design and build phases",
        "See how Omniverse digital twins connect design intent with operational reality across the lifecycle",
        "Understand how agentic AI and RAG enable scalable, high-skill technical services for electro-intensive operations",
        "Discover how AI augments scarce human expertise to improve uptime, safety, and sustainability"
      ],
      "nvidia_technology": "CUDA, Omniverse",
      "session_id": "S81556",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Schneider Electric USA",
          "name": "Natasha Nelson",
          "title": "CTO Services"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Designing and Operating an Energy Efficient AI Factory",
      "topic": "Retrieval-Augmented Generation (RAG)",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81556/"
    },
    {
      "description": "Learn how Rivian built a real-time digital twin for rapid engineering design through NVIDIA Omniverse, Apollo AI Physics models, and CUDA-X libraries.",
      "format": "In-Person",
      "industry": "Automotive / Transportation",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "How to use Omniverse and kit-cae to build a digital twin",
        "How to use NVIDIA PhysicsNeMo and Apollo AI Physics models for real-time design",
        "How to use GPU-accelerated computer-aided engineering codes to quickly generate high-quality training data"
      ],
      "nvidia_technology": "Omniverse, Modulus, CUDA-X, Blackwell",
      "session_id": "S81651",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Neil Ashton",
          "title": "Distinguished Engineer"
        },
        {
          "company": "Rivian",
          "name": "Andy McAllister",
          "title": "Sr. Manager of Aerodynamics and Environmental Performance"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Develop a Real-Time Digital Twin for Rapid Automotive Engineering Design",
      "topic": "CAE",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81651/"
    },
    {
      "description": "Create intelligent operational dashboards for digital twin applications using NVIDIA Omniverse. Build compact, real-time monitoring interfaces with React to track factory metrics, robot states, and sensor data streams. Embed custom panels into Kit applications and deploy dashboards across local and cloud environments. Learn to integrate LLM agents for natural language interaction with your digital twin data. Through hands-on exercises, learn the workflow for dashboard creation for streaming deployment, enabling intelligent monitoring and control of complex industrial operations. Familiarity with REST APIs, data streaming, and real-time visualization concepts Basic understanding of factory operations, IoT metrics, and monitoring workflows Experience with Python scripting for backend integration and automation Knowledge of LLM concepts and natural language processing",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Build compact, real-time dashboards using React to visualize live factory metrics.",
        "Embed custom monitoring panels into Kit applications for real-time tracking of robots, sensors, and simulation states.",
        "Deploy and stream dashboards in a cloud environment that communicated with a digital twin.",
        "Integrate LLM agents to enable natural language interaction with digital twin dashboards."
      ],
      "nvidia_technology": "",
      "session_id": "DLIT81650",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Victor Yudin",
          "title": "Software Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Justine Lin",
          "title": "Technical Marketing Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Develop AI-Powered Operational Dashboards for Digital Twin Applications",
      "topic": "Industrial Digitalization / Digital Twin",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81650/"
    },
    {
      "description": "In this session, we will share how Foxconn leverages its own advantages in the era of Physical AI, together with Project Genesis, a group-level centralized AI platform initiative, to cooperate with global ecological partners such as NVIDIA, BCG, ABB, Intrinsic, Siemens, etc., to build “AI Factory” and “Data Factory,” and dive into the application of various automation and robots in production lines that integrate the concept of Physical AI technology to jointly promote the transformation of Foxconn manufacturing into “AI intensive.”",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Understand Hon Hai’s strategy of gradually transforming from labor-intensive to automation-intensive to AI-intensive in the physical AI era."
      ],
      "nvidia_technology": "",
      "session_id": "S82100",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Hon Hai Precision Industry Co., Ltd.",
          "name": "Zhe Shi",
          "title": "Chief Digital Officer"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Develop Physical AI Applications and Build Data Factories With the Robotics Ecosystem (Presented by Foxconn)",
      "topic": "Robot Manipulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82100/"
    },
    {
      "description": "This training lab will explore new techniques for building robust, production-ready AI agents by focusing on an evaluation-driven design workflow. You'll learn how we use human-created datasets for iterative improvement and how to apply continuous eval and iterate cycles, including prompt changes and experimentation, to significantly improve agent performance. The prerequisites for this course are practical familiarity with AI/ML fundamentals, including Large Language Models (LLMs), and programming proficiency in Python. Attendees should also have a working understanding of basic prompt engineering and experience with the overall lifecycle of building an AI application, as the lab is focused on the advanced, production-ready stage of evaluation and iterative design.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Implement an evaluation-driven design (EDD) methodology to continuously measure and refine agent performance using both human-created and synthesized datasets."
      ],
      "nvidia_technology": "NeMo, NVIDIA AI Enterprise",
      "session_id": "DLIT81725",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Dhruv Nandakumar",
          "title": "Cybersecurity AI Solutions Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Develop Production Agents with Eval-Driven Design",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81725/"
    },
    {
      "description": "Learn how world foundation models generate physics-based data to advance physical AI development. Explore open tools for video processing, tokenization that accelerates training, and frameworks to customize models for autonomous vehicles, robotics, and humanoids. Get help from experts to optimize your physical AI development pipelines with NVIDIA Cosmos. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "What are world models, and how do they advance physical AI?",
        "Learn about NVIDIA Cosmos—the world foundation model development platform."
      ],
      "nvidia_technology": "Cosmos",
      "session_id": "CWES81669",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Pranjal Joshi",
          "title": "Product Marketing Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Joshua Bapst",
          "title": "Product Manager - NVIDIA Cosmos"
        },
        {
          "company": "NVIDIA",
          "name": "Aiden Chang",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Alexander Schwarz",
          "title": "Solutions Architect"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Develop World Foundation Models With NVIDIA Cosmos",
      "topic": "Video Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81669/"
    },
    {
      "description": "Learn how Blue Origin has been fine-tuning and using the NVIDIA Apollo AI physics models for their hypersonic spacecraft re-entry challenges. You’ll hear about how they use simulation within their research and design processes; how they see the role of AI within their overall workflows and work done to fine-tune and optimize AI Physics models for their applications.",
      "format": "In-Person",
      "industry": "Aerospace",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Discover how Blue Origin fine-tunes and deploys NVIDIA Apollo AI physics models to solve complex hypersonic reentry challenges."
      ],
      "nvidia_technology": "Modulus, CUDA-X",
      "session_id": "S81767",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Neil Ashton",
          "title": "Distinguished Engineer"
        },
        {
          "company": "Blue Origin",
          "name": "David Halaas",
          "title": "Sr. Technical Fellow"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Developing AI Physics Models for Spacecraft Reentry",
      "topic": "CAE",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81767/"
    },
    {
      "description": "Nasdaq's OmniMarket is a cutting-edge generative AI platform that creates ultra-realistic synthetic market order books and order flow data using advanced deep learning architectures. By leveraging sequential generative models and transformer-based networks, OmniMarket produces statistically accurate limit order book simulations that preserve the statistical properties and microstructure dynamics of real financial markets. The platform generates high-fidelity synthetic data indistinguishable from actual market conditions, while providing sophisticated vector embeddings for enhanced search and retrieval applications.",
      "format": "In-Person",
      "industry": "Financial Services",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Representation Learning on Orderbooks: Apply deep sequential models to capture structural relationships across price levels, sides, and quantities.",
        "Market Digital Twin: Create a system that runs in parallel to the order-matching engine that mirrors and contextualizes market behavior.",
        "Downstream Applications: Enable predictive modeling, liquidity characterization, and strategy prototyping using learned market embeddings."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, DGX Spark",
      "session_id": "S81530",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Nasdaq",
          "name": "Douglas Hamilton",
          "title": "VP - Head of AI Research and Engineering"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Developing the First Generative AI Model of Global Financial Markets",
      "topic": "Synthetic Data Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81530/"
    },
    {
      "description": "Diffusion models have exploded from research labs into the mainstream, powering everything from photorealistic image generation to creative co-pilots. But moving these models from a notebook to a scalable, low-latency, production-ready service is one of the greatest engineering challenges in modern AI. The \"magic\" of generation hides a complex stack of optimization and infrastructure hurdles. We will bypass high-level concepts to debate the concrete architectural trade-offs required for low latency and high throughput, covering unique training challenges, up-and-coming model architectures, inference and runtime optimization, low-precision and NVFP4, and scaling topologies.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "Navigate the architectural shift in generative AI by evaluating the performance and cost implications of moving from convolutional neural network-based U-Nets to diffusion transformers (DiTs) and mixture of experts (MoEs)",
        "Deploy architecture-specific scaling topologies, contrasting spatial parallelism for traditional U-Nets against DeepSpeed-Ulysses and Ring Attention for long-sequence DiTs.",
        "Tune the precise trade-off between latency and image quality by optimizing noise schedules, sampler selection, and classifier-free guidance parameters.",
        "Accelerate generation pipelines by applying diffusion-native optimization techniques, including consistency distillation, step caching, and NVFP4 quantization."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, RTX GPU, DGX Platform, CUDA, TensorRT, Infiniband Networking, cuDDN, Interconnect Networking, NVLink / NVSwitch, Triton",
      "session_id": "S81684",
      "session_type": "Panel",
      "speakers": [
        {
          "company": "Black Forest Labs",
          "name": "Andreas Blattmann",
          "title": "Co-Founder"
        },
        {
          "company": "Inception Labs",
          "name": "Stefano Ermon",
          "title": "CEO"
        },
        {
          "company": "Runway",
          "name": "Anastasis Germanidis",
          "title": "CTO and Co-Founder"
        },
        {
          "company": "NVIDIA",
          "name": "Ming-Yu Liu",
          "title": "VP Generative AI Research"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Diffusion Unlocked: Advanced Techniques for Training, Inference, and Deployment",
      "topic": "Image Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81684/"
    },
    {
      "description": "As AI data center complexity explodes, \"build-then-test\" is no longer viable. Industry leaders now use digital twins to co-develop and validate solutions. Join NVIDIA and Palo Alto Networks to see how NVIDIA Air enables partners to build, test, and certify software on virtualized hardware before physical deployment. We’ll dive into how collaborative simulation accelerates next-gen security integration into the AI factory, ensuring software is battle-tested for Day 1.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Virtual-First Validation: Discover how to utilize high-fidelity digital twins to build, test, and certify software stacks on virtualized hardware months before physical deployment.",
        "Infrastructure Interoperability: Learn how partners like Palo Alto Networks validate seamless integration between third-party security services and high-speed fabrics like NVIDIA Spectrum-X Ethernet.",
        "Agile Ecosystem Enablement: Understand how to leverage simulation platforms to synchronize software development with rapid hardware release cycles across the entire AI infrastructure stack.",
        "Automated Reference Architectures: Master the cloud-native workflow for developing and testing certified reference architectures that ensure Day 1 stability in complex AI factories."
      ],
      "nvidia_technology": "Ethernet Networking, Cumulus",
      "session_id": "S82176",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Palo Alto Networks",
          "name": "Jasmine Punia",
          "title": "Sr. Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Amit Katz",
          "title": "VP Networking Products"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Digital Twin–Driven Co-Development and Validation for Next-Generation AI Data Center Applications",
      "topic": "InfraOps",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82176/"
    },
    {
      "description": "We invite you to join us as we explore NVIDIA's Deep Learning Institute Training Catalogue, uncovering practical use cases and real-world applications that extend beyond standard course descriptions. In this session, our NVIDIA experts will guide you through some of our most popular and in-demand learning paths, demonstrating how they can deepen your AI knowledge, strengthen your skills, and support your career development.",
      "format": "Virtual",
      "industry": "Academia / Higher Education",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Find out where to locate NVIDIA’s learning paths and how to access the right courses and certifications for your career goals.",
        "See practical examples that demonstrate how NVIDIA courses apply to solving real-world challenges across various industries.",
        "Receive direct advice on building your own learning plan and obtaining professional certifications that add value to your career."
      ],
      "nvidia_technology": "CUDA",
      "session_id": "S81560",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Juan Jose Durillo Barrionuevo",
          "title": "DLI Principal Instructor"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Discover Top In-Demand Learning Paths Across Europe, the Middle East, and Africa",
      "topic": "Programming Languages / Compilers",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81560/"
    },
    {
      "description": "Explore how model distillation and compression techniques are redefining the role of large language models in quantitative finance. In this expert talk, we’ll examine strategies for reducing cost, latency, and complexity while preserving accuracy for real-time alpha generation and risk prediction. You'll gain insights into the essential techniques behind efficient model design and discover how these methods drive scalable, high-performance deployment on NVIDIA’s accelerated computing platform.",
      "format": "In-Person",
      "industry": "Financial Services",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn the principles of model distillation, transferring knowledge from large teacher models to smaller student models, to achieve faster inference and lower resource consumption without sacrificing accuracy.",
        "Understand how large language models are applied in quantitative finance for alpha generation, risk prediction, and automated analysis, and how to optimize for cost and latency.",
        "Develop a deep understanding of how pruning, quantization, and knowledge distillation work together as the foundational techniques for building efficient distilled models, and learn to integrate them for optimal performance in financial workflows.",
        "Gain a strategic perspective on the trade-offs between accuracy, latency, cost, and hardware constraints influence decisions in compressing and deploying models for production environments at the edge."
      ],
      "nvidia_technology": "CUDA-X, NeMo, NVIDIA NIM, NVIDIA AI Enterprise",
      "session_id": "S81522",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Lavinia Ghita",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Dhruv Desai",
          "title": "Sr. Solutions Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Distillation Recipes and Synthetic Data Fine-Tuning for Algorithmic Trading",
      "topic": "Transfer Learning / Fine Tuning",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81522/"
    },
    {
      "description": "NVIDIA Dynamo is a distributed inference serving framework built to run LLMs efficiently, reliably, and at massive scale. It helps AI providers reduce cost-per-token, meet strict service level objectives, and scale deployments using Kubernetes across large GPU clusters. Learn how Dynamo delivers high-performance distributed inference across frameworks such as SGLang, TensorRT-LLM, and vLLM. We’ll explore key capabilities — including disaggregated serving, KV-cache-aware routing, topology-aware scheduling, and intelligent memory and cache management — that make Dynamo a powerful foundation for production workloads. Whether you’re evaluating Dynamo, experimenting it, or deepening your understanding of its capabilities and real-world impact, join us to ask questions, see how the system works under the hood, and get practical guidance from the experts who build and deploy it. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Practical insights for deploying, tuning, and scaling LLMs in real-world environments, informed by production examples",
        "A deeper understanding of advanced inference techniques such as disaggregated inference, KV caching to storage, and topology-aware scheduling",
        "Clear guidance on the tools and recipes available to help you get up and running quickly with Dynamo using your preferred inference framework — SGLang, TensorRT-LLM , or vLLM"
      ],
      "nvidia_technology": "",
      "session_id": "CWES81461",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Akshatha Kamath",
          "title": "Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Neelay Shah",
          "title": "Principal Software Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Vikram Sharma Mailthody",
          "title": "Sr. Research Scientist"
        },
        {
          "company": "NVIDIA",
          "name": "Harry Kim",
          "title": "Principal Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Kyle Kranen",
          "title": "Team Lead/Manager - Deep Learning Algorithms"
        },
        {
          "company": "NVIDIA",
          "name": "Anish Maddipoti",
          "title": "Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Amr Elmeleegy",
          "title": "Product Marketing Manager"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Distributed Inference at Scale Using Dynamo",
      "topic": "AI Inference",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81461/"
    },
    {
      "description": "我们将深入探讨三种面向 DiT 视频模型的全新解决方案，解决 training-free 加速技术中存在的精度损失问题。随着高效、高质量的视频生成在降本增效和提升用户体验方面变得愈发迫切，本次分享希望能提供一些具有参考价值的思路与经验。",
      "format": "Virtual",
      "industry": "Media & Entertainment",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "无损量化（training-free）：一套端到端的混合精度推理工作流，专为视频生成场景设计，最终应用于 Kling 模型并服务了超过 4500 万 AIGC 用户，实现了几乎零额外开销的量化，同时几乎不带来精度损失",
        "ScalingCache：一种利用特征缓存复用的 DIT 推理加速策略，在几乎不损失精度的情况下实现了 2x–5x 的加速",
        "ScalingAttention：一种新的稀疏注意力技术，能够实现更宽的稀疏范围和更大的加速比，并建立精度—稀疏度曲线，探索稀疏性的极限"
      ],
      "nvidia_technology": "CUDA",
      "session_id": "S81970",
      "session_type": "Talk",
      "speakers": [],
      "technical_level": "Technical - Advanced",
      "title": "DiT 视频模型推理加速新方案：兼顾精度与效率的平衡之道",
      "topic": "Video Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81970/"
    },
    {
      "description": "Explore practical techniques for maximizing Tensor Core performance in your CUDA kernels. We’ll discuss CUTLASS-based coding patterns to unlock efficient pipeline utilization and streamlined data management. Together, we’ll use Nsight Compute to analyze, optimize, and fine-tune your workflows for optimal Tensor Core performance.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Discover the fundamentals of Tensor Cores on NVIDIA hardware, and learn how to harness their power using the CUTLASS library.",
        "Gain practical skills in Nsight Compute for profiling and optimizing your CUDA kernels for Tensor Cores.",
        "This talk is part of an ongoing series about optimizing CUDA kernel performance with Nsight Compute."
      ],
      "nvidia_technology": "CUDA, NSight Comute",
      "session_id": "S81772",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Magnus Strengert",
          "title": "Software Engineering Manager"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Don’t Leave Tensors on the Table: Programming and Optimizing Tensor Cores",
      "topic": "Profilers / Debuggers / Code Analysis",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81772/"
    },
    {
      "description": "The next wave of enterprise innovation hinges on successfully deploying large language models with predictable cost and performance. This session demystifies the token economy and highlights why the critical, yet often overlooked, network and security infrastructure layer, together with the accelerated Intelligent AI Proxy, has become essential for Gen AI success. Learn how this infrastructure layer enables two fundamental shifts: • Intelligent Traffic Management to Optimize Tokens, the Currency of AI Factories: How you can increase output token throughput by more than 50% and drive significantly higher return on investment from their AI factories. • Disaggregated Inference, a Critical Architectural Shift for Token Efficiency: How you can significantly improve GPU utilization and enable a far more scalable enterprise AI factories, delivering up to 3X increase in inference performance.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Learn how to implement a reference architecture with intelligent AI proxies to improve tokenomics, optimize inference, and accelerate enterprise AI factory performance.",
        "Understand how intelligent prompt routing improves response quality, strengthens security and regulatory compliance, and minimizes cost and latency through optimal resource use.",
        "See how disaggregated inference separates context understanding from response generation to maximize GPU utilization and enable efficient scaling across data centers."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, DOCA, Ethernet Networking, NCCL, NVIDIA NIM, NVIDIA AI Enterprise, Dynamo",
      "session_id": "S81794",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "F5, Inc.",
          "name": "Kunal Anand",
          "title": "Chief Product Officer"
        },
        {
          "company": "NVIDIA",
          "name": "Kevin Deierling",
          "title": "SVP, Networking"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Drive Down Costs: Enhanced Tokenomics and Smart Routing for High-Scale Enterprise AI",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81794/"
    },
    {
      "description": "Join our session to learn how benchmarking recipes can evaluate performance and inform infrastructure tuning to optimize tokens/watt. Learn about the intersection of performance and total cost of ownership (TCO) through NVIDIA's performance tools, and understand the NVIDIA Exemplar Cloud initiative.",
      "format": "In-Person",
      "industry": "Cloud Services",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Tuning for Optimal Perf: Improve the value of your infrastructure by tuning your platform for optimal performance per TCO.",
        "Data for Expected Perf: Leverage baseline application level results to understand expected performance.",
        "How to Navigate Change: Performance per dollar is an evolving field that should be accessible to all. Empower users and providers with data, metrics, and tools.",
        "What NVIDIA is Doing: Access expert guidance and support from NVIDIA as needed to tune your infrastructure for optimal tokens/watt on GB300, GB200, B200, H100, and more.",
        "The Importance of Ecosystem: See how NVIDIA is partnering across ecosystem to drive real-world workload and TCO performance metrics."
      ],
      "nvidia_technology": "DGX Platform, HGX, Infiniband Networking, Hopper, Interconnect Networking, NCCL, NeMo, Blackwell, NVIDIA AI Enterprise, DGX Cloud",
      "session_id": "S81845",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Emily Potyraj",
          "title": "Product Manager"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Drive Optimal Tokens per Watt on any AI Infrastructure Using Benchmarking Recipes",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81845/"
    },
    {
      "description": "Discover the transformative potential of NVIDIA’s edge AI platform in shaping the future of smart edge and autonomous robotics. This session will provide an overview of the platform’s capabilities across embedded, industrial, and enterprise edge environments, with a focus on how general intelligence at the edge enables machines to perceive, reason, and make real-time decisions, directly where data is generated. Learn about the best practices and software frameworks that power AI-driven robots and intelligent vision systems—from advanced multi-modal sensor processing to low-latency, on-device inference—unlocking greater autonomy without reliance on the cloud. Explore how these innovations are accelerating the rise of generative physical AI systems and reshaping industries including healthcare, retail, robotics, transportation, manufacturing, and beyond.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Explore the fundamentals and benefits of edge computing.",
        "Learn the key technological drivers powering edge computing revolution.",
        "Dive deep into NVIDIA's edge platform and software framework.",
        "Gain insight into the latest edge AI and robotic use cases.",
        "Connect With the Experts",
        "Workshops, Training Labs & Certification",
        "Sponsors & Exhibitors",
        "Privacy Policy Your Privacy Choices Terms of Service Accessibility Corporate Policies Product Security Contact",
        "Copyright © 2026 NVIDIA Corporation",
        "NVIDIA uses cookies to improve your experience on our web site. We and our third-party partners also use cookies and other tools to collect and record information you provide as well as information about your interactions with our websites for performan"
      ],
      "nvidia_technology": "Jetson, AGX, EGX, TensorRT, Isaac, Metropolis, IGX, JetPack, TAO Toolkit, DGX Spark",
      "session_id": "S81842",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Chen Su",
          "title": "Sr. Technical Product Marketing Manager"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Edge Computing 101: Introduction to Smart Edge and Autonomous Robots",
      "topic": "Embedded Edge",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81842/"
    },
    {
      "description": "In this hands-on lab you'll learn, as an AI factory administrator, how real workloads are run on a cluster. Intended for IT types (not data scientists!), this lab will have you configuring Run:ai, allocating and monitoring resources, and building a workload with NVIDIA NIMS to understand just what the user experience is, and what tools are at your disposal to make the AI factory workloads actually work. To be successful, participants should have an understanding of AI clusters, Linux, and containers. Bring a laptop as this session is very interactive!",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Learn how NVIDIA Mission Control, including Run:ai, can be used to increase cluster utilization.",
        "Configure data sources, projects, and environments in Run:ai for efficient resource consumption.",
        "Experience depoying a NIM using Run:ai to understand how users would interact with an AI factory."
      ],
      "nvidia_technology": "DGX Platform, HGX, Infiniband Networking, LaunchPad, Base Command Manager, NVIDIA NIM, NVIDIA AI Enterprise, NVIDIA Run:ai, Mission Control",
      "session_id": "DLIT82002",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Max Steele",
          "title": "Sr. Technical Instructor in AI and Data Center"
        },
        {
          "company": "NVIDIA",
          "name": "Terrell Bennett",
          "title": "Technical Training Content Developer"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Efficient Workload Management for AI Factories: What Every Admin Needs to Know",
      "topic": "InfraOps",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit82002/"
    },
    {
      "description": "Join Wistron technology leaders to explore how the company’s digital twin platform, built on NVIDIA Omniverse libraries and OpenUSD, is accelerating factory deployment, from Hsinchu to Dallas. Discover how these factory-scale digital twins are enabling many use cases; including streamlined factory design, layout validation, and manufacturing process simulation in the early planning. Get a deep dive into how these digital twins are enabling virtual commissioning and helping identify integration issues among robots, conveyors, and automation systems before physical installation. Learn about the real business outcomes being generated across Wistron’s operations, from faster iteration cycles to fewer deployment risks and eliminated rework during ramp-up.",
      "format": "In-Person",
      "industry": "Manufacturing",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Understand how digital twins were applied in Hsinchu to accelerate factory design, layout validation, and process simulation, enabling real-time collaboration across manufacturing, automation, IT, and operations teams within a shared digital twin environment."
      ],
      "nvidia_technology": "Omniverse",
      "session_id": "S82050",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Wistron Corp.",
          "name": "John Lu",
          "title": "Sr. Director"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Empower US Manufacturing Through Wistron's Smart Manufacturing (Presented by Wistron Corp.)",
      "topic": "Industrial Digitalization / Digital Twin",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82050/"
    },
    {
      "description": "AI continues to reshape enterprise applications, boosting productivity, streamlining operations, and delivering better outcomes. To realize these gains, enterprises need infrastructure that can keep pace with modern AI. Traditional infrastructure has hit its limits, diverting compute and power from AI workloads and creating performance bottlenecks and security gaps at AI scale. Learn how Red Hat OpenShift, NVIDIA BlueField DPUs, and high-performance Ethernet fabrics provide optimal infrastructure for enterprise AI. DPU-based isolation separates infrastructure services from user workloads, reducing the attack surface and enforcing policy boundaries. We'll also cover how OpenShift automates provisioning, updates, monitoring, and troubleshooting for BlueField-enabled nodes to simplify adoption at enterprise scale.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Learn design principles to combine purpose-built acceleration and security, without adding operational overhead, with OpenShift.",
        "Understand how accelerated, software‑defined networking using BlueField integrated into OpenShift can deliver secure, high‑performance AI infrastructure and eliminate performance bottlenecks."
      ],
      "nvidia_technology": "BlueField DPU, DOCA, Ethernet Networking",
      "session_id": "S81673",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Yael Shenhav",
          "title": "VP of Networking Products"
        },
        {
          "company": "Red Hat",
          "name": "Chris Wright",
          "title": "CTO and SVP, Global Engineering"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Enable Enterprise AI Factories With Secure, High-Performance Networking",
      "topic": "AI Networking, 5G/6G",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81673/"
    },
    {
      "description": "Learn how Fincantieri, one of the world’s largest shipbuilders, designed and implemented humanoid robots engineered to perform complex tasks in dangerous shipyard environments characterized by high variability and stringent safety requirements. Leveraging NVIDIA’s simulation and AI orchestration solutions, EY helped Fincantieri accelerate workflow automation and reduce manual interventions. Dive into Fincantieri’s strategic vision and understand the challenges for integrating humanoids into an industrial environment.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "How to design and implement humanoid robots engineered to perform complex tasks in dangerous environments",
        "How to accelerate workflow automation and reduce manual interventions using NVIDIA’s simulation and AI orchestration solutions",
        "Understand the strategic vision and the challenges for integrating humanoids into an industrial environment."
      ],
      "nvidia_technology": "Isaac, Omniverse, NVIDIA NIM, NVIDIA AI Enterprise",
      "session_id": "S82289",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "EY",
          "name": "Giuseppe Perrone",
          "title": "AI and Data Leader, Italy"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Enable Seamless Human-Humanoid Collaboration in a Shared Operational Environment (Presented by EY)",
      "topic": "Humanoid Robots",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82289/"
    },
    {
      "description": "In production, LLM inference performance is shaped by model architecture, hardware, workload patterns, and service-level agreement constraints. This session presents a practical workflow for profiling inference stacks, identifying real bottlenecks, and evaluating optimizations across kernels, scheduling, and system design, showing how to leverage fast-moving open-source innovations while applying our own enhancements to achieve production-grade performance and stability, and how to feed lessons from production back into upstream development.",
      "format": "In-Person",
      "industry": "Cloud Services",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how to profile LLM inference stacks and identify real bottlenecks across kernels, scheduling, and system design."
      ],
      "nvidia_technology": "CUDA, NCCL, Triton, Blackwell",
      "session_id": "S82026",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Together AI",
          "name": "Yineng Zhang",
          "title": "Principal AI Researcher"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Engineering Real-World LLM Inference: Bridging Open-Source and Production Systems",
      "topic": "AI Inference",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82026/"
    },
    {
      "description": "Learn how European startups are leveraging NVIDIA platforms to advance robotics and automation across real-world industries. From teaching robotic hands dexterity with scalable world models and automating smart agriculture using edge AI to reinventing hospitality operations through simulation-based design and applying adaptive robotics to healthcare and rehabilitation—discover how these innovators integrate foundation models, teleoperation, and digital twins to unlock new possibilities for autonomous systems in manufacturing, agriculture, services, and beyond.",
      "format": "Virtual",
      "industry": "Manufacturing",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "European startups are using NVIDIA technologies to accelerate robotics and automation across industries such as manufacturing, agriculture, healthcare, and hospitality.",
        "These companies combine advanced tools like foundation models, edge AI, teleoperation, and digital twins to build more adaptive and intelligent autonomous systems.",
        "Their innovations include robotic dexterity training, AI-driven smart farming, simulation-based service design, and robot-assisted rehabilitation, demonstrating practical real-world impact."
      ],
      "nvidia_technology": "Jetson, TensorRT, Isaac, Omniverse, IGX, JetPack, Omniverse Replicator, TAO Toolkit, Cosmos",
      "session_id": "S81521",
      "session_type": "Panel",
      "speakers": [
        {
          "company": "Extend Robotics",
          "name": "Chang Liu",
          "title": "CEO and Chief Designer"
        },
        {
          "company": "AI.Land",
          "name": "Josef Franko",
          "title": "Founder and CEO"
        },
        {
          "company": "NVIDIA",
          "name": "Snehal Deshmukh",
          "title": ""
        },
        {
          "company": "Mimic Robotics",
          "name": "Elvis Nava",
          "title": "Co-Founder & CTO"
        },
        {
          "company": "BellBoy Robotics",
          "name": "Sandy Hefftz",
          "title": "CEO"
        },
        {
          "company": "Generative Bionics",
          "name": "Daniele Pucci",
          "title": "Chief Executive Officer (CEO)"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "European Robotics Startup Showcase",
      "topic": "Robot Manipulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81521/"
    },
    {
      "description": "This session will explore Europe’s dynamic AI compute landscape and guide founders through leveraging both publicly funded AI factories—designed for open research—and private NCP/AI factories focused on commercial activity, using the concept of a unified digital continuum for strategic growth. Discover actionable pathways to access sovereign AI infrastructure, including a comprehensive overview of services from major players and how startups can engage with high-speed, secure NVIDIA-powered systems through tailored programs and partnerships developed for European innovators. We'll also highlight how new regional AI growth zones and an expanding network of AI factories offer localized access to advanced technology, talent, and venture funding—enabling startups to build competitive advantages rooted in European digital sovereignty and regional collaboration.",
      "format": "Virtual",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Europe is building a unified public-private AI compute ecosystem for startup growth.",
        "Startups can access sovereign, high-performance AI infrastructure and services through dedicated European programs and partnerships.",
        "Regional AI growth zones and AI factories provide local access to technology, talent, and funding."
      ],
      "nvidia_technology": "Cloud / Data Center GPU",
      "session_id": "S81898",
      "session_type": "Panel",
      "speakers": [
        {
          "company": "GENCI",
          "name": "Cedric Auliac",
          "title": "AI2F Program Director"
        },
        {
          "company": "NVIDIA",
          "name": "Pierre-Antoine Beaudoin",
          "title": "Regional Lead, Startup Ecosystem, Southern Europe"
        },
        {
          "company": "University of Bristol, BriCS",
          "name": "Sadaf Alam",
          "title": "CTO"
        },
        {
          "company": "Probabl.ai",
          "name": "Yann Lechelle",
          "title": "Co-founder & CEO"
        },
        {
          "company": "Sferical",
          "name": "Jenny Nordlöw",
          "title": "CEO"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Europe’s AI Launchpad: Unlock Startup Growth Through Sovereign AI Infrastructure",
      "topic": "Code / Software Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81898/"
    },
    {
      "description": "In this hands-on lab, you’ll gain expertise on workflows for simulation-based policy evaluation, including how to setup simulation environments with Isaac Lab Arena, how to post-train GR00T models for custom tasks and how to evaluate trained policies in simulation at scale. NVIDIA Isaac Lab Arena is a framework for scalable policy evaluation in simulation. NVIDIA Isaac GR00T provides robot foundation models for cognition and control, built on NVIDIA Omniverse™ and Cosmos™. Intermediate experience with Python, including working with packages, virtual environments, and basic scripting. Understand command-line usage for running scripts and managing project files. Have familiarity with robotics or simulation workflows (for example, using simulation tools or working with physics-enabled assets).",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how to easily prototype tasks in simulation using Isaac Lab-Arena.",
        "Post-train a policy using NVIDIA Isaac GR00T.",
        "Evaluate policies at scale with Isaac Lab Arena."
      ],
      "nvidia_technology": "Isaac",
      "session_id": "DLIT81698",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Asawaree Bhide",
          "title": "Robotics Technical Marketing Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Edith Llontop",
          "title": "Solutions Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Evaluating GR00T Robot Policies in Simulation with NVIDIA Isaac Lab - Arena",
      "topic": "Robotics Simulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81698/"
    },
    {
      "description": "Learn what goes into an AI Factory. Hardware, software, networking, storage ... nothing is off limits. Want to understand more about how RTX PRO can be used with Spectrum-X networking and Run:ai? Have questions about how NVIDIA Mission Control helps GB300 systems run jobs at peak efficiency? This is the place to be. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Understand AI Factory reference architectures",
        "Understand AI Factory system hardware (RTX PRO Servers, DGX B300 , DGX GB300 SuperPOD, etc.)",
        "Understand AI Factory MEP and physical integration",
        "Network/storage/compute considerations for an AI Factory"
      ],
      "nvidia_technology": "BlueField DPU, Grace CPU, RTX GPU, DGX Platform, HGX, Infiniband Networking, Ethernet Networking, MGX, Interconnect Networking, NVLink / NVSwitch, Blackwell, NVIDIA AI Enterprise, NVIDIA Run:ai, Mission Control",
      "session_id": "CWES81458",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Yang Yang",
          "title": "Solution Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Scott Ellis",
          "title": "Sr. Director, Solutions Architecture and Engineering"
        },
        {
          "company": "NVIDIA",
          "name": "Jeff Weiss",
          "title": "Sr. Director"
        },
        {
          "company": "NVIDIA",
          "name": "Robert Magno",
          "title": "Manager Solution Architects"
        },
        {
          "company": "NVIDIA",
          "name": "John Fragalla",
          "title": "Principal Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Hans Mortensen",
          "title": "Grace Product Specialist – NVIDIA Solutions Architecture"
        },
        {
          "company": "NVIDIA",
          "name": "Paul Bryan",
          "title": "DGX Solution Architect"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Everything You Wanted to Know About AI Factories!",
      "topic": "InfraOps",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81458/"
    },
    {
      "description": "Autonomous forklifts and mobile robots are transforming loading docks and warehouse aisles, but relying only on onboard sensing leaves them vulnerable to blind spots, occlusions, and conservative safety behaviors that increase cost. KION will show how they use NVIDIA Halos for outside-in safety on NVIDIA IGX, combining infrastructure cameras, low-latency detections, and safety logic to supervise multiple robots with virtual fences, tripwires, and dynamic zones. Learn how outside-in safety architecture boosts throughput, reduces false stops, and enables closer human–robot collaboration for use cases such as autonomous forklift trailer loading and autonomous mobile robot (AMR) zoning with Metropolis VSS and Cosmos Reason.",
      "format": "In-Person",
      "industry": "Manufacturing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Outside-in functional safety reduces blind spots and false stops so autonomous forklifts and AMRs can run faster while staying compliant and safe.",
        "Combining NVIDIA Halos, IGX, Metropolis VSS, and Cosmos Reason creates a supervisory safety layer that coordinates multiple robots with virtual fences, tripwires, and dynamic zones.",
        "This approach augments existing robot systems to boost throughput and enable closer human–robot collaboration in real industrial workflows, like trailer loading and AMR zoning."
      ],
      "nvidia_technology": "Jetson, Metropolis, IGX",
      "session_id": "S81838",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "KION Group",
          "name": "Johannes Hinckeldeyn",
          "title": "Director of Advanced Core Technologies"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Extend Robot Perception With Industrial Safety Agents",
      "topic": "Sensor Fusion",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81838/"
    },
    {
      "description": "Dive deep into theory and practice of low-latency inference by deploying NVIDIA TensorRT-LLM with advanced speculative decoding techniques. You'll train an Eagle-3 draft head to propose candidate tokens efficiently, serve it, and benchmark it using AIPerf to quantify how these strategies minimize latency.",
      "format": "In-Person",
      "industry": "Cloud Services",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn about the current diversity of the ways to address the autoregressive nature of transformers.",
        "Train an Eagle‑3 head to accelerate token generation via speculative decoding.",
        "Deploy LLMs for real-time inference using trtllm-serve.",
        "Benchmark latency and throughput using AIPerf, and observe the speedups."
      ],
      "nvidia_technology": "TensorRT, Dynamo",
      "session_id": "DLIT81945",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Dmitry Mironov",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Mireille Fares",
          "title": "Sr. AI Solution Architect"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Faster Together: Train and Deploy a Speculative Decoding Model for Low-Latency LLM Inference",
      "topic": "AI Inference",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81945/"
    },
    {
      "description": "Learn about using NVIDIA Nsight Systems for bottleneck detection in AI pipelines and explore new features to optimize and scale the execution of your applications. This lab will guide you through the performance analysis process of GPU-accelerated AI applications that run across multiple, possibly containerized, compute instances. In addition to profiling of CPU, GPU, network and I/O activity, we'll show how NVTX and plugin mechanisms enable the collection of custom data to support an even more comprehensive and tailored analysis on the target platform. We'll also show how the recipe system can be used to identify bottlenecks in applications running across multiple nodes. This lab is for both beginners and experienced developers who want to learn about the latest features, tips, and tricks for performance analysis to get the most out of their hardware.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how to detect bottlenecks in AI applications.",
        "Dive into the analysis of reports from multi-node application runs."
      ],
      "nvidia_technology": "NSight Systems",
      "session_id": "DLIT81641",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Sneha Latha Kottapalli",
          "title": "Sr. Systems Software Engineer"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Find the Bottleneck: Optimize AI Pipelines With Nsight Systems",
      "topic": "Performance Optimization",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81641/"
    },
    {
      "description": "Network operation centers (NoC) are the central nervous system of telecommunications, but they are often overwhelmed by \"alarm storms.\" In a traditional setup, engineers manually validate alarms, swivel between multiple dashboards, check topologies, and perform root-cause analysis. This manual process is time-consuming and prone to fatigue. To solve this, we are moving toward zero-touch, self-healing networks. In this tutorial, we'll walk through creating a fine-tuning playbook that integrates synthetic data generation, training, and evaluation pipelines. We'll demonstrate how to build an AI-driven reasoning model capable of autonomously performing NoC engineer workflows — detecting issues, calling tools, and remediating incidents without human intervention.  Familiarity with Python, LLM fine-tuning concepts, and basic containerization.",
      "format": "In-Person",
      "industry": "Telecommunications",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "How to process raw incident data into training datasets.",
        "How to use a \"Teacher\" model to generate synthetic reasoning traces.",
        "How to fine-tune a \"Student\" model using NVIDIA NeMo.",
        "How to evaluate the agent's ability to call tools and solve real-world problems."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, CUDA, NeMo",
      "session_id": "DLIT81936",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Amparo Canaveras",
          "title": "Sr. Solutions Architect Generative AI"
        },
        {
          "company": "NVIDIA",
          "name": "Aiden Chang",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Ari Uskudar",
          "title": "Telco AI Principal"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Fine-Tune a Telco Reasoning Model: A Guide to Synthetic Data, Tool Calling, and Evaluation",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81936/"
    },
    {
      "description": "GPU clusters are fast, until they're not. We'll walk through five performance pitfalls from real researcher code, the fixes that made them disappear, and how to use technologies like CUDA streams, CUDA graphs, and custom kernels along the way.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "Learn how to spot when software bottlenecks—not GPU limits—are actually slowing down your training.",
        "Discover a small set of repeatable fixes that unlock outsized performance gains in real-world training runs.",
        "Unpack how CUDA streams, CUDA graphs, and custom kernels work together to drive consistently higher GPU utilization."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, Grace CPU, CUDA, CUDA-X, Blackwell",
      "session_id": "S82065",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Jane Street",
          "name": "Sylvain Gugger",
          "title": "ML Engineer"
        }
      ],
      "technical_level": "General Interest",
      "title": "Five Bottlenecks, Five Fixes: How We Avoid Leaving Training Performance on the Table",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82065/"
    },
    {
      "description": "Molecular dynamics simulation, using the explicit equations of quantum mechanics (e.g., density functional theory) is the workhorse of computational materials science and chemistry. Until recently, more than a third of academic supercomputer time was spent on this task. We can now speed up this task by a factor of a million or more using machine learning, without substantial loss of accuracy. This amount of speedup leads to a qualitative change in what is possible to simulate, and therefore to understand, in a wide range of application fields, all the way from catalysis and battery materials to glasses and alloys, and even drug discovery. While some, by now classic, rules of thumb continue to deliver (e.g., more data yield better accuracy), the problem may be special in some ways. We find that encoding the known laws of physics, such as symmetry and locality, help generalization.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Research: Academic",
      "key_takeaways": [
        "Foundation models for chemistry and materials science generalize widely.",
        "Molecular dynamics for materials (and maybe soon, organic molecules, too) is now routinely done using machine learning potentials.",
        "Adopting NVIDIA tools such as ALCHEMI Toolkit and cuEquivariance keeps models on the cutting edge.",
        "Collaborating with NVIDIA to accelerate models is essential for success."
      ],
      "nvidia_technology": "CUDA, Hopper",
      "session_id": "S81802",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "University of Cambridge",
          "name": "Gabor Csanyi",
          "title": "Professor"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Foundation Models Revolutionize Molecular Dynamics Simulations for Chemistry and Materials Science",
      "topic": "Computational Chemistry / Materials Science",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81802/"
    },
    {
      "description": "Join a Stripe engineering lead for an inside look at how Stripe’s AI‑powered fraud prevention solution, Radar, is built with NVIDIA hardware and software to detect and block fraudulent transactions at massive scale, including emerging risks and new fraud vectors from agentic commerce to free trial abuse prevention. Trained on patterns derived from more than $1.4 trillion in annual payment volume, Radar uses advanced ML techniques to surface high‑confidence signals that reduced fraud rates for its users by 17% year-over-year.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "How Stripe builds for new fraud risks that come with agentic commerce and designs our models to identify and detect emerging fraud patterns",
        "How Stripe uses NVIDIA GPUs and CUDA and AWS infrastructure to train and deploy our fraud-prevention models",
        "How Stripe incorporates new signals and data points to evolve and train our AI models"
      ],
      "nvidia_technology": "Cloud / Data Center GPU, CUDA",
      "session_id": "S82252",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Stripe",
          "name": "Nathan Brasher",
          "title": "Tech Lead, Radar Machine Learning"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Fraud Prevention in the Age of Agentic Commerce",
      "topic": "Fraud Detection",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82252/"
    },
    {
      "description": "AI venture funding reached $100.8 billion in 2024—37% of all venture capital (VC) investment. Yet, most promising AI startups don’t fail due to weak markets or teams, but from challenges bridging research and production: scaling infrastructure, solving domain-specific problems, and building defensible technical moats. The focus is now on “doing AI right” for real impact. Top VCs now treat NVIDIA partnership as essential portfolio infrastructure—not just for compute, but as a strategic accelerator that helps founders solve hard problems, lead technically, and speed up R&D. This session shares how startups used NVIDIA’s ecosystem to achieve breakthrough results.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "How to Bridge Research and Production: Learn strategies for scaling infrastructure, solving domain-specific challenges, and building defensible technical moats.",
        "NVIDIA as a Strategic Accelerator: Discover how partnering with NVIDIA can help startups solve complex problems, establish technical leadership, and accelerate R&D.",
        "Real-World Success Stories: Gain insights from portfolio companies that leveraged NVIDIA’s full-stack ecosystem to achieve breakthrough results and meaningful impact."
      ],
      "nvidia_technology": "NVIDIA NIM",
      "session_id": "S81830",
      "session_type": "Panel",
      "speakers": [
        {
          "company": "Sensible Biotechnologies",
          "name": "Miroslav Gasparek",
          "title": "CEO and Co-Founder"
        },
        {
          "company": "NVIDIA",
          "name": "Alyss Noland",
          "title": "DGX Cloud Dev Ecosystem Development - DevRel"
        },
        {
          "company": "Roboflow",
          "name": "Joseph Nelson",
          "title": "Co-Founder and CEO"
        },
        {
          "company": "Aible",
          "name": "Arijit Sengupta",
          "title": "Founder and CEO"
        },
        {
          "company": "Radical Ventures",
          "name": "David Katz",
          "title": "Partner"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "From Challenge to Competitive Advantage: How VC Startups Leverage NVIDIA to Win Markets",
      "topic": "Code / Software Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81830/"
    },
    {
      "description": "This session explores proven strategies, real-world challenges, and ecosystem shifts shaping the journey from building breakthrough technologies to securing critical funding for women tech founders. Despite receiving only a fraction of global venture capital, women-led startups are achieving remarkable growth and impact—particularly in deep tech and AI sectors. Join a dynamic conversation with investors, entrepreneurs, and advocates to uncover pathways for women in tech to access capital, expand networks, and drive innovation, ensuring talent and ambition translate into tangible business success.",
      "format": "Virtual",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Learn how to balance innovation with the need to secure funding.",
        "Hear about the real-world challenges and opportunities faced by female tech founders.",
        "Learn about inspirational stories to inspire your own collaboration and growth."
      ],
      "nvidia_technology": "Audio2Face",
      "session_id": "S81720",
      "session_type": "Panel",
      "speakers": [
        {
          "company": "Women in AI",
          "name": "Moojan Asghari",
          "title": "Founder, Women in AI"
        },
        {
          "company": "NVIDIA",
          "name": "Jessica Driscoll",
          "title": "Regional Lead, Startup Ecosystem, UK and Ireland"
        },
        {
          "company": "ConceptionX",
          "name": "Carrie Baptist",
          "title": "CSO"
        },
        {
          "company": "Normain",
          "name": "Sara Landfors",
          "title": "Co-Founder and CEO"
        }
      ],
      "technical_level": "General Interest",
      "title": "From Code to Capital: Empowering Women Tech Founders",
      "topic": "",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81720/"
    },
    {
      "description": "While AI practitioners push rapid innovation, IT teams struggle with fragmented guidance and operational overhead when deploying secure, high-performance AI infrastructure across data center and edge to meet training and inference needs. Discover how prescriptive, validated reference designs such as Cisco Secure AI Factory with NVIDIA simplifies deployment and operations of trusted AI infrastructure with the same capabilities and operating model across data center and edge. Get actionable patterns to accelerate deployment and focus on innovation.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Identify the infrastructure bottlenecks slowing your AI deployment across data center and edge",
        "Discover the security and observability stack that makes AI infrastructure production-ready",
        "Learn how validated blueprints eliminate architectural guesswork and deployment risk",
        "See Cisco + NVIDIA designs in action: consistent operations for distributed AI at scale",
        "Walk away with patterns to deploy faster, operate unified, and free teams to innovate"
      ],
      "nvidia_technology": "",
      "session_id": "S82186",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Cisco Systems",
          "name": "Kevin Wollenweber",
          "title": "SVP and GM Networking – Data Center and Internet Infrastructure"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "From Complexity to Confidence: Core-to-Edge Secure AI Factories with Consistent Operations (Presented by Cisco Systems)",
      "topic": "AI Networking, 5G/6G",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82186/"
    },
    {
      "description": "As Jaguar Land Rover (JLR) continues its digital transformation journey, AI surrogates are unlocking new frontiers in computer-aided engineering (CAE). This session explores how the Aerodynamics team pioneered AI as an essential engineering “pilot,” proving its value in one of the most complex and high-impact CAE domains. Through robust data workflows and scalable deployment strategies, this approach has enabled faster, cleaner, and more reliable delivery of AI-driven insights. Discover how this foundation is now being expanded across broader CAE disciplines, transforming modeling efficiency, data integrity, and decision-making throughout the organization.",
      "format": "In-Person",
      "industry": "Automotive / Transportation",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "From Experimentation to Execution: How the aerodynamics domain validated AI’s role in high-value CAE workflows",
        "Data at the Core: Approaches to building robust, repeatable, and transparent data pipelines for AI model development and deployment",
        "Scalable Impact: Lessons learned in extending aerodynamic AI surrogates across other CAE functions at JLR"
      ],
      "nvidia_technology": "CUDA-X",
      "session_id": "S81736",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Jaguar Land Rover",
          "name": "Christopher Johnston",
          "title": "Sr. Technical Specialist, Vehicle Simulation"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "From Concept to Capability: Scaling AI Surrogates Across JLR CAE",
      "topic": "CAE",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81736/"
    },
    {
      "description": "The next wave of business innovation is powered by AI developer platforms that grant agents deep access to your enterprise knowledge. In this session, discover practical strategies for leveraging these platforms to connect information from across business systems, automate workflows, and support smarter decision-making. Live demonstrations will highlight how you can use AI-powered tools to streamline processes, enhance compliance, and guide innovation in your own organization. You’ll leave with frameworks and real-world examples to help scale knowledge, efficiency, and impact—no matter your role or industry.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Integrate Enterprise Data to Empower AI Agents: The session emphasizes how connecting information across business systems allows AI agents to access and act on enterprise knowledge, transforming isolated data into actionable insights.",
        "Automate and Optimize Workflows for Smarter Decisions: Learn practical methods to use AI-driven platforms to automate routine operations, enhance compliance, and improve decision accuracy and speed across teams.",
        "Adopt Scalable Frameworks for Innovation and Impact: See real-world examples and reusable frameworks to help organizations of any size scale efficiency, knowledge sharing, and innovative capabilities through AI integration."
      ],
      "nvidia_technology": "NeMo, NVIDIA NIM, NVIDIA AI Enterprise, cuVS",
      "session_id": "S81570",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Rachel Allen",
          "title": "Engineering Manager, Cybersecurity AI"
        },
        {
          "company": "NVIDIA",
          "name": "Roopa Prabhu",
          "title": "Sr. Director of Engineering"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "From Data to Decisions: Enabling AI Agents With Business Knowledge",
      "topic": "Retrieval-Augmented Generation (RAG)",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81570/"
    },
    {
      "description": "In this session, we’ll explore how K2K is applying data-compliant Vision-Language Models (VLMs) integrated with NVIDIA Cosmos Reason and Metropolis Blueprint for Video Search and Summarization (VSS) to turn passive footage into proactive decision-making. We’ll walk through how VLM-driven understanding enables cities to automate operational processes, anticipate incidents, and optimize resource allocation through granular semantic insight rather than traditional detection alone. We’ll also share how generated data, synthetic data and simulation pipelines are being used to scale robust urban AI without exposing personal identity data. The result: AI agents that don’t just “see,” but interpret, reason, and support real-time city operations in a trustworthy way.",
      "format": "Virtual",
      "industry": "Smart Cities / Spaces",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how the creation of a new VLM, trained with EU AI Act-compliant data, can generate more revenues in a city.",
        "Understand how the VLM helps to see a tangible return on investment."
      ],
      "nvidia_technology": "DeepStream, Metropolis, TAO Toolkit, NVIDIA NIM, Cosmos, Blueprint",
      "session_id": "S81867",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "K2K",
          "name": "Giuditta Zardoni",
          "title": "Executive Manager"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "From Data to Meaning: Vision-Language Models Shaping the Cities of Tomorrow",
      "topic": "Video Analytics",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81867/"
    },
    {
      "description": "As teams move from model training to deployment, the hardest challenges sit in inference: unpredictable latency, escalating GPU costs, inconsistent output quality, and agentic workflows requiring multiple reasoning hops and tool calls. Post-training and fine-tuning amplify these issues by introducing new model variants, quality requirements, and performance constraints. This session explores architecting distributed agentic inference pipelines that span GPU cloud, regional compute, and global edge environments. We’ll focus on the infrastructure patterns that reduce cost, improve throughput, stabilize quality, and support multi-step reasoning at scale. Learn practical frameworks for deciding what should run where, how to route inference across heterogeneous environments, and how to build post-training and fine-tuning outputs into a reliable, production-grade agentic pipeline.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Why inference—not training—is the hardest part of production AI, with latency, cost, and reliability emerging as the dominant constraints once models move into real-world use.",
        "How post-training and fine-tuning change the operational equation, introducing more model variants, stricter quality expectations, and tighter performance trade-offs",
        "What agentic and multi-step inference workflows require from infrastructure, including support for repeated reasoning hops, tool calls, and coordination across services",
        "Which inference architecture patterns are emerging, and how teams are beginning to balance performance, cost, and output quality as these patterns continue to evolve"
      ],
      "nvidia_technology": "RTX GPU, Blackwell",
      "session_id": "S82071",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Akamai Technologies",
          "name": "Jon Alexander",
          "title": "SVP Cloud Technology Product Management"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "From Fine-Tuning to Production: Designing Distributed Inference Systems (Presented by Akamai Technologies)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82071/"
    },
    {
      "description": "Specialized models are becoming necessary to win in the enterprise. Come speak to our field team about your challenges with deploying specialized models. They can share real stories from their experience with customization strategies, their successes and their failures. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Engage in Q&A with NVIDIA solution architects specializing in working with AI models in the field.",
        "Learn strategies for data curation, synthetic data generation, model evaluation, fine-tuning, and reinforcement learning.",
        "Leave with actionable strategies for customizing and optimizing AI models for production."
      ],
      "nvidia_technology": "NeMo, NVIDIA AI Enterprise",
      "session_id": "CWES81457",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Seth Henneman",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Eric Pham-Hung",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Yang Yu",
          "title": "Solutions architect"
        },
        {
          "company": "NVIDIA",
          "name": "Narimane Hennouni",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Aastha Jhunjhunwala",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Sukrit Rao",
          "title": "Solutions Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "From Foundation to Full-Throttle: Expert Advice on AI Model Customizations",
      "topic": "Training AI Models",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81457/"
    },
    {
      "description": "In the past two years, the NVIDIA Grace CPU has reshaped the foundation of modern data centers. Its successor, the NVIDIA Vera CPU, will become available soon. Vera marks an exciting evolution of CPU design. Based on a custom ARM CPU core, this new CPU provides unprecedented performance and efficiency. But what does this mean for system integrators and developers? In this talk, we will discuss how Vera improves upon Grace. We'll analyze the new features of the upcoming Vera CPU, and how you can use them to unleash Vera’s full potential. We will also revisit typical applications and usage scenarios, and detail how Vera’s architecture is particularly geared toward speeding up such workloads.",
      "format": "In-Person",
      "industry": "Hardware / Semiconductor",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn what NVIDIA’s next-generation data center CPU platform has to offer.",
        "Find out what workloads benefit most from Vera’s architectural improvements.",
        "Understand how you can maximize your application’s performance on both Grace and Vera CPUs."
      ],
      "nvidia_technology": "Grace CPU",
      "session_id": "S81680",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Matthias Langer",
          "title": "AI and DevTech Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Lukas Alt",
          "title": "DevTech Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "From Grace to Vera: NVIDIA’s Next ARM-Based Data Center CPU",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81680/"
    },
    {
      "description": "As AI workloads grow in size and complexity, infrastructure teams are under pressure to deliver high GPU utilization, predictable performance, and strong operational controls across a mix of Kubernetes and bare-metal environments. Limited visibility into GPU behavior can quickly lead to inefficiency, cost overruns, and governance gaps. This session examines practical approaches to observing, provisioning, and optimizing GPU-accelerated infrastructure for AI workloads. We’ll discuss strategies for end-to-end GPU monitoring, automation patterns for Kubernetes and bare-metal environments, and techniques for aligning performance optimization with policy and governance requirements. The focus is on repeatable, scalable practices that infrastructure and platform teams can apply in real-world AI environments.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "GPU Observability: Techniques for gaining actionable insight into GPU utilization, performance, and health across the AI stack",
        "Kubernetes and Bare-Metal Automation: Patterns for provisioning and managing GPU resources using policy-driven workflows across heterogeneous environments",
        "Compute Optimization and Governance: Approaches to improving resource efficiency while maintaining operational control and compliance for AI workloads, with the support of the NVIDIA AI Computing by HPE portfolio"
      ],
      "nvidia_technology": "Blackwell",
      "session_id": "S82139",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "HPE",
          "name": "Constantinos Venetsanopoulos",
          "title": "Director of Product"
        },
        {
          "company": "HPE",
          "name": "Rajeev Bhardwaj",
          "title": "VP/Chief Product Officer - VME/Morpheus/OpsRamp"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "From Idle to Optimized: Managing GPUs, Kubernetes, and Automated Compute at Scale (Presented by HPE)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82139/"
    },
    {
      "description": "Join us as we explore SHI's approach to supporting customers' AI journies and adoption of generative AI within their existing enterprise environments. We’ll share best practices and challenges encountered in identifying, prototyping, building, and scaling AI applications.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Hear about a novel and effective framework and approach for business leaders, developers, and IT operations teams.",
        "Imagine and ideate with advanced AI technologies including vision AI, digital twins, agentic frameworks, digital AI ambassadors, and other high-impact generative and agentic AI patterns for high-value use case prioritization.",
        "Rapidly prototype and experiment with those high-impact use cases tailored to their unique business needs and desired outcomes.",
        "Properly plan for successful production adoption of advanced AI solutions to achieve those outcomes."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, RTX GPU, DGX Platform, CUDA, Omniverse, RAPIDS, Audio2Face, Avatar Cloud Engine (ACE), Base Command Manager, Multi-Instance GPU (MIG), NeMo, NVLink / NVSwitch, NVIDIA NIM, Blackwell, NVIDIA AI Enterprise, Cosmos, Blueprint, DGX Spark, DGX Station, NVIDIA Run:ai, Mission Control, Nemotron",
      "session_id": "EX82210",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "SHI International Corp.",
          "name": "Jack Hogan",
          "title": "VP, Advanced Growth Technologies"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "From Imagination to Experimentation to Full-Scale Adoption: Ensure a Successful Generative AI Project (Presented by SHI)",
      "topic": "Retrieval-Augmented Generation (RAG)",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82210/"
    },
    {
      "description": "Get a comprehensive guide to constructing a complete, high-performance data science pipeline that leverages GPU acceleration throughout its life cycle. We'll sequentially cover the pipeline's core components: data ingestion, exploration, processing, and model training. For the initial stages of data ingestion and exploration, we'll demonstrate the use of GPU-accelerated frameworks, specifically Polars and/or cuDF pandas, to achieve significant speedups in data manipulation and analysis. Then we'll showcase the power of the cuML library (part of the RAPIDS ecosystem) for training and testing various machine learning models entirely on the GPU. Attendees need basic Python skills and introductory data science knowledge, but no prior GPU experience is required.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "End-to-End Pipeline Creation: Construct a seamless GPU-accelerated pipeline that integrates data ingestion, processing, and model training for large-scale workflows.",
        "Accelerated Data Operations: Utilize Polars and cuDF pandas to achieve significant speedups in exploratory data analysis and manipulation tasks.",
        "GPU Model Training: Gain hands-on experience training and tuning standard machine learning models, such as logistic regression and random forests, entirely on the GPU using cuML.",
        "Performance Optimization: Master the use of nvidia-smi and profiling tools to monitor system metrics, identify bottlenecks, and maximize GPU utilization.",
        "Practical Migration Strategy: Apply a provided checklist and reproducible notebooks to effectively migrate legacy CPU-based workflows to high-performance GPU environments."
      ],
      "nvidia_technology": "RAPIDS, cuDF, cuML",
      "session_id": "DLIT81754",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "William Hill",
          "title": "Developer Advocate, Data Science"
        },
        {
          "company": "NVIDIA",
          "name": "Allison Ding",
          "title": "Developer Advocate - Data Science"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "From Ingestion to Inference: Mastering the High-Performance GPU Data Science Pipeline",
      "topic": "Data Analytics / Processing",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81754/"
    },
    {
      "description": "Generative AI is rapidly becoming the defining workload of modern computing, but securing these powerful systems without compromising performance is still a challenge for most enterprises. This session dives into how you can lock down your most valuable AI assets—models, data, and prompts—while continuing to push the limits on speed, scale and insights. Google and NVIDIA will share how confidential computing on NVIDIA GPUs can protect your data and model using accelerated computing, so your information stays shielded even while in use.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Discover how Blackwell and Vera Rubin NVL72 unlock cutting-edge confidential computing for large-scale AI.",
        "Learn how workloads can run securely in the cloud, on premises, or in hybrid environments to meet diverse enterprise needs.",
        "Hear how real customers are already using NVIDIA GPU confidential computing to move critical AI projects into production with confidence."
      ],
      "nvidia_technology": "Hopper, Blackwell",
      "session_id": "S81638",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Google",
          "name": "Nelly Porter",
          "title": "Director of Product Management, Trusted Cloud"
        },
        {
          "company": "NVIDIA",
          "name": "Emily Sakata",
          "title": "Product Manager"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "From Isolation to Integration: Evolving Confidential Computing for a Scalable, Secure Future",
      "topic": "Confidential Compute",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81638/"
    },
    {
      "description": "As conventional development of machine vision solutions is time-consuming and costly, and AI-driven automation accelerates, the ability to virtually commission vision systems before physical deployment is a game-changer. Basler has developed a comprehensive workflow for virtual commissioning by creating digital twins of machine vision systems built on NVIDIA Omniverse and OpenUSD. This approach helps companies configure, simulate, and optimize vision solutions in a virtual environment—cutting prototyping costs, reducing deployment risks, and speeding up AI-driven automation. This talk will explore how Basler transformed traditional camera hardware into interoperable, scalable virtual models that enable sensor realistic simulation of vision systems and synthetic data generation for AI training.",
      "format": "Virtual",
      "industry": "Manufacturing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how OpenUSD provides a unified structure for vision system data, ensuring interoperability across platforms and workflows.",
        "Learn how integrating with NVIDIA Omniverse helps build high-fidelity digital twins that replicate real-world conditions.",
        "Learn how domain randomization and synthetic data generation accelerates model training, improves performance in edge cases, and ensures models trained in simulation perform reliably in real-world environments.",
        "Learn how the end-to-end virtual commissioning workflow—configure, simulate, train, optimize, deploy, iterate—reduces prototyping costs, accelerates development cycles, and minimizes deployment risks.",
        "Basler’s roadmap for advancing vision system virtualization to support next-generation AI and robotics applications"
      ],
      "nvidia_technology": "Jetson, RTX GPU, TensorRT, Omniverse, Omniverse Replicator, Triton",
      "session_id": "S81993",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Basler AG",
          "name": "Justus Basler",
          "title": "Application Engineer"
        },
        {
          "company": "Basler AG",
          "name": "Kamil Lelowicz",
          "title": "Machine Learning Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "From Physical Vision Systems to OpenUSD: Building Digital Twins for Virtual Commissioning",
      "topic": "Industrial Digitalization / Digital Twin",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81993/"
    },
    {
      "description": "Large language models can now do far more than autocomplete code—they can act as real teammates. In this session, we’ll walk through how to turn an LLM into an autonomous engineer that can plan work, call tools, and ship real features. We’ll unpack the core loop behind modern coding agents: breaking down tickets, reading docs, editing code, running tests, and self-correcting when things fail. You’ll see how to wire agents into your existing stack—IDEs, repos, CI, and issue trackers—using only standard developer workflows. We’ll discuss patterns for letting agents safely debug their own mistakes, roll back bad changes, and open production-ready pull requests. Finally, we’ll look ahead at “long-term context:” agents that remember whole work sessions, not just a single prompt, and what that unlocks for reliability, velocity, and real-world software teams of every size.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Autonomous AI requires agents that can plan, execute, and self-correct using standard developer tools.",
        "To solve complex tasks, an AI must be able to read documentation and debug its own code.",
        "The future of software AI is long-term context, moving from single prompts to entire work sessions."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, Grace CPU, DGX Platform, HGX, CUDA, TensorRT, Infiniband Networking, Hopper, CUDA-X, cuDDN, NCCL, Triton, Blackwell",
      "session_id": "S81825",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Cognition",
          "name": "Silas Alberti",
          "title": "Founding Team"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "From Prompt to Pull Request: Engineering Coding Agents into Your Stack",
      "topic": "Code / Software Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81825/"
    },
    {
      "description": "In this introductory session, we’ll show how we’re taking ML for \"World of Tanks: HEAT\" from notebooks into the battlefield using NVIDIA Triton. Starting by setting the machine learning context, we'll clarify what a “strategic recommendation system” means in practice within a game, and how we prepare data, choose a model, and evaluate it in a way that aligns with player experience. Then we’ll unpack the serving side: what changes when your models run constantly during battles, why naive in-process inference doesn’t fit, and how Triton helps us standardize deployment across CPU and GPU targets. We’ll discuss how we plan for CPU to GPU migration, evaluate dynamic batching for high-frequency models, leverage ONNX and TensorRT, and use Triton’s Model Analyzer to explore the performance envelope before real players arrive.",
      "format": "In-Person",
      "industry": "Gaming",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "How to bridge the gap between an ML prototype and an always-on production model in a live game",
        "An end-to-end view of a Triton-powered inference path in a real-time game, from request to response",
        "A practical “question checklist” to evaluate your own readiness for Triton-based production inference even before launch",
        "Concrete ways Triton features (dynamic batching, shared memory, TensorRT integration, Model Analyzer) can help you scale responsibly"
      ],
      "nvidia_technology": "TensorRT, Triton, NVIDIA AI Enterprise",
      "session_id": "S81622",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Wargaming Group",
          "name": "Dmitry Nozhnin",
          "title": "Product Director, AI"
        },
        {
          "company": "NVIDIA",
          "name": "Dora Csillag",
          "title": "Sr. Solutions Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "From Prototype to Playable: Bringing Scalable ML Into \"World of Tanks: HEAT\" With NVIDIA Triton",
      "topic": "Performance Optimization",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81622/"
    },
    {
      "description": "Join our experts for a candid, face-to-face discussion on the critical steps needed to take your AI agents from proof-of-concept to production. We'll go beyond the basics, diving into real-world challenges like building for scale, ensuring robust security, and maintaining performance in enterprise environments. This is your chance to get practical advice, ask your toughest questions, and learn what it takes to build agents that truly deliver value for the business. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Learn how to design a robust and scalable architecture necessary to support high-volume, continuous usage in demanding enterprise environments.",
        "Learn the critical steps for embedding enterprise-grade security, access controls, and strict data governance to ensure agent solutions remain compliant and trustworthy.",
        "See practical strategies for maintaining consistent performance and reliability through continuous monitoring, and the effective management of the agent life cycle.",
        "Understand the strategic shift needed to define success based on measurable return on investment and business value, moving beyond simple technical feasibility."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, Ethernet Networking, NeMo, NVIDIA NIM, Blackwell, NVIDIA AI Enterprise",
      "session_id": "CWES81626",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Meghana Puvvadi",
          "title": "Director of Engineering, AI/ML Enterprise"
        },
        {
          "company": "NVIDIA",
          "name": "Michael Demoret",
          "title": "Engineering Manager, Cybersecurity AI"
        },
        {
          "company": "NVIDIA",
          "name": "Rachel Allen",
          "title": "Engineering Manager, Cybersecurity AI"
        },
        {
          "company": "NVIDIA",
          "name": "JB Blair",
          "title": "Solutions Architect Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Dhruv Nandakumar",
          "title": "Cybersecurity AI Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Rich Harang",
          "title": "Principal AI Security Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Hsin Chen",
          "title": "Sr. Data Scientist, Cybersecurity AI"
        },
        {
          "company": "NVIDIA",
          "name": "Nic Borensztein",
          "title": "Principal Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Matt Penn",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Sean Lopp",
          "title": "Sr. Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "From Sandbox to Scale: Build Production-Ready AI Agents for Your Business",
      "topic": "Retrieval-Augmented Generation (RAG)",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81626/"
    },
    {
      "description": "In today's real-time markets, every millisecond and every signal matters. Financial firms are overwhelmed by the scale and variety of data, from ultra-low-latency market feeds to an explosion of unstructured and alternative content such as news, filings, research, and transcripts. Traditional research and trading stacks struggle to turn these data into production-grade strategies fast enough and with the accuracy needed to generate sustainable alpha. By fusing precise quantitative time-series data with unstructured intelligence and running it on NVIDIA GPUs, the bank can build domain-accurate models, reduce hallucinations through time-aware retrieval and vector search, and safely deploy agentic AI that supports quants, researchers, and traders in real time; shortening the path from research to live deployment, and improving productivity through research copilots and execution agents.",
      "format": "In-Person",
      "industry": "Financial Services",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Holistic AI for Capital Markets : How combining time-series market data with unstructured and alternative data unlocks richer, more actionable insights for alpha generation, liquidity discovery, and risk management",
        "Tier 1 Bank Case Study: How a leading global bank moved from siloed research and trading infrastructure to an AI-first stack using KX and NVIDIA to build, train, and deploy models faster and with greater confidence",
        "Agentic AI on NVIDIA's AI Factory Stack: How the joint KX and NVIDIA solution uses NVIDIA AI Enterprise such as NeMo, NIM, cuVS, CUDA, GPU, and AI acceleration like NVLink to power research assistants, execution copilots, and dynamic risk engines",
        "Built for Trading Speed: How real-time ingestion, time-aware vector search, and retrieval-augmented generation-enhanced LLMs turn raw market data and documents into executable trading strategies in seconds, instead of hours.",
        "Production-Ready AI Factory Blueprint: A take-home AI factory blueprint and implementation checklist for scaling financial AI programs with domain accuracy, explainability, and guardrails, while maximizing return on investment and minimizing total cost of ownership"
      ],
      "nvidia_technology": "CUDA, CUDA-X, cuDF, NeMo, NVLink / NVSwitch, NVIDIA NIM, NVIDIA AI Enterprise, cuVS",
      "session_id": "S81957",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "KX",
          "name": "Ashok Reddy",
          "title": "CEO"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "From Signal to Strategy: Unlock Alpha With AI-Powered Research and Trading",
      "topic": "Retrieval-Augmented Generation (RAG)",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81957/"
    },
    {
      "description": "Discover how to turn the vision of AI into a tangible reality within the industry. This session explores the organization, design, and deployment of “AI factories,” environments that integrate data, algorithms, and processes to accelerate innovation. We will share our experience creating real use cases on the NVIDIA platform, collaborating with strategic partners, and driving operational efficiency—a complete journey through digital transformation that turns AI into a true value engine for the organization.",
      "format": "Virtual",
      "industry": "Manufacturing",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Understand how the RTX Pro 6000SE versatility serves as the reference architecture for industry, enabling seamless multi-workload processing across enterprise AI, physical AI manufacturing, and engineering simulation workflows.",
        "Hear real-world deployment strategies for autonomous manufacturing systems, including quality control automation and digital twin integration. (Metropolis, Isaac Sim-lab, VSS)",
        "See a step-by-step methodology for deploying AI across business operations, from automated decision-making systems to intelligent resource allocation and supply chain optimization. (NIM, Blueprints, data fly-wheel)",
        "Learn how AI transforms and accelerates V-development beyond software GPU acceleration."
      ],
      "nvidia_technology": "Omniverse, Metropolis, CUDA-X, NeMo, Cosmos",
      "session_id": "S81987",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Horse Technologies",
          "name": "Raquel Hernansanz",
          "title": "Metaverse and Industrial Excellence Director"
        },
        {
          "company": "Deloitte",
          "name": "Andres Benitez Pozo",
          "title": "Manager"
        }
      ],
      "technical_level": "General Interest",
      "title": "From Vision to Reality: Building AI Factories",
      "topic": "Image / Video Detection & Recognition",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81987/"
    },
    {
      "description": "This course delivers a hands-on introduction to GPU-accelerated computing in Python, empowering developers to build fast, scalable applications using NVIDIA’s CUDA ecosystem. Through guided notebooks, participants master CuPy for array acceleration, cuDF for GPU DataFrames, and the cuda-python API to write custom kernels—all without leaving Python. Real-world exercises in data science, machine learning, and scientific computing emphasize performance, interoperability, and end-to-end efficiency. Learners progress from drop-in speedups to fully integrated GPU pipelines, mastering data movement, asynchronous execution, and profiling with Nsight tools. By the end, attendees can transform CPU-bound Python code into production-grade GPU solutions, bridge prototyping to deployment, and apply best practices for performance and reproducibility in modern accelerated computing. Certificate: Upon successful completion of the assessment, you’ll receive an NVIDIA certificate to recognize your subject matter competency and support your professional career growth. Basic Python competency including familiarity with variable types, loops, conditional statements, functions, and array manipulations. NumPy competency including the use of ndarrays and ufuncs. No previous knowledge of CUDA programming is assumed.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Build end-to-end GPU-accelerated applications using CUDA Python and interoperable libraries",
        "Write, compile, and launch custom CUDA kernels directly in Python",
        "Accelerate numerical and analytical workloads with CuPy and cuDF as drop-in replacements for NumPy and Pandas",
        "Integrate GPU operations seamlessly into data science, ML, and HPC pipelines",
        "Ensure reproducibility, performance, and scalability using Nsight profiling and best-practice design patterns"
      ],
      "nvidia_technology": "CUDA",
      "session_id": "DLIW82265",
      "session_type": "Full-Day Workshop",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Bryce Lelbach",
          "title": "Principal Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Katrina Riehl",
          "title": "Principal Technical Product Manager"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Fundamentals of GPU-Accelerated Workflows with CUDA Python",
      "topic": "Programming Languages / Compilers",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dliw82265/"
    },
    {
      "description": "This hands-on lab introduces roboticists, AI developers, and pipeline engineers to NVIDIA Cosmos world foundation models (WFMs) for synthetic data generation in robot learning pipelines. Participants will learn how to leverage simulation scenes from Isaac Sim and real-life images and videos to provide initial datasets and augment them with Cosmos Transfer, then evaluate data quality with Cosmos Reason—all within a production-ready robotics pipeline. Through this session, you'll experience an end-to-end data pipeline workflow, leveraging Cosmos WFMs to generate synthetic data from real-world and physically-accurate simulated data, and using Cosmos Reason as an intelligent critic to validate data quality. By completing this lab, you'll unlock the ability to generate high-quality training datasets, accelerating your action model development. Familiarity with robotics simulation concepts (what is synthetic data, why it matters) Basic Python scripting experience (reading/writing scripts, understanding function calls) Understanding of machine learning fundamentals (training, fine-tuning, evaluation) Familiarity with Cosmos (recommended course: An Introduction to NVIDIA Cosmos for Physical AI)",
      "format": "In-Person",
      "industry": "Manufacturing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Capture diverse synthetic robot training data using Isaac Sim and Omniverse RTX libraries.",
        "Generate synthetic datasets using Cosmos Transfer to create variations with structural and visual consistency.",
        "Evaluate synthetic data quality using Cosmos Reason to validate usability for downstream model training.",
        "Integrate Cosmos world models into production robotics pipelines for continuous data generation."
      ],
      "nvidia_technology": "RTX GPU, Isaac, Omniverse, DLSS, Omniverse Replicator, NVIDIA NIM, Blackwell, NVIDIA AI Enterprise, Cosmos",
      "session_id": "DLIT81644",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Stefanie Manzinger",
          "title": "Sr. Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Alexander Schwarz",
          "title": "Solutions Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Generate Synthetic Data for Physical AI with NVIDIA Cosmos World Foundation Models",
      "topic": "Synthetic Data Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81644/"
    },
    {
      "description": "Marvik presents a real-world use case demonstrating how computer vision, simulation, and NVIDIA Isaac Sim can be used to design more intuitive and safer human–machine interfaces for heavy equipment. The project explores gesture-based control of an excavator, using a standard camera and real-time pose estimation to translate operator movements into machine commands inside a high-fidelity simulated environment. By validating control logic, latency, and usability in simulation before touching physical hardware, this approach reduces development risk, shortens iteration cycles, and lowers costs. The session highlights how simulation-first workflows enable scalable foundations for future applications such as remote operation, semi-autonomous machinery, and intelligent operator assistance, while improving safety and operational efficiency in industrial settings.",
      "format": "In-Person",
      "industry": "Manufacturing",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Simulation-first development enables faster iteration, lower risk, and reduced cost when designing complex human–machine control systems.",
        "Gesture-based interfaces can significantly improve usability and safety for heavy machinery by reducing operator cognitive load and training time.",
        "A modular, ROS 2–based architecture validated in NVIDIA Isaac Sim provides a practical pathway for sim-to-real deployment on physical equipment."
      ],
      "nvidia_technology": "CUDA",
      "session_id": "EX82215",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "Marvik",
          "name": "Santiago Ferreiros Cabrera",
          "title": "Tech Lead"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Gesture-Based Control of Heavy Equipment Using Simulation-First Robotics Workflows (Presented by Marvik)",
      "topic": "Robotics Simulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82215/"
    },
    {
      "description": "The pace of AI regulations is increasing, and the complexity of complying with different laws is challenging for global companies. For example, EU member countries are passing \"companion\" regulations for the Artificial Intelligence Act. Meanwhile, the U.S. federal government hasn't adopted any AI regs, but many states have, and the current administration's focus differs from that of the previous administration. At the same time, Congress is focused primarily on child safety. How can companies comply with laws in such a fragmented environment?",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Understand the current state of AI regulations and governance frameworks across different countries and regions.",
        "Gain insights into the core ethical principles that underpin AI governance, and learn how these principles are being integrated into global AI governance structures and practices.",
        "Stay updated on the latest advancements and future directions in AI safety research."
      ],
      "nvidia_technology": "Maxine, Audio2Face, BioNeMo, NeMo, NVIDIA NIM, NVIDIA AI Enterprise",
      "session_id": "S81548",
      "session_type": "Panel",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Iain Cunningham",
          "title": "VP and Deputy General Counsel"
        },
        {
          "company": "California Chamber of Commerce",
          "name": "Jennifer Barrera",
          "title": "President and CEO"
        },
        {
          "company": "Simmons & Simmons LLP",
          "name": "Minesh Tanna",
          "title": "Global AI Lead and Partner, Disputes & Investigations"
        }
      ],
      "technical_level": "General Interest",
      "title": "Global Governance and the Future of AI Safety Regulation",
      "topic": "Zero-Trust Security",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81548/"
    },
    {
      "description": "Vector databases with 1 trillion vectors cannot fit in memory, and VecDB ingestion and search requires the massive concurrency, memory bandwidth, and latency tolerance of GPUs. Come see new programming models, new storage server reference designs, new SSD offerings, and new customer-oriented scaled results.",
      "format": "In-Person",
      "industry": "Cloud Services",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "With a new SCADA API and infra, GPUs can effectively use O(100K) threads, HBM bandwidth, and latency tolerance to access storage much more effectively than CPUs.",
        "Industry partners in Storage-Next are adding high-IOPs, power-efficient SKUs to their roadmaps.",
        "We've launched cuGraph.WholeGraph and cuVS products based on SCADA GA that enable work on problems orders of magnitude bigger than possible before, with more than 2x better total cost of ownership.",
        "Customer proofs of concept show tangible benefits for document ingestion and search."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, Grace CPU, DGX Platform, HGX, CUDA, DOCA, RAPIDS, Infiniband Networking, Magnum IO, MGX, cuGraph, NSight Systems, NVLink / NVSwitch, Blackwell, cuVS, Dynamo",
      "session_id": "S81840",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "CJ Newburn",
          "title": "Distinguished Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Vikram Sharma Mailthody",
          "title": "Sr. Research Scientist"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "GPU Access to Unbounded Dataset Sizes Breaks Open Document Ingestion and Search",
      "topic": "Programming Languages / Compilers",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81840/"
    },
    {
      "description": "Explore how to build high-performance, GPU-accelerated image pre- and post-processing pipelines using nvImageCodec, NPP, DALI, and CV-CUDA across domains such as semiconductors, medical imaging, geospatial, and more. See how nvImageCodec (with nvTIFF, nvJPEG, nvJPEG2000, HTJ2K) accelerates decode/encode, how NPP provides low-level image primitives, and how CV-CUDA and DALI enable scalable, model-ready pipelines. We’ll share recent optimizations and roadmap highlights, and use this session to gather your feedback on requirements across these libraries. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Applications of accelerated image processing in domains such as semiconductor, medical imaging, and geospatial.",
        "Learn how to build high-performance, GPU-accelerated image pre- and post-processing pipelines using nvImageCodec, NPP, DALI, and CV-CUDA.",
        "Discover recent optimizations and roadmap highlights across nvImageCodec, HTJ2K, NPP, DALI, and CV-CUDA that can boost your end-to-end vision performance.",
        "Provide feedback on your image workloads and requirements to help shape future features in NVIDIA’s image processing libraries."
      ],
      "nvidia_technology": "CUDA, RAPIDS, CV-CUDA, DALI, MONAI, nvCOMP, Blackwell, Cosmos",
      "session_id": "CWES81910",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "David Lesage",
          "title": "Engineering Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Zoheb Khan",
          "title": "Sr. Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Makan Taghavi",
          "title": "Sr. Product Manager for Image Processing and Data Compression"
        },
        {
          "company": "NVIDIA",
          "name": "Mahesh Khadatare",
          "title": "Sr. CUDA Math Library Engineer and Team Lead"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "GPU-Accelerated Image Processing and Data Loading With NVIDIA Image Processing Libraries",
      "topic": "Image Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81910/"
    },
    {
      "description": "Join a special presentation from our 2025-26 Graduate Fellowship recipients to learn \"what's next\" from the world of research and academia. Sponsored projects involve a variety of technical challenges, including machine learning, humanoid robotics, computer vision, speech and audio, reliability, drug discovery, agentic AI and trustworthy AI. These minds lead the future in our industry, and we're proud to support the 2025-26 NVIDIA Graduate Fellows. We'll also announce the 2026-27 Graduate Fellows at this session. For more information on the NVIDIA Graduate Fellowship program, visit www.nvidia.com/en-us/research/graduate-fellowships",
      "format": "In-Person",
      "industry": "Academia / Higher Education",
      "intended_audience": "Research: Academic",
      "key_takeaways": [
        "Learn about some leading-edge research in academia from 10 top Ph.D. students.",
        "Become familiar with the students who are our 2025-26 NVIDIA Graduate Fellowship winners.",
        "Have an opportunity to inquire further into the Graduate Fellows' research."
      ],
      "nvidia_technology": "RTX GPU, IGX",
      "session_id": "S81467",
      "session_type": "Lightning Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Bill Dally",
          "title": "Chief Scientist and SVP of Research"
        },
        {
          "company": "University of Maryland, College Park",
          "name": "Sreyan Ghosh",
          "title": "Ph.D. Student"
        },
        {
          "company": "University of Southern California",
          "name": "Jiawei Yang",
          "title": "Ph.D. Student"
        },
        {
          "company": "University of Illinois Urbana-Champaign",
          "name": "Yunze Man",
          "title": "Ph.D. Student"
        },
        {
          "company": "University of Texas at Austin",
          "name": "Ruisi Cai",
          "title": "Ph.D. Student"
        },
        {
          "company": "Georgia Institute of Technology",
          "name": "Anish Saxena",
          "title": "Ph.D. Student"
        },
        {
          "company": "KAIST",
          "name": "Seul Lee",
          "title": "Ph.D. Student"
        },
        {
          "company": "Johns Hopkins University",
          "name": "Xiaogeng Liu",
          "title": "Ph.D. Student"
        },
        {
          "company": "Stanford University",
          "name": "Zhiqiang Xie",
          "title": "Ph.D. Student"
        },
        {
          "company": "Carnegie Mellon University",
          "name": "Tairan He",
          "title": "Ph.D. Student"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Graduate Fellowship Fast Forward Talks",
      "topic": "Image Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81467/"
    },
    {
      "description": "NVIDIA is teaming up with GCL for a hands-on lab on NVIDIA DGX Spark. In this lab, you will build an AI agent using the latest open-source NVIDIA and community model, and explore enhancing and extending the agent abilities.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Working understanding of agentic AI development",
        "The power of open-source models for local AI agents",
        "Benefits of DGX Spark for local AI development"
      ],
      "nvidia_technology": "DGX Spark",
      "session_id": "DLIT82308",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Michael Shen",
          "title": "Principal Product Manager"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Hands-On NVIDIA DGX Spark",
      "topic": "Code / Software Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit82308/"
    },
    {
      "description": "We explore the life cycle of AI-based high-resolution regional weather forecasting applications, which are used by businesses and governments to achieve independent local weather and climate prediction capabilities at high accuracy and unprecedented speed. We'll start by creating a custom high-resolution model, guiding you through preparing a training dataset, training a downscaling diffusion model with NVIDIA PhysicsNeMo, and packaging it for real-time use. We then use NVIDIA Earth2Studio to integrate our custom model with a global AI-based weather forecasting model, such as FourCastNet 3, to build a full regional forecasting workflow, enabling sharper and more actionable local forecasts. We'll then discuss leveraging custom weather observations to improve the accuracy of the predictions, and deploying the workflow in operations so it can be productized as an inference service. Previous DLI: https://learn.nvidia.com/courses/course-detail?course_id=course-v1:DLI+S-FX-31+V1",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn the principles of downscaling and nowcasting, why they are important for independent weather and climate forecasting capabilities, and how AI is revolutionizing these fields.",
        "Prepare training datasets for efficient regional high-resolution AI weather forecasting.",
        "Gain hands-on experience with using PhysicsNeMo to train a regional forecasting and downscaling models based on generative AI.",
        "Package the trained model for inferencing and build an interface for it in Earth2Studio.",
        "Combine the regional model with a global forecasting model such as FourCastNet 3 to build and deploy a regional forecasting pipeline."
      ],
      "nvidia_technology": "Modulus",
      "session_id": "DLIT81485",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Georg Ertl",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Ira Shokar",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Nicholas Geneva",
          "title": "Sr. Software Engineer, Modulus"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Hands-On With Earth-2: Building Local and National Weather Resilience With AI",
      "topic": "Climate / Weather / Ocean Modeling",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81485/"
    },
    {
      "description": "We present a unified, hardware-aware framework for scaling large video generation models—such as Diffusion Transformers (DiT)—without compromising visual fidelity. Learn how production-grade model compression techniques jointly eliminate the quadratic cost bottlenecks that traditionally limit long-horizon video modeling. The session covers domain-specialized sparse attention for video sequences, robust low-precision training pipelines including low-bit attention mechanism, and mixed-low-bit inference strategies alongside adaptive sparse attention. With industrial-scale results demonstrating 2x end-to-end acceleration from sparse attention and 60–70% savings via low-bit computation—all with near-zero accuracy loss—this talk provides a practical blueprint for building efficient, lossless, and deployable video generative models at scale.",
      "format": "In-Person",
      "industry": "Consumer Internet",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how domain-specific sparsification and supervised token selection reduce attention cost while preserving visual quality for ultra-long video sequences.",
        "Understand how MXFP8/INT8 quantization, smooth K/V transforms, randomized Hadamard transforms, and stochastic rounding jointly eliminate quantization bias and stabilize training in attention mechanism.",
        "Gain practical guidance for deploying video generation models using mixed-low-bit (e.g., FP4/FP8) kernels and lossless quantization workflows, and how ScalingAttention choose the optimal sparsity pattern for each workload, achieving maximum acceleration at minimal cost to fidelity."
      ],
      "nvidia_technology": "CUDA",
      "session_id": "S81693",
      "session_type": "Tutorial",
      "speakers": [
        {
          "company": "Kuaishou Technology",
          "name": "Guangyang Lu",
          "title": "Training Framework Engineer"
        },
        {
          "company": "Kuaishou Technology",
          "name": "Fei Li",
          "title": "Training Framework Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Guangyun Han",
          "title": "DevTech"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Hardware-Efficient and Lossless Video Generation at Scale: Native Sparse Attention and Mixed-Low-Bit Quantization",
      "topic": "Video Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81693/"
    },
    {
      "description": "Learn how an AI co-scientist that combines powerful reasoning models with science simulations can expand the aperture for human scientists to achieve breakthroughs in domains such as fusion energy and targeted alpha therapy for cancer. This session delves into strategies for overcoming the challenge of data scarcity for training, for employing reinforcement learning leveraging physics simulations, AI surrogates, emulators, and building AI agents that can reiterate autonomously.",
      "format": "In-Person",
      "industry": "Academia / Higher Education",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Trade-offs with domain-adaptive pre-training, supervised fine-tuning, reinforcement learning, and retrieval-augmented generation in data scarce regimes with open models",
        "Strategy on including data from experiments or simulations for model alignment",
        "Integrating and orchestrating AI agents running LLMs and physics simulations in an iterative workflow"
      ],
      "nvidia_technology": "Hopper, BioNeMo, NeMo",
      "session_id": "S81632",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Los Alamos National Laboratory",
          "name": "Radha Bahukutumbi",
          "title": "Group Leader"
        },
        {
          "company": "Los Alamos National Laboratory",
          "name": "Ping Yang",
          "title": "Deputy Director, Staff Scientist"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Harness Agentic AI and Reasoning Models to Accelerate Scientific Discovery",
      "topic": "Supercomputing",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81632/"
    },
    {
      "description": "The next era in surgery will be defined by the convergence of advanced technology and human-centered care. In this session, Neda Cvijetic, global head of robotics and digital R&D at Johnson & Johnson MedTech, will highlight the role of AI, simulation, and accelerated computing in Johnson & Johnson’s mission to unlock the full potential of medical innovation for those who dedicate their lives to care.",
      "format": "In-Person",
      "industry": "Healthcare & Life Sciences",
      "intended_audience": "Research: Non-Academic",
      "key_takeaways": [
        "Simulation is the next frontier in surgical robotics.",
        "AI-driven simulation has the potential to enhance differentiated technology development in support of delivering better patient outcomes.",
        "Leveraging NVIDIA Isaac for Healthcare, NVIDIA Omniverse, and NVIDIA Cosmos unlocks key AI-driven simulation capabilities, and ultimately physical AI."
      ],
      "nvidia_technology": "Isaac, Omniverse",
      "session_id": "S81647",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Johnson & Johnson",
          "name": "Neda Cvijetic",
          "title": "Global Head, Robotics and Digital R&D"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Harness AI to Accelerate Innovation in Healthcare Robotics",
      "topic": "Robotics Simulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81647/"
    },
    {
      "description": "Explore the NVIDIA Data Flywheel, which uses agent deployment artifacts from production applications to increase the overall accuracy and reduce the latency/cost of agentic AI systems. We'll explore how to utilize the NeMo Microservices Platform for programmatic control of datasets, fine-tuning, evaluation, and inference. Familiarity with Python, basic understanding of how agentic systems work, hands-on knowledge is encouraged but not required.",
      "format": "In-Person",
      "industry": "Healthcare & Life Sciences",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "After we've designed and deployed a healthcare agentic AI system, we can continuously optimize it.",
        "NVIDIA Data Flywheel can help us optimize our healthcare AI systems and AI agents.",
        "Different parts of the NeMo Microservices Platform help us achieve different tasks in the optimization for datasets, fine-tuning, evaluation, and inference."
      ],
      "nvidia_technology": "NeMo",
      "session_id": "DLIT81668",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Abood Quraini",
          "title": "Technical Marketing Engineering Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Ben Randoing",
          "title": "Technical Marketing Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Jin Li",
          "title": "Technical Marketing Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Harness NVIDIA AI Advanced Tools for Generative AI in Digital Health",
      "topic": "ModelOps",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81668/"
    },
    {
      "description": "Discover how to design, deploy, and scale agentic AI systems that transform research acceleration, student support, and campus operations while navigating governance, safety, and infrastructure challenges unique to higher education. Join NVIDIA's Higher Education and Research team to explore proven architectures, deployment patterns, and integration strategies that help you move from experimentation to production-ready AI agents across your institution. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "Academia / Higher Education",
      "intended_audience": "Research: Academic",
      "key_takeaways": [
        "Understand what agentic AI is and where it delivers the most value in higher education and research—accelerating literature reviews, lab workflows, student services, accessibility, IT support, and campus operations.",
        "Learn the NVIDIA reference stack and patterns to build safely: NVIDIA AI Enterprise, NeMo and NIM microservices, Triton/TensorRT for optimized inference, RAG with vector databases, and Guardrails—plus how to integrate agents with LMS/SIS/LIMS and HPC resources.",
        "Be introduced to deployment blueprints from prototype to production: agent orchestration and tool use, connectors to institutional data, monitoring and telemetry, GPU scheduling and cost control (on‑premises, cloud, or hybrid), and performance tuning.",
        "Leave with actionable next steps: quickstart labs, sample workflows and case studies, success metrics and ROI frameworks, and pathways to NVIDIA academic programs and resources to accelerate adoption."
      ],
      "nvidia_technology": "NVIDIA AI Enterprise",
      "session_id": "CWES81562",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Zahra Ronaghi",
          "title": "Sr. Manager, HER and Gen AI Solutions"
        },
        {
          "company": "NVIDIA",
          "name": "Kaleb Smith",
          "title": "Sr. Data Scientist"
        },
        {
          "company": "NVIDIA",
          "name": "Zoe Ryan",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Huiwen Ju",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Amanda Butler",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Kristopher Keipert",
          "title": "SA Manager, Higher Education Research"
        },
        {
          "company": "NVIDIA",
          "name": "Mahsa Lotfollahi",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Kyla Wilkes",
          "title": ""
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Harness the Potential and Navigate the Challenges of Agentic AI Across Colleges and Universities Worldwide",
      "topic": "Retrieval-Augmented Generation (RAG)",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81562/"
    },
    {
      "description": "Robotics and AI are redefining healthcare, accelerating diagnostics, surgery, patient care, and operational efficiency. We'll highlight how NVIDIA technologies drive this transformation, from physical AI systems that perform precision tasks and address workforce shortages to digital AI agents that interpret multimodal data, predict patient needs, and guide decision-making. We'll dive into the latest NVIDIA technology—spanning cutting-edge hardware, AI factories, and next-gen simulation—enabling both physical and digital robots. You’ll see how these technologies unify sensing, perception, planning, control, and generative reasoning into deployable devices. You'll also learn how medical technology developers are building on NVIDIA’s ecosystem of tools, reference workflows, and partner solutions to accelerate R&D, improve system intelligence, and bring autonomous capabilities to life.",
      "format": "In-Person",
      "industry": "Healthcare & Life Sciences",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn about three computer frameworks enabling physical AI in healthcare.",
        "Learn the most recent advancement in bridging sim-to-real, from digital + physical integration to advanced simulation.",
        "fearn the newest development in synthetic data generation frameworks and pipelines to bridge data gaps.",
        "Discover how NVIDIA’s newest runtime hardware and software enable deterministic edge AI deployment with faster development cycles."
      ],
      "nvidia_technology": "Jetson, AGX, Isaac, Omniverse, Metropolis, Clara Holoscan, IGX, MONAI, Cosmos, DGX Cloud, DGX Spark",
      "session_id": "S81599",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Mostafa Toloui",
          "title": "Healthcare Robotics Product Lead"
        },
        {
          "company": "NVIDIA",
          "name": "Prerna Dogra",
          "title": "Sr. Manager for Healthcare AI Products"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Healthcare Reimagined: Bridging Digital Intelligence and Physical Autonomy",
      "topic": "Robotics Simulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81599/"
    },
    {
      "description": "The Aerial Framework has been designed from the ground up to meet the needs of 3GPP radio access networks (RANs)—signal processing workloads with microsecond latency requirements. It is a platform that unites research, test beds, and production deployments to solve development challenges for real-time applications. The session will give a hands-on tutorial to design algorithms in JAX/Torch and instantly deploy to real-time GPU pipelines with inline networking. GitHub: https://github.com/NVIDIA/aerial-framework",
      "format": "In-Person",
      "industry": "Telecommunications",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Show why Aerial Framework exists, who it is for, and what it can do.",
        "Show how to leverage the CUDA-X ecosystem to prove it is possible to accelerate research in Python to real-time applications, demonstrate AI-native workflows, and demonstrate algorithm profiling.",
        "Show what is coming next for Aerial Framework."
      ],
      "nvidia_technology": "Aerial, CUDA-X",
      "session_id": "S82158",
      "session_type": "Tutorial",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Kobi Cohen-Arazi",
          "title": ""
        },
        {
          "company": "NVIDIA",
          "name": "Roy Timo",
          "title": "Principal Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "High Performance AI-RAN Made Simple; Python to CUDA Signal Processing at the Speed of Light",
      "topic": "",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82158/"
    },
    {
      "description": "How do you go from a state-of-the-art foundation model to a globally available usage-based API? This session provides an overview of the roadmap and architecture from model performance optimization to multi-region GPU-based infrastructure setup to the precise mechanics of request prioritization, token accounting, and rate limiting. Building on an inference stack that includes technologies like NVIDIA Dynamo, TensorRT-LLM, and CuTe, we'll trace the path from model weights to a robust production service.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Production inference requires both optimized runtime and robust infrastructure.",
        "With NVIDIA Dynamo, Baseten has achieved state-of-the-art performance on leading open-weight models.",
        "Model performance techniques (quantization, speculation, caching, parallelism, disaggregation) vary by modality and use case."
      ],
      "nvidia_technology": "",
      "session_id": "EX82108",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "Baseten",
          "name": "Philip Kiely",
          "title": "Head of Developer Relations"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "High-Performance Inference for Frontier AI Models (Presented by Baseten)",
      "topic": "AI Inference",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82108/"
    },
    {
      "description": "Drug discovery depends on modeling and manipulating matter at the bio scale, but existing computational methods are often too slow or inaccurate, requiring costly experiments. We present a digital chemistry framework that combines many-GPU high-performance computing, quantum chemistry, and machine learning. With new algorithms and software optimizations, it overcomes long-standing cost and efficiency barriers to scalable quantum methods. Awarded the 2024 ACM Gordon Bell Prize, this work achieved the first quantum-accurate biomolecular simulations with near-experimental fidelity and the first exaflop-scale computation in double precision. Using exascale computing and FP64 emulation, automated workflows, and ML trained on quantum-accurate data, the platform enables adaptive, automatable, and cost-effective molecular modeling—accelerating discovery and reshaping in-silico research.",
      "format": "In-Person",
      "industry": "Academia / Higher Education",
      "intended_audience": "Research: Academic",
      "key_takeaways": [
        "Learn how combining many-GPU high-performance computing, quantum chemistry, and machine learning overcomes long-standing speed and accuracy barriers in drug discover.",
        "Learn how FP64 emulation on Blackwell accelerate HPC workflows.",
        "See how exascale algorithms and workflows, recognized with the 2024 ACM Gordon Bell Prize, enabled the first quantum-accurate biomolecular simulations at an experimental fidelity.",
        "Understand how automated, adaptive, and cost-effective in-silico pipelines accelerate candidate discovery and molecular design while reducing reliance on costly lab experiments."
      ],
      "nvidia_technology": "CUDA, cuBLAS, Blackwell",
      "session_id": "S81503",
      "session_type": "Talk",
      "speakers": [],
      "technical_level": "Technical - Intermediate",
      "title": "High-Performance Quantum- and AI-Driven Drug Discovery: Accelerate EXESS With FP64 Emulation on Blackwell",
      "topic": "Supercomputing",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81503/"
    },
    {
      "description": "The economics of inference are changing faster than ever, with the cost of serving frontier models growing at an unprecedented rate. To continue advancing intelligence per unit cost, we must look beyond isolated optimizations and adopt a holistic, cross-layer design philosophy spanning silicon, software, and model architecture. This session will explore the opportunities of hardware/software/algorithm co-design, highlighting how aligning model innovations with NVIDIA hardware features can unlock orders-of-magnitude efficiency gains. By educating the community on these co-design principles, we can accelerate the development of the next generation of frontier models and deepen our strategic advantage through tighter integration between hardware and intelligence.",
      "format": "In-Person",
      "industry": "Cloud Services",
      "intended_audience": "Research: Non-Academic",
      "key_takeaways": [
        "Key Advances in Inference Efficiency: Understand emerging inference technologies, including speculative decoding, WideEP, and test-time scaling, that are redefining the performance and cost dynamics of large-scale model deployment.",
        "NVIDIA’s Co-Design Vision: Learn about NVIDIA’s strategic investments in holistic hardware/software/algorithm co-design, enabling tighter integration between model innovation and platform capabilities across the full AI stack.",
        "Best Practices for Hardware-Aware Optimization: Discover actionable best practices to leverage NVIDIA hardware features effectively from kernel-level optimizations to architecture-level adaptations to maximize efficiency and scalability."
      ],
      "nvidia_technology": "Blackwell",
      "session_id": "S82154",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Ritika Borkar",
          "title": "Principal Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Holistic Model Co-Design: Close the Loop With Software and Hardware for Inference Efficiency",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82154/"
    },
    {
      "description": "This session shows how AI coding can be used today by IT teams with the tools and systems they already have. You will see how common IT work such as maintaining legacy code and scripts, reducing technical debt, and creating tests and documentation can be done faster with AI, without changing platforms or processes. Through real-world examples and a short demo, we will show how AI coding fits into existing IT workflows instead of requiring new tools or large integrations. The focus is on practical and low-risk use cases that deliver quick value, reduce ongoing maintenance work, and help teams spend more time on higher-impact efforts like modernization and optimization.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "AI coding can be used today with the tools IT teams already have.",
        "The fastest value comes from reducing technical debt in existing systems and scripts.",
        "AI coding helps teams work faster while keeping people in control.",
        "Less time on maintenance means more time for modernization and improvement."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, DGX Platform, HGX, HPC SDK, NVIDIA NIM, Blackwell, NVIDIA AI Enterprise, Blueprint, DGX Cloud, DGX Spark",
      "session_id": "EX82180",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "CDW",
          "name": "Jeff Myers",
          "title": "Principal Field Solution Architect - AI"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "How AI Coding Augments IT Teams Without Replacing Them (Presented by CDW)",
      "topic": "Code / Software Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82180/"
    },
    {
      "description": "This NVentures-hosted panel explores how AI is redefining the way we work across industries, from healthcare to manufacturing. Hear from innovative portfolio leaders who are applying AI to streamline operations, enhance decision-making, and unlock new levels of human productivity.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Cross-Industry Impact: Discover how AI is transforming workflows in sectors like healthcare and manufacturing.",
        "Practical Applications: Learn real-world strategies from portfolio leaders using AI to optimize operations and improve decision-making.",
        "Boosting Productivity: Understand how AI unlocks new levels of human productivity and efficiency across diverse industries."
      ],
      "nvidia_technology": "NVIDIA NIM",
      "session_id": "S81832",
      "session_type": "Panel",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Chase Overlie",
          "title": ""
        },
        {
          "company": "Lovable",
          "name": "Fabian Hedin",
          "title": "Co-Founder and CTO"
        },
        {
          "company": "Generalist AI",
          "name": "Pete Florence",
          "title": "Co-Founder and CEO"
        },
        {
          "company": "Abridge",
          "name": "Shiv Rao",
          "title": "CEO and Co-Founder"
        },
        {
          "company": "Instrumental, Inc.",
          "name": "Anna-Katrina Shedletsky",
          "title": "CEO and Co-Founder"
        },
        {
          "company": "Factory",
          "name": "Matan Grinberg",
          "title": "Co-Founder and CEO"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "How AI Is Changing Everyday Work, Insights from NVentures Portfolio Companies",
      "topic": "Code / Software Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81832/"
    },
    {
      "description": "Explore how next-generation high-bandwidth memory (HBM4) improves large language model serving by addressing two critical inference constraints: memory bandwidth and capacity. Through detailed profiling and controlled scaling studies across modern LLMs and reasoning- and agent-based workloads, we show how HBM4’s substantially higher bandwidth and increased capacity translate into lower latency and higher throughput in LLM serving systems, and we project the performance gains production AI systems can expect from GPU platforms equipped with HBM4.",
      "format": "In-Person",
      "industry": "Hardware / Semiconductor",
      "intended_audience": "Research: Non-Academic",
      "key_takeaways": [
        "Why memory bandwidth and capacity have become the primary performance constraints in modern LLM serving systems, especially for long-context, reasoning, and agent-based workloads",
        "How HBM4 alleviates the “memory wall” in LLM inference by scaling both bandwidth and capacity, enabling lower latency and higher throughput in real-world serving scenarios",
        "What recent advances in high-bandwidth memory technology and the future HBM roadmap mean for GPU platforms and large-scale AI deployment"
      ],
      "nvidia_technology": "Cloud / Data Center GPU, CUDA, Multi-Instance GPU (MIG), NSight Comute, NSight Systems",
      "session_id": "S81977",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "SK hynix",
          "name": "Donguk Moon",
          "title": "Director"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "How HBM4 Unlocks Efficient LLM Serving at Scale (Presented by SK hynix)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81977/"
    },
    {
      "description": "Go under the hood of NASCAR’s rigorous engineering workflow, where computational fluid dynamics (CFD) now accounts for 80% of all aerodynamic development. We'll explain how simulation drives critical decisions regarding driver safety, thermal management, and the racing package itself—specifically through \"liftoff safety\" studies and multi-car traffic modeling. This is made possible by NASCAR’s transition from CPU-based CFD to the CUDA-X accelerated Siemens Simcenter STAR-CCM+ package, in partnership with TotalSim and the Ohio Supercomputer Center. Leveraging the latest NVIDIA GB200 GPUs, this workflow delivers a 10x improvement in turnaround time, enabling larger, higher-fidelity multi-physics models.",
      "format": "In-Person",
      "industry": "Automotive / Transportation",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Discover how NASCAR utilizes CFD for over 80% of aerodynamic development to critically enhance driver safety and vehicle thermal management.",
        "Learn how the transition to CUDA-X accelerated Siemens Simcenter STAR-CCM+ enables high-fidelity modeling previously constrained by CPU limitations.",
        "Examine benchmarks demonstrating how NVIDIA GB200 GPUs deliver a 10x speedup, reducing simulation solve times from over 11 hours to under one hour."
      ],
      "nvidia_technology": "CUDA-X, Blackwell",
      "session_id": "S81752",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Neil Ashton",
          "title": "Distinguished Engineer"
        },
        {
          "company": "NASCAR",
          "name": "Eric Jacuzzi",
          "title": "VP of Vehicle Performance"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "How NASCAR Uses GPU-Accelerated Simulation for Safer, More Exciting Racing",
      "topic": "CAE",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81752/"
    },
    {
      "description": "Join this engaging session led by NVIDIA experts who collaborate closely with the global academic research community, supporting leading researchers in leveraging NVIDIA software technologies to drive scientific advancements across diverse fields. We'll provide insights into how various NVIDIA teams, representing different functional areas, can assist in accelerating your scientific discoveries at multiple levels. The team will outline their approach to working with individual researchers and groups, share success stories, and introduce a range of developer support programs, including the Academic Grant Program, DLI courses and teaching kits, etc. We'll also focus on aligning discussions with specific research domains such as robotics, biology, Gen AI, data processing, and climate science—while showcasing tools and resources available to help researchers enhance research outcomes. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "Academia / Higher Education",
      "intended_audience": "Research: Academic",
      "key_takeaways": [
        "Learn how NVIDIA helps the academic research community adopt NVIDIA software technologies.",
        "Learn how different teams at NVIDIA can assist in accelerating your scientific discoveries on multiple levels.",
        "Learn about various supporting programs NVIDIA offers today for research communities.",
        "Learn about various frameworks and tools that that can help researchers to accelerate their research outcomes across diverse fields of science.",
        "Learn how to empower your lab quickly in AI knowledge and beyond."
      ],
      "nvidia_technology": "CUDA, DeepStream, Morpheus, TensorRT, Clara, Isaac, Omniverse, RAPIDS, BioNeMo, Clara Holoscan, CUDA-X, cuDDN, cuDF, cuML, cuOPT, Data Science Workbench, JetPack, MONAI, NSight Comute, NSight Graphics, NSight Systems, TAO Toolkit, Triton, NVIDIA NIM, Cosmos",
      "session_id": "CWES81465",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Kaleb Smith",
          "title": "Sr. Data Scientist"
        },
        {
          "company": "NVIDIA",
          "name": "Will Ramey",
          "title": "Sr. Director"
        },
        {
          "company": "NVIDIA",
          "name": "Giuseppe Fiameni",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Johan Barthelemy",
          "title": "Strategic Researcher Engagement"
        },
        {
          "company": "NVIDIA",
          "name": "Tomasz Bednarz",
          "title": "Director of Strategic Researcher Engagement"
        },
        {
          "company": "NVIDIA",
          "name": "Kristopher Keipert",
          "title": "SA Manager, Higher Education Research"
        },
        {
          "company": "NVIDIA",
          "name": "Corentin Lapeyre",
          "title": "Strategic Researcher Engagement"
        },
        {
          "company": "NVIDIA",
          "name": "Hirofumi Suzuki",
          "title": "Strategic Researcher Engagement"
        },
        {
          "company": "NVIDIA",
          "name": "Oge Marques",
          "title": "Strategic Researcher Engagement Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Karin Sevegnani",
          "title": "Solutions Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "How NVIDIA Can Help You, as a Researcher, Accelerate Scientific Discovery",
      "topic": "AI Platforms",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81465/"
    },
    {
      "description": "Scaling mixture-of-experts (MoE) models in production while delivering high performance is challenging. This session explores how the NVIDIA Blackwell NVL72 makes MoE scaling practical and efficient with full stack innovation across compute, networking, and software. Hear from Perplexity AI on why they have adopted MoE models, detailing how they leverage Blackwell NVL72 to deliver real-time search with breakthrough performance.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn about the memory and communication challenges involved in deploying and scaling MoE models in production.",
        "Discover strategies for efficient MoE deployment on Blackwell NVL72 with techniques such as large expert parallelism and disaggregated serving.",
        "See throughput and interactivity performance profiles for popular MoE models."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, CUDA, TensorRT, NVLink / NVSwitch, Blackwell",
      "session_id": "S81701",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Perplexity AI",
          "name": "Nandor Licker",
          "title": "Technical Staff Member"
        },
        {
          "company": "NVIDIA",
          "name": "Shruti Koparkar",
          "title": "Sr. Manager, Product Marketing"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "How Perplexity AI Scales Real-Time Search With Breakthrough Mixture-of-Experts Performance",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81701/"
    },
    {
      "description": "This talk outlines how we design and operate a global GPU research cluster that supports AI and visual computing work for our console platform and game development studios. We use NVIDIA GPUs to train advanced models that run on PlayStation hardware to improve rendering, gameplay systems, and overall player experience. Running shared GPU infrastructure at scale brings challenges: securing data center space, long procurement cycles, multi-tenant Kubernetes management, and noisy-neighbor pressures as research needs shift. We will share lessons in cluster design, planning, security, and operations that help studios innovate and push the console experience forward.",
      "format": "In-Person",
      "industry": "Gaming",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Approaches for securing data center space and planning for growth in a rapidly changing hardware landscape",
        "Practical guidance for building a multi-tenant GPU environment that balances flexibility with stability",
        "Techniques for managing resource fairness and reducing noisy-neighbor effects",
        "Strategies for data protection and compliance in multi-tenant AI clusters",
        "Lessons learned from integrating new GPU generations and evolving AI workloads into a live R&D platform"
      ],
      "nvidia_technology": "CUDA, Infiniband Networking, Blackwell, NVIDIA Run:ai",
      "session_id": "S82025",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Sony Interactive Entertainment",
          "name": "Erick Flores",
          "title": "Director of Visual Computing Group"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "How Sony PlayStation Built a Multi-Tenant GPU Cluster to Accelerate R&D",
      "topic": "InfraOps",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82025/"
    },
    {
      "description": "Unlock faster, smarter AI development in this hands-on introduction to the NVIDIA Run:AI platform. In this lab, you’ll explore how Run:AI streamlines machine learning operations, removes infrastructure bottlenecks, and simplifies workload management to accelerate outcomes. Learn how dynamic resource allocation transforms efficiency across enterprise teams, ensuring workloads run faster, smoother, and at scale. Then apply what you learn directly in your lab environment as instructors guide you through real-world examples of accelerated ML operations. You’ll leave with practical skills and a strong foundation for optimizing performance with Run:AI. Basic understanding of machine learning workflows and terminology. General familiarity with containerized environments (Docker or Kubernetes preferred).",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Understand how Run:AI speeds up AI and ML workflows across the development life cycle.",
        "See how dynamic resource allocation increases throughput, utilization, and team efficiency.",
        "Gain hands-on experience running accelerated ML operations in a live lab environment."
      ],
      "nvidia_technology": "Mission Control",
      "session_id": "DLIT82181",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "David Whitehouse",
          "title": "NVIDIA Academy Lab Manager"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "How to Accelerate AI Workflows With NVIDIA Run:AI",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit82181/"
    },
    {
      "description": "Physical AI brings intelligence to machines, industrial arms, autonomous mobile robots (AMRs), and humanoids, enabling them to perceive, reason, and act in the real world in real time. These robots go beyond scripted behavior to make intelligent decisions on the fly: moving materials, assembling parts, and collaborating safely with people in dynamic environments. In this session, experts will walk through practical, end-to-end workflows for physical AI development using NVIDIA Isaac and Omniverse technologies. You’ll learn workflows spanning data collection and curation, digital twin creation, model training and scaling, high-fidelity simulation, and deployment to real hardware. We’ll highlight proven toolchains and real-world examples that developers can apply to accelerate the path to smarter, more adaptable robots.",
      "format": "In-Person",
      "industry": "Manufacturing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "End-to-End Physical AI Pipeline: Building intelligent robots requires integrated workflows—from data collection and curation to model training, simulation, and deployment on real hardware.",
        "Digital Twins for Scalable Development: High‑fidelity digital twins enable safe, repeatable, and large‑scale training and validation of robot behaviors before real-world deployment.",
        "Accelerated by NVIDIA Isaac and Omniverse: These platforms provide the toolchains for simulation, synthetic data generation, and deployment, reducing the gap between research prototypes and production‑ready robots."
      ],
      "nvidia_technology": "Jetson, AGX, Isaac, Omniverse, OVX, CUDA-X, Blackwell, Cosmos, DGX Cloud, DGX Spark",
      "session_id": "S81478",
      "session_type": "Tutorial",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Akul Santhosh",
          "title": "Solution Architect, Robotics"
        },
        {
          "company": "NVIDIA",
          "name": "Edith Llontop",
          "title": "Solutions Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "How to Build End‑to‑End Physical AI Systems for Humanoid Robots",
      "topic": "Synthetic Data Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81478/"
    },
    {
      "description": "The era of gigawatt-scale AI factories demands a radical shift in infrastructure co-design. Traditional methods fail to keep pace with the exponential growth of AI and the need for extreme energy efficiency. This panel introduces NVIDIA Omniverse DSX, a blueprint and open framework uniting design, simulation, and real-time operations within a single, physically accurate digital twin environment. Learn how industry leaders leverage DSX, OpenUSD, and AI-powered digital twins to co-design the building, power, cooling, and compute stack, setting a new standard for scalability, efficiency, and sustainability in AI infrastructure.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Learn the radical new approach required for gigawatt-scale AI factories, co-designing the building, power, cooling, and compute stacks simultaneously to achieve extreme energy efficiency and scale.",
        "Unifying Design with Digital Twins: Understand how industry leaders are leveraging AI-powered digital twins, OpenUSD, and physically accurate simulation environments to accelerate infrastructure design and optimize real-time operations.",
        "Maximize Tokens per Watt: Learn how integrating power, cooling, and compute design is the only pathway to sustainable, gigawatt-scale AI deployment."
      ],
      "nvidia_technology": "RTX GPU, Omniverse, Hopper, CUDA-X, Blackwell, Blueprint",
      "session_id": "S81671",
      "session_type": "Panel",
      "speakers": [
        {
          "company": "Cadence Design Systems, Inc.",
          "name": "Vivek Mishra",
          "title": "Corporate VP"
        },
        {
          "company": "NVIDIA",
          "name": "Scott Wallace",
          "title": "Director Data Center Engineering"
        },
        {
          "company": "Crusoe Energy Systems, Inc.",
          "name": "Chris Dolan",
          "title": "Chief Data Center Officer"
        },
        {
          "company": "Schneider Electric",
          "name": "Natasha Nelson",
          "title": "CTO of Services and VP of EcoStruxure Power"
        },
        {
          "company": "PTC",
          "name": "Catherine Kniker",
          "title": "Chief Marketing and Sustainability Officer"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "How to Build Planetary-Scale AI Infrastructure",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81671/"
    },
    {
      "description": "Get grounded in the practical fundamentals of deploying NVIDIA Run:AI from the ground up. This lab walks you through every stage of installation—from environment preparation to final validation—so you can confidently stand-up Run:AI within your own organization. Follow a guided, instructor-led deployment in your provided lab environment, learn how to verify installations, and practice diagnosing and resolving common setup issues. Whether you’re preparing for full production rollout or building a proof of concept, this session delivers the skills to deploy successfully and maintain stability. General experience with Kubernetes cluster administration. Basic understanding of containerized workloads and GPU infrastructure.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Understand the full setup requirements and architecture for installing Run:AI.",
        "Complete an end-to-end, instructor-led installation in a real lab environment.",
        "Learn how to validate deployment success and troubleshoot installation challenges."
      ],
      "nvidia_technology": "Mission Control",
      "session_id": "DLIT82184",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "David Whitehouse",
          "title": "NVIDIA Academy Lab Manager"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "How to Provision and Install NVIDIA Run:AI",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit82184/"
    },
    {
      "description": "In this lab, we explore how to build AI surrogate models for crash simulations, one of the most computationally intensive tasks in computer aided-engineering (CAE). Learn how modern AI architectures can reproduce high-fidelity physics simulations at a fraction of the cost, enabling faster design exploration and decision-making. We'll walk through the full workflow for developing an AI surrogate for CAE applications, including data preparation and analysis, training and optimization, loading and running inference with pre-trained models, and evaluating model accuracy and uncertainty. We'll also discuss how AI-accelerated workflows are transforming CAE across various industries and areas of research, and how these tools can integrate seamlessly into existing engineering pipelines. Basic understanding of engineering or scientific modeling workflows and the associated technical terminology Basic programming knowledge in Python (data structures, functions, basic libraries) Understanding of deep learning concepts (neural networks, training, inference) Basic understanding of data preprocessing techniques Understanding of model validation and verification methods",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how to construct an AI surrogate to accelerate CAE workflows.",
        "Understand how modern AI architectures model dynamic crash behavior.",
        "Learn how to assess accuracy and error quantification for AI models.",
        "Learn how to integrate AI surrogates into existing engineering workflows.",
        "Understand how AI-accelerated CAE is used across industrial engineering workflows."
      ],
      "nvidia_technology": "Modulus",
      "session_id": "DLIT81484",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Mark Hobbs",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Pablo Hermoso Moreno",
          "title": "Sr. Solutions Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "How to Run AI-Powered Computer-Aided Engineering Simulations",
      "topic": "CAE",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81484/"
    },
    {
      "description": "Join us to explore how NVIDIA's latest generation of ARM-based CPUs can boost data center performance and energy efficiency. We'll help you optimize your applications for the current NVIDIA Grace architecture while providing insights into NVIDIA Vera CPU, the next-generation processor based on a custom NVIDIA core — available in the second half 2026. Learn practical strategies to enable your code to run efficiently on these CPUs and understand how to take advantage of the next architectural improvements. We welcome participants interested in evaluating and deploying Grace today, as well as those considering adopting Vera in the future. You should have a foundational understanding of CPU programming. Bring your application-specific and general questions — we're here to help you prepare for both current and future generations of NVIDIA's data center processors. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Opportunity to ask questions about running CPU and accelerated workloads on NVIDIA CPUs based on the ARM architecture.",
        "Enabling your codebase to run efficiently on data center Arm CPUs is not a complex task, and has long-lasting benefits.",
        "Learn about optimization techniques for HPC, enterprise, and cloud workloads and the ecosystem around Grace and Vera, including but not limited to NVIDIA tools and libraries.",
        "Learn about NVIDIA products and offerings with the Grace- and Vera-based solutions."
      ],
      "nvidia_technology": "Grace CPU, NSight Systems",
      "session_id": "CWES81771",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Mathias Wagner",
          "title": "Sr. Developer Technology Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Lukas Alt",
          "title": "Developer Technology Engineer"
        },
        {
          "company": "Nvidia",
          "name": "Filippo Spiga",
          "title": "Technical Product Manager, Accelerated Compute Workloads and Performance"
        },
        {
          "company": "NVIDIA",
          "name": "Holly Wilper",
          "title": "Manager, System Software Tools"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "How to Run and Optimize Your Workloads on the NVIDIA Grace and Vera CPUs",
      "topic": "Performance Optimization",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81771/"
    },
    {
      "description": "In this full-day workshop, we'll guide you through an end-to-end robotic workflow—from simulation-based training to real-world robot deployment. You'll simulate the robot in NVIDIA Isaac Sim™, train the policies in Isaac Lab, validate the trained skills through software-in-the-loop testing, and deploy them to physical robots. Through practical exercises, you'll gain end-to-end experience in the techniques that bridge the sim-to-real gap: synthetic data generation, policy training and refinement, hardware testing on edge devices, and real-robot deployment. This workshop demonstrates how simulation accelerates robot learning and reduces the time and cost of bringing AI-powered automation to production environments. Certificate: Upon successful completion of the assessment, you’ll receive an NVIDIA certificate to recognize your subject matter competency and support your professional career growth. Intermediate experience with Python, including working with packages, virtual environments, and basic scripting. Familiarity with 3D content concepts such as meshes, materials, transforms, and scene hierarchies from tools like Blender, Maya, or similar DCC applications. Understand command-line usage for running scripts and managing project files. Understand basics of world foundation models. Have familiarity with robotics or simulation workflows (for example, using simulation tools or working with physics-enabled assets).",
      "format": "In-Person",
      "industry": "Retail / Consumer Packaged Goods",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Train robot policies and behaviors using simulation-based workflows",
        "Validate trained policies through software-in-the-loop (SIL) and hardware-in-the-loop (HIL) testing methodologies",
        "Apply sim-to-real techniques to bridge the gap between simulation validation and physical robot performance",
        "Deploy trained models to physical robots and execute real-world tasks",
        "Debug and optimize policies on real-world robots"
      ],
      "nvidia_technology": "Isaac",
      "session_id": "DLIW82273",
      "session_type": "Full-Day Workshop",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Kartik Sachdev",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Lior Ben Horin",
          "title": "Sr. Developer Relations Manager - Physical AI Ecosystem"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "How to Simulate, Train, Validate, and Deploy an End-to-End Robotics Workflow with NVIDIA Isaac",
      "topic": "Robotics Simulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dliw82273/"
    },
    {
      "description": "Domain-specific languages (DSLs) like COBOL remain critical in enterprise systems, but are underserved by today’s LLMs. Learn how to build continuously learning coding agents using NVIDIA NeMo to build data flywheels. We’ll show how to create high-quality datasets with a synthetic data generation pipeline, share results from supervised fine-tuning and progressive fine-tuning, and explore reinforcement learning in custom environments. Finally, we’ll walk through an end-to-end experience with the NeMo Microservices platform, including a take-home notebook for hands-on experimentation. This session delivers practical lessons and tools to help you make coding agents fluent in your domain-specific language.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how to achieve state-of-the-art accuracy on COBOL and other DSLs using the NVIDIA Data Flywheel approach."
      ],
      "nvidia_technology": "NeMo",
      "session_id": "S81707",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Jeff Farris",
          "title": "Principal Research Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Santiago Pombo",
          "title": "Generative AI Product Manager"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "How to Teach Coding Agents Your Domain-Specific Language: SDG to Fine-Tuned Models, COBOL Edition",
      "topic": "Code / Software Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81707/"
    },
    {
      "description": "Discover how NVIDIA Warp enables the next generation of GPU-accelerated physics, geometry processing, and differentiable programming. This training lab introduces Warp’s core capabilities and modules, then moves into a hands-on notebook where participants build a high-performance physics solver entirely in Python. This is demonstrated through a practical computational fluid dynamics example based on a 2-D Navier–Stokes formulation. You'll leave with an understanding of what Warp is designed for, how its core building blocks work, and how it can be used alongside deep learning frameworks like PyTorch and JAX in modern computational engineering workflows.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Unified Python Framework for Physics and AI: NVIDIA Warp enables developers to build GPU‑accelerated physics solvers directly in Python while integrating smoothly with deep learning, bridging computational physics, geometry processing, and deep learning.",
        "Interoperability with DL Frameworks: Learn how Warp complements frameworks such as PyTorch and JAX while interoperating with them through zero-copy data sharing and gradient exchange.",
        "Warp Tile-Based Primitives: Gain hands-on experience using Warp’s tile-based programming model for various matrix operations, making GPU programming more accessible.",
        "Rapid Development of High‑Performance Solvers: The lab will demonstrate how Warp’s modular, Python-native, and expressive design allows fast prototyping of physics solvers, using 2-D Navier Stokes solver as an example."
      ],
      "nvidia_technology": "CUDA-X",
      "session_id": "DLIT81837",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Sheel Nidhan",
          "title": "Sr. Technical Marketing Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Eric Shi",
          "title": "Sr. Engineering Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Mohammad Mohajerani",
          "title": "Sr. Product Manager"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "How to use NVIDIA Warp to Build GPU-Accelerated Computational Physics Simulations",
      "topic": "CAE",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81837/"
    },
    {
      "description": "NVIDIA IGX and Jetson are the world's leading computing platform for AI at the edge. It offers high-performance compute and advanced NVIDIA AI software stack, as well as an extensive ecosystem to help you create and take products to market. Join with NVIDIA engineers and experts to ask your questions and join discussion on topics ranging from hardware design to system software, from the latest AI models to application frameworks like Metropolis, Isaac, and Holoscan.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Developer Journey on Jetson Thor and IGX Thor",
        "System design and AI model performance at the edge",
        "Demystify application development on IGX and Jetson."
      ],
      "nvidia_technology": "Jetson, AGX, Isaac, Metropolis, IGX, JetPack",
      "session_id": "CWES81795",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Chen Su",
          "title": "Sr. Technical Product Marketing Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Suhas Hariharapura Sheshadri",
          "title": "Sr. Software Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Aayush Pathak",
          "title": "Hardware Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Chitoku Yato",
          "title": "Sr. Technical Product Marketing Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Thejas Gupta Shivashankar",
          "title": "Partner Ecosystem Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Neel Patel",
          "title": "Solutions Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "IGX and Jetson: Autonomous Robots and Physical AI Applications",
      "topic": "Embedded Edge",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81795/"
    },
    {
      "description": "Learn to build scalable data pipelines for training robot policies using Teleop Core, an open-source framework that unifies simulation and real-world teleoperation. We will demonstrate how to standardize integrations with human interface devices such as PICO headsets and controller to accelerate the creation of very large robotics datasets.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Architect a horizontal data pipeline that seamlessly bridges simulation and real-world environments to train robust robotic policies at scale.",
        "Leverage the open-source Teleop Core framework to standardize inputs from diverse human interface devices for consistent data formatting.",
        "Integrate high-fidelity hardware, including PICO VR headsets and Manus haptic gloves, to capture precise manipulation data.",
        "Deploy a unified reference design that enables the wider ecosystem to contribute to and benefit from massive, standardized robotics datasets."
      ],
      "nvidia_technology": "CloudXR, Isaac, Omniverse",
      "session_id": "S81999",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Tiffany Chen",
          "title": "Distinguished Software Engineer and Manager"
        },
        {
          "company": "PICO",
          "name": "Ke Jing",
          "title": "Lead Product Manager"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Immersive Robotics Teleop 101: Standardize Large-Scale Data Collection and Processing Pipeline",
      "topic": "AR / VR Development Tools",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81999/"
    },
    {
      "description": "This session will dive into the current difficulties in addressing common data center conundrums, with a specific focus on the intricate world of AI hardware interaction and failure analysis. With a comprehensive overview of existing strategies for effective troubleshooting and a detailed case study of real-world data center diagnostic scenarios, we will analyze the limitations of current methodologies. Furthermore, this talk will explore forward-looking perspectives beyond present-day solutions and outline the desired features and functionalities that next-gen AI hardware could incorporate to transform and simplify the diagnostic landscape. Gain actionable insights into current best practices and contribute to the advancement of shaping the future design of resilient data center hardware.",
      "format": "In-Person",
      "industry": "Cloud Services",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "An overview of existing strategies for effective troubleshooting and a detailed case study of real-world data center diagnostic scenarios",
        "Outline the desired features and functionalities that next-gen AI hardware could incorporate to transform and simplify the diagnostic landscape.",
        "Gain actionable insights into current best practices and the future design of resilient data center hardware."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, HGX, Blackwell",
      "session_id": "S82091",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Aivres Systems, Inc.",
          "name": "James Zou",
          "title": "R&D Fellow"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Improving AIDC Operational Efficiency with AI Hardware Diagnostic Innovations (Presented by Aivres)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82091/"
    },
    {
      "description": "With drastically increasing power and thermal density in new-generation AI chips and servers, AI data centers face enormous challenges in effectively and efficiently cooling their AI computing infrastructure at all levels—node, rack, pod, and data center. In this session, we will share Aivres energy-efficient liquid-cooled AI server products and solutions based on NVIDIA GPUs that fully meet the needs and technical challenges of new-generation AI data centers. Building on the primary liquid-cooling loop design, we extend the discussion to the secondary cooling loops, coolant distribution units, power supply, and liquid-cooled server rack designs. Finally, we will showcase our reference solutions and deployments for new-generation liquid-cooling AI data centers.",
      "format": "In-Person",
      "industry": "Manufacturing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Energy-efficient liquid-cooled AI server products and solutions based on NVIDIA GPUs that meet technical challenges of new-generation AI data centers",
        "The discussion to the secondary cooling loops, coolant distribution units, power supply, and liquid-cooled server rack designs",
        "Showcase the reference solutions and deployments for new-generation liquid-cooling AI data centers"
      ],
      "nvidia_technology": "Cloud / Data Center GPU, HGX, Blackwell",
      "session_id": "S82095",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Aivres Systems, Inc.",
          "name": "Jie Hu",
          "title": "R&D Principal Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Innovating Liquid Cooling Solutions for New-Generation AI Data Centers (Presented by Aivres)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82095/"
    },
    {
      "description": "AI infrastructure must deliver predictable, high‑throughput pipelines for large‑scale training and inference while enforcing strict security, data sovereignty, and multi‑tenant isolation across cloud and on‑premises environments. These demands can overwhelm traditional host‑based networking and security, reducing utilization and fragmenting policies. This session will bring together NVIDIA technologists building NVIDIA’s own DGX Cloud platform powered by NVIDIA Networking and show how they are extending the learnings to sovereign DGX AI factory solutions. Understand design patterns such as zero‑trust for hardened multi‑tenant clusters, accelerated Kubernetes networking, intelligent data platforms, and security services that enable consistent, high‑performance AI factories everywhere.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Understand how NVIDIA designs and operates its own cloud and DGX AI factory networking stack.",
        "Learn proven blueprints for zero‑trust multi‑tenant clusters and accelerated Kubernetes networking.​",
        "Learn how to integrate intelligent data platforms and DPU‑based security services into existing cloud and on‑prem environments to build consistent, high‑performance AI factories."
      ],
      "nvidia_technology": "BlueField DPU, DGX Platform, DOCA, DGX Cloud",
      "session_id": "S81856",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Alexander Petrovskiy",
          "title": "Sr. Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Arts Yang",
          "title": ""
        },
        {
          "company": "NVIDIA",
          "name": "Andrew Forgue",
          "title": ""
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Inside NVIDIA DGX AI Factory: Accelerating Networking for AI Across Cloud, Core, and Edge",
      "topic": "AI Networking, 5G/6G",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81856/"
    },
    {
      "description": "AI factories are transforming AI workloads, but maintaining uptime in these GPU-powered environments is a critical challenge—where even a single fault can disrupt weeks of progress and cost millions. Learn how NVIDIA’s advanced fault detection, predictive analytics, and innovative In-System Test (IST) facilitate rapid diagnostics, enabling organizations to quickly identify and resolve issues. By minimizing downtime and preventing costly interruptions, these technologies drive a significant reduction in total cost of ownership and deliver improved return on investment. This session will highlight how automated recovery and hierarchical diagnostics empower organizations to deploy and scale AI infrastructure with confidence and efficiency.",
      "format": "In-Person",
      "industry": "Cloud Services",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Discover how proactive fault prediction and self-healing capabilities drive higher uptime and business continuity for AI factories.",
        "Walk away with knowledge of why rapid diagnostics and automated recovery reduce operational costs and maximize return on investment.",
        "Learn how intelligent resiliency features empower organizations to scale AI confidently, unlocking new levels of efficiency and reliability."
      ],
      "nvidia_technology": "Cloud / Data Center GPU",
      "session_id": "S81951",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Shantanu Sarangi",
          "title": "Sr. Director"
        },
        {
          "company": "NVIDIA",
          "name": "Mahmut Yilmaz",
          "title": "Director"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Inside Resilient AI Factories: How Predictive Analytics and IST are Driving Next-Level Data Center Reliability",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81951/"
    },
    {
      "description": "Discuss with experts everything related to inter-GPU communication through NVLink, Infiniband, or other networks. We'll cover all communication libraries: NCCL, MPI, UCX and NVSHMEM. This is the perfect place to discuss performance benefits of CUDA stream/graph aware communication, GPU/Kernel initiated communication using device APIs, GPU Direct, NVLink, Infiniband, and SHARP to accelerate your deep learning training workload or your HPC application. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Understand available GPU Communication Libraries.",
        "Understand GPUDirect and related technologies and how they are leveraged by GPU Communication Libraries.",
        "Understand how to efficiently map your communication patter to GPU Communication Library primitives."
      ],
      "nvidia_technology": "CUDA, Infiniband Networking, Ethernet Networking, Magnum IO, Interconnect Networking, NCCL, NSight Systems",
      "session_id": "CWES81615",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Jiri Kraus",
          "title": "Principal Developer Technology"
        },
        {
          "company": "NVIDIA",
          "name": "Jim Dinan",
          "title": "Distinguished Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Arnav Goel",
          "title": "Sr. Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Oded Green",
          "title": "Sr. Developer Technology Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Sylvain Jeaugey",
          "title": "Distinguished Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Pak Markthub",
          "title": "Sr. Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Akshay Venkatesh",
          "title": "Sr. Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Thomas Gillis",
          "title": ""
        },
        {
          "company": "NVIDIA",
          "name": "Benjamin Glick",
          "title": "Senior Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Ke Wen",
          "title": "Principal Software Architect"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Inter-GPU Communication Techniques and Libraries for HPC and AI",
      "topic": "AI Networking, 5G/6G",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81615/"
    },
    {
      "description": "The session will describe the development of the digital twin for the DIII-D Experimental Fusion Reactor. The talk will describe the development of the AI models and live sensor data integrated with the CAD data to develop an interactive digital twin.",
      "format": "In-Person",
      "industry": "Energy",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Digital twins offer a new tool to help the community achieve commercial fusion energy.",
        "AI/ML models are critical to enable interactivity with high fidelity.",
        "The integration of live sensor data allows the digital models to be tested relative to the physical asset operation.",
        "Omniverse provides a superior set of capabilities to allow for the development of the highly coupled digital twin."
      ],
      "nvidia_technology": "DGX Platform, Clara Holoscan, NSight Systems, Omniverse Replicator",
      "session_id": "S81875",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "General Atomics",
          "name": "Raffi Nazikian",
          "title": "Director"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Interactive Digital Twin for Fusion With Integrated Sensors and Agentic AI",
      "topic": "Industrial Digitalization / Digital Twin",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81875/"
    },
    {
      "description": "Learn how neural rendering is transforming real-time graphics and simulation by embedding machine learning for massive performance gains and fidelity. This session introduces the core computational approaches and shows how the differentiable GPU programming layer essential to this work critically expands the high-performance compute ecosystem.",
      "format": "In-Person",
      "industry": "Media & Entertainment",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Deployable Now: Use compact multilayer perceptrons for practical techniques like neural compression, redefining asset representation for efficiency, quality, and hardware-accelerated performance.",
        "Differentiable Systems Core: Leverage Autodiff to turn complex algorithms (rendering, simulation) into trainable neural functions for automated GPU optimization.",
        "Production E2E Workflow: Seamless GPU pipeline from Python training to optimized runtime inference, guaranteeing strict, low-latency deployment"
      ],
      "nvidia_technology": "RTX GPU",
      "session_id": "S81661",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Shannon Woods",
          "title": "Slang Product Lead"
        },
        {
          "company": "NVIDIA",
          "name": "Alexey Bekin",
          "title": "Sr. DevTech Engineer"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Introduction to Neural Rendering",
      "topic": "Rendering Engines / Pipelines / Tools",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81661/"
    },
    {
      "description": "We'll give an overview of the first practical system for enabling pipeline parallelism (PP) for JAX on NVIDIA GPUs. JaxPP accepts simple annotations on the training step functions, and then will introduce Multiple Programs operating on Multiple Data (MPMD). This system is a compiler that works on top of JAX and its XLA compiler. We will give an introduction to pipeline parallelism and how it works on training. We'll also describe our experience of using JaxPP effectively on Cohere's training models.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Pipeline parallelism is an important method for distributed training and inference.",
        "JAX for GPUs can support it out of the box without changes to underlying compiler.",
        "JaxPP is a tool for PP on a large distributed cluster of GPUs."
      ],
      "nvidia_technology": "CUDA",
      "session_id": "S82141",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Vinod Grover",
          "title": "Sr. Distinguished Engineer"
        },
        {
          "company": "Cohere",
          "name": "Nikolas Gritsch",
          "title": "Member of Technical Staff"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Large-Scale JAX Training With Pipeline Parallelism",
      "topic": "AI Networking, 5G/6G",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82141/"
    },
    {
      "description": "Join this interactive session with NVIDIA experts to discover how the convergence of artificial intelligence and high performance computing (HPC) is reshaping algorithmic trading. As trading strategies grow in complexity, the need for high-throughput processing and advanced modeling capabilities becomes critical. Speak with our experts about the latest methods to leverage acceleration for data-intensive tasks, from training sophisticated AI models to running massive-scale simulations. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "Financial Services",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Methods for accelerating computationally intensive feature construction, backtesting, and portfolio construction",
        "Best practices for training and deploying AI models in capital markets",
        "Latest results for compression and inference at the edge",
        "Optimizing infrastructure for the next stage of quantitative trading and risk management"
      ],
      "nvidia_technology": "Cloud / Data Center GPU, Grace CPU, CUDA, TensorRT, RAPIDS, Infiniband Networking, Hopper, cuBLAS, CUDA-X, cuDDN, cuDF, cuML, cuOPT, NCCL, NSight Systems, NVLink / NVSwitch, Triton, Blackwell, NVIDIA Run:ai, Mission Control",
      "session_id": "CWES81440",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Yuliana Zamora",
          "title": "Sr. Developer Relations Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Pooja Aniker",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Mark Bennett",
          "title": "Sr. Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Derek Beattie",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Siddharth Samsi",
          "title": "Sr. Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Manasa Murthy",
          "title": "Digital Marketing Organization Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Peihan Huo",
          "title": "Solution Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Bogdan Vioreanu",
          "title": "Sr. Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Roman Yokunda Enzmann",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Lavinia Ghita",
          "title": "Solutions Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Leverage Acceleration for AI and HPC in Algorithmic Trading",
      "topic": "Performance Optimization",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81440/"
    },
    {
      "description": "Join NVIDIA experts to explore the field of digital biology. Discuss the latest advancements in accelerated computing and foundation models for drug discovery and genomics — from bioinformatics tooling to protein structure prediction, molecular dynamics to molecular generation, and much more. Learn how NVIDIA BioNeMo, Parabricks, and RAPIDS-singlecell, together with NVIDIA NIM, empower developers, researchers, and enterprises to quickly create generative AI solutions across chemistry, biology, and genomics. Engage directly with our product managers, developer relations, and solution architects to address your challenges and answer your questions. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "Healthcare & Life Sciences",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how to accelerate the development and training of large scale AI models for drug discovery through NVIDIA CUDA-X libraries like BioNeMo Framework, cuEquivariance, and Transformer Engine.",
        "Learn how to accelerate your genomics and single-cell data processing workflows leveraging Parabricks and RAPIDS-singlecell.",
        "Learn how to accelerate your drug discovery process through BioNeMo NIMs, NIM Agent Blueprints, and Clara Open Models.",
        "Engage directly with NVIDIA's product managers, engineers, and researchers to discuss challenges and seek solutions."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, CUDA, TensorRT, Clara, RAPIDS, BioNeMo, Clara Parabricks, cuBLAS, CUDA-X, cuGraph, cuML, DALI, MONAI, Triton, NVIDIA NIM, Blackwell, NVIDIA AI Enterprise, cuVS, Blueprint, DGX Spark",
      "session_id": "CWES81555",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Kyle Gion",
          "title": "Product Manager, Digital Biology Research"
        },
        {
          "company": "NVIDIA",
          "name": "Neha Tadimeti",
          "title": "Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Severin Dicks",
          "title": ""
        },
        {
          "company": "NVIDIA",
          "name": "Daniel Puleri",
          "title": "HPC Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Steven Kothen-Hill",
          "title": "Sr. Deep Learning Bioinformatics Scientist"
        },
        {
          "company": "NVIDIA",
          "name": "Peter St. John",
          "title": "Machine Learning Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Pankaj Vats",
          "title": "Sr. Bioinformatics/Genomics Scientist"
        },
        {
          "company": "NVIDIA",
          "name": "Jonathan Mitchell",
          "title": "Sr. Machine Learning Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Mahan Salehi",
          "title": "Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Xin Yu",
          "title": "Sr. Solution Architect"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Leverage AI and Accelerated Computing for Digital Biology: Parabricks, BioNeMo and Clara Open Models",
      "topic": "Training AI Models",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81555/"
    },
    {
      "description": "Multi-modal liquid biopsy tests with paired clinical data have the potential to unlock deep learning (DL) models to improve population-level multi-cancer detection to reach the 120 million people in the United States alone who are eligible for cancer screening. Such tests produce data for millions of molecules, such as cell-free DNA (and billions of bases) per patient, before incorporating EMR, imaging, and other data types. Learn more about the unique challenges and opportunities that Freenome and others are collaborating on to validate, launch, and improve ML/DL models for multi-cancer, and ultimately multi-disease, detection.",
      "format": "In-Person",
      "industry": "Healthcare & Life Sciences",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "Overview of Freenome's first screening test for colorectal cancer currently under FDA review with commercial launch in 2026 for the 120 million people in the US eligible for the screening as the foundation to expand into 10+ cancer types through the same laboratory workflow",
        "How Freenome is leveraging an attention-based DL model that is beginning to outperform state-of-the-art ML approaches to spot cancer patterns among the millions of cell-free DNA fragments and billions of DNA bases processed from a single blood draw, in spite of the immense input space and low training case counts",
        "How NVIDIA technology enables efficient scaling to millions of multi-modal datasets that integrate molecular with electronic health record data, creating a virtuous-cycle feedback loop that supports test performance improvements and new future applications"
      ],
      "nvidia_technology": "Cloud / Data Center GPU, DGX Platform",
      "session_id": "S81820",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Freenome",
          "name": "Riley Ennis",
          "title": "Co-Founder"
        }
      ],
      "technical_level": "General Interest",
      "title": "Leverage DL to Enable Blood-Based Multi-Cancer Detection and Empower Health Systems to Reach More Patients",
      "topic": "Bioinformatics / Genomics",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81820/"
    },
    {
      "description": "Dive into the technology behind the new era of physical AI that's redefining Europe's supply chain and facility automation. Otto Group presents the “robotic coordination layer,” which connects robot fleets in real warehouses to a digital twin, enabling the deployment of the latest technology for intelligent automation. Discover how Otto Groups, NVIDIA, and Reply collaborated to not only launch the latest AI and robotics systems in the field of logistics across Europe, but also create a scalable blueprint for the future of logistics.",
      "format": "In-Person",
      "industry": "Retail / Consumer Packaged Goods",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how to enable intelligent robotic orchestration for faster deliveries and better service.",
        "Discover how the digital twin enables virtual reconfiguration of warehouse areas for process optimization and dynamic simulations to support peak management.",
        "Learn about technologies that integrate with the warehouse management system to enable seamless transformation of facilities and workflows."
      ],
      "nvidia_technology": "Isaac, Omniverse, cuOPT",
      "session_id": "S81730",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Otto Group",
          "name": "Kay Schiebur",
          "title": "Member of Executive Board"
        },
        {
          "company": "Otto Group",
          "name": "Martin Umland",
          "title": "VP Supply Chain Management"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Leverage Physical AI to Simulate and Orchestrate Robotic Fleets for Retail Fulfillment Centers",
      "topic": "Robotics Simulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81730/"
    },
    {
      "description": "As rack power densities surge beyond 100 kilowatts (kW), achieving 100% heat capture becomes increasingly essential for thermal management and operational efficiency. Current coldplate designs focus on high-thermal design power components such as CPUs and GPUs, but future architectures demand complete heat removal across all components. Even at 90% heat capture, a 500kW rack leaves 50kW for air cooling, which is a stretch for even the best air-cooled implementations. Enabling 100% heat capture will be critical for supporting next-generation AI infrastructure. Join us to learn how integrated cooling loops and advanced coldplate technologies enable complete heat removal across the entire server.",
      "format": "In-Person",
      "industry": "Hardware / Semiconductor",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Understand why 100% heat capture is essential as rack power densities exceed 100kW and air cooling becomes insufficient.",
        "Learn how integrated cooling loops address thermal challenges beyond CPUs, GPUs, cooling components like OSFP modules, HBMs, and DIMMs."
      ],
      "nvidia_technology": "",
      "session_id": "S82004",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "CoolIT Systems",
          "name": "Andrew Buckrell",
          "title": "Sr. Thermal Mechanical Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Liquid Cooling: How to Achieve 100% Heat Capture on 500kW+ Racks (Presented by CoolIT Systems)",
      "topic": "Confidential Compute",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82004/"
    },
    {
      "description": "Explore ASRock Rack AI server solutions from edge to cloud and discover how ASRock Rack adopts liquid-cooling approaches in next-generation AI platforms to unlock higher performance and scalability. Learn how these liquid-cooled server designs are validated in ASRock Rack’s Liquid Cooling Lab to ensure reliability, efficiency, and readiness for modern AI workloads.",
      "format": "In-Person",
      "industry": "Cloud Services",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Learn how ASRock Rack AI server solutions scale from edge to cloud to support diverse AI workloads.",
        "Gain insights into system-level liquid-cooling design and validation that address increasing power, efficiency, and reliability requirements.",
        "Understand ASRock Rack’s AI server roadmap for the latest NVIDIA Rubin platform."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, RTX GPU, HGX, Blackwell",
      "session_id": "EX82045",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "ASRock Rack",
          "name": "Charlotte Li",
          "title": "Director of Product Marketing"
        }
      ],
      "technical_level": "General Interest",
      "title": "Liquid-Cooled AI Server Platforms for Next-Generation Performance (Presented by ASRock Rack, Inc.)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82045/"
    },
    {
      "description": "Dive deep into why current large language models struggle to generate performant CUDA kernels for accelerated systems, despite being capable of functional code. We'll explore the latest advancements in benchmarking, agentic frameworks, and industry efforts that are building the necessary infrastructure to close this performance gap and enable you to write \"speed of light\" AI-coded CUDA.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "CUDA coding capabilities have lagged behind more generalist languages and frameworks—not just anecdotally, now demonstrably based on benchmarks like KernelBench and our own ComputeEval.",
        "The gap is closing, but it's not the whole story—you write CUDA for performance, which has bad support in frontier models.",
        "Tool calling, agentic frameworks, and execution environments are evolving fast, but integrating performance feedback is niche, secret sauce, and early.",
        "You can vibe-code functional CUDA today, but the infrastructure isn't there for AI-generated performant CUDA, and there are as many approaches as there are accelerated computing problem domains and target architectures.",
        "Efforts like GPUMode's KernelBot are pushing the state of the art in advanced techniques for performant AI-coded CUDA, with clear support from frontier labs."
      ],
      "nvidia_technology": "CUDA, NSight Comute, NSight Systems, NVIDIA NIM",
      "session_id": "S81653",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Mark Gabel",
          "title": "Sr. Manager, AI Quality for Development Tools"
        },
        {
          "company": "Meta Platforms",
          "name": "Mark Saroufim",
          "title": "Software Engineer"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "LLM-Generated CUDA Kernels: Are We There Yet?",
      "topic": "Large Language Models (LLMs)",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81653/"
    },
    {
      "description": "Whether you're hand-tuning CUDA kernels for the latest GPUs or vibe-coding your way to the next great AI app, intelligent developer tools are here to help. This session will put you on the fast track to bug-free GPU-accelerated code by introducing the latest developer tool technologies from NVIDIA. We'll start with a brief overview of the free tools available to all developers, and then dive deep on the latest features that provide even more insights into application behavior and performance. These include Python, Tile, and AI framework profiling enhancements, new debugging and cloud-integrated workflows, and customized AI copilots to help along the way. No matter what level you're focused on in the software stack, you'll leave this session with new ideas and next steps to unlock the full potential of the latest GPU platforms.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "How to use customized AI copilots to develop and optimize CUDA apps",
        "Common GPU performance issues and how to detect and fix them with tools",
        "New tools and features for GPU-accelerated Python workloads"
      ],
      "nvidia_technology": "CUDA, NSight Comute, NSight Systems",
      "session_id": "S81590",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Jackson Marusarz",
          "title": "Technical Product Manager"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Lower the Barriers: How New Tools Unlock GPU Acceleration for Everyone",
      "topic": "Profilers / Debuggers / Code Analysis",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81590/"
    },
    {
      "description": "Hear about an intelligent, self-learning, and adaptive system where manufacturing expertise is accelerated, orchestrated, and augmented in real time through a unified, virtual platform (digital twin). The platform is accessible to operators, engineers, maintenance teams, and managers via natural interfaces and smart tools. All knowledge, experience, and best practices related to production—world-class manufacturing—are transformed into structured digital models, continuously enriched by data and made available through AI agents acting as “digital mentors.”",
      "format": "Virtual",
      "industry": "Manufacturing",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Significant performance boost across service levels, flexibility, quality, productivity of all production factors, and overall resilience",
        "Human operators are empowered through natural interfaces (chatbots, voice assistants, augmented/virtual reality), no-code/low-code tools, predictive dashboards, and decision-support systems. AI acts as an amplifier of human expertise, enabling a factory that is agile, resilient, and capable of self-optimization.",
        "A learning factory model with lower fixed and operating costs, ensuring that even small-scale plants have full access to AI-powered, advanced capabilities and solutions typically reserved for large-scale operations."
      ],
      "nvidia_technology": "Omniverse Replicator",
      "session_id": "S82204",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Ariston Group",
          "name": "Pierluigi Astorino",
          "title": "COO"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Manufacturing Intelligence: Agentic AI to Unlock the Next Level of Performance",
      "topic": "3D Model Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82204/"
    },
    {
      "description": "Subgraph isomorphism is a major bottleneck in graph analytics, scientific computing, and quantum-compilation pipelines, yet most solutions rely on backtracking that cannot exploit modern GPUs. Motivated by real quantum-compilation challenges, we present a GPU-native formulation that replaces serial search with parallel relational tabular operations built from reusable “motifs,” enabling efficient joins and filters using NVIDIA RAPIDS. Across diverse benchmarks, we achieve up to 600x speedups on a single NVIDIA H200 GPU. By combining Q-CTRL’s quantum performance management infrastructure software and NVIDIA’s GPU platforms, we demonstrate a data-centric method that simplifies high-performance graph analysis, accelerates large-scale applications, and opens a path toward emerging hybrid HPC–quantum architectures and future deployment on HPC–quantum interconnect systems like NVQLink.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Research: Non-Academic",
      "key_takeaways": [
        "Learn practical design principles for turning traditionally sequential graph algorithms into parallel data-operations to unlock GPU acceleration.",
        "See how these techniques integrate into end-to-end pipelines, enabling faster execution in both large-scale graph analytics and quantum-software workflows.",
        "Gain a clear roadmap for adopting GPU-centric graph processing today, and how these techniques prepare the path for future hybrid HPC–quantum integrated systems."
      ],
      "nvidia_technology": "CUDA, cuDF",
      "session_id": "S81533",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Oded Green",
          "title": "Sr. Developer Technology Engineer"
        },
        {
          "company": "Q-CTRL",
          "name": "Yulun Wang",
          "title": "Staff Scientist, Quantum Control"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Massive Parallelization of Subgraph Isomorphism for Scalable Quantum Compilation and Faster Graph Analytics",
      "topic": "Quantum Computing",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81533/"
    },
    {
      "description": "Your AI factory is ready for action—now it’s time to put it to work. In this session, two NVIDIA senior solutions architects will walk you through the essential steps for deploying and managing your AI infrastructure. Aiming at teams beginning their AI factory journey, we'll focus on real-world deployment practices and insights drawn from leading customer environments. Together, we’ll discuss how to streamline cluster onboarding, configuration, and monitoring, laying the foundation for efficient and resilient operations from Day One.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Best practices for initializing HGX/DGX clusters and configuring storage, networking, and compute fabrics",
        "Techniques to reduce mean time between failures and accelerate time-to-first-token",
        "Deep dives into GB200/GB300 NVLink architecture, system management, and tooling advancements"
      ],
      "nvidia_technology": "BlueField DPU, Cloud / Data Center GPU, Grace CPU, DGX Platform, HGX, Infiniband Networking, Ethernet Networking, Hopper, Interconnect Networking, Blackwell",
      "session_id": "S81422",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Nicola Bianchi",
          "title": "Sr. Solution Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Kristof De Brouwer",
          "title": "Sr. Solution Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Master AI Infrastructure At Scale: A DevOps Playbook",
      "topic": "Software-Defined Data Center",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81422/"
    },
    {
      "description": "Extended reality (XR) applications powered by agentic AI will revolutionize how frontline workers do their jobs. Join NVIDIA experts for a deep-dive discussion on how to design and train agentic AI systems for enterprise use cases. In this session, we'll share strategies and best practices for: leveraging the Nemo Agent Toolkit to build powerful, scalable, and optimized agents for XR use cases, optimizing agentic AI to accurately interpret and guide complex workflows, and deploying AI to the edge using NVIDIA Cosmos for low-latency feedback. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how to leverage the Nemo Agent Toolkit to build powerful, scalable, and optimized agents for XR use cases.",
        "Learn how to optimize agentic AI to accurately interpret and guide complex workflows.",
        "Deploy AI to the edge using NVIDIA Cosmos for low-latency feedback."
      ],
      "nvidia_technology": "CloudXR, Riva, Metropolis, NeMo, NVIDIA NIM, NVIDIA AI Enterprise, Cosmos, DGX Spark, DGX Station",
      "session_id": "CWES81784",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Greg Barbone",
          "title": "XR Product and Partner Management"
        },
        {
          "company": "NVIDIA",
          "name": "Chao-Yeh Chen",
          "title": "Principal Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Tiffany Chen",
          "title": "Distinguished Software Engineer and Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Katherine Huang",
          "title": "Solutions Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Master Immersive Real-Time Visual Process Understanding and Guidance",
      "topic": "AR / VR AI",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81784/"
    },
    {
      "description": "Join this interactive session for direct insights and recommendations from NVIDIA specialists on fully utilizing your virtualized GPUs to deploy graphic-intensive apps, digital twin simulations, AI development, and more. Engage with experts who've navigated virtualization complexities, offering practical advice and proven methodologies. Bring your questions and leave with actionable strategies to enhance your virtualized data center operations. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Configure multi-tenancy with NVIDIA MIG and NVIDIA vGPU technologies.",
        "Learn the best practices to leverage your existing virtualized platform to start prototyping AI.",
        "Utilize the latest NVIDIA Blackwell GPUs to support mixed workloads and enhance user density.",
        "Get GPU sizing recommendations for your demanding workloads."
      ],
      "nvidia_technology": "Omniverse",
      "session_id": "CWES81960",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Jimmy Rotella",
          "title": "Sr. Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Anshul Fadnavis",
          "title": "Product Management"
        },
        {
          "company": "NVIDIA",
          "name": "Kelly Siggers",
          "title": "Solution Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Phoebe Lee",
          "title": "Sr. Product Marketing Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Randall Siggers",
          "title": "Sr. Solution Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Maximize GPU Resources in Your Virtualized Environment: Best Practices for Sizing and Deployment",
      "topic": "Graphics Virtualization",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81960/"
    },
    {
      "description": "This session focuses on practical and system-level techniques to increase large language model throughput while reducing GPU cost and waste. It covers both training and inference optimization, with an emphasis on real-world bottlenecks such as memory bandwidth, kernel inefficiencies, batching limits, and underutilized GPUs.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "Uncover the True Limits of LLM Throughput: Learn why memory bandwidth and scheduling—not raw compute—usually hold you back, and how optimizing attention kernels, KV-cache usage, and batching can unlock major performance gains.",
        "Boost GPU Utilization with Smarter Batching and Decoding:"
      ],
      "nvidia_technology": "Cloud / Data Center GPU, Hopper, Blackwell, DGX Cloud",
      "session_id": "S82225",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Cohere",
          "name": "Dwarak Talupuru",
          "title": "Staff Research Engineer (Training Lead)"
        },
        {
          "company": "Cohere",
          "name": "Bharat Venkitesh",
          "title": "Staff Research Engineer (Inference Lead)"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Maximize LLM Throughput and Improve GPU Efficiency",
      "topic": "Retrieval-Augmented Generation (RAG)",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82225/"
    },
    {
      "description": "Struggling to fully utilize GPU memory bandwidth on modern architectures? In this interactive session, NVIDIA engineers will work with you in real time to diagnose and optimize memory-bound kernels. Bring your own code, performance traces, or ideas and get hands-on guidance on loop unrolling, vectorization, prefetching, and asynchronous Tensor Memory Accelerator (TMA) pipelines. Learn to recognize common pitfalls, explore TMA patterns, and apply producer–consumer pipelines to maximize throughput. Walk away with practical, actionable solutions tailored to your workloads. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Get direct guidance from NVIDIA engineers on why your memory-bound kernels may not be saturating bandwidth on modern GPUs, and how architectural trends affect your performance limits.",
        "Learn which optimization techniques—unrolling, vectorization, prefetching, and TMA—best fit your specific workload, with personalized advice on applying them.",
        "Bring your own kernels or performance traces and walk through concrete steps to diagnose bottlenecks and move toward peak bandwidth.",
        "Understand how to avoid common pitfalls in your own code, including loop peeling, misalignment, too-small kernels, and tricky TMA corner cases."
      ],
      "nvidia_technology": "CUDA, Hopper, Blackwell",
      "session_id": "CWES81670",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Sam Mish",
          "title": "DevTech Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Francis Tseng",
          "title": "Principal Accelerated Computing Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Benedikt Dorschner",
          "title": "Sr. DevTech Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Kate Clark",
          "title": "Distinguished DevTech Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Matthew Martineau",
          "title": "Sr. Developer Technology Engineer"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Maximize Memory Bandwidth on Modern GPUs",
      "topic": "Performance Optimization",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81670/"
    },
    {
      "description": "Modern GPU generations (since Volta) have increased memory bandwidth faster than streaming multiprocessor (SM) count, which makes the classic “add more threads” recipe insufficient for many memory-bound kernels. This tutorial gives hands-on, implementation-focused guidance to saturate memory bandwidth on contemporary NVIDIA GPUs using loop unrolling, vectorization, asynchronous pipelining (Tensor memory accelerator, or TMA), cache prefetching, and other techniques. Through step-by-step worked examples (e.g., stencils, SpMV), we show common pitfalls (peeling, misalignment, tiny kernels) and practical fixes, plus variants of TMA usage and a producer-consumer TMA pattern you can reuse.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Understand why saturating memory bandwidth is affected by architectural trends, and the impact of kernel design.",
        "Learn practical techniques—unrolling, vectorization, software prefetching, and asynchronous pipelining (TMA)—and when each is most effective.",
        "Apply step-by-step optimization workflows to real kernels (e.g., stencils, SpMV) to move from naïve implementations to bandwidth-saturated performance.",
        "Recognize and fix common pitfalls such as loop peeling, misalignment, and too-small kernels."
      ],
      "nvidia_technology": "CUDA, Hopper, Blackwell",
      "session_id": "S81666",
      "session_type": "Tutorial",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Matthew Martineau",
          "title": "Sr. Developer Technology Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Benedikt Dorschner",
          "title": "Sr. DevTech Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Sam Mish",
          "title": "DevTech Engineer"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Maximize Memory Bandwidth on Modern GPUs: Practical Techniques, Patterns, and Worked Examples",
      "topic": "Performance Optimization",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81666/"
    },
    {
      "description": "This panel explores how AI technologies designed and developed in Europe are revolutionizing the healthcare sector, particularly in the realms of speech recognition and transcription. Panelists will discuss the challenges and opportunities of building robust, privacy-focused, and multilingual AI tools for clinical and patient-facing applications across the continent. Gain insights into cutting-edge advancements, collaboration models, and ethical considerations shaping the future of AI-driven speech solutions in European healthcare.",
      "format": "Virtual",
      "industry": "Healthcare & Life Sciences",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Discover the impact of scalable, flexible speech AI architectures that support easy integration across telehealth, hospitals, and clinics.",
        "Learn how ambient speech transforms healthcare workflows, allowing clinicians to focus more on patient care and less on administrative burden.",
        "Discover how AI-powered transcription minimizes manual data entry, saving clinicians time and improving data quality in healthcare records.​",
        "Learn how real-time, multilingual speech recognition enables accurate clinical documentation for diverse patient populations across Europe."
      ],
      "nvidia_technology": "DGX Platform, HGX, Riva, NeMo, Triton, NVIDIA NIM, DGX Cloud",
      "session_id": "S81464",
      "session_type": "Panel",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Cedric Steenbeke",
          "title": "Digital Health Product DevRel"
        },
        {
          "company": "Therapyside",
          "name": "Alessandro De Sario",
          "title": "CEO and Co-Founder"
        },
        {
          "company": "PolyAI",
          "name": "Alex Brown",
          "title": "Strategic Alliances, Healthcare Director"
        },
        {
          "company": "Speechmatics",
          "name": "Ricardo Herreros-Symons",
          "title": "Founding Member & Chief Strategy and Revenue Officer"
        }
      ],
      "technical_level": "General Interest",
      "title": "Medical Speech and Transcription Frontiers: Transforming European Healthcare Delivery",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81464/"
    },
    {
      "description": "Physical AI is redefining how next-generation medical devices perceive, decide, and act in the real world. From advanced imaging systems and autonomous interventional workflows to intelligent surgical robots, developers now require a unified stack that bridges data, simulation, and real-time runtime and control. NVIDIA’s platforms — such as Holoscan, Isaac for Healthcare, Omniverse, Cosmos and Metropolis — deliver an end-to-end foundation for building and scaling physical AI: model training with domain-specific foundation models, high-fidelity digital twins for procedure-aware simulation, and deterministic low-latency deployment at the edge. Join NVIDIA experts to learn how to leverage our full-stack technologies to accelerate your development — from imaging and interventional systems to next-generation robotics — and bring safer, smarter MedTech to market faster. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "Healthcare & Life Sciences",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Build deterministic real-time edge AI with Holoscan.",
        "Develop robotic and interventional systems with Isaac for Healthcare.",
        "Leverage Cosmos and Omniverse for high-fidelity synthetic data generation and simulation."
      ],
      "nvidia_technology": "",
      "session_id": "CWES81646",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Zhijin Li",
          "title": "Sr. Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Jay Carlson",
          "title": "Holoscan Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Daguang Xu",
          "title": "Sr. Research Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Oliver Kutter",
          "title": "Solution Architect Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Mikael Brudfors",
          "title": "Sr. Solution Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Andres Diaz-Pinto",
          "title": "Sr. AI Scientist"
        },
        {
          "company": "NVIDIA",
          "name": "Mahdi Azizian",
          "title": "Sr. Director, Holoscan Platform and Medical Technologies"
        },
        {
          "company": "NVIDIA",
          "name": "Mostafa Toloui",
          "title": "Healthcare Robotics Product Lead"
        },
        {
          "company": "NVIDIA",
          "name": "Anas Abidin",
          "title": "Manager of Healthcare Solution Architecture and Engineering"
        },
        {
          "company": "NVIDIA",
          "name": "Rahul Choudhury",
          "title": "Sr. Software Engineering Manager"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "MedTech: Physical AI for Imaging, Intervention, and Robotics",
      "topic": "Medical Devices",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81646/"
    },
    {
      "description": "Join this session as Megatron core customers Reflection, Periodic Labs, XHS Rednote, and ByteDance describe their engineering use cases and best practices for pre-training their workloads. You will also hear about their contributions into this OSS project, features they leveraged for NVIDIA's latest hardware systems, and tuning for best performance at scale.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Megatron development fully open source, and GitHub First improving external contributor velocity",
        "Customer use cases that demonstrate extreme scalability, advanced parallelism, and cutting-edge research features to power state-of-the-art LLMs and multi-modal models",
        "Learn advanced optimizations such as FP8 mixed precision, memory-saving techniques, and fast distributed checkpointing to maximize performance and efficiency."
      ],
      "nvidia_technology": "CUDA, NeMo, Nemotron",
      "session_id": "S82214",
      "session_type": "Lightning Talk",
      "speakers": [
        {
          "company": "Periodic Labs",
          "name": "Costa Huang",
          "title": "Member of Technical Staff"
        },
        {
          "company": "ByteDance Seed",
          "name": "Wang Zhang",
          "title": "Software Engineer of ML System Architecture"
        },
        {
          "company": "RedNote Hilab",
          "name": "Yuhong Ge",
          "title": "AI Infra Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Santosh Bhavani",
          "title": "Sr. Software Product Manager, Deep Learning"
        },
        {
          "company": "Reflection AI",
          "name": "Aakanksha Chowdhery",
          "title": "Research Lead"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Megatron Core: Customer Stories on the Open-Source Training Framework for Ultra Speed at Scale",
      "topic": "Deep Learning Frameworks",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82214/"
    },
    {
      "description": "The next leap in enterprise AI is industrializing intelligence, where automation and human input combine to improve agility, decision quality, and process efficiency. This session shows how NVIDIA applies lean manufacturing principles and agentic AI to its own AI operations through three core MLOps platforms: the Data Factory for synthetic data generation and labeling, the Eval Factory for model benchmarking, and the NIM Factory for inference optimization. From these real-world examples, we distill a practical playbook for representing any process in AI-native terms, identifying high-impact automation opportunities, embedding continuous improvement, and building a culture of operational excellence. The result is an AI-first organization with a repeatable model for AI-enhanced operations, one that extends beyond MLOps to transform the entire enterprise and its business model.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "See how NVIDIA designed and scaled its AI operations (MLOps) to support high-quality data generation and labeling, foundation model benchmarking, and inference performance optimization.",
        "Equip your organization with a playbook for AI-first operations, enabling leaders and engineers to drive transformation beyond MLOps into core business processes.",
        "Learn how to optimize and scale a business process with agentic AI via process decomposition, automation, feedback loops, and setting the right abstraction boundaries."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, DGX Platform, HGX, CUDA, TensorRT, Hopper, NCCL, NeMo, NVLink / NVSwitch, Triton, NVIDIA NIM, Blackwell, NVIDIA AI Enterprise, Cosmos, DGX Cloud, NVIDIA Run:ai",
      "session_id": "S81657",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Jesse Oliver",
          "title": "Sr. Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Nik Spirin",
          "title": "Director, Generative AI and LLMOps Platform"
        },
        {
          "company": "NVIDIA",
          "name": "Seph Mard",
          "title": ""
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "MLOps 201: Master Operational Excellence With Agentic AI",
      "topic": "ModelOps",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81657/"
    },
    {
      "description": "Modern production AI is more than a foundation model. The model monolith is evolving into the data center-scale distributed system made of modular microservices. This talk shows how NVIDIA brings the full stack together by applying extreme software and hardware co-design to maximize performance across infrastructure, orchestration, and application layers. We cover an AI project journey from zero to production. Starting with a simple cloud API, we progressively add complexity while introducing key NVIDIA technologies, including NVIDIA AI Enterprise, NIM, NeMo, Run.ai, Dynamo, and GPU infrastructure, to explain how each of them addresses real engineering and operational challenges. Get a clear guide for turning AI prototypes and isolated experiments into enterprise-grade systems, and understand the decisions, trade-offs, and architectural patterns behind modern production AI.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Trace a real AI project’s evolution from a simple cloud API to a full production system, seeing where complexity emerges and how to manage it.",
        "Understand the architectural principles behind distributed, microservice-based AI systems and why this model replaces the “single foundation model” monolith.",
        "Learn how software-hardware co-design drives performance at data-center scale, and what engineering patterns matter most on modern GPU infrastructure.",
        "See how NVIDIA technologies fit together, each addressing specific challenges in modern AI inference stack.",
        "Get a practical guide for scaling from prototype to production and balancing accuracy, latency, throughput, reliability, cost, and operational complexity in real-world environments."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, Grace CPU, DGX Platform, HGX, CUDA, Fleet Command, TensorRT, Infiniband Networking, Ethernet Networking, Hopper, MGX, Base Command Manager, Interconnect Networking, Multi-Instance GPU (MIG), NCCL, NeMo, NVLink / NVSwitch, Triton, NVIDIA NIM, Blackwell, NVIDIA AI Enterprise, Blueprint, DGX Cloud, DGX Spark, DGX Station, NVIDIA Run:ai",
      "session_id": "S81662",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Nik Spirin",
          "title": "Director, Generative AI and LLMOps Platform"
        },
        {
          "company": "NVIDIA",
          "name": "Erik Bohnhorst",
          "title": "Director of Product Management"
        },
        {
          "company": "NVIDIA",
          "name": "Michael Balint",
          "title": "Director, Product Architecture"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "MLOps 202: From Models to Production AI Systems at Scale",
      "topic": "ModelOps",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81662/"
    },
    {
      "description": "Learn practical ways to implement MLOps first principles as you create autonomous AI agents using LangGraph and NVIDIA NIM by building an agentic system that demonstrates tool integration, multi-step reasoning, and adaptive planning. Master the foundational agent architecture that MLOps engineers use to automate tedious workflows like incident investigation, experiment documentation, and model health monitoring — then customize it for your own use cases.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Understanding of the four core components of any AI agent (directly applicable to MLOps automation)",
        "A working foundational agent that demonstrates tool use, planning, and reasoning",
        "Knowledge of how to build agents using LangGraph and NVIDIA NIM",
        "A turnkey, portable development environment ready for customization",
        "Your own customized agent ready to share as a launchable (adapt it for MLflow documentation, model cards, incident reports, etc.)"
      ],
      "nvidia_technology": "Hopper, NVIDIA NIM, Nemotron",
      "session_id": "DLIT82062",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Ryan Kraus",
          "title": "Sr. Technical Marketing Engineer"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "MLOps Best Practices: Build an AI Agent",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit82062/"
    },
    {
      "description": "Europe’s drive toward sovereign AI is redefining the way open, transparent, and multilingual large models are built at scale. This panel explores the technical foundations that drive this effort. Speakers will discuss best practices in data quality, domain adaptation, and culturally aligned evaluation for low-resource languages, showing how sovereign and open models can meet European requirements while delivering state-of-the-art performance. The discussion will also highlight how NVIDIA Nemotron enables European, Middle Eastern, and African (EMEA) developers to build and fine-tune competitive large models, accelerating innovation while reducing dependence on proprietary platforms. In addition, the panel will touch on how European developers and companies are preparing these models for real-world inference and deployment across diverse applications.",
      "format": "Virtual",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Discover how EMEA-based companies are shaping the next generation of open and sovereign AI models—and what this means for developers and innovators working across diverse languages and domains."
      ],
      "nvidia_technology": "NeMo",
      "session_id": "S81994",
      "session_type": "Panel",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Meriem Bendris",
          "title": "Sr. Deep Learning Data Scientist"
        },
        {
          "company": "Dicta",
          "name": "Shaltiel Shmidman",
          "title": "Software Developer"
        },
        {
          "company": "University College London",
          "name": "Pontus Stenetorp",
          "title": "Professor and Deputy Director"
        },
        {
          "company": "ETH Zurich",
          "name": "Imanol Schlag",
          "title": "Research Scientist"
        },
        {
          "company": "1MillionBot",
          "name": "Andres Desantes",
          "title": ""
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Model Builders at the Frontier of EMEA’s Open and Sovereign AI Movement",
      "topic": "Pre-Trained / Foundation Models",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81994/"
    },
    {
      "description": "Enterprise developers are moving beyond text-based retrieval-augmented generation (RAG) to harness multi-modal data—video, audio, and text. This Connect with the Experts session explores best practices for designing and optimizing multi-modal agentic RAG systems. Learn to combine speech recognition, visual understanding, and LLMs to enable context-rich, dynamic enterprise AI applications. The discussion covers real-world implementation including vector indexing of multi-modal embeddings, latency optimization for large-scale deployments, and fine-tuning for domain-specific retrieval. Gain insights into integrating NVIDIA Nemotron models and cuVS accelerated vector search to create unified RAG pipelines that reason across diverse data types. Engage with NVIDIA experts and fellow developers to exchange design patterns and best practices for powering next-generation enterprise intelligence. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how to design and implement a multi-modal RAG pipeline that unifies video, audio, and text for enterprise use cases, from ingestion to retrieval to generation.",
        "Understand how to generate and store embeddings for different modalities, including best practices for vector indexing, latency, and cost controls in production.",
        "See how to plug in tools for automatic speech recognition, vision models, and LLMs to build an end-to-end workflow that your existing applications can call via simple APIs."
      ],
      "nvidia_technology": "NeMo, NVIDIA NIM, cuVS",
      "session_id": "CWES81538",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Maryam Najafian",
          "title": "Principal SWE, Agentic AI Tech Lead"
        },
        {
          "company": "NVIDIA",
          "name": "Rachel Allen",
          "title": "Engineering Manager, Cybersecurity AI"
        },
        {
          "company": "NVIDIA",
          "name": "Nave Algarici",
          "title": "Generative AI Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Annie Surla",
          "title": "Developer Advocate Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Edward Li",
          "title": "Technical Marketing Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Bo Liu",
          "title": "Sr. Research Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Sean Sodha",
          "title": "Sr. Software Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Daniel Fatade",
          "title": "Sr. Solutions Architect, Gen AI"
        },
        {
          "company": "NVIDIA",
          "name": "Michael Demoret",
          "title": "Engineering Manager, Cybersecurity AI"
        },
        {
          "company": "NVIDIA",
          "name": "Randy Gelhausen",
          "title": "Sr. Engineering Manager"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Multi-Modal RAG: Build Agentic AI Systems With Video, Audio, and Text",
      "topic": "Retrieval-Augmented Generation (RAG)",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81538/"
    },
    {
      "description": "Hear from two of NVIDIA’s leading researchers the story of NVIDIA Nemotron—an open-source ecosystem for the full AI life cycle. NVIDIA Nemotron is a collection of multi-modal and reasoning models, datasets, and training techniques designed to leverage NVIDIA’s experience in accelerated computing for each step of your AI journey.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn why NVIDIA, primarily known as a hardware company, invested in a world-class family of multi-modal reasoning models.",
        "Understand the components of Nemotron and how you can use Nemotron tools with other models.",
        "Hear about key uses Nemotron can be used to tackle with enterprise-grade accuracy: advanced reasoning, coding, visual understanding, agentic tasks, safety, and information retrieval.",
        "Learn how NVIDIA Nemotron uses cutting-edge model architectures to speed inference performance while balancing leading accuracy to provide a model that is state of the art and practical to use."
      ],
      "nvidia_technology": "NeMo",
      "session_id": "S81719",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Bryan Catanzaro",
          "title": "VP, Applied Deep Learning Research"
        }
      ],
      "technical_level": "General Interest",
      "title": "Nemotron Unpacked: Build, Fine-Tune, and Deploy NVIDIA's Open Models",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81719/"
    },
    {
      "description": "Both enterprises and NVIDIA Cloud Partners (NCPs) must establish consistent tenant isolation across NVIDIA Spectrum-X Ethernet, NVIDIA Quantum InfiniBand, NVLink Multi-Node, DPU, virtual, and edge networks. This requires hard isolation across bare metal, virtual machines (VMs), and container workloads on all involved fabrics to support data sovereignty and compliance requirements.",
      "format": "In-Person",
      "industry": "Cloud Services",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Operational Lessons Learned from Launching 20-plus Multi-Tenant NCPs and AI Factories: Gain real-world insights from launching 20+ NCP clusters over the last year, including architectural challenges, scaling considerations, and operational trade-offs encountered when deploying shared GPU-based AI clusters.",
        "How to Deliver Dynamic and Secure Multi-Tenancy using Hardware-Level Isolation: The session will cover how true multi-tenancy is enabled through hardware-level isolation across bare metal, VM, and container workloads. This hard isolation—now the industry standard for data sovereignty and compliance—is applied on switches, DPUs, and fabrics by defining endpoints, with Netris algorithms automatically generating and applying the correct configurations across all network elements.",
        "Why Network Automation is Critical for Deploying and Operating Shared GPU-based AI Clusters: The session will explain why network automation is required to deploy, onboard tenants, and operate shared GPU-based AI clusters where isolation, consistency, and ongoing change are critical."
      ],
      "nvidia_technology": "BlueField DPU, Infiniband Networking, Ethernet Networking, NVLink / NVSwitch",
      "session_id": "EX82094",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "Netris",
          "name": "Alex Saroyan",
          "title": "CEO and Co-Founder"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Network Automation Lessons Learned From Launching 20+ Multi-Tenant Clouds and AI Factories (Presented by Netris)",
      "topic": "AI Networking, 5G/6G",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82094/"
    },
    {
      "description": "AI has revolutionized surrogate models in the field of computational physics. However, in this session we take a look at how AI tools can benefit physics-based modeling. We will introduce the neural physics approach, which writes multi-grid solvers as convolutional neural networks whose weights are set from numerical theory, rather than by training. We give demonstrations that indicate the breadth of application possible from complex environmental problems to industrial problems; demonstrate the scalability of the approach; describe the potential for optimization tasks; and discuss future possibilities of combining physics-based models (untrained neural networks from neural physics) with surrogate models (trained neural networks).",
      "format": "Virtual",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "How AI tools can empower pure physics-based simulations",
        "How neural physics can be applied to model fluids, solids, particles and radiation",
        "Learn the approach gives rise to portable, differentiable code for physics-based problems."
      ],
      "nvidia_technology": "RTX GPU, NeMo",
      "session_id": "S81927",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Imperial College London (ICL)",
          "name": "Christopher Pain",
          "title": "Professor"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Neural Physics for AI-Powered Differentiable PDE-Solvers and Digital Twins",
      "topic": "Physics",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81927/"
    },
    {
      "description": "AI applications increasingly rely on fast, scalable access to both structured data and high-dimensional vector embeddings. Traditional CPU-based systems struggle to meet these demands at scale. We'll explore how to accelerate SQL databases and vector search engines with GPUs, enabling rapid index builds, low-latency queries, and seamless integration of analytical and AI workloads. Learn how GPU-accelerated data systems unlock new performance levels for modern AI pipelines. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Performance benefits of GPU-accelerated SQL and vector search",
        "How GPU indexing reduces build times and search latency at scale",
        "Real-world use cases and system design trade-offs"
      ],
      "nvidia_technology": "CUDA-X, cuDF, nvCOMP, cuVS",
      "session_id": "CWES81481",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Tanmay Gujar",
          "title": "Developer Technology Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Corey Nolet",
          "title": "Principal Engineer, ML, Data Mining, and Vector Search"
        },
        {
          "company": "NVIDIA",
          "name": "Todd Mostak",
          "title": "Sr. Director of Engineering"
        },
        {
          "company": "NVIDIA",
          "name": "Felipe Aramburu",
          "title": "Distinguished Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Manas Singh",
          "title": "TPM, Vector Search"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Next-Gen Data Systems: GPU Acceleration for SQL and Vector Databases",
      "topic": "Databases",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81481/"
    },
    {
      "description": "The session will provide insights on use of physical AI in BP's business—how BP is using NVIDIA's Omniverse and IsaacSIM to train, simulate, and deploy semi-autonomous inspection and remove humans from harm's way.",
      "format": "In-Person",
      "industry": "Energy",
      "intended_audience": "Business Executive",
      "key_takeaways": ["Secure and safe operations using physical AI"],
      "nvidia_technology": "Isaac, Omniverse",
      "session_id": "S81717",
      "session_type": "Talks or Panels",
      "speakers": [
        {
          "company": "BP",
          "name": "Ankur Bansal",
          "title": "Automation Lead"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Next-Gen Offshore Platforms: Safer, Smarter, and Efficient With Digital Twins",
      "topic": "Robotics Simulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81717/"
    },
    {
      "description": "Accelerate your scientific discovery pipeline by connecting directly with experts spanning autonomous AI agents for science, AI for molecular and materials simulation, and high-performance traditional atomistic simulation. You will explore how to leverage the latest GPU-optimized tools — including NVIDIA ALCHEMI Toolkit, cuEquivariance, VASP, and GROMACS — to maximize accuracy and computational throughput for your specific research challenges. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Deploy autonomous AI agents to orchestrate complex workflows and accelerate discovery in energy, agriculture, and materials science.",
        "Leverage the ALCHEMI Toolkit and cuEquivariance to accelerate machine learning interatomic potentials for high-fidelity molecular design.",
        "Maximize computational performance for electronic structure calculations in VASP, Quantum ESPRESSO, and CP2K using the latest GPU optimizations.",
        "Extend simulation timescales and improve sampling efficiency in classical molecular dynamics codes like GROMACS, NAMD, and LAMMPS."
      ],
      "nvidia_technology": "CUDA-X",
      "session_id": "CWES81635",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Justin Smith",
          "title": "Sr. Developer Relations Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Stefan Maintz",
          "title": "DevTech Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Yutong Zhao",
          "title": "Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Alan Gray",
          "title": "Principal Developer Technology Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Evan Weinberg",
          "title": "Sr. Compute Developer Technology Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Piero Altoe",
          "title": "Developer Relations Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Robert Parrish",
          "title": "Sr. Engineering Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Logan Ward",
          "title": "Application Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Next-Generation Discovery: Agentic AI for Science, AI-Driven Simulation, and GPU-Accelerated Chemistry",
      "topic": "Computational Chemistry / Materials Science",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81635/"
    },
    {
      "description": "Attend this introductory session for an overview of how NVIDIA’s AI platform sparked the agentic and physical AI revolution. We'll showcase real-world examples of how NVIDIA has disrupted some of the world’s major industries. An interactive Connect With the Experts session will follow the talk, where you can engage with NVIDIA’s hardware firsthand and ask any follow-up questions. Check out our Connect with the Experts session here.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Learn how NVIDIA’s range of software, including NVIDIA AI Enterprise, CUDA, and Omniverse, have enabled successful agentic and physical AI applications across different industries.",
        "Familiarize yourself with NVIDIA’s hardware, including GPUs and CPUs, and see how NVIDIA has moved beyond the limitations of Moore’s Law.",
        "See live demos of these agentic and physical AI applications in action."
      ],
      "nvidia_technology": "Grace CPU, Jetson, RTX GPU, HGX, CUDA, TensorRT, Isaac, Omniverse, RAPIDS, Infiniband Networking, Modulus, Interconnect Networking, NeMo, NVLink / NVSwitch, Omniverse Replicator, PhysX, Blackwell, DGX Spark",
      "session_id": "S81708",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Rambo Jacoby",
          "title": "Principal Engineer"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "NVIDIA 101: From Chips to Software—How NVIDIA's AI Platform Enables the AI Revolution",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81708/"
    },
    {
      "description": "This session is a continuation of the \"NVIDIA 101: From Chips to Software — How NVIDIA's AI Platform Enables the AI Revolution\" session led by Rambo Jacoby. In this session, you'll get the opportunity to touch and feel NVIDIA's hardware and ask any questions you may have about NVIDIA's platform, and to ask follow up questions from Rambo's earlier session, directly from Rambo himself. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Learn how NVIDIA’s range of software, including NVIDIA AI Enterprise, CUDA, and Omniverse, have enabled successful agentic and physical AI applications across different industries.",
        "Familiarize yourself with NVIDIA’s hardware, including GPUs and CPUs, and see how NVIDIA has moved beyond the limitations of Moore’s Law.",
        "See live demos of these agentic and physical AI applications in action."
      ],
      "nvidia_technology": "Blackwell",
      "session_id": "CWES81955",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Rambo Jacoby",
          "title": "Principal Engineer"
        }
      ],
      "technical_level": "General Interest",
      "title": "NVIDIA 101: From Chips to Software—How NVIDIA's AI Platform Enables the AI Revolution",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81955/"
    },
    {
      "description": "本講演にご参加いただくことで、最先端の自動運転AI開発に必須となる、NVIDIA Cosmosを活用した高性能データセット基盤の具体的な構築ノウハウと実装事例を習得できます。 今日の自動運転AIの性能向上のボトルネックとなっている、大規模データの効率的な管理手法や、AIのロバスト性を左右する収集困難なエッジケースデータの生成・補完技術を深く理解し、皆様のプロジェクトにおける開発コストの削減とAIの実用性の向上に直結する知見を得ることが重要です。 具体的には、Cosmosの機能とAutolabeling基盤を連携させることで、実世界での収集が難しいロングテールデータを効率的に補完しつつ、ラベル付きのエッジケースデータが同時に得られる手法を解説します。 これらの技術を組み合わせることで、より現実的で高性能な自動運転AIを実現するためのデータセット基盤構築の実践的な知識を提供します。",
      "format": "Virtual",
      "industry": "Automotive / Transportation",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "NVIDIA Cosmosを活用した自動運転データセット基盤の全体設計と実装事例: 大規模かつ高性能な自動運転AIデータセットを、Cosmosの各種機能を連携させて構築する具体的なアーキテクチャとワークフローを習得します。",
        "ロングテールデータの効率的な生成、補完、および自動ラベリング技術: 実世界での収集が困難なエッジケース（Cosmos Reasonによる検索、Cosmos Transferによる仮想生成、Cosmos Predictによる未知シーン補完）を効率的にカバーし、AIのロバスト性を高める実践的手法を理解します。また、データ準備工程における自動ラベリングの導入による効率化の手法を学びます。",
        "データ管理の最適化による開発コスト・時間の大幅な削減ノウハウ: データセットの高精度な検索・フィルタリングを通じて、必要なデータのみを効率的に利用し、開発・データ加工にかかる時間とコストを大幅に削減する方法論を学びます。",
        "AIの実用性を飛躍的に向上させるデータセット構築戦略: 従来のデータ収集方法の限界を克服し、生成AIでエッジケースを生成することで、より現実的で高性能な自動運転AIのデータセットを構築するための戦略的知見を得られます。"
      ],
      "nvidia_technology": "Cosmos",
      "session_id": "S81897",
      "session_type": "Talk",
      "speakers": [],
      "technical_level": "Technical - Intermediate",
      "title": "NVIDIA Cosmosを活用した自動運転向けデータセット基盤の実装事例",
      "topic": "Image Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81897/"
    },
    {
      "description": "We'll detail our journey, achievements, and insights gained from implementing the Apache Spark RAPIDS accelerator across Snap's most substantial data processing pipelines. A significant cost reduction of 40% has been realized in comparison to utilizing CPU-only jobs. We are currently collaborating with Google and NVIDIA to further deploy the RAPIDS accelerator to a broader range of our production workloads.",
      "format": "In-Person",
      "industry": "Consumer Internet",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Application of RAPIDS Accelerator for Apache Spark at petabyte scale",
        "Case studies on certain Spark jobs that benefit from GPU acceleration",
        "Managing and working with GPU resources at scale, from managed cloud solutions to self-managed Kubernetes clusters"
      ],
      "nvidia_technology": "RAPIDS",
      "session_id": "S81678",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Snap, Inc.",
          "name": "Liang Chen",
          "title": "Staff Software Engineer"
        },
        {
          "company": "Snap, Inc.",
          "name": "Prudhvi Vatala",
          "title": "Sr. Engineering Manager"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "NVIDIA GPU Acceleration on Snap's Experimentation Platform: A Cost-Benefit Analysis Demonstrating Million-Dollar Savings",
      "topic": "Data Analytics / Processing",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81678/"
    },
    {
      "description": "本次分享将介绍如何基于 NVIDIA Megatron-Core Trainer 与 vLLM/SGLang Rollouter 搭建一个高性能、可扩展的强化学习训练框架。该框架采用流式、全异步的架构设计，实现了样本生成与训练过程的完全解耦，大幅减轻了 rollout 阶段长尾样本带来的资源闲置问题，充分发挥 NVIDIA GPU 的并行算力。在此体系下，我们在 128 张 NVIDIA 加速卡上完成了 Qwen7B/Qwen-30B-A3B 等模型的强化学习实验，在效果持平的前提下，取得了 1.7 倍 ~ 2.7 倍的训练效率提升。",
      "format": "Virtual",
      "industry": "Consumer Internet",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "生产者-消费者模式的流式架构：对 Rollouter 和 Trainer 的资源进行隔离，Rollouter 按照样本粒度持续生成样本，“先完成先出”，攒够需要的 batch_size 后 Trainer 开启训练，实现一边生成一边训练",
        "多种训练模式：支持从 on-policy 到多步 off-policy 等多种训练策略，用户可根据实际场景尝试不同模式，达到训练效果和效率的最佳平衡点",
        "卓越的训练效率：和同步训练相比，异步框架取得了 2 倍以上的训练效率提升，且效果无损"
      ],
      "nvidia_technology": "NeMo, NSight Systems",
      "session_id": "S81620",
      "session_type": "Talk",
      "speakers": [],
      "technical_level": "Technical - Advanced",
      "title": "NVIDIA GPU 的 LLM 异步强化学习训练",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81620/"
    },
    {
      "description": "本講演では、NVIDIA NeMo Guardrails と Amazon Bedrock を用いて生成 AI アプリケーションにガードレールを実装し、有害コンテンツに対する検知性能をどのように評価・改善していくかを、実例とともに紹介します。言語間（英語・日本語）の違いや、検知率と誤検知率のバランスといった複数の観点から検知性能を比較し、安全性と利便性のバランスを意識した評価プロセスの具体例を共有します。",
      "format": "Virtual",
      "industry": "Manufacturing",
      "intended_audience": "Security Operations",
      "key_takeaways": [
        "生成 AI アプリケーションにおける代表的な有害コンテンツと、その検知に関する考え方を理解できます。",
        "NVIDIA NeMo Guardrailsと Amazon Bedrock を組み合わせた AI ガードレールの実装事例を学べます。",
        "安全性と利便性の両立を目指したガードレール評価プロセスの全体像を把握できます。",
        "Amazon Bedrock 上の複数モデルおよび英語・日本語コンテンツ間で、検知性能がどのように変化するかを理解できます。"
      ],
      "nvidia_technology": "NeMo",
      "session_id": "S81703",
      "session_type": "Talk",
      "speakers": [],
      "technical_level": "Technical - Intermediate",
      "title": "NVIDIA NeMo Guardrails と Amazon Bedrock による AI ガードレール構築と検知性能評価",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81703/"
    },
    {
      "description": "生成系 AI およびロボティクス領域では、モデル性能の向上と同時に、学習データの品質・安全性・効率性がこれまで以上に重要な競争軸となっています。 本セッションでは、NVIDIA NeMo Curator のフィルター機能を用いた高品質データの選別による学習効率化、Nemotron-Personas-Japan を活用した安全性を重視した合成データの作成、さらに NVIDIA NeMo Curator による動画データの効率的な分割・抽出を紹介します。 また、NVIDIA Cosmos を用いたロボット領域でのデータ生成を通じて、シミュレーションデータを活用した実践的な学習手法を紹介します。 LLM、VLM、Physical AI を紹介し、モデルの精度向上における、効率的な実務的アプローチを共有します。",
      "format": "Virtual",
      "industry": "Manufacturing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "NVIDIA NeMo Curator のフィルター機能活用による合成データ開発効率化",
        "Nemotron-Personas-Japan 活用による、LLM の安全性を向上させる合成データ作成",
        "NVIDIA NeMo Curator による動画のキュレーションを活用し、動画データの効率的な学習について",
        "NVIDIA Cosmos を活用したロボット領域での活用方法について"
      ],
      "nvidia_technology": "NeMo",
      "session_id": "S81964",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "APTO Inc.",
          "name": "Ryo Takashina",
          "title": "ML Engineer"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "NVIDIA サービスを活用し、LLM の精度向上からフィジカル AI への応用について",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81964/"
    },
    {
      "description": "本次演讲将介绍 NVIDIA 的辅助驾驶核心模块，从云端训练到车端推理的全栈解决方案，包括 NVIDIA Cosmos Reason（视频理解和 VLA 骨干网络）、Cosmos World Model（全局场景仿真）、 NuRec 神经重建和闭环仿真、ACCV-Lab 辅助驾驶训练工具包以及面向 NVIDIA DRIVE Thor 的 VLA 模型部署及展望",
      "format": "Virtual",
      "industry": "Automotive / Transportation",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "借助 NVIDIA ACCV-Lab 工具集加速驾驶辅助系统实现端到端高效训练",
        "面向 NVIDIA DRIVE Thor 的 VLA 模型部署及展望"
      ],
      "nvidia_technology": "TensorRT, Video Codec SDK, Video Storage Toolkit (VST), NVIDIA AI Enterprise",
      "session_id": "S82221",
      "session_type": "Talk",
      "speakers": [],
      "technical_level": "Technical - Advanced",
      "title": "NVIDIA 加速辅助驾驶 2.0 时代 AI 定义汽车的大规模应用",
      "topic": "Video Analytics",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82221/"
    },
    {
      "description": "本セッションでは、半導体製造装置の立ち上げ・保守作業などの支援を目的としたLLMのチューニング方法をご紹介します。NVIDIAが公開しているオープンソースの合成ペルソナデータセットであるNemotron-Personas-USA, Nemotron-Personas-Japanと合成データ生成ツールであるNVIDIA NeMo Data Designerを活用した独自データセットの構築から、NVIDIA NeMoを用いたモデルチューニングまでのプロセスを紹介し、製造業におけるAI活用の可能性を探求します。",
      "format": "Virtual",
      "industry": "Manufacturing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "AIエージェントへの人格埋め込みを、Nemotron Personasで簡単にできる方法を紹介",
        "NVIDIA NeMo Data Designerによる、独自データをもとにした合成データ生成方法を紹介",
        "NVIDIA NeMo RL によるモデルのチューニング方法を紹介"
      ],
      "nvidia_technology": "Hopper, NeMo",
      "session_id": "S81827",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "東京エレクトロン株式会社",
          "name": "Kenya Nishiguchi",
          "title": "データサイエンティスト"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "NVIDIA NeMo を活用した半導体領域知識を組み込む小規模 LLM のファインチューニング",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81827/"
    },
    {
      "description": "本講演では、富士通の自律型AIプラットフォーム「Kozuchi AI プラットフォーム」に対し、NVIDIAの多様なソフトウェア（NVIDA NeMo、NVIDA NIM、NVIDA Blueprints、NVIDA Metropolis など）をどのようにマッピング・統合することで、プラットフォーム全体の機能性や拡張性、そして適用可能なユースケースのカバー範囲がどのように進化しているのかを、全体アーキテクチャと具体的な取り組みを交えて解説します。AIプラットフォームが単体技術の集合から、学習・推論・AIエージェント構築・運用までを一体で支える基盤へと進化する中、ソフトウェアスタック全体を俯瞰した設計思想と統合アプローチを理解することは、AIを実業務で活用する上で極めて重要だからです。 Kozuchi AI プラットフォームでは、富士通独自のTakaneやマルチAIエージェント技術、セキュリティ機能を中核に据えつつ、NVIDIA NeMo によるモデル学習・カスタマイズ、NIM による推論およびデプロイ環境の標準化、Blueprint を活用したエージェント構築と最適化を組み合わせることで、信頼性と運用性を兼ね備えた自律型AI基盤の実現を目指しています。この構成は性能向上にとどまらず、AIの再利用性、スケーラビリティ、ガバナンスといった実装・運用上の課題に対応するためにも不可欠です。 特に本講演では、NVIDIA AI Blueprint for video search and summarization（VSS）の取り組みに焦点を当て、映像データを起点とした自律型AIエージェントの実装と進化を紹介します。製造現場や物流、公共空間などで生成される膨大な映像データを、VSS blueprint と Metropolis を連携させて検索・要約・理解し、業務知識やルールと結び付けて判断や提案へとつなげるアプローチは、現場AIの実用化を大きく前進させるためです。",
      "format": "Virtual",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Kozuchi AI プラットフォームにおける NVIDIA ソフトウェア統合の全体像",
        "自律型AIプラットフォーム設計におけるソフトウェアスタックの考え方",
        "VSS blueprint を中核とした現場AIの実装アプローチ",
        "Vision AI と生成AI・AIエージェントを統合する設計思想",
        "産業別・業務別AI展開を見据えたプラットフォーム進化の方向性"
      ],
      "nvidia_technology": "TensorRT, Riva, Metropolis, NeMo, NVIDIA NIM, Blueprint, Dynamo",
      "session_id": "S81963",
      "session_type": "Talk",
      "speakers": [],
      "technical_level": "Technical - Intermediate",
      "title": "NVIDIAソフトウェアとの統合により進化する富士通の自律型AIプラットフォーム",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81963/"
    },
    {
      "description": "Integrate clash detection into your OpenUSD-based digital twin workflows using NVIDIA Omniverse libraries. In this lab, we’ll ensure ingested assets are run through conflict analysis using the Clash Detection SDK. You’ll create and execute collision detection queries, inspect and visualize results in an Omniverse viewport, and implement robust overlap detection in complex 3D scenes. We will learn the fundamentals of simulation for digital twins, engineering validation, and automated design verification workflows. Familiarity with CAD workflows, 3D modeling concepts Basic understanding of simulation concepts and collision detection principles Experience navigating 3D viewports and interactive visualization environments",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Explore Omniverse Clash SDK, APIs and reference implementation for clash detection.",
        "Perform interactive clearance and conflict checks and what-if scenario analysis on USD stages with engineering and factory data.",
        "Validate layouts, visualize results, generate layout review reports and identify metadata for resolving clashes on complex stages.",
        "Connect With the Experts",
        "Workshops, Training Labs & Certification",
        "Sponsors & Exhibitors",
        "Privacy Policy Your Privacy Choices Terms of Service Accessibility Corporate Policies Product Security Contact",
        "Copyright © 2026 NVIDIA Corporation",
        "NVIDIA uses cookies to improve your experience on our web site. We and our third-party partners also use cookies and other tools to collect and record informati"
      ],
      "nvidia_technology": "Omniverse",
      "session_id": "DLIT81803",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Martin Vovk",
          "title": "Sr. Solutions Architect"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "OpenUSD Asset Integration and Data Review With Clash Detection",
      "topic": "Industrial Digitalization / Digital Twin",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81803/"
    },
    {
      "description": "Discover how OpenUSD principles and Python scripting can build scalable 3D data pipelines for manufacturing, robotics, and physical AI applications. Throughout this full-day workshop, you'll learn composition arc techniques, asset hierarchy design, and performance optimization strategies essential for production pipelines. Develop practical skills in automating 3D asset workflows, managing complex scene data, and preparing simulation environments for physical AI training. This workshop directly prepares you for the NVIDIA-Certified Professional: OpenUSD Development exam. Certificate: Upon successful completion of the assessment, you’ll receive an NVIDIA certificate to recognize your subject matter competency and support your professional career growth. Intermediate Python experience, including packages, virtual environments, and scripting fundamentals. Familiarity with 3D modeling concepts: meshes, materials, transforms, scene hierarchies, and asset organization. Understanding of data pipeline concepts and file-based workflows. Basic knowledge of composition and layering concepts in content creation tools.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Apply USD Python for programmatic scene creation and asset management",
        "Compose complex USD scenes using references, payloads, variant sets, and LIVERPS strength ordering",
        "Structure asset hierarchies and layer stacks to support production-scale pipelines",
        "Apply data exchange principles for extracting, transforming, and validating USD data",
        "Optimize scene performance through instancing and modular design for large-scale simulations"
      ],
      "nvidia_technology": "Omniverse",
      "session_id": "DLIW82272",
      "session_type": "Full-Day Workshop",
      "speakers": [
        {
          "company": "UME.Studio",
          "name": "Daniel Roizman",
          "title": "CEO"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "OpenUSD Crash Course: Build 3D Data Pipelines for Physical AI",
      "topic": "Industrial Digitalization / Digital Twin",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dliw82272/"
    },
    {
      "description": "This tutorial provides a practitioner-led deep dive into why KV-cache design is a first-order performance lever for production LLM inference, directly impacting throughput, tail latency, and GPU utilization. Drawing on real production experience, the session walks through KV-cache design principles and implementation approaches used in FlexKV, LMCache, and Dynamo KVBM. The discussion highlights practical lessons from deploying KV caching with modern attention patterns such as MQA, GQA, DS3.2, and speculative decoding. You'll leave with actionable recipes, code patterns, and profiling techniques to integrate KV caching into existing inference engines and scale beyond a single node.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Why KV-cache Design Matters: Learn how KV caching directly impacts throughput, tail latency, and GPU utilization for multi-token LLM inference in production setups, and why naïve caching leaves significant performance on the table.",
        "Understand from Practitioners Who have Deployed KV Cache in Production: Compare the core design philosophies of FlexKV (block-aware dynamic sizing), LMCache (compression + eviction), and Dynamo KVBM (tiered GPU/DRAM/SSD placement with asynchronous prefetch) using real workloads and profiling data.",
        "Lessons Learned from Production Deployments: Discussion and implications of different system configuration, and how to work with attention patterns in MQA/GQA/DS3.2/Speculative decoding workloads",
        "Actionable Guidance for Developers: Leave with practical recipes and code patterns to integrate KV caching into existing inference engines, run profiling tools to detect bottlenecks, and avoid the top “gotchas” that slow down scaling past a single node."
      ],
      "nvidia_technology": "Dynamo, NIXL",
      "session_id": "S82033",
      "session_type": "Tutorial",
      "speakers": [
        {
          "company": "Tencent Cloud",
          "name": "Fan YE",
          "title": "Director of Accelerated Computing"
        },
        {
          "company": "Tensormesh",
          "name": "Junchen Jiang",
          "title": "CEO"
        },
        {
          "company": "NVIDIA",
          "name": "Ziqi Fan",
          "title": "Sr. Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Vikram Sharma Mailthody",
          "title": "Sr. Research Scientist"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Optimize KV Caches for LLM Inference: Dynamo KVBM, FlexKV, LMCache",
      "topic": "AI Inference",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82033/"
    },
    {
      "description": "This training lab shows you how to optimize and scale LLM workflows with SGLang. We walk through practical performance tuning using the SGL-Cookbook, dive into profiling and bottleneck analysis for developers, and demonstrate its deep integration into reinforcement learning (RL) training frameworks by showing how to use SGLang in a real RL run with the Miles RL framework. Curious about how LLM systems are optimized and scaled in practice No prior experience with SGLang, performance tuning, or RL training is required",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how to tune and scale LLM inference with SGLang, including how to select and combine configurations for better latency and throughput.",
        "Gain hands-on experience profiling LLM workloads and identifying performance bottlenecks with developer-oriented tools and workflows.",
        "See how SGLang integrates into RL training frameworks by running a real RL job with the Miles RL framework."
      ],
      "nvidia_technology": "Hopper, Blackwell, DGX Cloud, DGX Station",
      "session_id": "DLIT82143",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "Radixark.ai",
          "name": "Baizhou Zhang",
          "title": ""
        },
        {
          "company": "Radixark.ai",
          "name": "Qiaolin Yu",
          "title": ""
        },
        {
          "company": "Radixark.ai",
          "name": "Yueming Yuan",
          "title": ""
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Optimize LLM Inference and RL Training With SGLang",
      "topic": "AI Inference",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit82143/"
    },
    {
      "description": "This ninja-level tutorial will guide you through the techniques to build a low-latency inference kernel for Blackwell using CUDA and PTX. You will learn about techniques like programmatic dependent launch, prologue minimization, lightweight CGA parallelism, and operand swapping in the Blackwell Tensor Cores.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn about challenges in parallelizing small GEMMs, and how Blackwell hardware features help overcome them.",
        "Learn about key features of the Blackwell Tensor Cores.",
        "Learn about the benefits of warp-specialization and latency hiding."
      ],
      "nvidia_technology": "CUDA, Blackwell",
      "session_id": "S81518",
      "session_type": "Tutorial",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Allard Hendriksen",
          "title": "Sr. Developer Technology"
        },
        {
          "company": "NVIDIA",
          "name": "Petrick Liu",
          "title": "Developer Technology Engineer"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Optimize Mixture-Of-Experts for Low-Latency Inference Decode on Blackwell",
      "topic": "AI Inference",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81518/"
    },
    {
      "description": "We explore how Lambda’s bare-metal cloud unlocks the full performance of NVIDIA rack-scale systems by removing overhead and delivering the hardware as designed. You’ll learn how direct access to GPUs, CPUs, NVIDIA NVLink, and high-bandwidth fabrics increases efficiency, reduces time-to-train, and enables scaling for frontier AI workloads.",
      "format": "In-Person",
      "industry": "Cloud Services",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Understand the architectural benefits of the Blackwell and Rubin platforms for specific large-scale AI and HPC workloads.",
        "Learn the design principles Lambda uses to deliver isolated, high-performance bare-metal environments optimized for next-gen NVIDIA platforms.",
        "Learn practical techniques for bare-metal workload optimization to achieve maximum compute performance and efficiency at scale."
      ],
      "nvidia_technology": "Cloud / Data Center GPU",
      "session_id": "S82151",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Lambda",
          "name": "Maxx Garrison",
          "title": "Product Manager for Cloud Infrastructure"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Optimize NVIDIA Vera Rubin NVL72 and GB300 NVL72 Workloads With Lambda’s Bare-Metal Cloud (Presented by Lambda)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82151/"
    },
    {
      "description": "As vision AI continues to expand into real-world applications—from multi-modal systems like vision language models (VLMs) to depth-aware robotics using stereo vision—optimizing model performance on edge devices has become mission-critical. This talk dives into practical strategies for deploying high-performance vision models on constrained hardware. We'll explore techniques for customizing large-scale VLMs while maintaining multi-modal alignment, and optimizing depth estimation models for real-time stereo processing with limited compute and power budgets. You'll leave with a clear understanding of how to balance accuracy, latency, and resource usage when bringing advanced vision capabilities to the edge.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Customize and adapt large-scale VLMs for edge deployment while preserving multi-modal alignment and robust visual-language reasoning.",
        "Optimize stereo depth estimation models and processing pipelines to achieve real-time performance under tight compute and power budgets.",
        "Design and evaluate edge inference strategies that balance accuracy, latency, and resource usage for mission-critical vision applications."
      ],
      "nvidia_technology": "Jetson, DeepStream, Metropolis, TAO Toolkit, DGX Spark",
      "session_id": "S81833",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Louise Huang",
          "title": "Product Manager"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Optimize Performance of Vision AI Models on the Edge",
      "topic": "Video Analytics",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81833/"
    },
    {
      "description": "Learn how to use a graphical user interface-based integrated development environment (IDE) purpose-built for deep neural network developers to manage the end-to-end process of going from PyTorch to a deployment-ready model that yields the best inference performance. Learn how to use a GPU-based performance profiler to guide sound decision-making for model optimizations. The core concepts of PyTorch and Deep Learning are needed to get through this course.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Explore a new workflow for optimizing deep neural network models in PyTorch that leverages GPU hardware performance data.",
        "Learn to graphically edit deep neural network models using the Nsight DL Designer IDE.",
        "Learn to deploy ONNX models directly to TensorRT and ONNX Runtime from within Nsight DL Designer.",
        "Connect With the Experts",
        "Workshops, Training Labs & Certification",
        "Sponsors & Exhibitors",
        "Privacy Policy Your Privacy Choices Terms of Service Accessibility Corporate Policies Product Security Contact",
        "Copyright © 2026 NVIDIA Corporation",
        "NVIDIA uses cookies to improve your experience on our web site. We and our third-party partners also use cookies and other tools to collect and record information you provide as well as information"
      ],
      "nvidia_technology": "CUDA, TensorRT, NSight Systems, TAO Toolkit",
      "session_id": "DLIT81579",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Manoj Kumar Yennapureddy",
          "title": "Sr. Deep Learning Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Optimize PyTorch Models for High-Performance Inference With Nsight Deep Learning Designer",
      "topic": "Performance Optimization",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81579/"
    },
    {
      "description": "Making split-second decisions that balance fraud, cost, and conversion is central to global payments. This session explores how machine learning techniques—such as reinforcement learning, contextual bandits, and transformers—can be applied to optimize decision-making at scale. Gain insights into deploying low-latency models in production environments, as well as research approaches for moving from manual feature engineering to self-supervised representation learning.",
      "format": "Virtual",
      "industry": "Financial Services",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Learn how to move from isolated models and decisioning to a unified system using reinforcement learning to optimize the entire payment life cycle (conversion, risk, cost) simultaneously.",
        "Look at the engineering choices and constraints of serving complex ensembles with P50 latencies of 20ms across a distributed, high-availability platform.",
        "Gain insight into our exploration of foundational models for payments, moving from manual feature engineering to learning directly from raw transaction sequences."
      ],
      "nvidia_technology": "Cloud / Data Center GPU",
      "session_id": "S82115",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Adyen",
          "name": "Dhruv Ghulati",
          "title": "Principal Product Manager"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Optimizing $1 Trillion in Payments: From Contextual Bandits to Foundational Models",
      "topic": "Code / Software Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82115/"
    },
    {
      "description": "Amazon Ads is using large language models to solve complex shopping problems. Learn the system choices driven by the business application, disaggregated serving patterns with NVIDIA Dynamo on AWS EKS, and benchmarking practices that enable adopting new optimizations without re-architecting the entire stack.",
      "format": "In-Person",
      "industry": "Retail / Consumer Packaged Goods",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Learn how to navigate through numerous LLM inference optimizations, starting from business application requirements, and make critical latency/cost trade-offs for large-scale inference.",
        "Dive deep into emerging shopping behaviors in the Gen AI era and learn how LLMs can improve the shopping experience.",
        "Discover the disaggregated LLM serving pattern using NVIDIA Dynamo on AWS EKS that allows for rapid adoption of new performance optimizations without a full stack re-architecture.",
        "Understand how to leverage industry-standard software and established benchmarking practices to deploy cutting-edge LLM solutions faster."
      ],
      "nvidia_technology": "Dynamo, Cloud / Data Center GPU, Hopper, NIXL",
      "session_id": "S81656",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Amazon",
          "name": "Muthu Muthukrishnan",
          "title": "VP Sponsored Products and Brands"
        },
        {
          "company": "Amazon",
          "name": "Runqing Yang",
          "title": "Sr. Machine Learning Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Optimizing the Shopping Experience at Amazon Ads With NVIDIA Dynamo",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81656/"
    },
    {
      "description": "This talk explores the Ozaki Scheme, including its two variants, as a practical approach for recovering high accuracy from low-precision computations in numerical linear algebra. Participation is important as it sheds light on how we can effectively respond to the accuracy challenges posed by next-generation accelerator architectures.",
      "format": "Virtual",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Research: Academic",
      "key_takeaways": [
        "The core concept of the Ozaki Scheme and its variants, and how high-accuracy results can be systematically recovered from low-precision arithmetic in numerical linear algebra.",
        "How the Ozaki Scheme addresses the accuracy–performance trade-off, enabling reliable numerical results while exploiting the high throughput of low-precision accelerators.",
        "Practical techniques for applying the Ozaki Scheme to matrix computations, including matrix multiplication and factorizations, on modern GPU and accelerator architectures."
      ],
      "nvidia_technology": "cuBLAS",
      "session_id": "S82285",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Shibaura Institute of Technology",
          "name": "Katsuhisa Ozaki",
          "title": "Professor"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Ozaki Scheme: Addressing the Accuracy Challenge in the Era of Low-Precision Accelerators",
      "topic": "Accelerated Computing Libraries",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82285/"
    },
    {
      "description": "Modern GPUs have gotten so fast that CPUs cannot launch new work to the GPUs fast enough, which reduces overall efficiency. CUDA Graphs can eliminate CPU overhead completely, but have traditionally been hard to use, so let’s break down the problems and fixes for them, focusing on PyTorch in particular.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "CUDA Graphs are hard to use because launching them depends upon global state, but CUDA Graphs don’t know this state on their own. The user must track this manually, which is error-prone.",
        "Global state errors can be classified into several categories: consistency problems between device and host, use-after-free errors, uncaptured work, and stale memory buffers.",
        "Debugging tools in PyTorch built on CUPTI and the CUDA API can be used to identify several of these errors.",
        "Users can be relieved of thinking about these problems if they make their model work with torch.compile(), via a feature called “parameterized CUDA graph launch.”"
      ],
      "nvidia_technology": "CUDA",
      "session_id": "S81709",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Daniel Galvez",
          "title": "Sr. AI DevTech Engineer"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Parameterized CUDA Graph Launch in PyTorch: CUDA Graphs Without the Pain",
      "topic": "Deep Learning Frameworks",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81709/"
    },
    {
      "description": "This tutorial guides you through the implementation of an FMHA kernel using cuTile, focusing on how to express complex dataflows—including matrix products, softmax reductions, and attention-value multiplication—directly in terms of multidimensional tiles. Explore how the cuTile compiler automates the most difficult aspects of GPU optimization, such as asynchronous memory movement, software pipelining, and Tensor Core utilization. By the end of the session, you'll understand how to move from a high-level algorithm description to a production-quality kernel that achieves near-peak performance while remaining readable and maintainable.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "The Tile-Centric Paradigm: Learn to define GPU execution by breaking algorithms into logical tiles, rather than managing individual thread workloads.",
        "Automated Memory Orchestration: Understand how cuTile automatically lowers high-level load and store operations into efficient hardware-specific asynchronous transfers.",
        "High-Performance FMHA implementation: Build a complete attention kernel step-by-step, managing the complex dependencies between scores and softmax results using intuitive array-like syntax.",
        "Implicit Latency Hiding: Discover how cuTile handles the overlap of data movement and compute, enabling maximum GPU utilization without complex manual synchronization.",
        "Integration and Profiling: Gain practical experience profiling DSL-based kernels and integrating them directly into modern AI frameworks as high-speed custom operators."
      ],
      "nvidia_technology": "CUDA, Blackwell",
      "session_id": "S81705",
      "session_type": "Tutorial",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Alessandro Morari",
          "title": "DevTech Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Vishal Mehta",
          "title": "DevTech Engineer"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Performance Optimization of cuTile Kernels",
      "topic": "Programming Languages / Compilers",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81705/"
    },
    {
      "description": "Artificial intelligence is driving unprecedented computational demands that rely on massive data access at low latency. Learn how advanced semiconductor technologies—high-bandwidth memory, low-power DRAM, and next-generation storage—have become critical to meeting these computing challenges in AI systems. We will spotlight how Micron’s innovations reduce power consumption in large-scale AI training and inference systems while delivering the bandwidth to advance next-generation generative AI.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Press / Analyst",
      "key_takeaways": [
        "Understand how data bandwidth directly relates to smarter, more capable AI.",
        "See how AI-driven improvements are accelerating semiconductor manufacturing.",
        "Gain insights into Micron’s view of future memory technologies."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, Grace CPU, RTX GPU, Hopper, Blackwell",
      "session_id": "S82079",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Micron Technology",
          "name": "Girish Cherussery",
          "title": "VP & GM, AI Memory Solutions"
        }
      ],
      "technical_level": "General Interest",
      "title": "Performance Without Compromise: How Memory Innovation is Central to AI System Effectiveness (Presented by Micron)",
      "topic": "Sustainable Computing",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82079/"
    },
    {
      "description": "Physical AI moves enterprise intelligence into real-world systems that sense, reason, and act. Scaling from pilots to enterprise operations remains challenging. In this session, TCS and a global enterprise discuss how physical AI is operationalized using NVIDIA-accelerated computing to deliver measurable outcomes across physical operations. The session walks through a representative use case integrating physical AI assets—combining real-time sensing, simulation, and decisioning using NVIDIA Omniverse for digital twin simulation, Isaac Sim for robotics and physical system modeling, and NVIDIA NIM for optimized inference. Speakers share production lessons learned on integrating physical data streams and sustaining performance and reliability at scale. You'll leave with a grounded view of how to scale physical AI in production and turn real-world intelligence into enterprise advantage.",
      "format": "In-Person",
      "industry": "Manufacturing",
      "intended_audience": "Consulting",
      "key_takeaways": [
        "How physical AI systems sense, reason, and act in real-world operations at scale",
        "How NVIDIA Omniverse, Isaac Sim, and NVIDIA NIM support physical AI workflows",
        "How real-time physical data is integrated into production AI systems",
        "Key learnings from scaling physical AI beyond pilots",
        "How physical AI delivers measurable enterprise outcomes"
      ],
      "nvidia_technology": "Omniverse, NVIDIA NIM",
      "session_id": "S82102",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Tata Consultancy Services",
          "name": "Naresh Mehta",
          "title": "Global Chief Technology and Innovation Officer"
        },
        {
          "company": "Tata Consultancy Services",
          "name": "Laksh Parthasarathy",
          "title": "Global Head, Smart Mobility Group"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Physical AI at Scale: Turn Real-World Intelligence Into Enterprise Advantage (Presented by Tata Consultancy Services)",
      "topic": "Robot Navigation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82102/"
    },
    {
      "description": "Discover the latest advancements in end-to-end autonomous driving as the industry accelerates toward scalable, Level 4 AI-driven systems. This session explores how NVIDIA’s Alpamayo—particularly reasoning vision-language-action models (VLA)—enable vehicles to interpret, reason about, and navigate complex real-world scenarios with greater humanlike understanding. We’ll highlight how this approach is strengthened by a continuous self-improving loop powered by NVIDIA Cosmos and safeguarded by NVIDIA Halos, NVIDIA’s comprehensive full-stack safety system for physical AI. Finally, we’ll discuss how NVIDIA is building an open ecosystem to empower partners in developing next-generation, robotaxi-ready autonomous vehicles.",
      "format": "In-Person",
      "industry": "Automotive / Transportation",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how NVIDIA uses Aplamayo, reasoning vision-language-action (VLA) models to advance autonomous-vehicle capabilities through trustworthy reasoning."
      ],
      "nvidia_technology": "DRIVE, CUDA, TensorRT, OVX, NSight Systems, NVIDIA NIM, Blackwell, Cosmos",
      "session_id": "S81779",
      "session_type": "Talks or Panels",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Marco Pavone",
          "title": "Sr. Research Director"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Physical AI for End-to-End Vehicle Autonomy",
      "topic": "Robot Navigation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81779/"
    },
    {
      "description": "One of the hardest problems in physical AI is transforming robots from task-specific machines to general-purpose, intelligent collaborators. NVIDIA Research is pioneering a unified AI- and simulation-driven approach to solve this challenge, working from theoretical advances all the way to high-performance deployment. Join this session to learn about research breakthroughs in sim-to-real, real-world learning, model architecture, and model training, as well as key milestones on the path to generalist embodied intelligence for real-world applications. Gain perspectives about where robotics is headed, what technical hurdles remain, and how the next wave of research innovation could unlock entirely new markets.",
      "format": "In-Person",
      "industry": "Manufacturing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "The future of robotics is a collaborative ecosystem where general-purpose humanoids and specialized traditional robots will coexist, driven by a shift from modular to end-to-end AI systems.",
        "Scaling these intelligent robots depends on closing the \"sim-to-real\" gap until simulated training data becomes virtually indistinguishable from real-world data.",
        "This advancement in physical AI will move robots from controlled settings into our daily lives, opening entirely new markets and applications."
      ],
      "nvidia_technology": "Jetson, AGX, Isaac, Omniverse, OVX, CUDA-X, Blackwell, Cosmos",
      "session_id": "S81479",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Yashraj Narang",
          "title": "Robotics Research Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Jim Fan",
          "title": "Principal Research Scientist/Sr. Research Manager"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Physical AI for the Real World: A Vision From NVIDIA Robotics Research",
      "topic": "Humanoid Robots",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81479/"
    },
    {
      "description": "Join us to explore how innovative collaboration drives real-world impact. We’ll dive deep into how our joint work with WWT, VERTIV, and NVIDIA is redefining performance, efficiency, and scalability through one of the first NVIDIA DGX GB300 liquid-cooled deployments in Europe, the Middle East, or Africa (EMEA). You’ll gain valuable insights into how this partnership turned complex challenges into breakthrough results, improving AI infrastructure and accelerating outcomes. Through a customer testimonial journey, we’ll highlight practical lessons, measurable benefits, and how you can apply these insights to your own AI and data center strategies.",
      "format": "In-Person",
      "industry": "Public Sector and Sovereign AI",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Explore how collaboration fuels innovation with NVIDIA and VERTIV to deliver next-generation AI infrastructure.",
        "Discover real-world insights from one of the first DGX GB300 liquid-cooled deployments in EMEA.",
        "Learn how to overcome implementation challenges and accelerate performance in demanding AI workloads.",
        "Take away practical strategies for building scalable, sustainable, and high-performance AI environments.",
        "Connect With the Experts",
        "Workshops, Training Labs & Certification",
        "Sponsors & Exhibitors",
        "Privacy Policy Your Privacy Choices Terms of Service Accessibility Corporate Policies Product Security Contact",
        "Copyright © 2026 NVIDIA Corporation",
        "NVIDIA uses cookies to improve your experience on our web site. We and our third-party pa"
      ],
      "nvidia_technology": "BlueField DPU, Cloud / Data Center GPU, DGX Platform, Infiniband Networking, Ethernet Networking, Base Command Manager, Interconnect Networking, NVLink / NVSwitch, Blackwell, NVIDIA AI Enterprise, Mission Control",
      "session_id": "S82130",
      "session_type": "Talk",
      "speakers": [],
      "technical_level": "Business / Executive",
      "title": "Pioneering NVIDIA DGX GB300 Liquid-Cooled Deployment in EMEA (Presented by PNY)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82130/"
    },
    {
      "description": "Join Dr. Wen-Mei Hwu and NVIDIA experts for a live Q&A exploring key updates in the CUDA ecosystem since the last edition of his book Programming Massively Parallel Processors (PMPP). This session offers a rare opportunity to engage directly with Wen-Mei and his team of CUDA engineers, who will answer technical questions, share new features introduced in PMPP Edition 5, and discuss emerging topics under consideration for the sixth edition.",
      "format": "In-Person",
      "industry": "Academia / Higher Education",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Understand how Programming Massively Parallel Processors (PMPP) is used by both academia and industry, and learn what to expect from PMPP Edition 5 as it becomes available around GTC 2026.",
        "Learn about new and updated content in PMPP Edition 5, including coverage of LLMs, advanced matrix multiplication, multi-GPU programming, filtering, wavefront algorithms, compute and memory architectures, and performance optimization, plus an updated code repository and new hands-on labs.",
        "Gain insight into the future of parallel computing and get your technical questions answered directly by NVIDIA architects and engineers."
      ],
      "nvidia_technology": "DGX Platform, CUDA, NSight Comute, NSight Systems, DGX Spark",
      "session_id": "QA81637",
      "session_type": "Q&A With NVIDIA Experts",
      "speakers": [
        {
          "company": "Group Product Manager - CUDA",
          "name": "Anshuman Bhat",
          "title": "Group Product Manager - CUDA"
        },
        {
          "company": "Senior Distinguished Research Scientist and Senior Research Director at NVIDIA",
          "name": "Wen-Mei Hwu",
          "title": "Sr. Distinguished Research Scientist"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "PMPP Edition 5 Unveiled: Ask Dr. Wen Mei and the CUDA Team Your Questions",
      "topic": "Programming Languages / Compilers",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-qa81637/"
    },
    {
      "description": "How do we move beyond simple instruction-following to models that truly reason, plan, and use tools? The answer lies in the rigor of the post-training pipeline. In this session, we unveil the technology behind Nemotron’s reasoning and tool use capabilities. While we will cover the essentials of imitation learning and synthetic data generation, the spotlight will be on the frontier of reinforcement learning (RL). We will demonstrate how multi-environment RL training serves as the catalyst for unlocking multi-step reasoning and robust tool use. By analyzing the interplay between reward modeling, reinforcement learning from human feedback, and environment-aware training, we will show how Nemotron achieves superior performance in complex agentic workflows. Join us to explore the specific techniques NVIDIA uses to align models not just for chat, but for cognitive action.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Research: Academic",
      "key_takeaways": [
        "Talk will cover recent developments in post-training Nemotron models",
        "How to use RL to add reasoning, tool use, and multi-step capabilities to AI models",
        "RL training in multi-environment setting with NeMo-RL & NeMo-Gym"
      ],
      "nvidia_technology": "NeMo",
      "session_id": "S81558",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Oleksii Kuchaiev",
          "title": "Director of Applied Research"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Post-Training Nemotron With RL: Multi‑Environment Training for Reasoning and Tool-Using Agents",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81558/"
    },
    {
      "description": "PayPal is leveraging NVIDIA’s open-source AI models to accelerate the evolution of agentic commerce—a new era where intelligent digital agents autonomously handle commerce tasks across industries. By integrating NVIDIA’s Nemotron and reasoning AI frameworks, PayPal enables businesses to automate customer engagement, personalize financial recommendations, and optimize payment flows in real time.",
      "format": "In-Person",
      "industry": "Financial Services",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Discover best practices for building and fine-tuning open-source AI agents using NVIDIA Nemotron in real-world commerce environments.",
        "Learn how to design scalable, secure, and globally compliant AI-powered commerce solutions.",
        "Identify concrete strategies for boosting developer productivity and accelerating time-to-market by leveraging open, adaptable AI frameworks—transforming both business outcomes and tooling ecosystems."
      ],
      "nvidia_technology": "DGX Platform, NeMo, NVIDIA NIM, NVIDIA AI Enterprise",
      "session_id": "S81734",
      "session_type": "Talks or Panels",
      "speakers": [
        {
          "company": "PayPal",
          "name": "Farad Farahani",
          "title": "Head of Agentic Commerce and Personalization"
        },
        {
          "company": "PayPal",
          "name": "Prakhar Mehrotra",
          "title": "SVP, Global AI"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Powering Agentic Commerce with Open-Source AI: Easy to Build, Train, and Deploy",
      "topic": "Retrieval-Augmented Generation (RAG)",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81734/"
    },
    {
      "description": "Effective use of AI, accelerated computing, and advanced infrastructure is essential to drive research innovation and scientific breakthroughs. Researchers need to focus on sustainability, funding approaches, and strategic collaborations to maximize the impact of their work and fully leverage NVIDIA technologies. This panel will provide successful research use cases of accelerated computing, showcasing how to better advocate and utilize research infrastructure available at academic institutions.",
      "format": "Virtual",
      "industry": "Academia / Higher Education",
      "intended_audience": "Research: Academic",
      "key_takeaways": [
        "Best practices from leading researchers from diverse fields, revealing how AI and accelerated computing can drive innovation and reshape research processes and scientific discovery",
        "How infrastructure optimization can foster the success of researchers and students alike",
        "Various models of funding research, sustainability challenges, and best practices for managing research infrastructure animated by various research use cases",
        "How to maximize impact and outcome of research through partnership with NVIDIA and its developer programs"
      ],
      "nvidia_technology": "Isaac, NeMo, NVIDIA NIM, NVIDIA AI Enterprise, Dynamo",
      "session_id": "S81942",
      "session_type": "Panel",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Tomasz Bednarz",
          "title": "Director of Strategic Researcher Engagement"
        },
        {
          "company": "University College London",
          "name": "Mirco Musolesi",
          "title": "Professor of Computer Science"
        },
        {
          "company": "Warsaw University of Technology",
          "name": "Tomasz Trzciński",
          "title": "Director of R&D and Professor"
        },
        {
          "company": "Mira Lab",
          "name": "Nadia Magnenat Thalmann",
          "title": "Founder"
        },
        {
          "company": "NVIDIA",
          "name": "Judit Szulagyi",
          "title": "Director of Higher Education and Research (EMEA)"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Powering Research: Real Academic Use Cases With NVIDIA Accelerated Computing and AI",
      "topic": "AI Platforms",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81942/"
    },
    {
      "description": "Building a truly performant LLM agent means fine-tuning for your unique use-case. This requires training data that span multiple specialized domains or tools; for most real-world applications, the data you need simply don't exist. This hands-on workshop leverages NVIDIA Data Designer, our recently open-sourced, state-of-the-art framework, to bridge this gap. We'll cover the latest techniques developed at NVIDIA for producing high-quality reasoning data, including STEM and tool-calling workflows used to scale Nemotron models. By the end of this session, you will gain the skills to speed up data experimentation and drive innovation in your own AI projects. Using datasets for supervised fine-tuning and evaluation (classification, generation, or instruction-following tasks) Optional: prior exposure to NVIDIA NeMo, Hugging Face datasets/models, or other LLM fine-tuning stacks.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Master the fundamentals — synthetic data generation, governance, and best practices.",
        "Learn NVIDIA's playbook — apply expert techniques for data design and experimentation used to scale state-of-the-art models like Nemotron.",
        "Build with Open Source — Get hands-on with NVIDIA Data Designer to generate high-quality training datasets from scratch."
      ],
      "nvidia_technology": "NeMo, NVIDIA AI Enterprise",
      "session_id": "DLIT81572",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Kirit Thadaka",
          "title": "Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Yevgeniy Meyer",
          "title": "Principal Research Scientist"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Powering Specialized Agents: Architecting Synthetic Data Pipelines with NVIDIA Data Designer",
      "topic": "Code / Software Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81572/"
    },
    {
      "description": "As AI workloads continue to grow in size, power density, and complexity, data centers must evolve beyond traditional architectures. This session explores how PEGATRON is enabling next‑generation AI factories through automated manufacturing and tightly integrated server platforms. From accelerated rack‑level deployment to end‑to‑end infrastructure integration, PEGATRON delivers a comprehensive solution spanning compute, networking, and storage—engineered to interconnect racks and clusters in modern AI data centers.",
      "format": "In-Person",
      "industry": "Cloud Services",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Accelerating AI factory build-out through automated assembly",
        "Expanding Pegatron’s NVIDIA GPU platform portfolio for high-compute rack demand",
        "Delivering an end-to-end L10–L13 server and infrastructure total solution"
      ],
      "nvidia_technology": "BlueField DPU, Grace CPU, RTX GPU, DGX Platform, HGX, Infiniband Networking, Ethernet Networking, Hopper, MGX, OVX, NVLink / NVSwitch, Blackwell, NVIDIA AI Enterprise",
      "session_id": "EX82041",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "Pegatron",
          "name": "Christopher Liang",
          "title": "VP of Product Department"
        }
      ],
      "technical_level": "General Interest",
      "title": "Powering the AI Factory: From Platform Integration to Scalable AI Data Center Solutions (Presented by Pegatron)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82041/"
    },
    {
      "description": "This session explores how NVIDIA 3 Computers—Train, Simulate, and Deploy—enable enterprises to turn AI models into real-world applications across computer vision, robotics, and edge AI. Learn how this end-to-end architecture accelerates AI deployment, reduces risk, and helps organizations scale AI from experimentation to production.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "See how NVIDIA’s AI software platforms accelerate industry-specific applications without starting from scratch.",
        "Understand how the Train–Simulate–Deploy workflow enables faster and more reliable AI deployment across vision, robotics, and edge environments.",
        "Gain practical insights into building a scalable, production-ready AI architecture that delivers measurable business value."
      ],
      "nvidia_technology": "Jetson, AGX, DGX Platform, CUDA, JetPack, NVIDIA AI Enterprise",
      "session_id": "EX82046",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "EDOM",
          "name": "Wilson Yen",
          "title": "Product Director"
        }
      ],
      "technical_level": "General Interest",
      "title": "Practical Applications of NVIDIA 3 Computers in Vision, Robotics, and Edge (Presented by EDOM Technology)",
      "topic": "Embedded Edge",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82046/"
    },
    {
      "description": "Learn how to use the Nsight Compute tool to profile CUDA and AI GPU kernels in the Python framework of your choice: Numba, Triton or PyTorch. Understand the optimization workflows provided by NVIDIA Nsight Compute UI and its ncu command line interface. Use the new scoreboard dependency analysis view and source page improvements like opcode to metric pipeline associations. Learn how to use the Nsight Jupyterlab Extension for profiling and streaming reports directly in Jupyter. Try the latest Nsight Python decorators for profiling from within any Python script.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "How to profile CUDA and AI GPU kernels in various Python frameworks",
        "How to use the Nsight Compute kernel optimization flow and guidance",
        "How to use the Nsight Jupyterlab Extension and Nsight Python"
      ],
      "nvidia_technology": "",
      "session_id": "DLIT81545",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Felix Schmitt",
          "title": "Principal System Software Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Profiling Python and AI workloads with Nsight Compute",
      "topic": "Profilers / Debuggers / Code Analysis",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81545/"
    },
    {
      "description": "In this session, we explore how agents and digital humans are transforming customer in store experiences by improving the connection with customers, employees and store operations. From answering questions in multiple languages to providing product education and recommendations, and seamless integrating with key systems, PUMA is redefining the way retailers work and the way customers shop.",
      "format": "In-Person",
      "industry": "Retail / Consumer Packaged Goods",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Discover the impact AI agents are having on customer experience in PUMA stores and online",
        "Learn about the incremental value AI agents deliver beyond the intended use case, unlocking new benefits for both store staff and customers",
        "Hear how we'll leverage initial key learnings as we expand rolling out AI agents to more stores"
      ],
      "nvidia_technology": "Blackwell",
      "session_id": "S81726",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "PUMA",
          "name": "Ivan Dashkov",
          "title": "Head of Emerging Marketing Tech"
        },
        {
          "company": "LiveX AI",
          "name": "Jia Li",
          "title": "President and Chief AI Officer"
        },
        {
          "company": "Google",
          "name": "Kapil Dabi",
          "title": "Americas Market Lead, Retail and Consumer"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Puma’s Smart Stores: Agents and Digital Humans Redefine Customer Interaction",
      "topic": "Speech Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81726/"
    },
    {
      "description": "NVIDIA CUDA Direct Sparse Solver (cuDSS) is increasingly being adopted in large-scale simulations for engineering and design, such as those pertinent to electrical engineering automation (EDA) and computer-aided engineering (CAE). In this session, we will cover recent advances, including most recent performance number, highlights of key features, and new APIs in cuDSS that enable users across various workflows to leverage GPU acceleration for sparse solvers.",
      "format": "In-Person",
      "industry": "Hardware / Semiconductor",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Discover how, with new optimizations for large-scale problems, cuDSS can now solve models with hundreds of millions of degrees of freedom and over a billion non-zeros on a single NVIDIA GPU.",
        "Learn about advanced features such as Hybrid Memory Mode and multi-GPU support, open the door to solving problems at unprecedented scale in a fraction of the time compared to traditional approaches."
      ],
      "nvidia_technology": "CUDA-X",
      "session_id": "S81824",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Azi Riahi",
          "title": "Principal Product Manager"
        }
      ],
      "technical_level": "General Interest",
      "title": "Push the Boundaries of CAE and EDA With NVIDIA cuDSS",
      "topic": "Accelerated Computing Libraries",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81824/"
    },
    {
      "description": "Reduced-precision formats like NVFP4 are shaping the future of AI by enabling faster training and inference without compromising model accuracy. In this talk, we share best practices, insights, and lessons learned from training and deploying convolutional neural network (CNN) and transformer models using advanced reduced-precision techniques while maintaining full accuracy relative to high-precision baselines. We'll explore how reduced precision, combined with optimized CUDA kernels, delivers significant speedups for both training and inference—pushing the performance frontier of computer vision workloads on modern GPUs. This methodology can be broadly applied across industrial use cases requiring highly accurate, low-latency computer vision models, including manufacturing, medical imaging, autonomous vehicles, robotics, and more.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn the design and advantages of NVIDIA’s advanced NVFP4 format",
        "Understand techniques to maintain full model accuracy leveraging low-precision computation.",
        "Learn how advanced reduced-precision formats accelerate training and inference for CNNs and vision transformers.",
        "Learn the methodology for adopting highly accurate, low-latency reduced-precision models for your use cases."
      ],
      "nvidia_technology": "RTX GPU, TensorRT, CUDA-X, NSight Comute, NSight Systems",
      "session_id": "S81547",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Leo Du",
          "title": "Sr. Solutions Architect, Generative AI"
        },
        {
          "company": "KLA",
          "name": "Pradeep Ramachandran",
          "title": "Director"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Push the Performance Frontier of Computer Vision Models With NVFP4 Reduced Precision",
      "topic": "Computational Imaging",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81547/"
    },
    {
      "description": "How would you write a Python library that uses CUDA today? Most Python GPU libraries—PyTorch, CuPy, RAPIDS, and others—still build on CUDA C++ because many core CUDA building blocks have only ever existed in C++. That’s now changing. New Python-native CUDA libraries bring formerly C++-only capabilities directly into Python, enabling high-performance GPU functionality without bindings or C++ backends. This talk shows how to build modern GPU libraries with these tools, and what becomes possible when it’s Python all the way down.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "The new generation of CUDA Python tooling, such as CCCL Python, cuTile, and nvmath-python, let you do much more in Python without needing to “drop down” to C++.",
        "These tools deliver the performance of hand-tuned CUDA kernels while enabling you to write high-level, maintainable libraries and applications.",
        "Learn what modern GPU development in Python can look like!"
      ],
      "nvidia_technology": "CUDA",
      "session_id": "S81531",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Ashwin Srinath",
          "title": "Senior Software Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Python All the Way Down: Speed-of-Light CUDA Without Leaving Python",
      "topic": "Accelerated Computing Libraries",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81531/"
    },
    {
      "description": "In this talk, we highlight how emerging quantum computing technologies are poised to impact the National Energy Research Scientific Computing Center’s scientific workload. We show how NVIDIA’s hardware and software stack already enable next-generation quantum device and application simulations, and explain how NERSC’s future quantum roadmap will build on these capabilities.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Quantum computing has the potential to influence a substantial portion of NERSC’s supercomputing workload.",
        "NVIDIA hardware and software are allowing next-generation simulations for quantum hardware and applications development.",
        "NERSC's future roadmaps for quantum computing research will continue to draw on accelerated computing."
      ],
      "nvidia_technology": "cuQuantum, CUDA Quantum, CUDA-Q",
      "session_id": "S81696",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Lawrence Berkeley National Laboratory",
          "name": "Katie Klymko",
          "title": "NERSC Quantum Computing Advisor"
        }
      ],
      "technical_level": "General Interest",
      "title": "Quantum Computing at NERSC and Lawrence Berkeley National Laboratory",
      "topic": "Quantum Computing",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81696/"
    },
    {
      "description": "Get a hands-on introduction to NVIDIA CUDA-Q, the platform for seamlessly orchestrating both quantum and classical processors alongside each other in accelerated quantum supercomputers. Whether you are new to the field or an experienced developer, this session walks you through building a hybrid application to demonstrate the execution of quantum-classical workflows across NVIDIA GPUs and IQM’s quantum hardware.",
      "format": "In-Person",
      "industry": "Academia / Higher Education",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Understand how accelerated quantum supercomputing integrates quantum processors (QPUs) as dedicated accelerators, alongside CPUs and GPUs.",
        "Learn the CUDA-Q programming model to write hybrid code that seamlessly blends classical logic with quantum kernels in a single source.",
        "Build a quantum walk algorithm from scratch to practically explore quantum computing concepts such as superposition and entanglement.",
        "See how easily you can switch targets to NVIDIA GPUs for accelerated simulation or to IQM’s quantum processors for execution."
      ],
      "nvidia_technology": "CUDA Quantum, CUDA-Q",
      "session_id": "S81553",
      "session_type": "Tutorial",
      "speakers": [
        {
          "company": "IQM",
          "name": "Danny Bulmash",
          "title": "Education Lead"
        },
        {
          "company": "NVIDIA",
          "name": "Monica Van Dieren",
          "title": "Sr. Technical Marketing Engineer, Quantum and HPC"
        },
        {
          "company": "NVIDIA",
          "name": "Mark Wolf",
          "title": "Technical Marketing Engineer"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Quick Start to Accelerated Quantum Supercomputing",
      "topic": "Quantum Computing",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81553/"
    },
    {
      "description": "This session introduces the development of the Qwen model family, including LLM, coder, vision language, Omni, vision generation models, etc. It introduces the knowledge of scaling happening in these models and points out the future direction for the next generation of models and agents.",
      "format": "In-Person",
      "industry": "Academia / Higher Education",
      "intended_audience": "Research: Non-Academic",
      "key_takeaways": [
        "Learn how scaling happens in pre-training.",
        "Understand the design of a multi-modal model and system.",
        "Learn about the next-generation model architecture, training methods, etc."
      ],
      "nvidia_technology": "CUDA, TensorRT, Isaac, cuBLAS, cuDDN, NCCL, Triton",
      "session_id": "S81688",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Alibaba",
          "name": "Junyang Lin",
          "title": "Tech Lead of Qwen Team"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Qwen: Toward Generalist Models",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81688/"
    },
    {
      "description": "This session presents a technical blueprint for integrating NVIDIA Omniverse, Isaac Sim, and cuOpt into Mercedes-Benz’s MO360 digital production ecosystem to enable real-time, optimization-driven factory logistics. A high-fidelity factory digital twin—built in Omniverse using USD, CAD/BIM data, and physics-based simulation—provides a unified environment for validating material-flow strategies and layout configurations. Within this virtual factory, Isaac Sim models robots with accurate kinematics, while physics and AI-driven simulation modules help to cater various manufacturing planning scenarios and faster decision-making. Leveraging this digital infrastructure, NVIDIA cuOpt performs GPU-accelerated multi-constraint route optimization. Together, these technologies illustrate a roadmap for simulation-native manufacturing in MO360.",
      "format": "In-Person",
      "industry": "Automotive / Transportation",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "Digital Twin Hub of Factories Using Omniverse and Issac Sim",
        "AI-Driven Simulation and Robotics",
        "Route Optimization Using CuOpt"
      ],
      "nvidia_technology": "cuOPT, Omniverse Replicator",
      "session_id": "S81850",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Mercedes-Benz Research and Development India",
          "name": "Aritra Ghosh",
          "title": "VP, Manufacturing Engineering (RD/IME), and IT for MO (ITH/IM)"
        },
        {
          "company": "Mercedes-Benz Research and Development India",
          "name": "Jagdish Mishra",
          "title": "Sr. Manager"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Real-Time Planning With Digital Twins, Robotics, and Integrated AI",
      "topic": "Industrial Digitalization / Digital Twin",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81850/"
    },
    {
      "description": "Physical engineering is moving from static reports and siloed workflows to real‑time, interactive insight. This session shows how AI physics meets simulation‑driven engineering by putting NVIDIA’s GPU‑accelerated libraries and NVIDIA Omniverse open libraries first. We’ll cover accessing common computer-aided engineering (CAE) formats via OpenUSD technologies in kit-cae; high‑fidelity field visualization with RTX and IndeX; Warp‑accelerated CAE/visualization algorithms; physics NeMo surrogates made from solver-generated ground truth; and application streaming for instant, hardware‑agnostic review and collaboration. We’ll highlight current industry use cases and advancements in the area of digital twins for science and engineering. You’ll leave today with an adoption path and a clear view of how AI physics and Omniverse libraries can accelerate your organization.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Omniverse Kit-CAE enables integrating CAE data into Omniverse digital twins",
        "PhysicsNeMo and NVIDIA Apollo enable building AI physics surrogate models",
        "Digital twins accelerate the process of scientific and engineering discovery"
      ],
      "nvidia_technology": "RTX GPU, CUDA, Omniverse, Modulus, CUDA-X, DLSS, Triton, NVIDIA NIM, Blackwell",
      "session_id": "S81781",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Jeff Larkin",
          "title": "HPC Architect"
        }
      ],
      "technical_level": "General Interest",
      "title": "Real-Time Science and Engineering With AI Physics and Kit-CAE",
      "topic": "Industrial Digitalization / Digital Twin",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81781/"
    },
    {
      "description": "This hands-on lab introduces mechanical and simulation engineers to GPU-accelerated, real-time CFD workflows that combine interactive visualization with physics-AI models. Participants will use NVIDIA technologies to connect CFD simulations, PhysicsNeMo, and digital twin environments for faster iteration and more reliable design insights. We will configure and run real-time CFD cases, integrate PhysicsNeMo for accelerated prediction, and visualize results interactively for design exploration. The session will show how to move from traditional batch-style CFD to responsive, GPU-driven workflows that support rapid concept evaluation and collaborative design reviews. Familiarity with CAE workflows and core computational fluid dynamics concepts. Experience with engineering simulation principles, including mesh, boundary conditions, and solver workflows. Understanding of 3D scene composition, transforms, and visualization in simulation or 3D environments. Basic knowledge of physics-based simulation and design optimization processes.",
      "format": "In-Person",
      "industry": "Manufacturing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how to integrate real time CFD workflows for fast design iteration and interactive reviews."
      ],
      "nvidia_technology": "Omniverse, NeMo",
      "session_id": "DLIT81801",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Abigail Breazeale",
          "title": "Sr. Technical Marketing Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Robert Cervellione",
          "title": "Sr. Product Manager – Omniverse"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Real-Time Simulation for Real-Time Results: How AI Physics Can Accelerate Your AI Factory",
      "topic": "CAE",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81801/"
    },
    {
      "description": "Modulate’s VoiceWear enables real-time voice transformation, demanding high-fidelity audio and near-zero latency. This session details how Modulate and NVIDIA migrated critical VoiceWear workloads from CPU to GPU, significantly reducing latency within their streaming pipeline. We’ll explore the process of identifying bottlenecks, the impact of precision and framework selection on performance, and how targeted CUDA and inference-stack optimizations delivered measurable gains. Attendees will gain practical strategies for accelerating sequential, real-time ML workloads for production.",
      "format": "In-Person",
      "industry": "Gaming",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "How GPU acceleration shifts data prep, training, and inference workflows from hours to minutes",
        "How to identify high-impact CPU workloads with strong ROI for GPU acceleration",
        "Practical guidance for measuring and operationalizing cost and latency improvements"
      ],
      "nvidia_technology": "RTX GPU, HGX, CUDA, TensorRT",
      "session_id": "S81744",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "James Maki",
          "title": "Sr. Solutions Architect - Gen AI"
        },
        {
          "company": "Modulate",
          "name": "Carter Huffman",
          "title": "CTO and co-founder"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Real-time Voice Transformation with GPU Accelerated AI",
      "topic": "Performance Optimization",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81744/"
    },
    {
      "description": "Federated learning is transforming data-centric industries such as healthcare, life sciences, finance, and transportation — domains where privacy, compliance, and data governance are essential. NVIDIA FLARE enables collaborative model training across institutions without moving sensitive data, unlocking insights from distributed datasets that would otherwise remain siloed. Today, organizations worldwide are deploying federated learning in production, using NVIDIA FLARE to build more accurate diagnostic models, accelerate drug discovery, improve financial risk modeling, and optimize transportation systems. With Confidential Computing on NVIDIA GPUs, FLARE supports secure, decentralized pipelines that maintain data protection and execution integrity across every site, even when running complex multi-party workflows. Meet the experts behind NVIDIA FLARE and join the discussion. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "Healthcare & Life Sciences",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "Flexible Collaboration: NVIDIA FLARE enables multi-institutional AI development without sharing sensitive data, supported by simple, flexible, and production-ready APIs.",
        "Accelerated AI Innovation: Secure access to distributed data leads to more accurate models for diagnosis, drug discovery, financial analysis, and personalized treatment development.",
        "End-to-End Security and System Integrity: FLARE ensures that decentralized AI workflows remain secure and trustworthy from start to finish — protecting data in transit, enforcing robust validation and auditing mechanisms, and maintaining consistent system integrity across all participating sites."
      ],
      "nvidia_technology": "CUDA",
      "session_id": "CWES81554",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Holger Roth",
          "title": "Principal Federated Learning Scientist"
        },
        {
          "company": "NVIDIA",
          "name": "Yuan-Ting Hsieh",
          "title": "Senior Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Chester Chen",
          "title": "Sr. Product and Engineering Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Isaac Yang",
          "title": "Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Jiahui Guan",
          "title": "Senior Solution Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Zhijin Li",
          "title": "Sr. Solutions Architect"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Real-World Federated Learning With NVIDIA FLARE",
      "topic": "Data Analytics / Processing",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81554/"
    },
    {
      "description": "Quantum computers are fundamentally different than conventional computers. They promise to address certain problems that are practically prohibitive, and even impossible, to solve using today’s supercomputers. The challenge is building one that is large enough to be useful. In this talk, we will provide an overview of contemporary quantum computing at an intuitive level, including the technology, the promise, the hype, the importance of high-performance classical computing in this endeavor, and the challenges ahead associated with realizing useful quantum computers at scale.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "A quantum computer at scale will outperform conventional computers for specific tasks (not all tasks).",
        "High-performance classical computing is integral to realizing a large-scale, performant quantum computer.",
        "Quantum error correction will be required to realize the full promise of quantum computation.",
        "Partnerships such as that between MIT and NVIDIA are allowing state-of-the-art technologies to advance quantum computing research."
      ],
      "nvidia_technology": "Grace CPU, DGX Platform, Hopper, CUDA Quantum, CUDA-Q",
      "session_id": "S82295",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Massachusetts Institute of Technology",
          "name": "William Oliver",
          "title": "Henry Ellis Warren (1894) Professor"
        }
      ],
      "technical_level": "General Interest",
      "title": "Realizing the Promise of Quantum Computation",
      "topic": "Quantum Computing",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82295/"
    },
    {
      "description": "Delivering a mainstream enterprise AI solution requires an ecosystem that not only works, but is engineered as a cohesive set of components that fit seamlessly to drive success. To truly provide a seamless enterprise solution, you start in the open, where the components can be visualized, vetted, and cleanly integrated. Red Hat and NVIDIA have taken those steps to begin Day 0 support for Red Hat Enterprise Linux and Red Hat OpenShift on NVIDIA systems, starting with Vera Rubin. In this talk, we will look at the work being done behind the scenes to deliver what is a fundamental change in how open moves faster to deliver an enterprise experience.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "The Power of \"Day 0\" Ecosystem Integration",
        "\"Starting in the Open\" for Seamless Security and Reliability",
        "A Fundamental Shift in Speed-to-Market",
        "Red Hat and NVIDIA, Better Together"
      ],
      "nvidia_technology": "",
      "session_id": "EX82275",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "Red Hat",
          "name": "Scott Herold",
          "title": "Director, Product Management - Red Hat Enterprise Linux"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Red Hat & NVIDIA: Driving Mainstream Enterprise AI Solutions to Unlock Wide-Scale Adoption of AI (Presented by Red Hat)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82275/"
    },
    {
      "description": "SandStar, Lawson, and True Corp. will explain how to use both NVIDIA edge AI and cloud AI solutions to change thousands of retail stores' daily operations, and attain positive return on investment (ROI) within three to six months.",
      "format": "In-Person",
      "industry": "Retail / Consumer Packaged Goods",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Learn how the NVIDIA tech stack has been leveraged to accelerate end-to-end AI transformation and scale reliably across thousands of retail locations.",
        "Learn proven playbooks that deliver positive ROI within 3–6 months through operational automation and generative insight.",
        "Hear lessons from Lawson’s and True Corp. on long-term NVIDIA AI deployments that boost efficiency and deepen customer engagement."
      ],
      "nvidia_technology": "Jetson, HGX, DeepStream, NVIDIA AI Enterprise, Cosmos",
      "session_id": "S81675",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "SandStar",
          "name": "Yili Wu",
          "title": "CEO"
        },
        {
          "company": "Lawson China",
          "name": "Takashi Tokita",
          "title": "Deputy General Manager"
        },
        {
          "company": "True Corporation Public Co., Ltd.",
          "name": "Ekaraj Panjavinin",
          "title": "Chief Digital Officer"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Reimagining Store Operations at Scale to Accelerate Retail ROI",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81675/"
    },
    {
      "description": "Reinforcement learning (RL) is entering a new phase—one defined as much by systems engineering as by algorithmic innovation. In this panel, we convene industry leading experts to discuss current challenges in scaling RL along with emerging RL paradigms, which together can unlock scientific discovery, multi-modal reasoning, collaborative agents, and continual learning—the vanguard of more adaptive and intelligent systems.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "RL is a critical component to unlock advanced reasoning and intelligence.",
        "Further advances in RL will require systems engineering to efficiently orchestrate complex and dynamic workflows at scale.",
        "The next generation of intelligence will require scalable systems that can learn, adapt, and reason in the wild."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, Hopper, Blackwell",
      "session_id": "S82182",
      "session_type": "Panel",
      "speakers": [
        {
          "company": "Humans&",
          "name": "Yuchen He",
          "title": "Founding Engineer"
        },
        {
          "company": "Applied Compute",
          "name": "Linden Li",
          "title": "Co-Founder and Chief Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Vartika Singh",
          "title": "Strategic AI Lead"
        },
        {
          "company": "Periodic Labs",
          "name": "William Fedus",
          "title": "Co-Founder"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Reinforcement Learning at Scale: Engineering the Next Generation of Intelligence",
      "topic": "Reinforcement Learning",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82182/"
    },
    {
      "description": "Explore new solutions for navigating the expanding landscape of reinforcement learning (RL) — know when to use RL, which flavor fits your domain, and what tooling accelerates your path to production. Get practical guidance on stacks, environments, and implementation strategies from hands-on practitioners. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Understand when reinforcement learning adds value over supervised fine-tuning or prompt engineering, and when simpler approaches are the better choice.",
        "Learn how to select the right RL flavor for your use case, including RLHF for preference alignment, RLVR for verifiable reasoning tasks, and other emerging techniques.",
        "Discover which tooling stacks and simulation environments map to specific domains.",
        "Get a practical roadmap for integrating RL into your workflows, from initial experimentation through production deployment."
      ],
      "nvidia_technology": "NeMo",
      "session_id": "CWES81912",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Chris Alexiuk",
          "title": "Deep Learning Developer Advocate"
        },
        {
          "company": "NVIDIA",
          "name": "Shashank Verma",
          "title": "Sr. Technical Marketing Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Christopher Wing",
          "title": "Senior Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Wenwen Gao",
          "title": "Senior Product Manager"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Reinforcement Learning in the Modern AI Stack",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81912/"
    },
    {
      "description": "Clinical decision support is evolving from static, rule-based systems to adaptive, agentic AI that can reason, learn, and collaborate with clinicians. This talk explores how OpenEvidence is building an agentic system that combines real-world clinical data and clinical literature to deliver context-aware insights at the point of care. It'll also discuss the critical role of a continuously learning data flywheel—where every interaction enhances reasoning, accuracy, and safety—and how this dynamic feedback loop can redefine trust and performance in medical AI systems.",
      "format": "In-Person",
      "industry": "Hardware / Semiconductor",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Clinical decision support is being transformed by agentic AI.",
        "Importance of a data flywheel in continuously learning from user feedback",
        "Open models ecosystem is pivotal for building these new digital health platforms."
      ],
      "nvidia_technology": "NeMo",
      "session_id": "S81813",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "OpenEvidence",
          "name": "Daniel Nadler",
          "title": "CEO"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Reinventing Clinical Decision Support With Agentic AI and a Data Flywheel",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81813/"
    },
    {
      "description": "This session unpacks how Koerber is building a humanoid “robot brain” for parcel logistics using NVIDIA Omniverse, Isaac Sim, and Jetson across simulation and real-world deployments. We’ll walk through how Unitree class humanoid hardware is trained in Omniverse scenes with integrated physics and flow simulation, how Gr00t and Cosmos are evaluated as foundation models for control, and how the same policies are deployed on Jetson-powered lines for adaptive, vision-driven and safety-aware parcel handling.",
      "format": "Virtual",
      "industry": "Retail / Consumer Packaged Goods",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Learn how to design a simulation-first data and training pipeline for humanoid parcel handling, from Omniverse/Isaac Sim scenes to third-party robot hardware.",
        "Hear how Koerber integrates Jetson for adaptive control, vision AI, and safety functions directly on parcel handling lines.",
        "See how physics and flow simulation in Omniverse inform real-world behaviors, enabling scalable deployment of humanoids into high-throughput logistics operations."
      ],
      "nvidia_technology": "Omniverse",
      "session_id": "S81982",
      "session_type": "Talk",
      "speakers": [],
      "technical_level": "General Interest",
      "title": "Revolutionize Automated Parcel Handling With Humanoid Robots",
      "topic": "Humanoid Robots",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81982/"
    },
    {
      "description": "Discover how Flex, a global manufacturing leader, is transforming warehouse and shipping operations in collaboration with EY using NVIDIA’s advanced AI-driven optimization technologies. This session will explore how Flex rebooted their operational designs toward achieving measurable improvements in efficiency, cost, cashflow, and safety through an innovative solution powered by NVIDIA CuOpt. Get insights into the strategic objectives and scalable deployment model, and how this approach will drive rapid innovation across global logistics operations.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "How to implement Global Standard Process to optimize Warehouse and Shipping",
        "How to drive timely, accurate, and efficient execution across a global footprint",
        "How to enable scalable impact on revenue, cost, cashflow, and safety"
      ],
      "nvidia_technology": "cuOPT",
      "session_id": "S82288",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "EY",
          "name": "Sundip Naik",
          "title": "EY Americas, Logistics Lead"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Revolutionize Warehouse and Shipping Operations Through AI Optimization (Presented by EY)",
      "topic": "Logistics / Route Optimization",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82288/"
    },
    {
      "description": "Enterprise robotics is moving fast, driven by advancements in AI and accelerated by NVIDIA’s ecosystem. But what is actually working in production today, and what is still stuck in prototypes and labs? In this session, HCLTech will take a practical look at real-world deployments of AI-powered robotics deployments across industries. We will discuss how we transformed operations for one of the largest mining companies with simulations built on Omniverse, robots trained with Cosmos and Isaac Sim, and video feeds processed with HCLTech IPs, alongside NVIDIA's Isaac robotics platform. We will share our lessons learned, perspective on what is working, what is promising but not quite ready, and where we are seeing the most momentum going into 2026.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "A field-level view of how enterprises are using NVIDIA’s robotics stack, including Jetson and Isaac",
        "What has made it into production versus what remains experimental",
        "Lessons learned from deployments in logistics, utilities, and manufacturing",
        "Where digital twins and simulation are helping—and where they are not",
        "Guidance for enterprises planning to scale robotics programs over the next year"
      ],
      "nvidia_technology": "Jetson, AGX, DGX Platform, EGX, DeepStream, TensorRT, LaunchPad, Clara Holoscan, cuLitho, CV-CUDA, IGX, JetPack, Omniverse Replicator, TAO Toolkit, NVIDIA NIM, NVIDIA AI Enterprise, cuVS, Cosmos, DGX Cloud, DGX Spark, DGX Station, Nemotron",
      "session_id": "EX82058",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "HCLTech",
          "name": "Tamas Foldi",
          "title": "SVP and Head of Robotics"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Robotics in the Wild: What’s Real, What’s Not, and What’s Coming Next (Presented by HCLTech)",
      "topic": "Robotics Simulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82058/"
    },
    {
      "description": "Large language model training has scaled to tens of thousands of GPUs, and keeps expanding. As cluster sizes continually scale, training runs can incur failures, interrupting the run and requiring mitigation. Examples may include NaN values, CUDA errors, and job hangs that challenge stability. High-efficiency LLM training infrastructure requires minimal interruption, fast diagnosis, and strong fault tolerance—we present ByteRobust, a GPU infrastructure management system tailored for robust LLM training. Leveraging LLM training’s parallelism and characteristics, it achieves large-capacity fault tolerance, prompt fault demarcation, and localization via a data-driven approach to ensure continuous efficient training. Deployed on a production GPU platform, ByteRobust sets a new level in training robustness, with 97% effective training time ratio over a three-month training job on 9,600 GPUs.",
      "format": "In-Person",
      "industry": "Cloud Services",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Core Challenge: Large-scale LLM training (tens of thousands of GPUs, multi-month cycles) can be impacted by failures (hardware/software/implicit failures) and risks from continuous code iterations. Traditional handling methods result in significant unproductive time, limiting effective training time ratio (ETTR).",
        "Core Design: Guided by the principles of \"rapid isolation, compatibility with human errors, and controlled recovery,\" ByteRobust achieves fast failure handling through an automated fault tolerance framework, data-driven over-eviction, and efficient recovery mechanisms (hot updates/standby machines/checkpoint optimization).",
        "Key Outcome: The 9,600-GPU-scale training achieves a 97% ETTR, with recovery speed 5-11x faster than traditional solutions and near-zero checkpoint overhead, adapting to the complex needs of ultra-large-scale LLM pre-training."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, Infiniband Networking, Hopper",
      "session_id": "S81421",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "ByteDance Seed",
          "name": "Shuguang Wang",
          "title": "Engineer"
        },
        {
          "company": "ByteDance Seed",
          "name": "Zuquan Song",
          "title": "Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Robust LLM Training Infrastructure at ByteDance",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81421/"
    },
    {
      "description": "Learn how leading broadcasters are using NVIDIA Holoscan for Media to deliver simultaneous multilingual versions of live content with precise lip-sync, localized graphics, and broadcast-grade latency—without writing code. We will explore how complete-frame localization pipelines can be operationalized across GPU-accelerated environments to expand audience reach, reduce human workload, and protect creative integrity at scale.",
      "format": "In-Person",
      "industry": "Media & Entertainment",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Understand how zero-code localization pipelines leverage GPU clusters and model inference inside Holoscan for Media to support real-time multilingual delivery.",
        "Analyze technical approaches for complete-frame versioning, including automated language translation, neural lip-sync alignment, and dynamic virtual graphics.",
        "Integrate Holoscan workflows into existing master control room environments while preserving industry standards (SDI/NDI, SMPTE) and acceptable frame-level latency.",
        "Evaluate how multi-domain localization reduces manual workload, unlocks new market segments, and supports sustainable monetization strategies across live media."
      ],
      "nvidia_technology": "OVX",
      "session_id": "S81557",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Monks",
          "name": "Anthony Walasik",
          "title": "Lead Product Manager"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Scalable AI Localization for Live Media Using NVIDIA Holoscan for Media",
      "topic": "Video Analytics",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81557/"
    },
    {
      "description": "As modern data centers evolve into AI factories, training foundational models becomes critical. But scaling these training pipelines across tens of thousands of GPUs introduces significant challenges—such as node failures, network errors, checkpointing overhead—that can quickly turn occasional anomalies into frequent production issues. In our hands-on lab, learn about the challenges of running training pipelines and how to integrate resiliency and fault-tolerance features. We'll present various fault-tolerance mechanisms provided in frameworks like NVRx and PyTorch, and demonstrate how to achieve robust, scalable training in modern AI factories using NVIDIA NeMo. Gain practical insights into techniques for measuring I/O and overall infrastructure performance. You should be familiar with Python and deep learning frameworks, understand LLM training concepts, and be comfortable with Linux.",
      "format": "In-Person",
      "industry": "Public Sector and Sovereign AI",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Understand fault-tolerance and resiliency mechanisms available in large-scale training frameworks and libraries (Megatron-Bridge, Megatron-LM, NVRx, PyTorch).",
        "Learn how to use and quantify the benefits of advanced checkpointing strategies, such as asynchronous, local, and hierarchical checkpointing.",
        "Understand how to measure end-to-end training performance in AI factories, including checkpointing.",
        "Implement fault-tolerant techniques to detect and handle failures, such as hung ranks, stragglers, and job restart mechanisms."
      ],
      "nvidia_technology": "CUDA, NCCL, NeMo",
      "session_id": "DLIT82259",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Mohak Chadha",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Pramod Kumbhar",
          "title": "Sr. Solutions Architect - HPC & AI"
        },
        {
          "company": "NVIDIA",
          "name": "Shreya Gupta",
          "title": "Solutions Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Scalable and Resilient Training With Megatron-Bridge and NeMo",
      "topic": "Training AI Models",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit82259/"
    },
    {
      "description": "Algorithmic trading is entering a new era defined by AI, massive data volumes, and unprecedented compute scale. Many leading firms now operate more like advanced AI labs than traditional trading desks, relying on large fleets of NVIDIA GPUs and consuming petabytes of real-time and historical market data. The main barrier is no longer the models, but accessing and processing the data fast enough to power them. This session traces how the industry has evolved over the past 20 years, where firms are finding real success with AI/ML, and what frontiers are emerging as compute and data converge. We’ll show how new approaches to data delivery, storage, and accelerated networking, together with solutions from Databento and NVIDIA, are opening the door to the next wave of innovation in finance.",
      "format": "In-Person",
      "industry": "Financial Services",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Leading firms now compete on AI/ML capabilities and research speed, rather than simple models or latency.",
        "GPU-accelerated deep learning and nonlinear methods are driving measurable gains in signal generation and algorithmic execution.",
        "Firms are expanding beyond price prediction into textual data and LLM-powered front-office tools.",
        "Databento enables teams to access and analyze petabytes of data with ease, helping firms accelerate research and scale AI efforts more effectively.",
        "NVIDIA hardware is enabling more cost-effective storage and networking for modern financial AI workflows."
      ],
      "nvidia_technology": "BlueField DPU, Cloud / Data Center GPU, CUDA, Ethernet Networking, Cumulus",
      "session_id": "S81543",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Databento",
          "name": "Christina Qi",
          "title": "CEO"
        }
      ],
      "technical_level": "General Interest",
      "title": "Scalable Data Access for Next-Gen Algorithmic Trading",
      "topic": "Time Series Forecasting",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81543/"
    },
    {
      "description": "Learn how to move data-intensive applications from proof of concept (PoC) to production scale using a reference architecture that pairs high-capacity JBOD storage with NVIDIA BlueField 4 DPUs to accelerate data access, reduce CPU overhead, and improve end-to-end efficiency. This session presents a practical blueprint for solution architects designing cost-effective, high-throughput storage systems to support modern, data-driven and AI-enabled applications at scale.",
      "format": "In-Person",
      "industry": "Hardware / Semiconductor",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Apply a scalable storage blueprint that combines dense JBOD architectures with NVIDIA BlueField to overcome PoC-to-production bottlenecks.",
        "Understand how offloading data movement and security functions to BlueField 4 DPUs improves throughput and system efficiency.",
        "Design cost-optimized, high-capacity data pipelines that meet performance requirements for real-world AI workloads."
      ],
      "nvidia_technology": "BlueField DPU",
      "session_id": "EX82205",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "Seagate Technology",
          "name": "Mohamad El-Batal",
          "title": "Chief Technologist, Seagate CSG; and Technologist, CTO Office"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Scale AI Data Pipelines With High-Capacity JBODs and NVIDIA BlueField-4 DPUs (Presented by Seagate Technology)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82205/"
    },
    {
      "description": "As AI models evolve from billions to trillions of parameters, the infrastructure requirements have shifted from simple compute to complex, vertically integrated systems. In this session, Google Cloud product leaders will unveil the latest advancements in the AI hypercomputer architecture, designed for the NVIDIA Blackwell era. We will dive deep into the performance benchmarks and architectural breakthroughs of our newest GPU instances, including A4X VMs (powered by NVIDIA GB200 NVL72 GPU), A4X Max (powered by NVIDIA GB300 NVL72) and G4 VMs (powered by NVIDIA RTX PRO 6000 Blackwell Server edition). Learn how Google Cloud leverages its third-generation liquid cooling, Jupiter network fabric, storage, and Titanium ML adapters to eliminate bottlenecks, reduce costs, improve GPU utilization, and provide the most scalable environment for the next wave of generative and physical AI.",
      "format": "In-Person",
      "industry": "Cloud Services",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Large-Scale Implementation: How Google’s custom liquid-cooling and power infrastructure optimizes thermal management for high-density GPU deployments",
        "Networking and Offload: Utilizing Google's Jupiter fabric and Titanium adapters to offload host networking and maximize effective GPU bandwidth",
        "System Integration: Strategies for leveraging the full AI hypercomputer stack to reduce latency and improve training performance at scale"
      ],
      "nvidia_technology": "RTX GPU, Blackwell",
      "session_id": "S82243",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Google Cloud",
          "name": "Ruslan Mursalzade",
          "title": "Product Marketing"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Scale Foundation Models With Google Cloud AI Hypercomputer (Presented by Google Cloud)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82243/"
    },
    {
      "description": "Engage directly with the Megatron Core MoE development team to explore mixture of experts (MoE) training at scale. This interactive session offers a unique opportunity to discuss the latest advancements in MoE training framework, roadmap, and best practices with the engineers who build it. We'll cover how to train state-of-the-art models like DeepSeek-V3 and Qwen, and share strategies for achieving optimal performance on the latest Grace Blackwell hardware. We'll dive into key features such as leveraging FP8 for accelerated training, implementing efficient expert parallelism with DeepEP/HybridEP, maximizing communication efficiency with 1F1B EP Overlap, and minimizing memory footprint. Whether you're building the next foundation model or optimizing an existing training pipeline, this session delivers actionable insights to maximize throughput and harness the full power of Megatron Core. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "Consumer Internet",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Gain insights into Megatron Core MoE roadmap and best practices directly from the development team.",
        "Learn best practices for MoE training, including model parallelism tuning strategies, memory footprint minimization through recomputation and offloading, and techniques to handle expert load imbalance effectively.",
        "Learn how to leverage FP8 precision to accelerate MoE training while maintaining model quality on Grace Blackwell hardware."
      ],
      "nvidia_technology": "NeMo",
      "session_id": "CWES81540",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Jianbin Chang",
          "title": "DevTech Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Yigong Qin",
          "title": "Developer Technology Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Hongxiao Bai",
          "title": "Sr. DevTech Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Hongbin Liu",
          "title": "Developer Technology"
        },
        {
          "company": "NVIDIA",
          "name": "Zijie Yan",
          "title": "Sr. DevTech Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Yuzhong Wang",
          "title": "DevTech Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Dennis Liu",
          "title": "Sr. DevTech Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Kunlun Li",
          "title": "Devtech工程师"
        },
        {
          "company": "NVIDIA",
          "name": "Xin Yao",
          "title": "Sr. DevTech Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Zhongbo Zhu",
          "title": "DevTech Engineer, AI"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Scale MoE Training With Megatron Core: Features, Optimizations, and Best Practices",
      "topic": "Deep Learning Frameworks",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81540/"
    },
    {
      "description": "Learn about Coupang’s journey in adapting vision language models (VLMs) for ecommerce at scale, processing billions of records and millions of updates daily. We detail a pipeline combining continual pre-training (CPT), supervised fine-tuning (SFT), and reinforcement learning (RL) to unify multiple VLMs into a localized foundational model powering over 20 apps such as multi-modal retrieval, matching, and review summarization. The system outperforms closed-source models in cost and accuracy. We will show how scalable agentic architecture on NVIDIA’s accelerated ecosystem delivers 10x throughput and 99.9% availability, and share strategies for content integrity and adversarial filtering in real-world environments.",
      "format": "In-Person",
      "industry": "Retail / Consumer Packaged Goods",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Construct a unified VLM training pipeline leveraging CPT, SFT, and RL to distill and localize models for high-volume commercial deployment, while optimizing performance through direct hyperparameter tuning on Megatron Core and in-depth profiling of Python and Java code.",
        "Architect scalable agent-based systems on NVIDIA's ecosystem that replace expensive closed-source models while increasing throughput by 10x.",
        "Implement robust safety guardrails to maintain content integrity and mitigate against adversarial input."
      ],
      "nvidia_technology": "DGX Platform, Infiniband Networking, Ethernet Networking, Hopper, Base Command Manager, Interconnect Networking, NVLink / NVSwitch, NVIDIA Run:ai, Mission Control",
      "session_id": "S81828",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Coupang",
          "name": "Dongbai Guo",
          "title": "VP, Emerging Commerce Engineering"
        },
        {
          "company": "Coupang",
          "name": "Ashish Suryavanshi",
          "title": "VP, Engineering"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Scale Open-Source VLMs to Billions of Interactions for Ecommerce",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81828/"
    },
    {
      "description": "Imagine that you could generate the world’s most in-depth dataset for sports using the same video you see on your TV screen. In this presentation we’ll show you how we combine computer vision and our own proprietary multimodal foundation model that takes away the need for in-venue hardware and processing and scales infinitely across the game of soccer and beyond; uncovering and explaining the language of sport. We’ll show how GPU acceleration allows us to model entire matches as unified multi-agent sequences, and how this foundation model powers the next generation of soccer intelligence - from tactical reconstruction to semantic embeddings - within the Opta OS ecosystem. The session will detail the model design, engineering challenges, and key breakthroughs.",
      "format": "In-Person",
      "industry": "Media & Entertainment",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Explore how to design and train a domain-specific multi-modal foundation model in the tracking data space.",
        "Dive into techniques for full-match multi-agent sequence modeling using custom spatial–temporal attention.",
        "Uncover how generative tracking (“ghosting”) and soccer semantic embeddings emerge from the model.",
        "Learn how a sports domain foundation model becomes the intelligence layer for data generation, analysis, and expert assistance within Opta OS.",
        "Connect With the Experts",
        "Workshops, Training Labs & Certification",
        "Sponsors & Exhibitors",
        "Privacy Policy Your Privacy Choices Terms of Service Accessibility Corporate Policies Product Security Contact",
        "Copyright © 2026 NVIDIA Corporation",
        "NVIDIA uses cookies to improve your experience o"
      ],
      "nvidia_technology": "Cloud / Data Center GPU, CUDA, DeepStream, TensorRT, Optical Flow SDK, Video Codec SDK",
      "session_id": "S81633",
      "session_type": "Talks or Panels",
      "speakers": [
        {
          "company": "Stats Perform",
          "name": "Patrick Lucey",
          "title": "Chief Scientist"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Scale Sports Intelligence and Insight With Foundational Models",
      "topic": "Video Analytics",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81633/"
    },
    {
      "description": "Discover how enterprises go from AI proof-of-concept (PoC) to deploying intelligent agents in production, automating and optimizing business processes in SAP environments. We'll explore the design and deployment of SAP’s Joule Agents, which support complex, multi-step workflows for procurement, finance, supply chain, and ops. We show the technical architecture of Joule Agents, including how SAP Business Technology Platform works with modern AI infrastructure to coordinate and connect agents to business data, maintaining reliability at scale. Key topics include data integration, agent orchestration patterns, governance, risk management, and performance tuning via advanced AI infrastructure. Gain actionable guidance on building AI agents from concept to production using SAP’s partner ecosystem to speed time-to-value while ensuring scalability, security, and operational resilience.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how enterprises transition from AI pilots to deploying intelligent agents that automate and optimize essential SAP business processes.",
        "The session provides an in-depth look at the technical architecture behind SAP’s Joule Agents, emphasizing integration with modern AI infrastructure and business data for reliable, scalable operations.",
        "Gain practical insights into designing, orchestrating, and governing multi-step workflows using Joule Agents in procurement, finance, supply chain, and operations."
      ],
      "nvidia_technology": "",
      "session_id": "S82080",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "SAP",
          "name": "Walter Sun",
          "title": "SVP and Global Head of AI, SAP"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Scaling AI Agents Across SAP Apps, From PoC to Production (Presented by SAP)",
      "topic": "Retrieval-Augmented Generation (RAG)",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82080/"
    },
    {
      "description": "DuckDB has redefined what a lightweight online analytical processing database can do, but as an in-process engine, it faces inherent scale limitations. Sirius bridges that gap by bringing GPU-native execution to DuckDB—without changing how users write queries. In this session, we’ll explore how Sirius offloads query workloads to GPUs using the open Substrait format, accelerating analytics by over 8x while enabling DuckDB to handle data sizes far beyond a single node. Learn how this new architecture combines DuckDB’s simplicity with the power of GPU compute, unlocking interactive analytics on massive datasets without giving up the elegance of a single-node engine.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "DuckDB on GPUs is incredibly fast",
        "DuckDB can now scale to larger datasets",
        "GPUs accelerate data processing"
      ],
      "nvidia_technology": "DGX Platform, CUDA, CUDA-X, cuDF, NCCL, nvCOMP",
      "session_id": "S81870",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "University of Wisconson",
          "name": "Xiangyao Yu",
          "title": "Assistant Professor"
        },
        {
          "company": "NVIDIA",
          "name": "Bobbi Yogatama",
          "title": "Sr. Systems Software Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Scaling DuckDB Beyond Its Limits: GPU-Accelerated Analytics With Sirius",
      "topic": "Accelerated Computing Libraries",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81870/"
    },
    {
      "description": "As AI factories grow to hundreds of thousands, and soon millions, of interconnected GPUs, the network has become the defining architecture of large-scale AI. Achieving uncompromised performance requires fabrics that not only scale up and out within a single data center, but scale across multiple sites with predictable latency, lossless throughput, and end-to-end resilience. In this session, NVIDIA unveils the full breadth of networking innovations that enable the construction of giga-scale AI infrastructure. Learn how advancements in networking, including ultra-high-radix switches, next-generation NVIDIA Quantum-X InfiniBand and Spectrum-X Ethernet platforms with co-packaged optics, and accelerator-optimized SuperNICs, deliver the performance required to train massive foundation models, deploy real-time inference pipelines, and interconnect federated AI factories globally.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Learn why large-scale AI performance is increasingly defined by the network, and how fabrics must scale up, out, and across data centers to support hundreds of thousands to millions of GPUs.",
        "Learn how NVIDIA’s latest networking innovations deliver predictable latency, lossless throughput, and unmatched scalability.",
        "Gain practical insights into building AI infrastructures spanning multiple sites, including topologies, co-packaged optics, and full-stack co-design techniques that enable training, inference, and global AI interconnect at massive scale."
      ],
      "nvidia_technology": "BlueField DPU, Infiniband Networking, Ethernet Networking, Interconnect Networking",
      "session_id": "S81561",
      "session_type": "Talks or Panels",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Gilad Shainer",
          "title": "SVP Networking"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Scaling Out and Across: Networking Innovations for Giga-Scale AI Systems",
      "topic": "AI Networking, 5G/6G",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81561/"
    },
    {
      "description": "In this hands-on lab, General Robotics will guide you through a complete, end-to-end solution development workflow built on the NVIDIA ecosystem. You'll learn to: • Capture teleoperated demonstrations of real manual tasks • Augment, vary, and scale those tasks in simulation using NVIDIA Isaac • Generate high-volume synthetic datasets to expand coverage • Fine-tune modern robotics models using NVIDIA’s training infrastructure • Test policies and reinforcement learning models in advanced simulation, leveraging NVIDIA AI models like GraspGen, CuMotion, and Foundation Stereo • Deploy learned behaviors to real robots running on Jetson • Analyze system performance and iterate using the latest LLMs and agentic tools • Bridge the human–robot gap across development, validation, and deployment Enthusiasm to learn the latest techniques in AI and robotics.",
      "format": "In-Person",
      "industry": "Manufacturing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how to simulate and deploy learned AI behaviors to real robots.",
        "Access the latest and greatest AI models from NVIDIA and across the industry.",
        "Accelerate your robotics automation development journey with GRID from General Robotics."
      ],
      "nvidia_technology": "",
      "session_id": "DLIT81881",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "General Robotics",
          "name": "Sai Vemprala",
          "title": "CTO and Co-founder"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Scaling Robotic AI Development from Prototype to Production",
      "topic": "Robot Manipulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81881/"
    },
    {
      "description": "As AI shifts to continuous, real-time, agentic workloads, power and thermal stability—not model complexity—now limit AI factory scale. GPUs running large-scale inference require predictable, full-performance operation, yet air cooling introduces throttling and inefficiency. This session presents Neuralwatt’s evaluation of ZutaCore HyperCool two-phase direct-to-chip cooling. Results show zero thermal throttling and events, up to 30% lower peak GPU temperatures, and a 22.5% increase in tokens per watt. Learn how two-phase cooling enables higher compute density and throughput without adding power capacity, while supporting waterless, high-temperature, production-ready AI infrastructure.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Why thermal and power ceilings—not model complexity—are the new bottlenecks for AI factory scale",
        "How two-phase cooling maintains stable, full-performance GPU operation under real-world agentic workloads",
        "Neuralwatt validation results of ZutaCore HyperCool: 30% lower peak temperatures, 22.5% more tokens per watt, zero throttling",
        "How to increase compute density and throughput without increasing available power",
        "Practical guidance for adopting waterless, high-temperature two-phase cooling in production AI facilities"
      ],
      "nvidia_technology": "HGX, Blackwell",
      "session_id": "EX82049",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "ZutaCore",
          "name": "My Truong",
          "title": "CTO"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Scaling the AI Factory: Eliminating Power and Thermal Barriers With Two-Phase Cooling (Presented by ZutaCore)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82049/"
    },
    {
      "description": "Scaling complex models requires infrastructure that evolves with your code. From training to deployment, learn how to optimize your NVIDIA infrastructure for training and inference workloads at scale. Join Erwan Menard, senior vice president of product, for a technical deep dive into Crusoe Cloud’s high-performance, climate-aligned platform.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Deep-dive into Crusoe’s latest capabilities, optimized for the newest NVIDIA GPU architectures.",
        "Learn how to navigate the technical demands of massive-scale model training, efficient fine-tuning, and low-latency inference.",
        "Dive into a real-world case study and how a leading AI innovator leveraged Crusoe’s compute platform to accelerate their time-to-market."
      ],
      "nvidia_technology": "HGX, Hopper, NVIDIA NIM, Blackwell",
      "session_id": "S82018",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Crusoe",
          "name": "Erwan Menard",
          "title": "SVP, Product"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Scaling the AI Life Cycle: Next-Gen Training and Inference With Crusoe Cloud (Presented by Crusoe)",
      "topic": "Training AI Models",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82018/"
    },
    {
      "description": "Financial services leader Intercontinental Exchange and AHEAD share how the Aurora platform is scaling enterprise AI across cloud and on-premises environments. The session highlights how to apply a proven hybrid AI playbook combining NVIDIA accelerated platforms, AI software, standardized pipelines, proprietary data, and governance frameworks to move AI from experimentation to production. Learn how this approach is delivering measurable impact across Intercontinental Exchange while improving performance, cost control, operational efficiency, and trust.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "The Power of a Hybrid Architecture: A standardized hybrid architecture, anchored by NVIDIA accelerated platforms, is the critical enabler for seamlessly scaling AI workloads across distinct on-premises and cloud environments.",
        "Operationalization Over Experimentation: Moving from disparate experiments to production-grade utility requires a reproducible playbook that integrates software and pipelines to drive rapid adoption in complex sectors like FIDS and mortgage technology.",
        "Ecosystem Synergy as a Force Multiplier: Success in enterprise AI is accelerated by strategic partnerships (e.g., ICE and AHEAD) that combine deep domain expertise with technical prowess to navigate the complexities of deploying proprietary data models.",
        "Governance as a Performance Driver: Integrating robust governance frameworks with proprietary data is not just a compliance measure, but the engine that delivers cost control, operational efficiency, and the essential trust required for enterprise AI.",
        "Measurable Business Outcomes: Effective AI strategy transcends technical metrics, focusing instead on tangible KPI improvements—specifically cost optimization and operational speed—to validate the investment in high-performance computing infrastructure."
      ],
      "nvidia_technology": "DGX Platform, NeMo, NVIDIA NIM, Blackwell, NVIDIA AI Enterprise, NVIDIA Run:ai",
      "session_id": "S82117",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "ICE",
          "name": "Ryan New",
          "title": "CIO"
        },
        {
          "company": "AHEAD",
          "name": "Josh Perkins",
          "title": "VP, Emerging Technologies"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Scaling Trusted AI: A Hybrid Playbook for Impact in FinServ (Presented by AHEAD)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82117/"
    },
    {
      "description": "This session will demonstrate how to move beyond basic spatial-temporal modeling and implement domain-specific video intelligence using techniques inspired by NVIDIA VSS to build robust, life cycle-aware platforms for real-world applications like brand safety and contextual advertising. Explore new solutions for accelerating large-scale video ingestion with modern multi-modal tooling like NVIDIA NeMo Curator, and discover strategies for achieving high-throughput inference through embedding-level optimization and evolving to next-generation, end-to-end validation frameworks. We'll present proven methodologies with customer success examples from enterprises like Samsung Electronics, LG Electronics, and Kenvue to help you deploy enterprise-grade, trustworthy AI systems that analyze video content at scale.",
      "format": "In-Person",
      "industry": "Media & Entertainment",
      "intended_audience": "Marketing / Sales",
      "key_takeaways": [
        "Moving Beyond Spatial-Temporal Modeling with Domain-Based Video Intelligence: Learn how domain-specific cues, such as content life cycle patterns, real-world context, and viewer intent, enable deeper, more accurate video understanding necessary for applications like brand safety and contextual advertising.",
        "Apply Multimodal Structuring Techniques Inspired by NVIDIA VSS:"
      ],
      "nvidia_technology": "DGX Platform, CUDA, Metropolis, NeMo, Blackwell, Blueprint, DGX Station",
      "session_id": "S81617",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "PYLER",
          "name": "Jaeho Oh",
          "title": "CEO"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Scaling Trustworthy Multi-Modal Video Intelligence: A Life Cycle-Aware Approach for Domain-Specific Real-World Platforms",
      "topic": "Video Analytics",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81617/"
    },
    {
      "description": "探索基于 Megatron-Core 构建的 SeamlessFlow 可扩展 RL框架，了解如何通过 Trainer-Agent 隔离和标签驱动调度在 NVIDIA GPU 集群上消除流水线气泡。我们将展示如何利用时空复用技术最大化资源利用率，实现相比 VERL 提升 100% 的吞吐量并将训练时间缩短 62%。",
      "format": "Virtual",
      "industry": "Consumer Internet",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "基于 Megatron-Core 的 RL 训练与 Rollout 解耦，实现智能体对服务中断无感知",
        "标签驱动的调度范式：将硬件抽象为带能力标签的资源，统一了共置（Colocated）与分离（Disaggregated）架构，实现高效部署",
        "消除气泡的时空复用流水线： 在训练-Rollout 分离架构下，动态重分配空闲训练节点用于Rollout，充分利用 GPU 资源",
        "卓越的性能提升： 相比 VERL，平均吞吐量提升100%，总训练时间减少 62%"
      ],
      "nvidia_technology": "Hopper",
      "session_id": "S82321",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "快手科技",
          "name": "Yinghan Cui",
          "title": "Senior Expert"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "SeamlessFlow：NVIDIA GPU 集群上基于 Megatron-Core 的 Trainer-Agent 隔离与标签调度无空泡强化学习框架",
      "topic": "Code / Software Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82321/"
    },
    {
      "description": "This session examines how DCAI, a sovereign neo-cloud operator, supports secure multi-tenant access to high-performance AI compute on the Gefion supercomputer. We will show how organizations can share infrastructure while maintaining isolated environments for data, workloads, and networks. The talk covers fractional GPU allocation for efficient resource utilization, pre-configured AI workspaces, and new automation developed with NVIDIA Run:ai that speeds deployment of secure environments. Learn practical approaches for enabling multiple teams to develop and scale AI workloads on shared infrastructure without compromising security or sovereignty.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Learn how a sovereign neo-cloud enables multiple organizations to share the Gefion supercomputer while preserving strict isolation of data, workloads, and networks.",
        "Understand how fractional GPU allocation and preconfigured AI workspaces improve utilization and reduce friction for teams developing and scaling AI workloads.",
        "See how new automation built with NVIDIA Run:ai accelerates the creation of secure, sovereign AI environments without compromising governance or security."
      ],
      "nvidia_technology": "NVIDIA Run:ai",
      "session_id": "S82177",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Omri Geller",
          "title": "VP Product Management"
        },
        {
          "company": "Danish Centre for AI Innovation",
          "name": "Ali Syed",
          "title": "SVP Infrastructure"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Secure Multi-Tenant AI: Sovereign Neo-Cloud Infrastructure With Fractional GPU Sharing",
      "topic": "InfraOps",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82177/"
    },
    {
      "description": "As organizations integrate AI into production systems, new and complex security challenges are emerging. In this panel discussion, we’ll explore where AI models are most vulnerable—data poisoning, evasion, and prompt-based exploitation—and how adversarial red teaming can help identify and mitigate these risks before attackers do. Learn how research–industry collaboration drives the development of scalable defenses and practical tools for securing machine learning pipelines. By the end of this session, you’ll have a high-level playbook for building resilient AI systems in an evolving threat landscape.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Understand common AI attack vectors, including poisoning, evasion, and prompt-based exploits.",
        "Discover how red teaming strengthens model and pipeline security.",
        "Learn how collaboration between research and industry speeds defensive innovation."
      ],
      "nvidia_technology": "NVIDIA NIM, NVIDIA AI Enterprise",
      "session_id": "S81494",
      "session_type": "Panel",
      "speakers": [
        {
          "company": "Cisco",
          "name": "Omar Santos",
          "title": "Distinguished Engineer, Cisco Product Security Incident Response Team Security Research and Operations"
        },
        {
          "company": "Darktrace",
          "name": "Nicole Carignan",
          "title": "SVP, Security and AI Strategy, Field CISO"
        },
        {
          "company": "NVIDIA",
          "name": "Daniel Rohrer",
          "title": "VP of Software Product Security"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Securing AI Application: Red Teaming the Models and Applications",
      "topic": "Security for AI",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81494/"
    },
    {
      "description": "As AI-powered coding assistants become more capable—and begin interacting directly with developer environments—their security risks grow alongside their utility. This session offers a practical demonstration of how vulnerabilities can emerge when these assistants gain computer-use capabilities, exposing new attack surfaces inside the software development life cycle. We then dive into Cursor’s real-world mitigation strategies, showing how hard security controls around critical functions create a flexible balance between capability and risk. Finally, we connect lessons from AI security research, secure application design, and red-teaming exercises to illustrate how organizations can safely unlock higher levels of automation. Together, these practices chart a path toward powerful AI development tools with far lower operational risk.",
      "format": "In-Person",
      "industry": "Public Sector and Sovereign AI",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "AI coding assistants are powerful, but adding computer use capabilities also increases their risk.",
        "Enabling hard security controls around certain critical functions enables a flexible trade-off between capabilities and risk.",
        "By combining AI security expertise, secure application design, and lessons learned from red teaming these applications, we can enable a high degree of automation with much lower risk to the organization."
      ],
      "nvidia_technology": "NCCL, NVIDIA NIM",
      "session_id": "S81493",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Rich Harang",
          "title": "Principal AI Security Engineer"
        },
        {
          "company": "Cursor",
          "name": "Kody Fisher",
          "title": "Engineering Manager"
        },
        {
          "company": "Cursor",
          "name": "Tom Daniels",
          "title": "CISO"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Securing Coding Assistants",
      "topic": "Security for AI",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81493/"
    },
    {
      "description": "As AI and large language models move from experimentation into mission-critical systems, organizations are building intelligent platforms to power decision-making, automation, and innovation at scale, creating a new security reality. From data pipelines and model development to deployment, inference, and real-world impact, AI systems expand the attack surface in ways traditional security can't address. We'll explore a practical Security for AI Framework, aligned with the OWASP Top 10 for LLMs, and see how modern threats like adversarial manipulation, data poisoning, prompt injection, model abuse, and synthetic media attacks are reshaping the landscape. Learn how organizations are embedding governance, access control, validation, monitoring, and response into their AI platforms and AI factory architectures, turning security into a foundation for trusted, production-ready intelligence.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Security Operations",
      "key_takeaways": [
        "Understand how the AI and LLM threat landscape evolves as models move into production.",
        "Learn practical approaches to managing adversarial, data, and model-level risks."
      ],
      "nvidia_technology": "NeMo",
      "session_id": "EX82075",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "Trend Micro",
          "name": "Fernando Cardoso",
          "title": "VP of Product Management"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Securing Intelligence: A Practical Framework for AI and LLM Security (Presented by Trend Micro)",
      "topic": "Code / Software Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82075/"
    },
    {
      "description": "As enterprises build AI factories, massive data flows, distributed compute, and real-time inferencing push infrastructure to new limits while introducing new security risks. An AI factory is a specialized data center built for intelligence at scale, yet traditional defenses struggle to secure dynamic AI pipelines and emergent model behavior without impacting performance. This session explores how a secure-by-design architecture powered by Palo Alto Networks AIRS and accelerated by NVIDIA BlueField delivers real-time, inline protection at the speed of AI. By embedding security controls at the infrastructure layer, organizations gain agentless visibility and enforcement across the AI life cycle. AIRS detects threats, blocks prompt-level attacks, and governs agent actions as systems scale. Together, these capabilities enable trustworthy, governed, high-throughput AI operations.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "See how accelerated computing and AI deliver real-time defense so security teams can detect and respond to threats inline, without slowing AI workloads.",
        "Learn how to embed zero trust controls into the AI factory, ensuring security and performance scale together as AI adoption grows.",
        "Understand how to modernize AI infrastructure security by shifting from perimeter controls to continuous, cloud-connected runtime protection for prompt-level attacks and agentic workflows.",
        "Gain practical guidance on governing AI systems through unified visibility and consistent policy enforcement across data, models, and pipelines."
      ],
      "nvidia_technology": "BlueField DPU, RTX GPU, DOCA, Ethernet Networking, NVIDIA NIM, Blackwell, NVIDIA AI Enterprise",
      "session_id": "S81691",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Ofir Arkin",
          "title": "Sr. Distinguished Engineer"
        },
        {
          "company": "Palo Alto Networks",
          "name": "Rich Campagna",
          "title": "SVP for Network Security"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Securing the AI Factory: Accelerate Runtime Security for Enterprise Innovation",
      "topic": "Security for AI",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81691/"
    },
    {
      "description": "AI agents are swiftly evolving toward true autonomy using self-coding, feedback-driven, and adaptive architectures. This session explores frameworks where agents self-improve via data flywheels—constantly refining their models through feedback and learning from every interaction. We'll spotlight data designer agents that not only process, but actively generate and curate training data, enabling ongoing evolution without routine human coding. Critical topics include self-repairing code, reinforcement-driven data loops, and orchestration of agent collectives for collaboration and scale. These advances are powering scalable, reliable, and collaborative autonomous systems for enterprise and research, ushering in agents that can continually adapt and co-create with humans in real time.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Understand Self-Coding Intelligence: Learn how AI agents can autonomously generate, refine, and repair their own code through feedback loops and adaptive learning frameworks.",
        "Harness Data Flywheels for Continuous Improvement: Discover how data designer agents create and curate training data to fuel ongoing evolution, enabling systems that learn efficiently without constant human intervention.",
        "Scale Collaboration Among Intelligent Agents:"
      ],
      "nvidia_technology": "NeMo, NVIDIA NIM, NVIDIA AI Enterprise, cuVS",
      "session_id": "S81569",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Bartley Richardson",
          "title": "Sr. Director of Engineering"
        },
        {
          "company": "NVIDIA",
          "name": "Kris Murphy",
          "title": "Technical Product Manager"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Self-Coding Agents: Architectures, Data Flywheels, and Autonomous Code Repair",
      "topic": "Retrieval-Augmented Generation (RAG)",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81569/"
    },
    {
      "description": "Achieving Level 4 (L4) autonomy at global scale requires both technical mastery and a verifiable safety case to earn public trust. This panel brings together engineering, mobility, and regulatory leaders from across the autonomous vehicle ecosystem to examine the breakthroughs enabling trustworthy autonomous vehicle deployment. We’ll explore the core challenges: integrating large foundation models, using reasoning vision-language-action models to handle edge cases, generating billions of validation miles using physically accurate simulation, and using unified safety frameworks like NVIDIA Halos to achieve certification and regulatory compliance. You’ll learn how teams are translating abstract safety mandates into verifiable software and hardware constraints to deliver truly trustworthy autonomy.",
      "format": "In-Person",
      "industry": "Automotive / Transportation",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "End-to-End vs. Modular Architectures: Technical comparison of system designs, evaluating the trade-offs of vision-first end-to-end learning versus multi-sensor modular stacks for reliability and interpretability",
        "AI for Validation: How generative AI and simulation produce synthetic, safety-critical data, accelerating the training and validation loop from cloud to car",
        "Safety Imperative: Learn how engineering teams use certified frameworks like NVIDIA Halos to integrate ecosystem, algorithmic, and platform safety to achieve regulatory compliance.",
        "Operationalizing Autonomy: Strategies for ensuring autonomous fleet reliability at scale, including safe over-the-air updates and continuous data feedback loops for continuous safety improvement"
      ],
      "nvidia_technology": "DRIVE, DGX Platform, CUDA, DRIVE SDK, Omniverse, OVX, DRIVE AV, DRIVE IX, Blackwell, Cosmos",
      "session_id": "S81786",
      "session_type": "Panel",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Ali Kani",
          "title": "VP of Automotive"
        },
        {
          "company": "Waabi",
          "name": "Raquel Urtasun",
          "title": "Founder and CEO"
        },
        {
          "company": "Tesla",
          "name": "Ashok Elluswamy",
          "title": "VP of AI"
        },
        {
          "company": "Uber Technologies",
          "name": "Noah Zych",
          "title": "Global General Manager, Autonomous Mobility and Delivery"
        }
      ],
      "technical_level": "General Interest",
      "title": "Shaping the Next Era of Autonomous Driving Through Safety-Centered AI",
      "topic": "Autonomous Machines",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81786/"
    },
    {
      "description": "As data demands scale to terabytes and petabytes, traditional GPU memory boundaries become critical bottlenecks. We introduce a unified strategy using composable, engine-agnostic building blocks to shatter these limits. We'll dive deep into cuCascade, a library for memory reservation and topology discovery. It prevents out-of-memory failures by gracefully spilling data between memory tiers—from GPU to host to disk—while mapping hardware throughput. We also introduce Quentrace, a semantic telemetry layer for \"always-on\" profiling, allowing developers to visualize query plans and resource consumption across GPUs in real time. We'll demonstrate these tools powering \"speed of light\" analytics on NVIDIA’s NVL72 systems (GB200) utilizing full NVLink. Learn to leverage these components to optimize engines, identify bandwidth bottlenecks, and achieve unprecedented TCO and performance.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Moving data to the GPU is difficult, and needs to be fast",
        "Composable, foundational building blocks are needed for accelerated data processing on GPUs",
        "Telemetry as a first-class citizen matters when building large distributed engines on GPUs"
      ],
      "nvidia_technology": "CUDA, cuDF, NCCL, NSight Systems, nvCOMP",
      "session_id": "S81873",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Felipe Aramburu",
          "title": "Distinguished Solutions Architect"
        },
        {
          "company": "NVIDI",
          "name": "Rodrigo Aramburu",
          "title": "Developer Relations for Data Processing"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Shatter the Memory Wall: Composable Building Blocks for Massive Scale Analytics",
      "topic": "Data Analytics / Processing",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81873/"
    },
    {
      "description": "True innovation has arrived for cabling AI PODs! Navigating cabinet depth, accessibility, cable volume and performance is simplified. Explore options that can save time on site!",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Consulting",
      "key_takeaways": [
        "Understand cabling challenges and possible solutions in an AI POD.",
        "Understand project staging, timing, and risk for off-site vs. on-site connectivity.",
        "Why you should build spares into cabling installation"
      ],
      "nvidia_technology": "Cloud / Data Center GPU, Grace CPU, Infiniband Networking, Ethernet Networking, Hopper, Interconnect Networking, Blackwell",
      "session_id": "EX82260",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "CommScope",
          "name": "Ken Hall",
          "title": "Data Center Solutions Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Simplifying high-density fiber cabling for AI PODs, improving site efficiency for Day 2 (Presented by CommScope)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82260/"
    },
    {
      "description": "This session explores how world models are emerging as a critical next step in advancing AI, enabling applications across physical AI, entertainment, and beyond. We'll go inside Runway's GWM-1: its architecture, capabilities, and why we believe video diffusion-based world models are the most direct path to general-purpose simulation. We'll also discuss the research and infrastructure challenges involved in deploying world models that run in real time.",
      "format": "In-Person",
      "industry": "Media & Entertainment",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how world models are emerging as a critical next step in advancing AI.",
        "Learn how world models will enable applications across physical AI, entertainment, and beyond.",
        "Learn about Runway's GWM-1 model architecture, capabilities, and why video diffusion-based world models are the most direct path to general-purpose simulation.",
        "Learn about the research and infrastructure challenges involved in deploying world models that run in real time."
      ],
      "nvidia_technology": "DGX Platform, Blackwell, DGX Cloud",
      "session_id": "S82147",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Runway",
          "name": "Anastasis Germanidis",
          "title": "CTO and Co-Founder"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Simulating Reality Inside General World Models",
      "topic": "Video Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82147/"
    },
    {
      "description": "As autonomous AI agents expand enterprise automation, they introduce a growing attack surface that traditional security controls were not designed to address. This session shows why visibility and policy enforcement in the AI factory are critical for trust, and how GPU-accelerated cyber models act as intelligent guardrails against threats such as prompt injection, model exfiltration, and data leakage. Through examples and an architectural walkthrough, we demonstrate a full-stack solution using NVIDIA NIM-based inference, NVIDIA Zero Trust Networking, the NVIDIA Morpheus SDK, DPU-accelerated firewalls, and F5 AI Guardrails. Learn how to offload security functions into an isolated trust domain to ensure zero-trust enforcement without compromising workload performance, and leave with an actionable blueprint for SecOps and AI teams to turn AI risk into a competitive differentiator.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Learn how F5 and NVIDIA jointly deliver end-to-end, scalable, and zero-trust security for the enterprise AI stack.",
        "Understand why centralized visibility, policy enforcement, and GPU-accelerated guardrails are essential to protecting AI inference from prompt injection and data exfiltration.",
        "Discover how the NVIDIA Morpheus SDK and DPU-accelerated firewalls prevent sensitive data leakage while preserving high-performance AI workloads."
      ],
      "nvidia_technology": "BlueField DPU, NVIDIA NIM, NVIDIA AI Enterprise",
      "session_id": "S81841",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Kevin Deierling",
          "title": "SVP, Networking"
        },
        {
          "company": "F5, Inc.",
          "name": "Kunal Anand",
          "title": "Chief Product Officer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Smart Agents, Safer Systems: Engineering AI for Trust and Transparency",
      "topic": "Security for AI",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81841/"
    },
    {
      "description": "Alibaba Cloud ACK RBG is a management tool for the full life cycle of large model inference deployment on Kubernetes, designed to address the challenges of large-scale deployment and operations of prefill-decode (PD) disaggregation architecture within Kubernetes clusters. It not only provides features such as multi-role management/collaboration, auto service discovery, topology-aware placement, elastic scaling, and atomic rollout/failure recovery, but also tightly integrates with NVIDIA Dynamo's AIConfigurator and Planner to achieve performance profiling and smarter scaling in complex PD disaggregation and mixture-of-experts (MoE) scenarios. This aims to further improve inference efficiency and reduce costs while meeting service level agreement (SLA) requirements, representing one of the effective ways for large-scale practical deployment of NVIDIA Dynamo.",
      "format": "In-Person",
      "industry": "Cloud Services",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Understand key technical challenges of landing LLM inference service in real enterprise production environment. Share the case study and best practice of how Alibaba Cloud help clients to deploy LLM inference services in Kubernetes at scale, and manage those with high performance and efficiency.",
        "Learn to use SGLang community project — RoleBasedGroup (RBG) — to manage the entire life cycle of Dynamo inference service just as easy as deploy, rolling update, failure auto-recover, monitor and orchestrate micro services.",
        "Integrate RBG with Dynamo AIConfigurator and Planner to autoscale the prefill-decode disaggregated and MoE inference instances based on model specific metrics to continuously balance the serving SLA (TTFT, TPOT) requirement and cost.",
        "Leverage Mooncake, Fluid such distributed cache technologies to reduce large model service cold start time 10X.",
        "Extend Kubernetes Gateway API (GIE) with prefix cache and KVCache awareness and LoRA affinity routing policy to reduce LLM serving latency over 40%, increase cache-hit 15%."
      ],
      "nvidia_technology": "NeMo, NVIDIA NIM",
      "session_id": "S81462",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Alibaba Cloud Intelligence",
          "name": "Tongyu Guo",
          "title": "Software Engineer"
        },
        {
          "company": "Alibaba Cloud Intelligence",
          "name": "Kai Zhang",
          "title": "Sr. Staff Engineer"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Solve the Day 1 Challenge of Landing LLM Inference in Enterprise",
      "topic": "ModelOps",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81462/"
    },
    {
      "description": "Regulated industries like healthcare and finance require running AI models locally for privacy and compliance, as moving up to 70% of sensitive enterprise data to the public cloud is costly and impractical. This necessitates bringing AI to the data. This creates a Trust Dilemma: Enterprises need strong guarantees that sensitive data remains private and inaccessible to external parties, while model providers need assurance that their proprietary models are protected when deployed outside their direct control. This dilemma often restricts enterprises to open-source models, with advanced proprietary models only available as SaaS. NVIDIA Confidential Computing resolves this by using trusted execution environments to protect both sensitive data and model intellectual property. We will showcase a reference architecture that securely deploys proprietary models into enterprise infrastructure without exposing models or data to operators or vendors. This approach allows AI providers to meet customers where their data resides, enabling enterprises to safely apply advanced AI to their most sensitive data.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Learn how NVIDIA Confidential Computing resolves trust dilemma by protecting data and model IP.",
        "Dive deep into a reference architecture with model providers, ISVs, and OEMs.",
        "Enterprises can use frontier models that were formerly SaaS-only without compromising data sovereignty or regulatory compliance."
      ],
      "nvidia_technology": "NVIDIA NIM, NVIDIA AI Enterprise, NVIDIA Run:ai",
      "session_id": "S82081",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Erik Bohnhorst",
          "title": "Director of Product Management"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Solving the Trust Dilemma by Ensuring Model and Data Privacy On Premises with Confidential AI",
      "topic": "InfraOps",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82081/"
    },
    {
      "description": "Sovereign AI is rapidly becoming a new growth engine for telcos as they build national AI clouds and monetize them in very different ways. This panel brings together leading operators from across the world to share how they are turning NVIDIA-powered sovereign AI infrastructure into real revenue and real impact for their nations. Panelists will share their customer stories and discuss a variety of business models. They will also highlight how innovation hubs are enabling local developers, startups, and enterprises to build country-specific AI applications. The session will close with concrete lessons learned, demand generation, and a call to action for telcos and app builders looking to participate in the sovereign AI opportunity.",
      "format": "In-Person",
      "industry": "Telecommunications",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Learn how leading telcos are positioning sovereign AI infrastructure for consumers, enterprises, and governments, and what the demand looks like in practice.",
        "Understand monetization models for sovereign AI and how these are driving ecosystem growth and new revenue for telcos.",
        "Practical lessons on ecosystem building, and go to market for telcos and application providers that want to leverage sovereign AI platforms"
      ],
      "nvidia_technology": "BlueField DPU, CUDA, TensorRT, Infiniband Networking, Ethernet Networking, Hopper, Interconnect Networking, NVLink / NVSwitch, NVIDIA NIM, Blackwell, NVIDIA AI Enterprise, Dynamo",
      "session_id": "S82019",
      "session_type": "Panel",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Joao Kluck Gomes",
          "title": "Director, Business Development – AI Factories and Applications"
        },
        {
          "company": "Indost Ooredoo Hutchison (IOH)",
          "name": "Vikram Sinha",
          "title": "President, Director, and CEO"
        },
        {
          "company": "TELUS",
          "name": "Chris Madan",
          "title": "VP, Customer Digital Solutions and Product"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Sovereign AI for Telcos: Offtake Stories and Monetization Models From Around the World",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82019/"
    },
    {
      "description": "World models demand long-horizon reasoning that rapidly outgrows GPU memory, while sovereign AI requires strict control over where data and models live. This session shows how a data intelligence architecture designed with native multi-tenancy, KV Cache, and hybrid cloud capabilities can extend the AI pipeline beyond the GPU to deliver ultra-low-latency, high-concurrency access patterns and policy-based governance to scale world models under sovereign constraints. You'll leave with practical design patterns for building NVIDIA-accelerated AI factories that maintain performance without sacrificing sovereignty or cost-per-token.",
      "format": "In-Person",
      "industry": "Public Sector and Sovereign AI",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Learn how how a data intelligence architecture designed with native multi-tenancy, KV Cache, and hybrid cloud capabilities can extend the AI pipeline beyond the GPU.",
        "Discover practical design patterns for building NVIDIA-accelerated AI factories that maintain performance without sacrificing sovereignty or cost-per-token.",
        "Learn how intelligent AI data pipelines deliver ultra-low-latency, high-concurrency access patterns and policy-based governance to scale world models under sovereign constraints."
      ],
      "nvidia_technology": "BlueField DPU, Cloud / Data Center GPU, Grace CPU, RTX GPU, DGX Platform, HGX, CUDA, Infiniband Networking, Ethernet Networking, Hopper, NeMo, NVIDIA NIM, Blackwell, NVIDIA AI Enterprise, Nemotron",
      "session_id": "S82096",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "DDN",
          "name": "Sven Oehme",
          "title": "CTO"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Sovereign World Models: Scaling AI Memory, Data Gravity, and Governance (Presented by DDN)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82096/"
    },
    {
      "description": "We will share our experience in building the first physical AI research lab in northern Spain, dedicated to empowering the local industrial ecosystem. This lab serves as a collaborative space where researchers can design, prototype, and validate cutting-edge technologies in robotics and automation. By providing access to advanced physical AI tools such as NVIDIA Omniverse and Isaac platforms, we enable rapid development and testing before deployment in real-world scenarios. Local industries can actively engage with these innovations, gaining hands-on experience and exploring practical applications that drive competitiveness and technological adoption. We'll show how collaboration between academic research and industrial implementation can foster a dynamic environment for innovation and growth.",
      "format": "Virtual",
      "industry": "Manufacturing",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Understand how creating a dedicated space for designing, prototyping, and validating robotics and automation technologies fosters faster, safer, and more effective innovation cycles.",
        "Learn how collaboration between academic institutions and local industries transforms cutting-edge research into practical, industry-ready solutions that enhance productivity and competitiveness.",
        "Discover how access to platforms like NVIDIA Omniverse and Isaac enables regional companies to experiment with and adopt AI-driven automation."
      ],
      "nvidia_technology": "Isaac, Omniverse, NVIDIA AI Enterprise",
      "session_id": "S81988",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Vicomtech",
          "name": "Jorge Posada",
          "title": "Scientific and Institutional Director"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Spain's First Physical AI Lab: Empowering the Northern Spanish Industrial Ecosystem",
      "topic": "Industrial Digitalization / Digital Twin",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81988/"
    },
    {
      "description": "Get direct answers to your implementation challenges. This open Q&A session is your chance to connect with the experts responsible for Nemotron Speech to discuss how to maximize the performance of automatic speech recognition (ASR) and text-to-speech (TTS) in your own projects. Whether you are building for a noisy industrial environment, need better multilingual support, or are trying to create intelligent voice agents, our team is here to troubleshoot and share actionable strategies. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Solving Real-World Challenges: How to apply Nemotron Speech capabilities—like noise robustness and zero-shot TTS—to fix specific pain points in your application",
        "Strategic Customization: Best practices for fine-tuning Nemotron Speech models to handle the unique jargon, accents, or requirements of your specific domain",
        "Achieving State-of-the-Art Quality: Practical techniques you can adopt right now to boost accuracy and naturalness in your deployment"
      ],
      "nvidia_technology": "NeMo",
      "session_id": "CWES82169",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Sanjay Singh Chauhan",
          "title": "Sr. Engineering Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Mayank Jain",
          "title": "Sr. Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Mikyas Desta",
          "title": "Sr. Deep Learning Scientist"
        },
        {
          "company": "NVIDIA",
          "name": "Rahul Mittal",
          "title": "Engineering Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Harishchandra Dubey",
          "title": "Sr. Machine Learning Scientist"
        },
        {
          "company": "NVIDIA",
          "name": "Yitagessu Gebremedhin",
          "title": "Sr. Speech AI Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Adi Margolin",
          "title": "Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Tripti Singhal",
          "title": "Senior Solutions Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Speech AI Model and Services Development and Customization",
      "topic": "Speech Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes82169/"
    },
    {
      "description": "Traditional automotive crashworthiness assessment relies on computationally expensive finite element simulations, creating a bottleneck in automotive design. This work with General Motors introduces machine learning surrogate models, utilizing MeshGraphNet and Transolver architectures within the NVIDIA PhysicsNeMo framework. Evaluated on a Body-in-White dataset of 150 simulations, the study compares time-conditional and stability-enhanced autoregressive modeling strategies. The models successfully predict spatiotemporal deformation trends using undeformed geometry and variable thickness inputs. Although full finite element accuracy remains out of reach, the approach achieves massive computational speedups. These results demonstrate the feasibility of ML-driven dynamics for rapid, early-stage crashworthiness optimization.",
      "format": "In-Person",
      "industry": "Automotive / Transportation",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Machine learning models, specifically MeshGraphNet and Transolver, can effectively learn and predict complex structural deformations in automotive crash scenarios.",
        "Data-driven models deliver orders-of-magnitude faster inference speeds compared to traditional finite element simulations, unlocking rapid design iteration.",
        "AI/ML based surrogate models serve as complements, rather than full replacements, for high-fidelity physics solvers."
      ],
      "nvidia_technology": "NeMo, PhysX",
      "session_id": "S81785",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "General Motors",
          "name": "Sudeep Chavare",
          "title": "Vehicle Optimization Lead"
        },
        {
          "company": "NVIDIA",
          "name": "Rishi Ranade",
          "title": "Sr. Technical Engineer – Physics ML"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Speed Meets Safety: Accelerating Automotive Crash Simulations With AI Physics",
      "topic": "CAE",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81785/"
    },
    {
      "description": "Master the deployment of massive mixture-of-expert architectures by configuring Dynamo’s PD Disaggregation with Large Expert Parallelism to maximize GPU efficiency. Learn to optimize distributed data flows including KV Cache Aware Routing to achieve an order-of-magnitude increase in serving throughput while meeting strict production latency demands. Familiarity with LLM inference pipelines (prefill-decode, kv-cache, etc) and compilers including TensorRT-LLM, vLLM, or SGLang",
      "format": "In-Person",
      "industry": "Cloud Services",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "Configure Dynamo to efficiently orchestrate models across diverse inference backends, including vLLM and SGLang.",
        "Integrate Prefill/Decode (PD) Disaggregation to independently scale computational phases and maximize GPU utilization.",
        "Deploy Smart KV Cache Aware Routing to eliminate redundant computation and optimize utilization."
      ],
      "nvidia_technology": "TensorRT, Dynamo",
      "session_id": "DLIT82000",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Arun Raman",
          "title": "Sr. Solution Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Kyle Huang (WWFO)",
          "title": "Gen AI Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Utkarsh Uppal",
          "title": "Sr. Deep Learning Solutions Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Split and Win: Dynamo's Prefill-Decode Disaggregation and Smart KV Routing for Extreme LLM Throughput",
      "topic": "AI Inference",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit82000/"
    },
    {
      "description": "This session examines strategic capital deployment in AI and infrastructure. Panelists will explore trends in investment, collaboration models, and the evolving business landscape that supports scalable AI growth.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Strategic Capital Fuels AI Innovation",
        "Collaboration Models are Evolving",
        "The Business Landscape is Rapidly Shifting"
      ],
      "nvidia_technology": "NVIDIA NIM",
      "session_id": "S81777",
      "session_type": "Panel",
      "speakers": [
        {
          "company": "A16Z",
          "name": "Anjney Midha",
          "title": "General Partner"
        },
        {
          "company": "NVIDIA",
          "name": "Vishal Bhagwati",
          "title": "Head of Corporate Development"
        },
        {
          "company": "Global Infrastructure Partners (Blackrock)",
          "name": "Will Brilliant",
          "title": "Partner and Global Head of Digital Infrastructure"
        },
        {
          "company": "Thinking Machines Lab",
          "name": "Mira Murati",
          "title": "Founder and CEO"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Strategic Capital and the Evolution of the AI Ecosystem",
      "topic": "Code / Software Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81777/"
    },
    {
      "description": "NVIDIA Dynamo is a distributed inference serving framework built to run large language models efficiently, reliably, and at massive scale. It enables AI providers to reduce cost-per-token, meet strict service-level objectives, and scale deployments across large GPU clusters. In this interactive session, you’ll learn how Dynamo delivers high-performance distributed inference across frameworks such as SGLang, TensorRT-LLM, and vLLM. We’ll cover how to compose KV-aware routing, disaggregated serving, and KV offloading to achieve the most optimal throughput and latency given available GPU resources. Additionally, we will detail how to configure Dynamo deployment for Kubernetes (K8s) with topology-aware hierarchical gang scheduling, enable auto-scaling to adapt to real-time changes in traffic, and ensure reliability with fine-grained LLM fault tolerance.",
      "format": "In-Person",
      "industry": "Cloud Services",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Practical insights for deploying, tuning, and scaling LLMs in real-world environments, informed by production examples",
        "A deeper understanding of advanced inference techniques such as disaggregated inference, KV caching, and topology-aware scheduling",
        "Learn how to accelerate popular inference use cases such as agents, codegen, and multi-modality."
      ],
      "nvidia_technology": "Triton, Dynamo, NIXL",
      "session_id": "S81762",
      "session_type": "Tutorial",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Harry Kim",
          "title": "Principal Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Kyle Kranen",
          "title": "Team Lead/Manager - Deep Learning Algorithms"
        },
        {
          "company": "NVIDIA",
          "name": "David Lu",
          "title": "Sr. Solutions Architect"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Supercharge LLM Inference With Dynamo",
      "topic": "AI Inference",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81762/"
    },
    {
      "description": "Fast experimentation in feature engineering is essential to quickly discover the most valuable features that improve model performance. In this tutorial, we leverage NVIDIA cuDF and cuML libraries to accelerate the experimentation pipeline on GPUs with their zero-code change features, enabling faster feature engineering and quicker development of more accurate models.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "First, you'll learn feature engineering techniques using a publicly available product review dataset.",
        "Then, you'll train gradient boosted trees and support vector classification models with engineered features to evaluate their impact.",
        "Before you leave, you'll understand how engineered features can boost ML models’ accuracy, and gain practical skills for real-world use cases."
      ],
      "nvidia_technology": "CUDA, RAPIDS, CUDA-X, cuDF, cuML",
      "session_id": "DLIT81546",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Chris Deotte",
          "title": "Sr. Data Scientist"
        },
        {
          "company": "NVIDIA",
          "name": "Ronay Ak",
          "title": "Sr. Data Scientist"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Supercharge Tabular ML Models With GPU-Accelerated Feature Engineering",
      "topic": "Data Analytics / Processing",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81546/"
    },
    {
      "description": "As AI adoption accelerates across financial services, firms face a dual challenge: moving fast enough to stay competitive while meeting growing expectations for safety, resilience, and governance. The Financial Conduct Authority’s Supercharged Sandbox addresses this by enabling firms to develop and test advanced AI use cases using high-quality synthetic data, secure compute, and structured regulatory engagement. Drawing on insights from the first cohort, this session shares how firms experienced the sandbox and how it reshaped their approach to AI development and assurance. Firms tested complex models against realistic scenarios, explored cases difficult to observe in live data, and integrated governance earlier in the development life cycle. The session highlights practical lessons and design patterns relevant beyond financial services.",
      "format": "Virtual",
      "industry": "Financial Services",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "What we’ve learned from the first Supercharged Sandbox cohort from the participating firms",
        "How synthetic data are being used in practice to unlock innovation while preserving confidentiality and privacy",
        "Common design challenges firms face when scaling AI solutions, and how early regulatory insight helps address them",
        "How collaborative sandbox models can fuel innovation for the benefit of both regulators and the industry"
      ],
      "nvidia_technology": "Cloud / Data Center GPU",
      "session_id": "S82044",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Financial Conduct Authority",
          "name": "Colin Payne",
          "title": "Head of Department"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Supercharging Responsible AI Innovation: Compute, Synthetic Data, and Regulatory-Informed Design",
      "topic": "Code / Software Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82044/"
    },
    {
      "description": "Dive deep into the engineering of Swallow, Japan’s sovereign AI, where we trained from Qwen3 and GPT-OSS using Megatron-LM with custom datasets like SwallowMath-v2 and SwallowCode-v2. Learn how our pipeline of continual pre-training, supervised fine-tuning (SFT), and reinforcement learning (RL) achieves high Japanese proficiency while preserving the original models’ reasoning capabilities, offering a scalable framework for sovereign AI development in other non-English languages.",
      "format": "In-Person",
      "industry": "Public Sector and Sovereign AI",
      "intended_audience": "Research: Non-Academic",
      "key_takeaways": [
        "Leverage high-performance, scalable training techniques using Megatron-LM to adapt large open models like Qwen3 and GPT-OSS.",
        "Mitigate catastrophic forgetting by integrating high-quality self-developed math and code datasets during continual pre-training.",
        "Implement a robust post-training pipeline combining SFT and RL to construct reasoning models.",
        "Gain a reproducible blueprint for developing sovereign AI in non-English languages without compromising the base model’s English performance."
      ],
      "nvidia_technology": "CUDA, Infiniband Networking, Hopper, cuDDN, NCCL, NeMo, NSight Systems, NVLink / NVSwitch",
      "session_id": "S81710",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Institute of Science Tokyo",
          "name": "Kazuki Fujii",
          "title": "Graduate Student"
        },
        {
          "company": "Institute of Science Tokyo, Institute of Integrated Research, Supercomputing Research Center",
          "name": "Rio Yokota",
          "title": "Professor"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Swallow LLM: Continual Pre-Training and RL for Sovereign AI",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81710/"
    },
    {
      "description": "High-density AI workloads demand a paradigm shift, from siloed infrastructure to integrated ecosystems. This presentation reveals how a combined entity is leveraging its expanded scale and expertise to deliver the first truly total solution for next-gen data centers. We will explore how unifying compute, power, and liquid cooling into a single, validated solution stack eliminates integration risk, slashes deployment time, and ensures optimal performance for NVIDIA's GPU architecture. Learn how to transition from fragmented purchasing to a seamless, factory-integrated, plug-and-play AI infrastructure solution.",
      "format": "In-Person",
      "industry": "Cloud Services",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "The Power of One Entity: Understand how consolidating compute, power, and liquid cooling expertise under a single entity eliminates procurement complexity and deployment risk in high-density environments.",
        "Factory Integrated Synergy: See the blueprint for a validated, end-to-end stack, from the powershelf to liquid-cooling system, designed to meet peak performance for the most demanding AI workloads.",
        "Scale for Velocity: Discover how the new Compal's ecosystem and scale lead to time-to-market for next-gen accelerated compute infrastructure.",
        "Beyond the Chip: Learn why integrated thermal management is the key differentiator for scaling AI, enabling sustainable operation of racks above 100kW density."
      ],
      "nvidia_technology": "Cloud / Data Center GPU",
      "session_id": "EX82039",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "USA Products & Operations",
          "name": "John Leung",
          "title": "Sr. Director"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Synergy at Scale: Unify Compute, Power and Cooling (Presented by COMPAL)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82039/"
    },
    {
      "description": "Tabular data powers entire industries, but building state-of-the-art models for structured data at scale remains complex. In this session, we’ll unveil a comprehensive workflow for building transformer-based tabular foundation models. Each stage of our reference architecture is GPU-accelerated: tabular synthetic data generation, feature engineering and tokenization, training large-scale tabular transformers, and deploying inference pipelines. We apply this workflow to the real-world problem of fraud detection in credit card transactions, highlighting best practices and NVIDIA’s ecosystem for scalable tabular AI. Participants should be comfortable with basic Python and familiar with core AI/ML and Foundation Model concepts (tokenization, pre-training, fine tuning). Experience working with financial services data or workflows (e.g., banking, payments) is recommended but not required; the material is designed for technical practitioners such as data scientists, ML engineers, and developers.",
      "format": "In-Person",
      "industry": "Financial Services",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "Learn to leverage tabular foundation models for structured data across enterprise use cases.",
        "Apply synthetic data generation to tabular data using NeMo Data Designer.",
        "Train and scale transformer-based tabular models easily with NeMo Framework.",
        "Run highly optimized inference using Dynamo-Triton."
      ],
      "nvidia_technology": "RAPIDS, cuDF, cuML, NeMo, Triton, NVIDIA AI Enterprise",
      "session_id": "DLIT81818",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Benjamin Wu",
          "title": "Sr. Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Flora Huang",
          "title": "Sr. Solutions Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Tabular Foundation Models for Financial Services",
      "topic": "Data Analytics / Processing",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81818/"
    },
    {
      "description": "One size doesn’t fit all for inference, and black-box APIs break down when real workloads demand clear service-level agreements (SLAs) around latency, cost, reliability, and output quality. This session will walk you step by step through how you can choose the right inference stack for your use case, build it yourself, and operate it in production, using practical techniques to balance performance trade-offs across real-time workloads.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Why one-size-fits-all inference stacks fail for real workloads",
        "How to create an inference system that meets your needs",
        "Nuances of inference systems built for production scale",
        "How to tailor inference deployments to your use case",
        "What optimization techniques you can apply in-house"
      ],
      "nvidia_technology": "TensorRT, Triton, NVIDIA NIM",
      "session_id": "EX82207",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "Simplismart",
          "name": "Amritanshu Jain",
          "title": "Founder"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Tailor-Made Inference: Building SLA-Focused AI Stacks for Enterprise Demands (Presented by Simplismart)",
      "topic": "AI Inference",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82207/"
    },
    {
      "description": "Discover how Wayve is evolving embodied AI foundation models from frontier research into production-ready driving intelligence that can scale across vehicle platforms, markets, and real-world conditions. Learn how AV2.0 techniques in end-to-end learning, fleet-scale training, and mapless operation enable safe, adaptable autonomy for automotive applications and robotaxi services.",
      "format": "In-Person",
      "industry": "Automotive / Transportation",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Understand how embodied AI foundation models learn robust driving behaviors from large, diverse fleets for scalable autonomy."
      ],
      "nvidia_technology": "DRIVE, AGX",
      "session_id": "S82149",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Wayve",
          "name": "Alex Kendall",
          "title": "Co-Founder and CEO"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Take AV2.0 to the Next Level With End-to-End Embodied AI",
      "topic": "Autonomous Machines",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82149/"
    },
    {
      "description": "Large language models for code are transforming how developers create, maintain, and understand software—but building these models from the ground up requires the right tools and know-how. In this talk, we’ll explore how to leverage the NVIDIA NeMo framework and NVIDIA’s accelerated computing infrastructure to train state-of-the-art (SOTA) coding LLMs capable of handling multiple programming languages. Gain practical insights into dataset preparation, multilingual training challenges, and effective strategies for balancing diverse code sources. We’ll cover workflow design for large-scale distributed training, discussing optimization techniques and best practices for scaling across GPU clusters. The session will also highlight real-world applications of multilingual coding LLMs—from powering intelligent developer assistants to managing complex enterprise codebases.",
      "format": "Virtual",
      "industry": "All Industries",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "A practical understanding of how to use NVIDIA NeMo and NVIDIA infrastructure to train SOTA coding LLMs",
        "Insights into dataset preparation, multilingual training challenges, and strategies to overcome them",
        "Knowledge of optimization techniques and best practices for scaling training across large compute environments",
        "Awareness of real-world applications for multilingual coding LLMs, from developer assistance to enterprise codebase management",
        "A clear roadmap for experimenting with and deploying advanced coding LLMs using NVIDIA’s technology stack"
      ],
      "nvidia_technology": "DGX Platform, TensorRT, NeMo, Triton, NVIDIA NIM, NVIDIA AI Enterprise, DGX Cloud, Nemotron",
      "session_id": "S82306",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Meriem Bendris",
          "title": "Sr. Deep Learning Data Scientist"
        },
        {
          "company": "NVIDIA",
          "name": "Miguel Martinez",
          "title": "Sr. Applied Deep Learning Researcher"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Teach AI to Code in Every Language With NVIDIA NeMo",
      "topic": "Code / Software Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82306/"
    },
    {
      "description": "Key Audience - Machine learning practitioners interested in building large language models for code. - Data scientists exploring multilingual model training with state-of-the-art frameworks. - Developers and solution architects seeking to leverage NVIDIA’s ecosystem for LLM training (NeMo, GPUs, and supporting toolkits). Technical leaders and innovators aiming to understand how multilingual coding models can accelerate productivity and enterprise applications.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "A practical understanding of how to use NVIDIA NeMo and NVIDIA infrastructure to train SOTA coding LLMs.",
        "Insights into dataset preparation, multilingual training challenges, and strategies to overcome them.",
        "Knowledge of optimization techniques and best practices for scaling training across large compute environments.",
        "Awareness of real-world applications for multilingual coding LLMs, from developer assistance to enterprise codebase management.",
        "A clear roadmap for experimenting with and deploying advanced coding LLMs using NVIDIA’s technology stack."
      ],
      "nvidia_technology": "NVIDIA AI Enterprise",
      "session_id": "DLIT82292",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Miguel Martinez",
          "title": "Sr. Applied Deep Learning Researcher"
        },
        {
          "company": "NVIDIA",
          "name": "Meriem Bendris",
          "title": "Sr. Deep Learning Data Scientist"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Teaching AI to Code in Every Language with NVIDIA NeMo",
      "topic": "Code / Software Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit82292/"
    },
    {
      "description": "As data centers and high performance computing environments scale to unprecedented levels, traditional power distribution architectures face critical limitations. This session explores the next frontier in energy delivery: the 1 megawatt (MW) rack and the evolution beyond sidecar designs through advanced low-voltage direct current (LVDC) strategies. We will examine how 800 VDC optimizes efficiency, reduces losses, and supports the growing demands of AI workloads and hyperscale infrastructure. Gain insights into design considerations, safety standards, and integration approaches that enable sustainable, high-density power distribution for future-ready facilities. Join us to understand why 800 VDC is not just an alternative—it’s the foundation for powering the next generation of compute.",
      "format": "In-Person",
      "industry": "Energy",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "High-voltage direct current (HVDC) architectures are essential for meeting the massive power demands of AI workloads and hyperscale data centers. Moving beyond traditional sidecar designs unlocks the ability to support 1 MW racks efficiently.",
        "HVDC reduces conversion losses, improves energy efficiency, and minimizes infrastructure footprint—critical for sustainable, future-ready facilities. Learn how optimized HVDC strategies lower operational costs while supporting green initiatives.",
        "Implementing HVDC at scale requires careful attention to design considerations, safety standards, and integration approaches. These best practices ensure reliable, secure, and scalable power distribution for next-generation compute environments."
      ],
      "nvidia_technology": "",
      "session_id": "S82090",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Schneider Electric",
          "name": "Jim Simonelli",
          "title": "CTO, Data Center Business"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "The 1 MW Rack and Going Beyond the Sidecar: Optimizing 800 VDC for Power Distribution (Presented by Schneider Electric)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82090/"
    },
    {
      "description": "",
      "format": "In-Person",
      "industry": "Cloud Services",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "NVIDIA Nemotron 3 critical for IOCS and Platform Engineering in 2026",
        "NetApp data pipelines critical for IOCS and Platform Engineering to deliver NVIDIA Nemotron 3",
        "Vultr pioneers integrated platform services powered by NVIDIA GB300, Nemotron 3 and NetApp for agentic AI"
      ],
      "nvidia_technology": "",
      "session_id": "S82052",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Vultr",
          "name": "Kevin Cochrane",
          "title": "Chief Marketing Officer"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "The 2026 Infrastructure, Operations, and Cloud Services Agenda: Scale Agentic AI With NVIDIA Nemotron 3 (Presented by Vultr)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82052/"
    },
    {
      "description": "Agentic AI and generative AI are not just incremental upgrades—they represent a fundamental paradigm shift in healthcare, transforming myriad medical specialties in their own unique ways. From managing complex cancer pathways to scaling mental health support, autonomous AI agents are moving beyond simple data analysis to become proactive collaborators, reasoning, planning, and executing complex tasks with minimal human oversight. This transformation is making a demonstrable difference in saving patient lives and driving significant savings for hospitals and health systems. This session will bring together leaders in diverse specialty care—where the domain challenges are complex and distinct—to showcase real-world agentic AI implementations and quantified outcomes.",
      "format": "In-Person",
      "industry": "Healthcare & Life Sciences",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Evolution to Autonomy: Learn how agentic AI is transcending traditional predictive models to become a proactive system that reasons, plans, and executes clinical workflows with minimal human intervention.",
        "Specialty-Specific Applications:"
      ],
      "nvidia_technology": "Riva, RAPIDS, Metropolis",
      "session_id": "S81610",
      "session_type": "Lightning Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Brad Genereaux",
          "title": "Global Lead, Healthcare Alliances"
        },
        {
          "company": "Sword Health",
          "name": "Ricardo Rei",
          "title": "Head of AI Research"
        },
        {
          "company": "Maven Clinic",
          "name": "Jaya Savkar",
          "title": "SVP of Product"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "The Agentic Revolution for Specialty Care, Powered by AI for Better Lives and Bottom Lines",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81610/"
    },
    {
      "description": "Ubisoft recently unveiled Teammates, its first playable AI-based R&D project. This experience redefines NPC interaction by allowing players to lead a proactive, intelligent squad via real-time voice commands. In this session, the developers behind Teammates will detail their process for selecting and tuning AI models to maximize player immersion. They will also provide a technical deep dive into their deployment strategy for high-performance, real-time inference.",
      "format": "In-Person",
      "industry": "Gaming",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Ubisoft developers will present their proven methods for evaluating and specializing AI models for gaming.",
        "They will explain how they set up infrastructure in the cloud to support real-time, latency-critical inference, capable of scaling up to serve many thousands of players simultaneously.",
        "Get insights into building tooling that allows designers to iterate quickly on AI behavior and game logic.",
        "They’ll show how similar models can be run on gaming-spec PCs to achieve comparable response quality and performance without cloud costs."
      ],
      "nvidia_technology": "RTX GPU",
      "session_id": "S81739",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Ubisoft Paris",
          "name": "Joel Gregoire",
          "title": "Technical Director Software Engineering"
        },
        {
          "company": "Ubisoft Paris",
          "name": "Maxime Sazadaly",
          "title": "Technical Lead ML Engineering"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "The AI behind Teammates, Ubisoft's Experimental Interactive Characters",
      "topic": "AI Inference",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81739/"
    },
    {
      "description": "Learn how Medivis is revolutionizing surgical care with the University of Pittsburgh Medical Center (UPMC) and MAIA, their groundbreaking extended reality (XR) and AI platform. Discover how MAIA gives surgeons real-time, conversational access to vast medical data through an advanced retrieval-augmented generation system. We'll also showcase its powerful image recognition capabilities powered by NVIDIA Nemo Agent Toolkit and Cosmos, delivering the next generation of surgical precision and care.",
      "format": "In-Person",
      "industry": "Healthcare & Life Sciences",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Discover how Medivis leverages advanced XR visualization and agentic AI systems to equip doctors with precise surgical planning and in-situ guidance, dramatically improving patient outcomes.",
        "Learn why the NVIDIA NeMo Agent Toolkit and NVIDIA Cosmos are the critical building blocks enabling Medivis to construct a highly flexible and scalable AI infrastructure.",
        "Hear directly from Edward Andrews at UPMC, who describes Medivis as a \"disruptive technology\" that is fundamentally changing neurosurgery."
      ],
      "nvidia_technology": "Riva, NeMo, NVIDIA NIM, Blackwell, Cosmos, Blueprint",
      "session_id": "S82005",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Medivis",
          "name": "Osamah Choudhry",
          "title": "Co-Founder and CEO"
        },
        {
          "company": "University of Pittsburgh Medical Center (UPMC)",
          "name": "Edward Andrews",
          "title": "Neurosurgeon"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "The AI-Augmented Surgeon: Inside the Operating Room of the Future",
      "topic": "AR / VR AI",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82005/"
    },
    {
      "description": "The next generation of extended reality (XR) devices is set to revolutionize the hands-on workforce. In this session, learn how Ramblr, Convai, and GridRaster built intelligent AI agents that deliver real-time guidance, boosting worker productivity and safety.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "We’ll explore how to use the NVIDIA Nemo Agent Toolkit and NVIDIA Cosmos to create scalable agentic systems that run anywhere—from edge devices to the cloud.",
        "Understand why XR is the premier interface for interacting with digital twins in frontline environments.",
        "Discover how these XR systems create a powerful data flywheel, capturing the critical training data needed to power the autonomous robotics of tomorrow."
      ],
      "nvidia_technology": "CloudXR, Riva, NeMo, NVIDIA NIM, Blackwell, Cosmos, DGX Spark",
      "session_id": "S81621",
      "session_type": "Lightning Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Greg Barbone",
          "title": "XR Product and Partner Management"
        },
        {
          "company": "Convai",
          "name": "Purnendu Mukherjee",
          "title": "Founder and CEO"
        },
        {
          "company": "GridRaster",
          "name": "Bhaskar Banerjee",
          "title": "Co-Founder and CTO"
        },
        {
          "company": "Ramblr",
          "name": "Roman Hasenbeck",
          "title": "Co-Founder and CEO"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "The Augmented Workforce: Build and Deploy AI Agents for Enterprise XR",
      "topic": "AR / VR Development Tools",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81621/"
    },
    {
      "description": "This session explores the architectural foundations required to build and scale enterprise AI factories for LLMs, agentic AI, physical AI, and HPC workloads. We’ll outline the full-stack infrastructure and software requirements of an AI factory and demonstrate how NVIDIA’s NV-Certified systems, enterprise reference architectures, validated designs, and exemplar AI factories distill our learnings from real-world deployments into repeatable patterns that help system partners accelerate implementation with consistency, performance, and efficiency.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Understand the architectural pressures created by LLMs, agentic AI, physical AI, and HPC—and why the pivot from traditional data centers to purpose-built AI factory designs is now required.",
        "Learn the essential infrastructure and software components needed to build an AI factory optimized for performance, efficiency, and repeatability.",
        "See how NVIDIA simplifies AI factory design with prescriptive patterns that reduce risk and accelerate deployment.",
        "Understand how exemplar factories are engineered, why their design points matter, and how to align each factory type with specific AI workloads.",
        "Walk away with tangible tools, documentation, and actionable ideas you can use immediately with system partners and NVIDIA."
      ],
      "nvidia_technology": "BlueField DPU, Grace CPU, RTX GPU, HGX, Omniverse, Ethernet Networking, Hopper, MGX, CUDA-X, Multi-Instance GPU (MIG), NVLink / NVSwitch, RTX Virtual Workstations (vWS), NVIDIA NIM, Blackwell, NVIDIA AI Enterprise, Blueprint, NVIDIA Run:ai, Mission Control",
      "session_id": "S81851",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Peter Lillian",
          "title": "Director of Product Management"
        },
        {
          "company": "NVIDIA",
          "name": "Matthew Hull",
          "title": "VP, Global AI Solutions"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "The Builder’s Toolkit: Scaling Enterprise AI Factories",
      "topic": "AI Platforms",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81851/"
    },
    {
      "description": "Deploying image processing workloads on NVIDIA GPUs at near speed-of-light performance has required hand-tuning kernels using CUDA C++, given the unique nature of the loads used in semiconductor manufacturing. However, maintaining and debugging CUDA C++ code is challenging, often requiring significant re-tuning when adopting new GPU architectures. Tile-based programming, enabled by domain-specific languages (DSLs) such as NVIDIA cuTile and OpenAI Triton, offers a more portable and maintainable alternative while achieving performance close to hand-tuned CUDA C++. We'll evaluate tile-based programming for image processing workloads used in semiconductor manufacturing, comparing development effort and performance when porting operations from CUDA C++ to cuTile and Triton. We share insights from implementing GPU-accelerated algorithms, debugging, and optimizing performance in these DSLs.",
      "format": "In-Person",
      "industry": "Hardware / Semiconductor",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Tile-based programs achieve performance close to that achieved by thread-level programs written in CUDA-C++.",
        "The improved abstraction of tile-level program results in enhanced developer productivity, allowing the programmer to focus on high-level algorithmic decomposition of workloads while low-level mapping and optimizations are handled by the compiler.",
        "cuTile's raised level of abstraction and parametrization friendly constructs enables quicker iteration time for performance optimization and tuning.",
        "Block-based programming offers improved debuggability and code maintainability when compared to CUDA C++.",
        "Kernels in the tile abstraction do not need to be redesigned from scratch when adopting newer hardware features like TMA, unlike kernels written in CUDA C++ that require hand-tuning."
      ],
      "nvidia_technology": "CUDA, Blackwell",
      "session_id": "S81894",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "KLA",
          "name": "Pradeep Ramachandran",
          "title": "Director"
        },
        {
          "company": "KLA",
          "name": "Arjun Vadakkeveedu",
          "title": ""
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "The Case for Block-Based Programming With cuTile and Triton for Image Processing Workloads Used in Semi Manufacturing",
      "topic": "Performance Optimization",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81894/"
    },
    {
      "description": "The era of generative AI has mastered digital content, but the next frontier lies in the physical world. For developers comfortable with LLMs and retrieval-augmented generation (RAG), the leap into physical AI—robotics, autonomous systems, and edge computing—can feel like starting from zero. This session will bridge that gap. We will deconstruct physical AI workflows, showing you how to translate digital reasoning into physical action. We'll explore how to move beyond text-based inputs to incorporate multi-modal sensing and spatial intelligence, turning static models into interactive, real-world agents. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Understand the hierarchy from foundation models to simulation (NVIDIA Isaac/Omniverse libraries, Cosmos) to real-world deployment.",
        "Learn how vision language models (VLMs) enable machines to perceive and interpret their surroundings.",
        "Identify the low-barrier use cases (logistics, smart spaces, inspection) where developers can apply existing Gen AI skills today.",
        "Understand why simulation is the \"compiler\" for physical AI, and how to use it to shorten your development cycle."
      ],
      "nvidia_technology": "Jetson, Isaac, Omniverse, Metropolis, Cosmos, Blueprint",
      "session_id": "CWES81843",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Abubakr Karali",
          "title": "Sr. Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Stephanie Rubenstein",
          "title": "Sr. Product Marketing Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Chiara Refaeuter",
          "title": "Startups Partner Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Cobus Bothma",
          "title": "Sr. Product Manager"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "The Developer’s Roadmap to Physical AI: Bridging Digital Models and Real-World Systems",
      "topic": "Computational Imaging",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81843/"
    },
    {
      "description": "We present the first-ever global simulation of the full Earth system at an unprecedented 1 km resolution—a milestone recognized with the 2025 Gordon Bell Prize in Climate Modeling. Led by researchers from the Max Planck Institute for Meteorology and the German Climate Computing Center, this simulation captures the dynamic flow of energy, water, and carbon across the atmosphere, ocean, and land, unlocking new frontiers in climate science. The team harnessed the full computational power of JUPITER (the world’s largest GH200 Superchip installation) and ALPS systems, building on a long-standing collaboration with NVIDIA to advance the ICON codebase. The heterogeneous architecture, balancing Earth system components on both Grace CPUs and Hopper GPUs, enabled innovative optimization and acceleration techniques, reducing the code complexity by half while increasing performance and portability.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Research: Non-Academic",
      "key_takeaways": [
        "Breakthrough in high-performance computing for climate science",
        "Long-term collaboration is a game-changer",
        "Major advances in software efficiency and portability",
        "Exascale-class systems redefine climate simulations"
      ],
      "nvidia_technology": "Grace CPU, Hopper",
      "session_id": "S82185",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Max Planck Institute for Meteorology (MPI-M)",
          "name": "Daniel Klocke",
          "title": "Group Leader"
        }
      ],
      "technical_level": "General Interest",
      "title": "The Earth System at 1 km Resolution: Breaking Frontiers in Climate Science",
      "topic": "Climate / Weather / Ocean Modeling",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82185/"
    },
    {
      "description": "This session explores how deep, pre-validated collaboration is the only path to de-risking the AI factory. We will discuss how digital twin-driven design allows the ecosystem to visualize and solve thermal and power challenges before physical deployment, enabling a \"design once, deploy everywhere\" approach. We will examine how this co-creation strategy supports the data center as a unit of compute, ensuring that power, thermal, and IT systems operate as a single, synchronized organism, rather than disjointed components. Anchored in the Vertiv Frontiers 2026 trends, we’ll show how ecosystem-driven integration becomes mission-critical as architectures evolve. We’ll also cover how digital twins—using physically accurate simulation—enable teams to validate designs, failure scenarios, and performance before steel is cut, reducing schedule and operational risk.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Why the Old Model Is Dead: Understand why the vendor–buyer paradigm cannot meet the demands of AI-scale infrastructure.",
        "The New Unit of Compute: Learn why system-level integration—not individual components—defines success in the AI era.",
        "Co-Creation as a Strategy:"
      ],
      "nvidia_technology": "Cloud / Data Center GPU, Grace CPU, DGX Platform, Hopper, Blackwell",
      "session_id": "S82232",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Vertiv",
          "name": "Martin Olsen",
          "title": "VP, Segment Strategy and Deployment"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "The Ecosystem Engine: Co-Creating the AI Factory (Presented by Vertiv)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82232/"
    },
    {
      "description": "Physical AI and edge supercomputing are both concerned with the timely delivery and processing of physical measurements on GPU to build the future of autonomous decision-making across industries and domains—from scientific instruments to industrial robotics. During this talk, we will hear from leading sensor manufacturers who are building real-time AI solutions with NVIDIA and the Holoscan platform; they'll share their best practices and their vision, and highlight transformational work toward a future of AI-powered instruments.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "The world's leading sensor manufacturers are building on NVIDIA and Holoscan for real-time AI solutions.",
        "Physical AI starts where data is generated—the edge of the edge; a tight integration of data acquisition systems, sensors, networking, and GPUs is essential for autonomous systems and experiments.",
        "Edge computing can span from embedded computing (small size, weight, and power) to large-scale data center deployments—as long as there's a tight coupling between compute and the instrument.",
        "Edge supercomputing is an emerging domain where high-speed sensors need data center-like GPU resources and networking for real-time processing."
      ],
      "nvidia_technology": "CUDA, DOCA, TensorRT, Clara Holoscan, CUDA-X",
      "session_id": "S81525",
      "session_type": "Lightning Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Adam Thompson",
          "title": "Principal Technical Product Manager - Sensor Processing"
        },
        {
          "company": "National Instruments",
          "name": "John Ammerman",
          "title": "Distinguished Engineer"
        },
        {
          "company": "DECTRIS",
          "name": "Felix Bachmair",
          "title": "Director of Market Strategy and Innovation"
        },
        {
          "company": "Texas Instruments",
          "name": "Giovanni Campanella",
          "title": "Industrial Automation and Robotics GM"
        },
        {
          "company": "Analog Devices",
          "name": "Paul Golding",
          "title": "VP of Edge AI"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "The Edge of the Edge: Redefining GPU-Enabled AI Sensor Processing",
      "topic": "Signal & Sensor Processing",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81525/"
    },
    {
      "description": "生成AIは、基盤となる Transformer モデルから、複雑なマルチモーダルシステム、さらには自律的 Agentic AI へと急速な進化を遂げています。この変化に対応し、未来の AI を創出し続けるためには、AI スーパーコンピュータの設計を大きく変える必要がでてきています。本セッションでは、生成 AI 時代における、AI スーパーコンピューターのアーキテクチャの進化とそれを支える HW/SW の協調設計について説明します。 ・アーキテクチャの進化: 高度化する生成 AI がもたらす爆発的な計算需要に対し、AI スーパーコンピュータのアーキテクチャはどのように進化すべきかを説明します。具体的には、従来の「学習中心」のパラダイムを超え、大規模な学習と推論のハイブリッドワークロードをネイティブにサポートする「生成 AI 時代の AI スーパーコンピュータ」が不可欠となっており、それを実現するアーキテクチャの進化について説明します。 ・相乗効果を生むプラットフォームの協調設計: 高度な生成 AI の開発には、AI創出のエンジンである NVIDIA Blackwell GPU、それらを繋ぐ神経網としての Spectrum-X の高速なネットワークファブリック、それらを制御する AI 開発のオペレーティングシステムとしての NVIDIA NeMo や NVIDIA TensorRT 等の先進ソフトウェア群の三位一体の協調設計が重要になります。それらの技術と設計を具体的に説明すると共に、何故緊密に協調設計された AI スーパーコンピュータが、単なる選択肢ではなく、生成AI時代を勝ち抜くための「必須要件」であるのかを説明します。",
      "format": "Virtual",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "生成 AI 時代における AI の進化とそれに伴う AI スーパーコンピュータの設計のパラダイムシフトについて説明します。",
        "生成 AI 時代における学習・推論をハイブリッドに行う AI スーパーコンピュータのアーキテクチャ設計について説明します。",
        "NVIDIA Blackwell GPU と Spectrum-X による NW ファブリック等の先端ハードウェア群と NVIDIA NeMo 等の先進ソフトウェア群を用いた協調設計について掘り下げて説明します。"
      ],
      "nvidia_technology": "HGX, DOCA, TensorRT, Ethernet Networking, NCCL, NeMo, NVLink / NVSwitch, Blackwell",
      "session_id": "S81513",
      "session_type": "Talk",
      "speakers": [],
      "technical_level": "Technical - Advanced",
      "title": "The Generative AI Revolution: AI スーパーコンピュータ設計のパラダイムシフト",
      "topic": "Software-Defined Data Center",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81513/"
    },
    {
      "description": "Our workshop highlights proven strategies for tabular data developed by NVIDIA’s Kaggle grandmasters, who have earned top honors in hundreds of international data science competitions. You'll practice rapid electronic design automation (EDA), large-scale feature engineering, model building, ensembling, and pseudo-labeling — all accelerated with GPUs for faster experimentation and better accuracy. Basic Python knowledge and basic understanding of machine learning models and tabular data.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "Learn how Kaggle grandmasters consistently win tabular data Kaggle competitions.",
        "Learn how to conduct fast experimentation to improve your model accuracy.",
        "Learn how to explore and combine many diverse models.",
        "Learn advanced techniques of hill climbing, stacking, and pseudo labeling.",
        "Learn important EDA and feature engineering principles."
      ],
      "nvidia_technology": "CUDA, RAPIDS, CUDA-X, cuDF, cuML",
      "session_id": "DLIT81565",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Chris Deotte",
          "title": "Sr. Data Scientist"
        },
        {
          "company": "NVIDIA",
          "name": "Gilberto Titericz Junior",
          "title": "Sr. Data Scientist"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "The Kaggle Grandmasters Playbook: Battle-Tested Modeling Techniques for Tabular Data",
      "topic": "Performance Optimization",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81565/"
    },
    {
      "description": "NVIDIA has been at the forefront of innovation, consistently delivering top-notch solutions for various sectors. The NVIDIA RTX PRO 6000 Workstation, Data Center, Embedded, and Edge Computing solutions are no exception. These solutions leverage the power of NVIDIA GPUs to enhance performance, accelerate workflows, and provide scalable solutions that can adapt to a wide range of applications. Whether you're working on complex simulations, AI-driven data analysis, or developing cutting-edge embedded systems, NVIDIA has a solution tailored to your needs. The NVIDIA RTX PRO 6000 Workstation solutions are perfect for those in the creative and design industries, offering unparalleled graphics performance and real-time ray tracing capabilities.",
      "format": "In-Person",
      "industry": "Hardware / Semiconductor",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": ["Embedded System Solutions"],
      "nvidia_technology": "Cloud / Data Center GPU, RTX GPU, RTX Virtual Workstations (vWS), Blackwell, DGX Spark",
      "session_id": "S82137",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "PNY Technologies",
          "name": "WILLY ORTIZ",
          "title": "Sr. Field Sales Engineer Manager"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "The NVIDIA Solutions for Workstation/Data Center/Embedded and Edge Computing (Presented by PNY)",
      "topic": "AI Platforms",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82137/"
    },
    {
      "description": "Long-context AI conversations create a \"memory wall\" where GPU high-bandwidth memory becomes a bottleneck for scaling inference during agentic AI. This session shares a practical study of how NVIDIA Dynamo’s KV Block Manager (KVBM) accelerates large scale inference. We'll examine Dynamo’s four-tiered architecture (G1–G4) that places data in the right tier at the right time to balance higher concurrency, throughput, and cost. We’ll show how VAST Data as the G3 tier achieves maximum memory and storage efficiency to eliminate the memory wall for long-running agents. Finally we’ll dive into the new NVIDIA Inference Context Management Storage platform, which provides a new class of AI-native storage infrastructure designed for gigascale inference. You'll leave with blueprints for architecting secure, persistent memory for the next generation of agentic AI.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "A Practical Study of Intelligent Offloading: Deep dive into the architecture of the KVBM to understand how it intelligently batches asynchronous writes and demands bulk synchronous reads, enabling GPUs to break through memory limits without expensive recompute operations.",
        "The Arithmetic of \"Infinite\" Context: Learn the formula for accurately sizing infrastructure for large-scale inference deployments. We break down the relationship between concurrent users, context window size, and retention multipliers to plan for the massive storage footprint required by long-running agents.",
        "Achieving Line-Rate Throughput: Review real-world data where VAST saturates 200Gbps links at ~99% line rate during retrieval, ensuring maximal efficiency and a 20x improvement in time-to-first-token.",
        "Governing the \"KV Cache Era\": Address new data management challenges. We discuss best practices for encryption and handling sensitive user information contained within cached prompt history.",
        "Establish an Optimized Context Memory Tier: Augment existing networked object and file storage by holding latency‑sensitive, reusable inference context and prestaging it to increase GPU utilization. Get AI-native data services while enabling 5x higher tokens-per-second with 5x more power efficiency compared to traditional storage."
      ],
      "nvidia_technology": "BlueField DPU, Cloud / Data Center GPU, Grace CPU, DGX Platform, HGX, DOCA, Ethernet Networking, Hopper, Magnum IO, MGX, Multi-Instance GPU (MIG), NeMo, NVIDIA NIM, Blackwell, NVIDIA AI Enterprise, Dynamo, NIXL",
      "session_id": "S82255",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Vast Data, Inc.",
          "name": "Alon Horev",
          "title": "Co-Founder and VP, Technology"
        },
        {
          "company": "VAST Data",
          "name": "Anat Heilper",
          "title": "Director of AI Architecture"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "The Physics of Long-Context Inference: Breaking the Memory Wall With NVIDIA Dynamo (Presented by VAST Data)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82255/"
    },
    {
      "description": "The automotive industry is rapidly pivoting toward a future where intelligence inside the vehicle defines brand differentiation, customer experience, and long-term value. Breakthroughs in GenAI, cloud–edge orchestration, and accelerated computing are enabling automakers to deliver safer, smarter, and more intuitive experiences—turning every vehicle into a continuously improving digital product. In this panel we will share how leaders across different companies in the ecosystem - AI infrastructure, cloud, Tier-1 platforms, and next-generation EV OEMs - are shaping the in-vehicle GenAI landscape. The session offers a view of where the industry is headed and what it will take to bring truly intelligent, AI-powered vehicles to market at global scale.",
      "format": "In-Person",
      "industry": "Automotive / Transportation",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "The shift from voice assistants to intelligent copilots, capable of orchestrating navigation, media, vehicle control, and personalized services through large language and vision-language models.",
        "Hybrid cloud–edge AI architectures enabling reasoning, memory, and real-time multimodal perception inside the car.",
        "Leveraging AI factories to build, evaluate, and deploy customized agentic pipelines that deliver safer, more intuitive cockpit and ADAS experiences.",
        "Collaborative innovation across the automotive value chain, demonstrating how OEMs, cloud providers, and platform partners can jointly unlock new revenue streams and service models.",
        "What the next 2–5 years look like as vehicles evolve into continuously improving AI-powered products."
      ],
      "nvidia_technology": "DRIVE, CUDA, Riva, NeMo, NVIDIA NIM, Blackwell, NVIDIA AI Enterprise",
      "session_id": "S81724",
      "session_type": "Panel",
      "speakers": [
        {
          "company": "Microsoft",
          "name": "Raj Paul",
          "title": "Americas Mobility Industry Leader"
        },
        {
          "company": "Bosch",
          "name": "Auston Payyappilly",
          "title": "Director - Product Management & Acquisitions, Cockpit & ADAS compute/ECU"
        },
        {
          "company": "NVIDIA",
          "name": "Sri Subramanian",
          "title": "Global Head of Generative AI, Automotive"
        },
        {
          "company": "Lucid Motors",
          "name": "Thomas Evans",
          "title": "CAIO Head of Artificial Intelligence"
        },
        {
          "company": "Ford Motor Company",
          "name": "Bryan Goodman",
          "title": "Executive Director, Artificial Intelligence"
        },
        {
          "company": "Tensor",
          "name": "Jewel Li",
          "title": "Chief Operating Officer"
        }
      ],
      "technical_level": "General Interest",
      "title": "The Road to Intelligent Mobility: How In-Vehicle GenAI Is Redefining the AI-Defined Vehicle",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81724/"
    },
    {
      "description": "Open-source AI is evolving at record speed, reshaping how we train, refine, and deploy models across industries. Join leading builders as we explore the current state of open models and tools, the breakthroughs driving their momentum, and the new capabilities needed as AI becomes more specialized, multi-modal, and agent-driven.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Understand how today’s leading open-model teams train and optimize.",
        "Learn why openness accelerates trust, transparency, and real-world adoption."
      ],
      "nvidia_technology": "NeMo",
      "session_id": "S81791",
      "session_type": "Panel",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Vartika Singh",
          "title": "Strategic AI Lead"
        },
        {
          "company": "NVIDIA",
          "name": "Jonathan Cohen",
          "title": "VP of Applied Research"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "The State of Open-Source AI",
      "topic": "Pre-Trained / Foundation Models",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81791/"
    },
    {
      "description": "Visit us and discuss ways to get started or deepen your understanding of NVIDIA tools that support vision AI applications. Our topic experts are offering one-on-one dialogue on topics related to training, fine-tuning, and deploying a range of vision AI models. Ask about NVIDIA Metropolis, Cosmos Reason VLM, DeepStream, TAO, deploying on edge through cloud, and more. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Get started creating vision AI apps.",
        "Add agentic AI to existing computer vision pipelines.",
        "Our technical experts can help to unblock your build."
      ],
      "nvidia_technology": "Jetson, DeepStream, Metropolis, TAO Toolkit, Cosmos",
      "session_id": "CWES81733",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Neel Patel",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Khoa Ho",
          "title": "Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Chintan Shah",
          "title": "Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Sarah Todd",
          "title": "Metropolis Product Marketing Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Debraj Sinha",
          "title": "Product Marketing Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Sammy Ochoa",
          "title": "Technical Marketing Engineer"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "The Vision AI Toolbox",
      "topic": "Image / Video Detection & Recognition",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81733/"
    },
    {
      "description": "AI isn’t just about chatbots and automation—startups are finding unexpected, creative ways to apply it across industries you might not imagine. In this session, we’ll spotlight 25 innovative use cases that reveal the breadth and depth of what’s possible when bold founders meet cutting-edge technology. Get ready to expand your view of AI’s potential and discover opportunities you didn’t see coming.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Technical Inspiration: see 25 surprising ways startups are leveraging AI models, APIs, and frameworks in innovative ways.",
        "Broadened Problem-Solving Perspective: Learn how AI is being applied across diverse industries, giving you new approaches to tackle technical challenges.",
        "Ideas for Hands-On Experimentation: Walk away with concrete concepts and techniques you can explore in your own projects or prototypes."
      ],
      "nvidia_technology": "NVIDIA NIM",
      "session_id": "S81735",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Ashutosh Joshi",
          "title": "Inception Partner Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Chris Milroy",
          "title": "Engineering Manager"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Think You Know AI? 25 Startups Prove You Wrong",
      "topic": "Code / Software Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81735/"
    },
    {
      "description": "Bring your questions about performance portability, integration with existing frameworks, and migrating from traditional SIMT kernels for an in-depth discussion of when and how to use each model effectively. Connect with CUDA experts who can help you navigate the new generation of tile-based GPU programming models, including OpenAI’s Triton, NVIDIA’s CuTe DSL in CUTLASS, and the recently announced cuTile tile programming model for CUDA. Come explore how these array- and tile-centric abstractions automate memory movement, pipelining, and Tensor Core utilization while still letting you write high-performance kernels in Python or C++ for AI, scientific computing, and HPC workloads. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Understand how tile-based programming models like Triton, CuTe DSL, and cuTile differ from traditional SIMT CUDA, and when each approach is most appropriate for your workloads.",
        "Learn how array- and tile-centric abstractions can automate memory movement, pipelining, and Tensor Core utilization while still delivering near hand-tuned performance.",
        "See concrete examples of AI, HPC, and scientific kernels written in Triton, CuTe DSL, and cuTile, and compare their programmability and performance characteristics side by side.",
        "Get practical guidance on integrating these models into existing Python and C++ codebases, including interop with frameworks (PyTorch, JAX) and migration paths from legacy kernels."
      ],
      "nvidia_technology": "CUDA",
      "session_id": "CWES81591",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Bryce Lelbach",
          "title": "Principal Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Stephen Jones",
          "title": "CUDA Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Jared Roesch",
          "title": ""
        },
        {
          "company": "NVIDIA",
          "name": "Jaydeep Marathe",
          "title": "Principal Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Andy Terrel",
          "title": "CUDA Python Product Lead"
        },
        {
          "company": "NVIDIA",
          "name": "Rob Armstrong",
          "title": "CUDA Technical PM Lead"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Tile Programming for GPUs",
      "topic": "Programming Languages / Compilers",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81591/"
    },
    {
      "description": "Explore new techniques for top-k selection, a critical operation for accelerating mixture of experts and scientific computing that efficiently extracts the most relevant items from massive datasets. Dive into the design of our state-of-the-art GPU algorithm, AIR Top-K (now centralized in CUB), and learn how leveraging these primitives can accelerate high-throughput workloads by over 30x compared to previous methods.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Discover the AIR Top-K algorithm design, which utilizes kernel fusion and adaptive buffering to maximize throughput on the GPU.",
        "Learn how to leverage cub::DeviceTopK to deploy optimized, device-wide ranking in your applications without the need for custom kernel maintenance.",
        "Examine real-world case studies demonstrating over 30x speedups in high-throughput workflows.",
        "Gain insights into the broader applicability of top-k algorithms for critical workloads, including mixture of experts."
      ],
      "nvidia_technology": "CUDA",
      "session_id": "S81614",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Christina Zhang",
          "title": "DevTech Compute Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Yue Weng",
          "title": "DevTech"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Top-K Selection at the Speed of Light",
      "topic": "Data Analytics / Processing",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81614/"
    },
    {
      "description": "Learn how to develop contact‑rich manipulation policies in Isaac Lab, and deploy them using NVIDIA Isaac ROS. In this hands‑on lab, you will walk through a gear-assembly workflow that uses Isaac Lab to train an insertion policy, and then deploy it in ROS 2, together with perception models (segmentation and 3D pose estimation) and cuMotion for GPU‑accelerated planning. You'll see how to configure an Isaac for Manipulation workflow that begins with a simulated UR robot and depth cameras for training the policy, and then tune to achieve robust, repeatable sim‑to‑real performance on contact‑rich tasks. Basic understanding of ROS (Robot Operating System). Intermediate experience with Python, including working with packages, virtual environments, and basic scripting. Familiarity with 3D content concepts such as meshes, materials, transforms, and scene hierarchies from tools like Blender, Maya, or similar DCC applications. Understand command-line usage for running scripts and managing project files. Have familiarity with robotics or simulation workflows (for example, using simulation tools or working with physics-enabled assets).",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Use Isaac Lab to train contact-rich manipulation policies.",
        "Use Isaac ROS for high‑performance perception and manipulation packages and pipelines in ROS 2.",
        "Deploy an Isaac for Manipulation gear‑insertion policy in simulation.",
        "Plan and execute motions with CUDA‑X cuMotion for fast, collision‑aware manipulation.",
        "Learn how to achieve sim-to-real zero shot deployment independent of the runtime framework."
      ],
      "nvidia_technology": "Isaac",
      "session_id": "DLIT81808",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Ashwin Varghese Kuruttukulam",
          "title": "coming soon"
        },
        {
          "company": "NVIDIA",
          "name": "Ayusman Saha",
          "title": "coming soon"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Train and Deploy Contact-Rich Robot Manipulation Skills With Isaac Lab and Isaac ROS",
      "topic": "Robot Manipulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81808/"
    },
    {
      "description": "This workshop will help you get started with the Alpamayo suite of models, frameworks, and data for building reasoning-based autonomous vehicles. Learn how to run neural reconstructions on the AlpaSim simulator with the NVIDIA Physical AI Open Datasets and connect it to the Alpamayo 1 end-to-end stack. By the end of the session, you'll have hands-on experience running neural simulations and will be more familiar working with end‑to‑end stacks.",
      "format": "In-Person",
      "industry": "Automotive / Transportation",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Gain hands‑on experience running a neural reconstruction simulation in AlpaSim.",
        "Learn how to integrate AlpaSim with the Alpamayo 1 end-to-end stack.",
        "Practice manipulating agent positions and trajectories within the simulation environment."
      ],
      "nvidia_technology": "",
      "session_id": "DLIT82311",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Itai Zadok",
          "title": "Sr. Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Stefanie Manzinger",
          "title": "Sr. Solutions Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Train and Test End-to-End Autonomous Vehicles with Alpamayo",
      "topic": "Autonomous Machines",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit82311/"
    },
    {
      "description": "Learn how to reconstruct a large scene for robotics testing using NVIDIA Omniverse NuRec Gaussian-based reconstruction technologies to perform multi-GPU training, neural enhancement, object segmentation and extraction. This lab will walk through core concepts with a step-by-step workflow for data capture, reconstruction, generative enhancement, and object level-integration for robotics simulation in NVIDIA Isaac Lab.",
      "format": "In-Person",
      "industry": "Public Sector and Sovereign AI",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn how to reconstruct a large scene with multiple GPUs to test robots using NVIDIA Omniverse NuRec, fVDB, and Isaac Sim.",
        "Understand core reconstruction and rendering technologies with hands-on experimentation with object segmentation.",
        "Apply AI-based enhancement to the scene model to improve reconstruction quality.",
        "Learn how to utilize data collection to train 3D Gaussian splats, extract 3D objects, and convert to USD for robotics simulation in Isaac Lab."
      ],
      "nvidia_technology": "RTX GPU, Omniverse, Blackwell",
      "session_id": "DLIT81757",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Zoe LaLena",
          "title": "Solutions Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Training Lab: Advance World Simulation With 3D Gaussian Splatting for Large-Scale Environment Reconstruction",
      "topic": "Robotics Simulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81757/"
    },
    {
      "description": "Dive into the MiTAC R1917GC, featuring the NVIDIA Grace C1 CPU and PCIe Gen 5 I/O in an MGX 1U form factor. Discover how its industry-leading energy efficiency significantly reduces total cost of ownership (TCO) for edge data center workloads.",
      "format": "In-Person",
      "industry": "Cloud Services",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Understand why the shift to Arm-based servers (e.g., MiTAC R1917GC with NVIDIA Grace and Vera) is an essential, long-term industry trend for addressing soaring utility costs and achieving aggressive power usage effectiveness goals in modern data centers.",
        "Understand how Grace’s high-bandwidth LPDDR5 and fast on-chip fabric architecture delivers a superior TCO by reducing operating expenses while maintaining full system performance.",
        "Learn how the combination of power efficiency and PCIe Gen 5 I/O prepares data center infrastructure for the next generation of accelerated cloud applications, ensuring long-term technological competitiveness."
      ],
      "nvidia_technology": "Grace CPU",
      "session_id": "EX81895",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "MiTAC Computing Technology USA",
          "name": "Raymond Huang",
          "title": "General Manager"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Transform Data Center Economics With NVIDIA Grace- and Vera-Powered Edge Compute Servers (Presented by MiTAC)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex81895/"
    },
    {
      "description": "Discover how Oxa transforms both real-world data (real-to-real) and data from SIM (sim-to-real) leveraging NVIDIA Cosmos for industrial domains such as ports, airports, or solar farms. We'll showcase results, both qualitative and quantitative, from training downstream tasks such as perception on this transformed data.",
      "format": "Virtual",
      "industry": "Automotive / Transportation",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "Explore edge cases where sim-to-real is the only way to acquire data in restricted-access environments, such as ports and warehouses.",
        "See examples combining world models with autolabelling to further extend data in cases where neither real-world nor sim data exist to speed up R&D.",
        "Learn the benefits of multi-modal search, and why Oxa uses it as part of an extended pipeline.",
        "Understand how this unlocks and enables workflow integration for rapid deployment in the industrial space.",
        "See examples (with quantitative metrics) of domains where real-to-real greatly benefitted the robustness of visual perception at deployment time.",
        "Connect With the Experts",
        "Workshops, Training Labs & Certification",
        "Sponsors & Exhibitors",
        "Privacy Policy Your Pr"
      ],
      "nvidia_technology": "DRIVE, Cosmos",
      "session_id": "S82230",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Oxa Autonomy Ltd.",
          "name": "Horia Porav",
          "title": "Principal Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Transform Data for Industrial Mobility Automation",
      "topic": "Image Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82230/"
    },
    {
      "description": "Large-model technologies are rapidly transforming ecommerce. We'll discuss how we successfully overcome major challenges in large-scale data processing and training efficiency to develop a 245 billion-parameter mixture-of-experts (MoE) model that achieves state-of-the-art performance across Southeast Asian languages and key ecommerce tasks—a critical milestone for AI-native ecommerce. See how our model powers traffic distribution, search, content generation, and customer service to enhance efficiency and user experience, while our AI Merchant Bot improves seller efficiency and drives tangible growth. We'll highlight breakthrough search capabilities of large models in ecommerce search, including query rewriting, attribute enrichment, semantic retrieval, and multi-modal search, and how they significantly improve search quality and user intent understanding.",
      "format": "In-Person",
      "industry": "Retail / Consumer Packaged Goods",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn practical strategies for building and optimizing large-scale, region-specific language models.",
        "Discover how advanced AI models can transform ecommerce traffic distribution, enhancing the efficiency and user experience of search, recommendation, and advertisement.",
        "Gain insights into how AI is helping ecommerce customer service, merchant workflows, and improving the efficiency of ecommerce operations."
      ],
      "nvidia_technology": "TensorRT, NCCL, NeMo, Blackwell",
      "session_id": "S81452",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Shopee",
          "name": "Peter Zhang",
          "title": "Sr. Principal Engineer"
        },
        {
          "company": "Shopee",
          "name": "Rex Zeng",
          "title": "Director"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Transform Shopee Ecommerce Using a Southeast Asia-Tuned LLM",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81452/"
    },
    {
      "description": "The difference between a stalled AI pilot and enterprise-wide AI agent adoption is often your AIOps platform. NVIDIA Enterprise AI Factory ecosystem partners will teach you how an end-to-end AIOps stack unifies observability, keeps efforts focused, and turns noisy logs into reliable performance. Learn best practices from NVIDIA's IT team and key partners on how to build and deploy your own AI factory.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Understand the key features of an end-to-end AIOps stack on NVIDIA Enterprise AI Factory.",
        "Learn how to connect observability, MLOps, and agentic workflows into a single, coherent platform.",
        "Select best-in-class ecosystem partners and codesign to align roadmaps to enterprise requirements.",
        "Learn how to turn unstructured data (logs, tickets, documents) into high-quality datasets for evaluation-driven development."
      ],
      "nvidia_technology": "NeMo, NVIDIA NIM, NVIDIA AI Enterprise, NVIDIA Run:ai, Nemotron",
      "session_id": "S82078",
      "session_type": "Tutorial",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Meghana Puvvadi",
          "title": "Director of Engineering, AI/ML Enterprise"
        },
        {
          "company": "NVIDIA",
          "name": "Nic Borensztein",
          "title": "Principal Solutions Architect"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Transform Your Organization With End-to-End AIOps",
      "topic": "InfraOps",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82078/"
    },
    {
      "description": "AI networking is at a turning point as performance and security demands surpass traditional infrastructure limits. Cisco and NVIDIA tackle bandwidth, latency, and security challenges head-on with innovations in silicon, systems, software, and optics. Scale fabric architectures to maximize AI workloads, ensuring efficient expansion and secure data in motion. Explore validated architectures that simplify deployment, optimize GPU use, and future-proof AI infrastructure. Chart your path to breakthroughs.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Turning AI networking challenges to opportunities with Cisco & NVIDIA",
        "Strategically scale AI workloads with the Cisco Nexus operating model",
        "Protect your AI data-in-motion with no disruption and real-time security",
        "Simplify AI deployment with proven, validated architectures",
        "Build faster, smarter, security infused, future-ready AI networks"
      ],
      "nvidia_technology": "",
      "session_id": "S82187",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Cisco",
          "name": "Will Eatherton",
          "title": "SVP, Networking Engineering"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Transforming Bottlenecks into Breakthroughs: Scaling Secure AI Networks for AI Factories (Presented by Cisco Systems)",
      "topic": "AI Networking, 5G/6G",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82187/"
    },
    {
      "description": "Bringing AI into the physical world is a massive challenge. Industrial environments are complex and noisy, yet businesses demand accurate forecasts from thousands of assets. Join NVIDIA and Cognite to see how we solve this. We’ll take a deep dive into the NVIDIA NV-Tesseract family of models and Cognite Data Fusion, focusing on how to deploy foundational time series models in the industrial world to identify process deviations, and potential reliability issues. You'll learn: • How to integrate NV-Tesseract with Cognite’s Industrial Knowledge Graph to scale across thousands of sensors. • Real-world case studies of customers applying these capabilities in production. • Benchmarks showing how GPU-accelerated analysis delivers faster insights. • Discover how to reduce costs, prevent disruptions, and free your teams from manual firefighting to focus on high-value innovation.",
      "format": "In-Person",
      "industry": "Manufacturing",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "Prove Business Value: Review real-world production case studies and performance benchmarks demonstrating how foundational time-series models reduce costs and prevent operational disruptions.",
        "Master the Integration: Learn how to deploy NVIDIA NV-Tesseract models within Cognite’s Industrial Knowledge Graph to scale accurate forecasting across thousands of complex sensor streams.",
        "Scale with Ease: Learn how to seamlessly deploy forecasting and anomaly detection across your entire asset portfolio."
      ],
      "nvidia_technology": "NVIDIA NIM",
      "session_id": "S81871",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Cognite",
          "name": "Peet Cremer",
          "title": "Principle AI architect"
        }
      ],
      "technical_level": "General Interest",
      "title": "Transforming Industrial Ops: Real-Time Forecasting and Anomaly Detection With Foundational Time-Series Models at Scale",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81871/"
    },
    {
      "description": "Explore cutting-edge methods for accelerating large language model (LLM) inference with NVIDIA technologies. This interactive session dives into deploying and scaling models using TensorRT-LLM with PyTorch integration, alongside modern speculation and quantization techniques that boost efficiency. Learn from NVIDIA experts how to fine-tune inference pipelines, optimize GPU performance, and build scalable, high-throughput deployments across advanced systems like the GB200 NVL72. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn advanced LLM inference optimization techniques using TensorRT-LLM, quantization (FP8/NVFP4), and speculation methods like Medusa and Eagle.",
        "Discover how to scale and orchestrate LLM serving with NVIDIA Dynamo and disaggregated architectures on platforms like GB200 NVL72.",
        "Gain practical insights into maximizing throughput and minimizing latency with custom attention and GEMM kernels in frameworks such as vLLM and SGLang."
      ],
      "nvidia_technology": "CUDA, TensorRT, Hopper, cuDDN, Blackwell",
      "session_id": "CWES81679",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Haohang Huang",
          "title": "Sr. AI DevTech Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Nikita Korobov",
          "title": "AI Developer Technology engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Martin Marciniszyn Mehringer",
          "title": "Sr. AI Developer Technology Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Robin Kobus",
          "title": "Developer Technology Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Julien Debache",
          "title": "AI Developer Technology Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Turbocharge Your LLM Inference: Expert Strategies for Lightning-Fast, Scalable AI Deployment",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81679/"
    },
    {
      "description": "Enterprises sit on massive data estates, often 100 exabytes or more of unstructured information. Yet 95% of AI pilots fail to reach production. The problem isn't a lack of data. It's that the data aren't AI-ready. In this session, discover how NetApp and NVIDIA are transforming enterprise data infrastructure into an AI-ready data platform that fuels AI factories. Learn how NetApp AI Data Engine, designed on NVIDIA AI Data Platform, refines raw enterprise data into AI-ready intelligence. See how NetApp AFX and validated AIPod configurations deliver the performance foundation your AI factory demands. The result is agentic AI that can truly understand and unlock the value hidden across your business.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Your data estate is untapped fuel for AI.",
        "AI factories need a data supply chain, not just storage.",
        "From AFX to AIDE, NetApp brings AI to your data."
      ],
      "nvidia_technology": "DGX Platform, HGX, NeMo, NVIDIA NIM, Blackwell, NVIDIA AI Enterprise",
      "session_id": "S82103",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NetApp",
          "name": "Mackinnon Giddings",
          "title": "AI Solutions Marketing Manager"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Turn Your Data Estate Into AI Fuel (Presented by NetApp)",
      "topic": "Data Storage",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82103/"
    },
    {
      "description": "As AI and data center infrastructure become powerful engines of economic growth, the next competitive edge is for telecom operators to plug sovereign AI directly into their national networks. In this session, you’ll get an inside look at how Nscale is partnering with telecom service providers in the U.K. to deploy full-stack NVIDIA-powered AI Infrastructure on existing telecom sites across the country, bringing AI closer to users.",
      "format": "In-Person",
      "industry": "Telecommunications",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Understand how coupling AI data centers with telecom networks drives national-scale economic benefits and new AI services.",
        "Learn how bringing compute closer to the network edge reduces latency, improves security for sensitive sovereign workloads, and increases power and spectrum efficiency.",
        "See how a deployment with a major U.K. operator (to be named in early 2026) can support both training and low-latency inference across consumer, enterprise, and public-sector use cases.",
        "Gain a practical checklist for telcos and cloud providers to assess sites, networking stacks, and organizational readiness for AI-era infrastructure."
      ],
      "nvidia_technology": "Hopper, Blackwell",
      "session_id": "S82173",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Nscale",
          "name": "Tom Burke",
          "title": "Chief Revenue Officer"
        }
      ],
      "technical_level": "General Interest",
      "title": "Turning Telecom Networks Into Sovereign AI-Ready Platforms (Presented by Nscale)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82173/"
    },
    {
      "description": "In this lab, we'll walk through large-scale training and deployment of a well-known mixture-of-experts (MoE) LLM. You'll learn how to scale MoE pre-training by composing several parallelism techniques (FSDP, Tensor Parallel, Pipeline Parallel), memory saving techniques (Activation Checkpointing, CPU Offloading), and torch.compile driven optimizations. We'll use TorchTitan as our pre-training framework. We'll also demonstrate how to deploy such a model using the vLLM framework. You'll learn how to apply CUDA-accelerated inference-specific optimizations available in vLLM. Knowledge of MoE LLM models like DeepSeek-V3 or LLAMA4-Scout. Knowledge of system concepts like docker, CUDA, distributed processes.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "TorchTitan demonstrates how PyTorch optimizations compose with each other for pre-training at scale.",
        "Fully Sharded Data Parallelism (FSDP) can be used to train billion-parameter models over a large number of GPUs.",
        "Different forms of parallelism (Tensor parallel, expert parallel, pipeline parallel, etc.), and memory-saving techniques like activation checkpointing can be composed together for scaling pre-training to 1,000+ GPUs.",
        "Torch.compile-based workflows can generate fast kernels and optimize a distributed model.",
        "Custom kernels (through FlashInfer, OAI Triton, CUTLASS) in vLLM enable high-throughput and memory-efficient inference for LLMs."
      ],
      "nvidia_technology": "BlueField DPU, Cloud / Data Center GPU, Grace CPU, DGX Platform, CUDA, Infiniband Networking, Ethernet Networking, Hopper, cuBLAS, CUDA-X, cuDDN, cuFFT, DALI, Interconnect Networking, NCCL, NSight Comute, NSight Systems, NVLink / NVSwitch, Triton, Blackwell, NVIDIA AI Enterprise, DGX Cloud, DGX Station",
      "session_id": "DLIT82021",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Syed Ahmed",
          "title": "Sr. Software Engineer"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Ultra Scale Runbook for PyTorch on NVIDIA GPUs for Training and Inference",
      "topic": "Deep Learning Frameworks",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit82021/"
    },
    {
      "description": "This talk presents a DeepResearch-powered long-context framework for financial multi-modal conversational models, two specialized financial benchmarks (BizFinBench and MME-Finance), the any-to-any omni architecture for real-time cross-modal agentic interactions, and a general evaluation engine for standardized AI performance evaluation across key capabilities.",
      "format": "In-Person",
      "industry": "Financial Services",
      "intended_audience": "Research: Academic",
      "key_takeaways": [
        "Learn about a unified DeepResearch-based framework empowering long-context understanding and reasoning across text, vision, and audio modalities in financial scenarios.",
        "Get insights into GAGE, a unified extensible evaluation engine to standardize, automate, and quantify AI performance in reasoning, multi-modal understanding, and agentic task execution.",
        "Discover any-to-any omni architecture that enables seamless, real-time full-duplex interaction across multiple modalities for agentic financial AI applications."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, DRIVE",
      "session_id": "S81729",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Tonghuashun",
          "name": "Wenchen Li",
          "title": "Product Director"
        },
        {
          "company": "Ainvest",
          "name": "Adam Wang",
          "title": "CEO"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Unified Architectures and Evaluation Frameworks for Financial AI Agents",
      "topic": "Retrieval-Augmented Generation (RAG)",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81729/"
    },
    {
      "description": "Join us for an interactive session and engage directly with NVIDIA's networking experts. We'll cover our full-stack networking platform, including NVLINK, Spectrum-X Ethernet, InfiniBand, and BlueField DPUs, and networking software like Cumulus. Learn how these technologies accelerate every phase of AI, from training to inference, and how tools like NVIDIA AIR can help you validate your infrastructure. This session is designed to answer your specific questions and provide the strategic insights you need to unleash your AI at scale. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Get a practical look at NVIDIA’s end-to-end networking stack — NVLINK, Spectrum-X Ethernet, InfiniBand, BlueField DPUs, and Cumulus — and how these pieces fit together to drive high-performance AI infrastructure.",
        "The session is built around attendee questions, offering strategic guidance tailored to real deployment needs — helping you and your organization chart a confident path to AI at scale.",
        "The experts will break down how each element of networking technology boosts training, fine-tuning, and inference efficiency, helping teams remove bottlenecks and scale their AI workloads more predictably."
      ],
      "nvidia_technology": "BlueField DPU, Rivermax, Infiniband Networking, Ethernet Networking, Cumulus, Magnum IO, Interconnect Networking, NCCL",
      "session_id": "CWES81447",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Taylor Allison",
          "title": "Sr. Networking Product Marketing Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Scot Schultz",
          "title": "Sr. Director, HPC and Technical Computing"
        },
        {
          "company": "NVIDIA",
          "name": "David Iles",
          "title": "Sr. Director, Ethernet Switching"
        },
        {
          "company": "NVIDIA",
          "name": "Moshe Lavi",
          "title": "Senior Director of Product Management"
        },
        {
          "company": "NVIDIA",
          "name": "Avi Alkobi",
          "title": "Sr. Director of Business Development and Technology Alliances"
        },
        {
          "company": "NVIDIA",
          "name": "Ranga Maddipudi",
          "title": "Director of Product Management, Networking"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Unleashing AI at Scale: Meet the NVIDIA Networking Experts",
      "topic": "AI Networking, 5G/6G",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81447/"
    },
    {
      "description": "This session will explain how KV cache technology improves AI inference performance and scalability. CXL memory can play a transformative role by expanding local and shared infrastructure to save processed results, enabling GPUs to focus on more high-value tasks. Learn how sharing cached work across multiple GPUs not only accelerates response times, but also unlocks new functionalities, driving higher system efficiency and reducing the total cost of inference. Attend this talk to explore the future of AI inference optimization through the benefits of KV cache.",
      "format": "In-Person",
      "industry": "Hardware / Semiconductor",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Scale Model Serving Efficiently: By decoupling KV cache from GPU memory, organizations can run larger models or more concurrent queries on existing GPU infrastructure.",
        "Increase Throughput for Real-World Workloads: Centralized KV storage frees GPUs to focus on compute and higher-level tasks, improving overall system utilization and accelerating token-per-second performance.",
        "Lower Inference Cost at Scale: Shared CXL memory appliances provide a cost-efficient alternative, delivering high performance for less hardware investment.",
        "Future-Proof Your Inference Architecture: A KV cache server allows infrastructure teams to adopt a modular, flexibly scalable architecture aligned with next-generation CXL-enabled AI deployments."
      ],
      "nvidia_technology": "RTX GPU, Blackwell",
      "session_id": "EX82068",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "Penguin Solutions",
          "name": "Phil Pokorny",
          "title": "CTO"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Unlock AI Performance: Three Ways CXL-Powered KV Cache Improves Inference Efficiency (Presented by Penguin Solutions)",
      "topic": "ModelOps",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82068/"
    },
    {
      "description": "Financial applications demand high precision and ultra-low latency. Learn firsthand from engineering leaders at Capital One how they're building distributed data pipelines to curate high-quality datasets that enable laser-focused foundation models for financial applications.",
      "format": "In-Person",
      "industry": "Financial Services",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "Discover best practices for curating high-quality datasets that boost the accuracy and reliability of foundation models in financial applications.",
        "Learn to deploy scalable, distributed data pipelines using RAY across multi-node and multi-GPU setups to handle massive datasets efficiently.",
        "Learn how to drive substantial benefits through acceleration of distributed Ray Tune workflows on NVIDIA GPU from engineering leaders at Capital One."
      ],
      "nvidia_technology": "CUDA-X",
      "session_id": "S81826",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Roman Yokunda Enzmann",
          "title": "Solutions Architect"
        },
        {
          "company": "Capital One",
          "name": "Brian Nguyen",
          "title": "Lead Software Engineer"
        },
        {
          "company": "Capital One",
          "name": "Nick Resnick",
          "title": "Lead AI Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Unlock Efficiency for Financial Agents With Scalable Data Curation",
      "topic": "Training AI Models",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81826/"
    },
    {
      "description": "Running interactive SQL at scale is still far slower, and more expensive, than it should be. This session explores how GPU acceleration fundamentally changes that equation. We’ll dive into open-source community work speeding up the popular open data lakehouse engine Presto—work that required rethinking not just the core execution engine, but also the surrounding system components that drive performance at scale. We'll walk through benchmark results, lessons from real enterprise deployments, and the architectural details that actually matter in practice. You'll leave with concrete guidance for GPU-accelerating your own data processing workloads to achieve better performance at lower cost.",
      "format": "In-Person",
      "industry": "Financial Services",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "GPU-accelerated data processing has better performance at lower cost.",
        "Presto GPU delivers excellent performance on standard benchmarks and production workloads.",
        "The Presto GPU software stack is open source and easy to get started."
      ],
      "nvidia_technology": "CUDA, Infiniband Networking, Ethernet Networking, Magnum IO, Interconnect Networking, NSight Systems, nvCOMP",
      "session_id": "S81563",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Greg Kimball",
          "title": "Software Engineering Manager"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Unlock Fast, Cost-Effective Interactive Analytics on Massive Data Lakehouses",
      "topic": "Databases",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81563/"
    },
    {
      "description": "Delivering peak performance and stability in large-scale AI infrastructure is increasingly challenging as workloads and hardware continuously evolve. This requires rapid, end-to-end benchmarking and performance debugging across every layer of the hardware and software stack. Join this interactive session to connect with NVIDIA experts and learn about the benchmarks, tools, and methodologies we use internally at NVIDIA — and make available to you — to measure, optimize, and operate AI infrastructure at scale. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Benchmarks and Tools: Discover a range of NVIDIA benchmarks and tools to measure and troubleshoot performance for both on-premises and cloud infrastructure.",
        "End-to-End Benchmarking: Learn how to validate performance throughout the life cycle — from initial deployment and acceptance testing to production operations.",
        "Performance Optimization: Understand strategies to identify and eliminate bottlenecks across compute, networking, and storage layers.",
        "Best Practices: Gain insights and lessons learned from some of the largest AI infrastructure deployments."
      ],
      "nvidia_technology": "",
      "session_id": "CWES81692",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Giovanni Mascari",
          "title": "Sr. Solutions Architect - HPC & AI"
        },
        {
          "company": "NVIDIA",
          "name": "Vasileios Karakasis",
          "title": "Sr. Systems Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Sriharsha Niverty",
          "title": "Systems Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Michal Marcinkiewicz",
          "title": "Engineering Manager, Deep Learning Algorithms"
        },
        {
          "company": "NVIDIA",
          "name": "Mohak Chadha",
          "title": "Solutions Architect"
        },
        {
          "company": "NVIDIA",
          "name": "Pramod Kumbhar",
          "title": "Sr. Solutions Architect - HPC & AI"
        },
        {
          "company": "NVIDIA",
          "name": "Patrick Atkinson",
          "title": "Sr. Developer Technology Engineer"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Unlock Peak Performance and Stability: End-to-End Benchmarking of AI Infrastructure",
      "topic": "InfraOps",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81692/"
    },
    {
      "description": "Discover how to resolve the trade-off between computational speed and model complexity in financial portfolio management. This session combines NVIDIA’s Quantitative Portfolio Optimization developer example with exclusive insights from Kendall Square Capital, demonstrating how to accelerate large-scale, complex portfolio optimization problems without compromising result quality. Learn how to formulate data-driven models to leverage cuOpt and CUDA-X Data Science libraries to unlock accelerations up to triple-digit speed-ups compared to CPU-based solutions. Gain insights into how these innovations transform portfolio optimization from a slow batch process into a dynamic workflow, unlocking new frontiers in automated investing research and real-time rebalancing. Python programming, basic knowledge of numerical optimization",
      "format": "In-Person",
      "industry": "Financial Services",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "GPU-Accelerated Libraries: Learn how NVIDIA cuOpt and the CUDA-X Data Science libraries accelerate each component of the portfolio optimization pipeline.",
        "Customer Spotlight – Kendall Square Capital & cuFolio: Hear how Kendall Square Capital uses cuFolio to accelerate large, complex optimization problems, ensuring zero compromise on solution quality while drastically reducing compute time.",
        "Backtesting at Scale: Discover how GPU acceleration enables near real-time strategy backtesting, allowing quant researchers and traders to iterate on strategies in minutes, rather than days.",
        "Advanced Risk Modeling: Dive into the mathematical foundations of portfolio optimization, focusing on Conditional Value at Risk (CVaR) risk measure to better capture tail-risk scenarios, and learn how to transform it into a computationally tractable formulation.",
        "Performance Benchmarking: Review benchmark results across NVIDIA H100, H200, B200, RTX 6000D, and DGX Sparks to understand how performance scales across varying problem sizes and GPU architectures."
      ],
      "nvidia_technology": "CUDA, CUDA-X, cuDF, cuML, cuOPT",
      "session_id": "DLIT81764",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Ioana Boier",
          "title": "Global Head of Capital Markets Strategy"
        },
        {
          "company": "NVIDIA",
          "name": "Peihan Huo",
          "title": "Solution Architect"
        },
        {
          "company": "Kendall Square Capital",
          "name": "Jianchi Chen",
          "title": "Chief Investment Officer"
        },
        {
          "company": "NVIDIA",
          "name": "Francisco Zhao",
          "title": "Senior Solutions Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Unlock Real-Time Financial Decisions With GPU-Accelerated Portfolio Optimization",
      "topic": "Accelerated Computing Libraries",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81764/"
    },
    {
      "description": "In this session, we showcase a real-world national deployment that enables confidential retrieval-augmented generation (RAG) and AI inference on protected data—without exposing those data to cloud operators, infrastructure administrators, model providers, or third-party vendors. Learn how NVIDIA Confidential Computing, along with the combined technology from OPAQUE’s Confidential AI Platform and the TII, the applied research arm of the Advanced Technology Research Council (ATRC), establishes hardware-backed trusted execution environments that deliver cryptographic guarantees of data confidentiality, integrity, and verifiable governance across the entire AI life cycle. This architecture enables real-time AI inference and analytics directly on encrypted data, unlocking high-value AI use cases while preserving zero-trust security boundaries.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Industries across the public and private sectors face a fundamental challenge with AI: extracting value from highly sensitive, regulated data while meeting strict privacy, security, and data-sovereignty requirements.",
        "Learn how to securely deploy AI to deliver world-class insights and deliver better outcomes across public health, financial services, and other industries that leverage sensitive, private data.",
        "Learn how to build cloud-scale confidential RAG and inference pipelines while maintaining complete control over sensitive data.",
        "Enhance outcomes through improved delivery of public services, while securely enabling research into regional population data while preserving privacy across the data pipeline.",
        "Understand the end-to-end architecture supporting sovereign large language models and domain-specific AI. Organizations retain full control of their encryption keys while securely collaborating on shared AI workloads, and cryptographic audit trails provide provable compliance for regulators and security teams."
      ],
      "nvidia_technology": "Triton, NVIDIA NIM, Blackwell",
      "session_id": "S82299",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Advanced Technology Research Council",
          "name": "Najwa Aaraj",
          "title": "CEO, Technology Innovation Institute"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Unlock Sovereign AI With Confidential RAG and Secure Inference for Regulated Data",
      "topic": "Confidential Compute",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82299/"
    },
    {
      "description": "In next-generation AI infrastructure, the data layer determines how efficiently GPUs run, how much energy and cooling are consumed, and how fast retrieval-augmented generation (RAG) pipelines and multi-modal workloads can operate. This session reveals why storage and data architecture built from the NVIDIA AI Data Platform reference design—including NVIDIA AI Enterprise—are the key drivers for AI factory success. Learn the core design patterns and pitfalls that decide whether turnkey, large-scale AI runs at full speed or stalls.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Dev / IT Operations",
      "key_takeaways": [
        "Discover why data, not compute, is the key to AI factory success.",
        "Learn why a turnkey, integrated platform built on NVIDIA's AIDP reference design is the fastest way to AI business outcomes.",
        "See a RAG pipeline in action with a demo featuring NVIDIA AI Enterprise."
      ],
      "nvidia_technology": "BlueField DPU, Cloud / Data Center GPU, Grace CPU, RTX GPU, DGX Platform, CUDA, DOCA, Ethernet Networking, Hopper, NeMo, NVIDIA NIM, Blackwell, NVIDIA AI Enterprise, cuVS, Dynamo, Nemotron",
      "session_id": "S82241",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Jacob Liberman",
          "title": "Director, Enterprise Product"
        },
        {
          "company": "DDN",
          "name": "James Coomer",
          "title": "Sr. VP, Products"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Unlocking Data at Scale for AI Factories (Presented by DDN)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82241/"
    },
    {
      "description": "AI is accelerating breakthroughs across scientific domains, transforming complex data into actionable insights for discovery and design. This session explores how agentic AI systems are being engineered to drive innovation—from decoding biological mechanisms for therapeutic science to modeling quantum matter for next-generation materials. Two visionary thought leaders will share how their research teams are applying robust, domain-specific AI frameworks to reshape the future of their fields.",
      "format": "In-Person",
      "industry": "Academia / Higher Education",
      "intended_audience": "Research: Academic",
      "key_takeaways": [
        "AI as a Hypothesis Generator: AI agents can move beyond pattern recognition to generate novel, testable hypotheses in complex biological systems.",
        "Teaching AI to \"Think\" Like a Physicist: Pioneering methods to instill the fundamental principles of physics into AI models, enabling them to analyze complex quantum data and identify previously hidden patterns in exotic materials.",
        "Power of Foundational Models in Science: Learn how large-scale, pre-trained AI models can be adapted to understand the intricate language of biology.",
        "From Data Overload to Insight: Learn how use of AI is solving a critical challenge: making sense of the enormous datasets generated by modern experimental techniques and facilities.",
        "A New Moore's Law for Materials: The synergy between AI, data accumulated from generations of research, chemistry, and physics creates an exponential acceleration in materials discovery, heralding a new era of technological advancement driven by purpose-designed quantum materials."
      ],
      "nvidia_technology": "Cloud / Data Center GPU",
      "session_id": "S81745",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Harvard Medical School",
          "name": "Marinka Zitnik",
          "title": "Associate Professor of Biomedical Informatics"
        },
        {
          "company": "Cornell University",
          "name": "Eun-Ah Kim",
          "title": "Professor of Physics"
        }
      ],
      "technical_level": "General Interest",
      "title": "Using Agentic AI to Transform Science: From Molecules to Materials",
      "topic": "Biology - Generative AI",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81745/"
    },
    {
      "description": "In this training lab, we'll demonstrate how state-of-the-art reasoning vision-language-actions (VLAs) models, like Alpamayo R1, can be used as part of a safety evaluation pipeline ready for next-generation, end-to-end AI-powered autonomous vehicles (AVs). Learn how to use these powerful reasoning models to aid in data curation for safety validation, using these models to mine the NVIDIA Physical AI Dataset to identify key scenarios to target with additional testing. Python (specifically, Jupyter Lab/Notebook-based computing)",
      "format": "In-Person",
      "industry": "Automotive / Transportation",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Gain hands-on experience with a state-of-the-art reasoning vision-language-action model for autonomous vehicle applications.",
        "Gain practical familiarity with recent NVIDIA open AV ecosystem releases, specifically Alpamayo-R1 and the PhysicalAI-AV dataset.",
        "Get insights into safety evaluation in the autonomous vehicle industry.",
        "Learn how to connect reasoning traces with downstream safety considerations, including using these traces to discover otherwise difficult-to-identify safety-relevant scenarios (e.g., based on counterfactual reasoning).",
        "Understand how reasoning, as a signal, is complementary to other AV model outputs and scenario metadata, empowering data curation workflows and enabling new insights on safety evaluation metrics."
      ],
      "nvidia_technology": "DRIVE, DRIVE SDK, DRIVE AV, Blackwell, Cosmos",
      "session_id": "DLIT81597",
      "session_type": "Training Lab",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Apoorva Sharma",
          "title": "Research Scientist"
        },
        {
          "company": "NVIDIA",
          "name": "Ed Schmerling",
          "title": "Sr. Research Scientist"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Using Reasoning VLAs to Develop Safer Autonomous Vehicles",
      "topic": "Pre-Trained / Foundation Models",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-dlit81597/"
    },
    {
      "description": "Finding the right investor for your startup can be tricky. Venture capital (VC) firms offer more than just funding. They provide mentorship, technical guidance, access to valuable partners and customers, and a range of other benefits. Making the right connections can help your business reach its potential. Join us to hear four visionary VCs pitch their firms and present their thesis. This event is for founders and business leaders looking to connect with some of the most impactful VCs in the technology industry. You'll also get a chance to connect with a VC of your choice for a deeper individual discussion.",
      "format": "Virtual",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Venture capital (VC) firms offer more than just funding. They provide mentorship, technical guidance, access to valuable partners and customers, and a range of other benefits.",
        "Making the right connections can help your business reach its potential.",
        "Connect directly with top VCs."
      ],
      "nvidia_technology": "CUDA",
      "session_id": "S81991",
      "session_type": "Panel",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Troy Estes",
          "title": "Developer Relations"
        },
        {
          "company": "TAL Ventures",
          "name": "Miriam Shtilman-Lavsovski",
          "title": "Partner"
        },
        {
          "company": "Tiny Supercomputer Investment Co.",
          "name": "Philipp Moehring",
          "title": "Co-Founder"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Venture Capitalist Reverse Pitch: EMEA",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81991/"
    },
    {
      "description": "Any fault-tolerant quantum computer must be tightly integrated with classical co-processors that decode quantum error correction measurements in real time. These decoders must operate with extreme speed, high accuracy, and broad versatility. In this talk, I will present our recently proposed Vibe Decoding strategy, in which an ensemble of diverse decoders is run in parallel to achieve performance beyond current state-of-the-art approaches. I'll also discuss its practical realization using CUDA-Q.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Research: Academic",
      "key_takeaways": [
        "Quantum error correction is a critical component in any fault-tolerant quantum computing architecture.",
        "Practical quantum error correction relies on fast and accurate decoding algorithms, and a collaboration between the University of Edinburgh and NVIDIA is developing advanced simulation and benchmarking tools using CUDA-Q.",
        "Vibe Decoding is a newly developed approach from the University of Edinburgh that is particularly well-suited to high-performance implementation on NVIDIA GPUs via CUDA-Q."
      ],
      "nvidia_technology": "Grace CPU, CUDA, Hopper, CUDA Quantum, CUDA-Q",
      "session_id": "S81722",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "University of Edinburgh",
          "name": "Joschka Roffe",
          "title": "EPSRC Quantum Technologies Fellow"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Vibe Decoding Quantum Error Correction With CUDA-Q",
      "topic": "Quantum Computing",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81722/"
    },
    {
      "description": "This session explores how bold product vision can be translated into real, production-grade software using AI-driven engineering practices. We’ll walk through the evolving role of AI across the software life cycle—from ideation and system design to coding, testing, deployment, and continuous improvement.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Turn Vision into Execution Velocity: Discover how AI transforms every stage of development—from design to deployment—so your teams ship high-quality products faster than ever.",
        "Engineer Smarter with Human-led, AI-Augmented Systems: Learn how to harness AI as a true partner, accelerating implementation and iteration while keeping human creativity and intent at the center.",
        "Build Strong Foundations to Scale AI Successfully:"
      ],
      "nvidia_technology": "DGX Platform, CUDA, Hopper, CUDA-X, RTX Virtual Workstations (vWS), Blackwell, DGX Cloud",
      "session_id": "S82224",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Poolside",
          "name": "Eiso Kant",
          "title": "Co-CEO and Co-Founder"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Vision to AI-Driven Software Engineering",
      "topic": "Code / Software Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82224/"
    },
    {
      "description": "As LLMs grow in size, context length, and architectural complexity, vLLM must evolve to meet new performance and scalability challenges. This talk presents key improvements in vLLM's core architecture, including a GPU-first design for zero CPU overheads and an architecture for cluster-scale serving deployment. It also highlights major optimizations in KV cache management and GPU kernels. Gain a detailed technical view of how vLLM is advancing to deliver next-level performance on NVIDIA GPUs at scale.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Evolution in LLMs poses new challenges in inference.",
        "vLLM is moving to a GPU-first architecture to eliminate CPU overheads and unlock higher, more stable performance for large-scale LLM inference.",
        "Cluster-scale serving is becoming a first-class design goal, enabling vLLM to efficiently scale across multi-node GPU deployments for production workloads."
      ],
      "nvidia_technology": "CUDA, NCCL, NIXL",
      "session_id": "S82059",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "University of California Berkeley",
          "name": "Woosuk Kwon",
          "title": "Co-Creator and Co-Lead"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "vLLM in 2026: Architectural Challenges and Performance Optimizations",
      "topic": "Large Language Models (LLMs)",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82059/"
    },
    {
      "description": "本セッションでは、現場の安全性確保と業務効率向上を同時に実現するため、視覚言語モデル(VLM)・要約技術（VSS）を活用した最新のアプローチを探求します。ウェアラブルデバイスを用いて、危険な動作や手順ミスのリアルタイム検知から技能継承に至るまで、現場の課題に対する具体的な技術手法を深く掘り下げます。",
      "format": "Virtual",
      "industry": "Manufacturing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "最新のアーキテクチャーを深く掘り下げる： NVIDIA Metropolis Blueprint for video search and summarization（NVIDIA VSS）と視覚言語モデル（VLM）を統合により、現場映像からリアルタイムで危険動作や手順ミスを検知する技術的な仕組みについて詳しく紹介します。",
        "パフォーマンス向上の実例を学ぶ： エレベーター点検における導入事例を通じて、AIによるリスク検知がいかにして事故を未然に防ぎ、安全管理の質を高めるかを確認します。",
        "新たな支援体制を探求する： デジタル技術と熟練者の経験知を融合することで、現場技術者を孤立させることなく、チーム全体で高度に支援するためのシステム構築手法について議論します。"
      ],
      "nvidia_technology": "Jetson, Metropolis, Blueprint",
      "session_id": "S81896",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "日立ビルシステム",
          "name": "Tatsunori Ohara",
          "title": "主任技師"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "VLM・VSSによる現場の安全性向上と作業効率向上の実現",
      "topic": "Video Analytics",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81896/"
    },
    {
      "description": "Get direct answers to your implementation challenges. This open Q&A session is your chance to connect with the experts responsible for Nemotron Speech to discuss how to maximize the performance of automatic speech recognition (ASR) and text-to-speech (TTS) in your own projects. Whether you are building for a noisy industrial environment, need better multilingual support, or are trying to create intelligent voice agents, our team is here to troubleshoot and share actionable strategies. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Solving Real-World Challenges: How to apply Nemotron speech capabilities—like noise robustness and zero-shot TTS—to fix specific pain points in your application",
        "Strategic Customization: Best practices for fine-tuning Nemotron speech models to handle the unique jargon, accents, or requirements of your specific domain",
        "Achieving State-of-the-Art Quality: Practical techniques you can adopt right now to boost accuracy and naturalness in your deployment"
      ],
      "nvidia_technology": "NeMo, NVIDIA NIM",
      "session_id": "CWES82024",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Negar Habibi",
          "title": "Sr. Manager of Technical Project Management"
        },
        {
          "company": "NVIDIA",
          "name": "Nikhil Srihari",
          "title": "Sr. Deep Learning Scientist"
        },
        {
          "company": "NVIDIA",
          "name": "Slim Essid",
          "title": "Sr. Deep Learning Scientist"
        },
        {
          "company": "NVIDIA",
          "name": "Dharmendra Choudhary (SW-TEGRA)",
          "title": "Sr. Systems Software Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Viet Anh Trinh",
          "title": "Sr. Applied Scientist"
        },
        {
          "company": "NVIDIA",
          "name": "Nourchene Ferchichi",
          "title": "Deep Learning Language Engineer"
        },
        {
          "company": "NVIDIA",
          "name": "Lily Lee",
          "title": "Deep Learning Scientist"
        },
        {
          "company": "NVIDIA",
          "name": "Adi Margolin",
          "title": "Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Tripti Singhal",
          "title": "Senior Solutions Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Voice Agent Applications and Models Development",
      "topic": "Speech Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes82024/"
    },
    {
      "description": "In this hands-on tutorial, two lead engineers walk through how to build real-time, voice-native conversational systems using integrated open-weight models and response generation. Learn when to open models to power low-latency, natural conversations in production. You’ll get practical guidance on customizing Nemotron Speech for your domain, wiring in function calling, and adapting models quickly to new intents, accents, and acoustic environments. We also explore how to use tools like Data Designer to tame fragmented or franchise data sources and turn them into robust, queryable pipelines for conversational AI. Through live coding and demos, you'll see how to stitch together end-to-end pipelines for high-value scenarios so you can develop voice agents with low latency and optimal efficiency at scale.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn when to use Nemotron Speech, versus full‑duplex Nemotron VoiceChat, to power low-latency, real-time, voice-native conversations across automatic speech recognition, natural language understanding, and response generation.",
        "Get hands-on experience customizing open-weight speech models with function calling and rapid domain adaptation for your specific intents, accents, and environments.",
        "See how to use Data Designer to turn fragmented or franchise data sources into robust pipelines that actually feed high-quality conversational AI.",
        "Walk through end-to-end implementations for high-value scenarios like quick-service restaurant ordering (including a Yum! Brands-style flow), contact-center automation, retail “Where is my order?” and returns, and consumer services.",
        "Leave with practical design patterns and code templates that prioritize latency, cost-efficiency, and reliability so you can ship production-ready voice agents after GTC."
      ],
      "nvidia_technology": "Riva, NeMo, NVIDIA NIM",
      "session_id": "S81704",
      "session_type": "Tutorial",
      "speakers": [
        {
          "company": "Yum! Brands",
          "name": "Rockford Yost",
          "title": "Senior Director, Data Science and AI"
        },
        {
          "company": "NVIDIA",
          "name": "Oluwatobi Olabiyi",
          "title": "Director of Engineering"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "Voice-Native Agents: Rearchitecting Conversational AI with Open, Low-Latency Speech Models",
      "topic": "Speech Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81704/"
    },
    {
      "description": "Several experts from the tools development and management teams will be available to talk about getting started, best practices, and advanced techniques for application debugging and optimization with NVIDIA developer tools. You can ask questions about anything from high-level concepts to specific tools details and also discuss plans and suggestions for improving the tools in the future. We can provide a brief overview of the NVIDIA Nsight family of design, profiling, and debugging tools, including an introduction to the latest features and copilot integrations for developing CUDA compute, AI, and graphics applications on the newest platforms. Important: Connect With the Experts sessions are interactive sessions that give you a unique opportunity to meet, in either a group or one-on-one setting, with the minds behind NVIDIA’s products and research to get your questions answered. Attendees can meet with experts on a first-come, first-served basis.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Best-known methods for profiling and debugging AI, CUDA, and graphics workloads",
        "Tips and tricks for identifying performance issues using built-in tools expertise",
        "Learn how to extend and customize tool functionality with recipes, scripts, and cloud features"
      ],
      "nvidia_technology": "CUDA, NSight Comute, NSight Graphics, NSight Systems",
      "session_id": "CWES81549",
      "session_type": "Connect With the Experts",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Rafael Campana",
          "title": "Sr. Engineering Director of CUDA Developer Tools"
        },
        {
          "company": "NVIDIA",
          "name": "Holly Wilper",
          "title": "Manager, System Software Tools"
        },
        {
          "company": "NVIDIA",
          "name": "Magnus Strengert",
          "title": "Software Engineering Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Jonathan Litt",
          "title": "Technical Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Jackson Marusarz",
          "title": "Technical Product Manager"
        },
        {
          "company": "NVIDIA",
          "name": "Gaoyan Xie",
          "title": "Sr. Manager of Software Engineering"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "What's in Your Developer Toolbox? CUDA, AI, and Graphics Profiling, Optimization, and Debugging Tools",
      "topic": "Profilers / Debuggers / Code Analysis",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-cwes81549/"
    },
    {
      "description": "As AI platforms scale from pilots to production, infrastructure decisions become strategic choices that affect cost, reliability, and long-term competitiveness. This session explores why an increasing number of AI platforms are moving away from fully centralized cloud models and toward owning dedicated GPU infrastructure on bare metal. We will discuss how AI platforms are using AI factory models to improve performance predictability, reduce unit costs, and monetize compute directly, while maintaining control over where and how their models run.",
      "format": "In-Person",
      "industry": "Public Sector and Sovereign AI",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "Learn how bare-metal GPUs improve performance consistency and cost predictability for production AI services.",
        "Identify deployment patterns that support sovereign and regional AI offerings, without rebuilding the platform stack.",
        "See how AI factory models help platforms scale globally, while maintaining control over utilization and margins."
      ],
      "nvidia_technology": "Cloud / Data Center GPU, RTX GPU, DGX Platform, Hopper, Blackwell",
      "session_id": "EX82240",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "Hydra Host",
          "name": "Aaron Ginn",
          "title": "CEO"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "Why AI Platforms are Reclaiming Their GPU Infrastructure (Presented by Hydra Host)",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82240/"
    },
    {
      "description": "Wireless network digital twins are a critical building block for AI‑native networks, especially as next‑generation 6G systems will be born first in simulation. NVIDIA Aerial Omniverse Digital Twin (AODT) creates realistic, physics‑based replicas of wireless networks that reflect radio conditions, topology, and traffic patterns, allowing operators and partners to model, test, and optimize 5G and 6G safely before touching the live network. In this session, NVIDIA and partners will show how AODT supports workflows from network planning and optimization to validating new AI‑for‑RAN algorithms and agentic AI scenarios, where agents explore actions in the twin first. Speakers will share real use cases and how integrating AODT helps them create and run more intelligent networks today, and why digital twins are becoming the default way to build AI‑native wireless networks of the future.",
      "format": "In-Person",
      "industry": "Telecommunications",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Learn about the capabilities, use cases, ecosystem solutions, and a roadmap of NVIDIA Aerial Omniverse Digital Twin for enabling 5G and 6G development.",
        "Discover how operators and partners use digital twins to plan networks, validate AI algorithms, and de-risk changes before they go into production.",
        "Gain practical guidance on how to get started with AODT—from integrating it into existing processes to using it for simulations to improve network performance."
      ],
      "nvidia_technology": "Aerial, Omniverse",
      "session_id": "S82022",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "KDDI Research",
          "name": "Satoshi Konishi",
          "title": "President and CEO"
        },
        {
          "company": "NVIDIA",
          "name": "CC Chong",
          "title": "Sr. Director, Head of Aerial Product Management"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "Wireless Network Digital Twins: Use Cases for 5G and 6G",
      "topic": "Industrial Digitalization / Digital Twin",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82022/"
    },
    {
      "description": "This presentation will focus on how scientists can leverage the Doudna supercomputer, to be deployed in 2027 at the National Energy Research Scientific Computing Center (NERSC), to manage and execute complex workflows, particularly those involving data sourced from external scientific experiments. We'll explore the system’s architecture, including the integration of Rubin GPUs, and detail how Doudna’s advanced workflow-centric capabilities accelerate science. We will discuss the relevance of these workflows for cutting-edge fields such as AI, modeling and simulation, and quantum computing, and how these new workloads enhance throughput and support of novel scientific discoveries. Gain insight into what goes into designing an HPC system to support these diverse workflows.",
      "format": "In-Person",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Overview of leveraging Doudna at NERSC for complex, data-driven workflows from external scientific experiments",
        "Understanding of Doudna’s architecture, networking, GPU configuration for driving diverse workflows for scientific discovery",
        "Guidance for supporting science users with new workflows and real-world examples of scientific impact"
      ],
      "nvidia_technology": "Grace CPU, CUDA, Infiniband Networking, Ethernet Networking, Clara Holoscan, Interconnect Networking, NCCL, NeMo, CUDA Quantum",
      "session_id": "S81634",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NERSC, Lawrence Berkeley National Laboratory",
          "name": "Rollin Thomas",
          "title": "Data Architect"
        }
      ],
      "technical_level": "General Interest",
      "title": "Workflow Revolution: How Doudna Drives HPC, AI, and Quantum Scientific Impact",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81634/"
    },
    {
      "description": "More than 80% of AI projects fail. The reason? Customers are not ready to put AI in production in areas where the agent can cause reputational or financial harm. Agent Bricks solves this by automatically generating domain‑specific synthetic data and task‑aware benchmarks, then using those benchmarks to auto‑optimize the agents for accuracy, latency and expense—eliminating the tedious trial‑and‑error cycle. In this session, you’ll see how to design and scale multi‑agent systems that combine retrieval, reasoning, and generation to answer complex, enterprise‑grounded queries, while tracking experiments, fine‑tuning models, and building governed, repeatable workflows that can be deployed at scale. By the end you’ll be equipped to launch production‑ready AI agents with confidence, speed, and budget‑control from Day 1.",
      "format": "In-Person",
      "industry": "All Industries",
      "intended_audience": "Data Scientist",
      "key_takeaways": [
        "Learn the primary challenges for building and deploying AI agents of high quality.",
        "Design and scale multi‑agent systems that combine retrieval, reasoning, and generation to answer complex, enterprise‑grounded queries.",
        "Launch production‑ready AI agents with confidence, speed, and budget‑control from Day 1."
      ],
      "nvidia_technology": "MGX, OVX, IGX, Cosmos",
      "session_id": "EX82242",
      "session_type": "Theater Talk",
      "speakers": [
        {
          "company": "Databricks",
          "name": "Nicolas Pelaez",
          "title": "Technical Marketing Engineer"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Your Agent is Expensive and Wrong (Let's Fix Both!) (Presented by Databricks)",
      "topic": "Retrieval-Augmented Generation (RAG)",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-ex82242/"
    },
    {
      "description": "Join a discussion featuring industry practitioners, along with representatives from both the public and private sectors, as they share how professional certifications add value in today’s AI-driven job market. The panel will highlight practical examples of how certification bridges the gap between education to employment, showing why and how to get started, what support is available, and how to accelerate your professional journey.",
      "format": "Virtual",
      "industry": "Academia / Higher Education",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Hear first-hand stories and examples from professionals across the public, private, and academic sectors regarding AI certifications.",
        "Receive practical tips and key takeaways to help you get started and distinguish yourself as you progress in your field.",
        "Gain clear guidance on selecting and preparing for the right certification.",
        "Participate in a Q&A session with panelists to receive straightforward answers and advice from experienced professionals."
      ],
      "nvidia_technology": "CUDA",
      "session_id": "S81544",
      "session_type": "Panel",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Karine Vardazaryan",
          "title": "Sr. Training Advisor at NVIDIA's Deep Learning Institute (DLI)"
        },
        {
          "company": "poradca ministra školstva",
          "name": "Radoslav Baťo Varga",
          "title": "Ministry of Education, Research, Development and Youth of the Slovak Republic, Adviser to the Minister"
        },
        {
          "company": "HTEC Group",
          "name": "David Senicic",
          "title": "Head of Technology"
        },
        {
          "company": "Capgemini",
          "name": "Etienne Grass",
          "title": "Global Chief AI Officer"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "Your Learning Pathway: Get Certified for Career Success",
      "topic": "Transfer Learning / Fine Tuning",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81544/"
    },
    {
      "description": "本講演では、CFD（数値流体力学）シミュレーションの新たな可能性として、Ansys FluentとNVIDIA Omniverseを統合し、リアルタイムで流体解析結果を高精度かつ直感的に可視化する実践的手法を紹介します。従来のCFD解析は膨大な計算時間と静的な結果表示に制約されていましたが、GPUアクセラレーションとOmniverseの仮想環境を活用することで、エンジニアや研究者は没入型の3D空間でシミュレーションを対話的に操作し、流れ場の変化を瞬時に確認できます。これにより、設計検討や意思決定のスピードが飛躍的に向上し、複雑な流体現象をより深く理解することが可能になります。 さらに、物理ベースのCFD解析とデジタルツイン基盤を組み合わせることで、製品開発や都市計画におけるシナリオ検証が容易になり、複雑な流体挙動を直感的に共有できるため、協調設計やチーム間コミュニケーションの質が向上します。教育分野では、物理の専門知識を持たない学生が視覚化やインタラクティブな学習を通じて計算工学に親しむための効果的な導入戦略を提案し、コンピュータサイエンス教育における新しい学習モデルを提示します。 加えて、東京工科大学のDGX B200スーパーコンピュータ「青嵐」を用いたGPU加速CFDによる東京都区内大規模流体解析の事例を紹介します。都市スケールのシミュレーションでは、膨大なメッシュデータ、境界条件の設定、並列計算の最適化など、技術的課題が山積しています。本講演では、これらの課題を克服するための最新手法と、国土交通省が主導するプロジェクト、PLATEAUのデータを使った都市環境における風況解析を現実応用へ拡張する際の将来展望についても考察します。",
      "format": "Virtual",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Research: Academic",
      "key_takeaways": [
        "Ansys FluentとNVIDIA Omniverseを統合し、ダイナミックな流体シミュレーションをリアルタイムで可視化する手法を習得します。これにより、エンジニアや研究者は没入型環境でCFD結果を対話的に探索できるようになります。",
        "物理ベースのCFDシミュレーションとデジタルツインプラットフォームを組み合わせることで、意思決定の高度化、協調的なエンジニアリング、複雑な流体現象の直感的なコミュニケーションが可能になります。",
        "物理学の背景を持たない学生が、視覚化やインタラクティブな学習を通じて計算工学に親しめるよう、物理ベースのシミュレーションをコンピュータサイエンス教育に取り入れるための戦略を紹介します。",
        "東京工科大学のDGX B200スーパーコンピュータ「青嵐」を用いて、東京都区内の流体力学をシミュレーションする大規模GPU加速CFD実験の事例を学び、都市スケールのシミュレーションを現実応用へ拡張する際の技術的課題と将来の可能性を考察します。"
      ],
      "nvidia_technology": "RTX GPU, DGX Platform, CUDA, Omniverse, Infiniband Networking, cuBLAS, Interconnect Networking, NVLink / NVSwitch, Blackwell, NVIDIA AI Enterprise, NVIDIA Run:ai",
      "session_id": "S81972",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "東京工科大学",
          "name": "Shoichiro Ikuno",
          "title": "ディレクター、デジタルツインセンター"
        }
      ],
      "technical_level": "Business / Executive",
      "title": "インタラクティブCFD最前線：Omniverseで体験するデジタルツインと都市シミュレーション",
      "topic": "Industrial Digitalization / Digital Twin",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81972/"
    },
    {
      "description": "参加者は、Dynamo と LMCache、NIXLを用いて 1,000 km 超にわたり KV キャッシュを共有し、分散データセンター間で大規模言語モデル（LLM）推論をスケールさせる方法を学びます。複数サイトの GPU クラスタや LLM サービスを運用する参加者にとって、本セッションは、分散データセンター間のサイロを解消しつつ、低遅延かつ省電力なサイト間推論を実現する実践的なアーキテクチャを提示する点で重要です。",
      "format": "Virtual",
      "industry": "Manufacturing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "遠隔データセンター間でオールフォトニクスネットワークを介したディスアグリゲーテッド推論処理を実行するための要点を学ぶ",
        "エンタープライズグレードのレベルで、Dynamoを効果的に管理する方法、具体的にはKVキャッシュの分散を制御し、プリフィルデコードのバランスを取る方法を学ぶ",
        "サイト間の KV キャッシュヒット率を高め、プレフィル時間と TTFT（第1トークンまでの時間）を短縮し、複数データセンターの部分キャッシュを融合する仕組みを理解する",
        "NIXL と DOCA オフロードが、多様な RDMA トランスポートを単一の低オーバーヘッドな経路に統合し、長距離リンク上でも GDS（GPU Direct Storage）の利点を維持する方法を学ぶ",
        "統合アーキテクチャが 100 km 超の距離でも低遅延の LLM 推論を維持し、分離されたサイトに比べて約 2 倍のスループットと約 3 倍のエネルギー効率を実現し、分散データセンターをリージョン規模で統合できることを確認する"
      ],
      "nvidia_technology": "DOCA, Dynamo, NIXL",
      "session_id": "S81884",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NTTドコモビジネス株式会社",
          "name": "Kota Tsuyuzaki",
          "title": "イノベーションセンター テクノロジー部門 担当課長 エンジニアリングマネージャー"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "オールフォトニクスネットワークとNVIDIA DynamoによるAIエージェント向けプラットフォームの最適化手法",
      "topic": "AI Inference",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81884/"
    },
    {
      "description": "楽天では、かねてより多様な領域でのAIの実装・活用を進めてきました。近年は、オープンかつ高性能なLLM開発・公開だけでなく、Vertical AI開発にも注力しており、特定の分野に特化したVertical LLMのファインチューニングに取り組んでいます。今回は、金融機関に不可欠な機密性・安全性を確保しつつ、従来モデルでは対応が難しかった金融関連タスクを実現するためのモデル開発における、設計思想や学習データ選定、開発プロセスについて紹介します。",
      "format": "Virtual",
      "industry": "Financial Services",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "従来モデルでは対応が難しかった金融特有のタスクに対して、ファインチューニングによってどのような性能向上が得られるのかを学ぶことができます。",
        "高精度な金融タスク向けモデルを実現するうえで不可欠となる、データの準備とそのデータセット構築プロセスを理解することができます。",
        "金融ドメインに適したベンチマークおよび評価データセットをどのように設計・準備すべきか、その考え方とプロセスを理解することができます。"
      ],
      "nvidia_technology": "NeMo",
      "session_id": "S81891",
      "session_type": "Talk",
      "speakers": [],
      "technical_level": "Technical - Intermediate",
      "title": "ファインチューニングによる金融領域に特化したVertical LLMの実現",
      "topic": "Large Language Models (LLMs)",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81891/"
    },
    {
      "description": "本次分享将结合云深处在实际项目中的训练和部署经验，为大家带来从之前的 Isaac Gym 框架，向模块化程度更高、仿真内容更丰富的 Isaac Lab 框架，迁移强化学习工作流的技巧和注意点。内容涵盖 API 适配、环境搭建与行业应用级部署等核心环节，同时会详解迁移流程简化的关键策略、仿真到现实迁移的性能优化，以及拓展至 Lite3/M20/X30/DR02 系列机器人等工业级应用场景的关键策略。",
      "format": "Virtual",
      "industry": "Academia / Higher Education",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "基于 M20 机器人优化轮腿式机器人的极限移动任务，重点关注强化学习仿真到现实的迁移技术",
        "基于 X30 机器人解决工业部署中的常见挑战，重点关注感控融合与 Isaac Lab 的集成方案",
        "基于 DR02 机器人及更多平台实现人形机器人的实地部署落地应用，重点关注模仿学习与 Isaac Lab 的集成方案"
      ],
      "nvidia_technology": "CUDA, Isaac",
      "session_id": "S81860",
      "session_type": "Talk",
      "speakers": [],
      "technical_level": "Technical - Advanced",
      "title": "从 Isaac Gym 迁移到 Isaac Lab，及未来技术演进",
      "topic": "Robotics Simulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81860/"
    },
    {
      "description": "",
      "format": "Virtual",
      "industry": "Automotive / Transportation",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "模型开发与工程化部署：基于 NVIDIA GPU 架构及 TensorRT-LLM 引擎，实现高效上车部署，提升端到端智能座舱的语音、视觉及行为理解能力。",
        "本地化推理与数据闭环：依托 NVIDIA DRIVE Orin/Thor 芯片，提供低时延模型压缩、动态资源调度、跨模态事件感知及实时多任务推理能力，为复杂车载场景下的个性化 AI 交互、场景适配及安全性能保障提供技术方案。",
        "模型迭代工程设计：探索端云一体架构下的数据同步、模型切换、远程升级及合规安全运维等技术细节，确保座舱 AI 系统的持续演进与定制化优化。"
      ],
      "nvidia_technology": "DRIVE, Blackwell",
      "session_id": "S82027",
      "session_type": "Talk",
      "speakers": [],
      "technical_level": "Technical - Intermediate",
      "title": "从云端到本地，重塑座舱AI",
      "topic": "",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82027/"
    },
    {
      "description": "本演讲将介绍一种将多模态输入转化为物理可驱动资产的生成式方法。我们将详细阐述如何利用 3D 大模型，从输入图像或自然语言，得到具有完整关节结构的3D资产，涵盖自动化关节估计、部件生成以及物理属性推断等技术。 This session introduces a generative method that transforms multimodal inputs into physically drivable assets. We will detail how we leverage 3D large models to transform images or natural language descriptions into fully articulated assets, featuring automated semantic joint estimation, part-aware mesh generation, and precise physical property inference.",
      "format": "Virtual",
      "industry": "Manufacturing",
      "intended_audience": "Research: Non-Academic",
      "key_takeaways": [
        "高保真 OpenUSD / URDF 生成：基于多模态提示词（图像或文本）生成可仿真的资产，自动合成网格、纹理及物理属性，实现即拿即用 High-Fidelity OpenUSD / URDF Generation: Generate simulation-ready assets from multimodal prompts (images or text), automatically synthesizing meshes, textures, and physical properties for immediate utility",
        "语义关节估计与分割：利用 3D 大模型进行语义结构分析，实现精确的部件级分割与运动学关节的自动识别 Semantic Joint Estimation & Segmentation: Leverage 3D large models for semantic structural analysis to enable precise part-level segmentation and automatic identification of kinematic joints",
        "生成式网格补全：利用 3D 大模型的生成先验知识，直接推断缺失的几何结构并合成优化后的拓扑，确保模型结构的完整性 Generative Mesh Completion: Utilize the generative priors of 3D large models to directly infer missing geometry and synthesize optimized topology, ensuring model integrity"
      ],
      "nvidia_technology": "Cloud / Data Center GPU, CUDA, TensorRT, Omniverse, cuBLAS, cuDDN, Triton",
      "session_id": "S81935",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "字节跳动",
          "name": "Dongping Li",
          "title": "Researcher"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "从视觉还原到物理交互： 物理可驱动3D 资产生成技术 Beyond Visuals: A Physics-Informed Generative Method for Articulated 3D Assets",
      "topic": "3D Model Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81935/"
    },
    {
      "description": "与传统的计算机视觉模型相比，基于 VLM 的 Cosmos VSS 服务在视频分析领域的意图识别相关场景中表现出色。在本次演示中，我们将深入探讨一个实际的商业项目，以介绍 VSS 在实际应用中的使用方式，以及如何利用它来构建能够检测交通违规行为的服务。本次演示将带您了解我们在部署 Cosmos VSS 时所面临的挑战、资源需求，以及我们如何利用真实数据集对其进行微调以满足客户特定的业务目标。",
      "format": "Virtual",
      "industry": "Smart Cities / Spaces",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "VLM 在视频分析领域中带来的可快速落地和复制的新业务价值",
        "利用 Cosmos VSS 相关的 Blueprint 和 NIMs 构架生产系统（架构、部署方案、业务模型等）",
        "Cosmos VSS 在智慧交通领域的真实业务场景和价值"
      ],
      "nvidia_technology": "Cosmos",
      "session_id": "S81823",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "图灵新智算（广州）科技集团有限公司",
          "name": "Frank Chen",
          "title": "研发副总裁"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "利用 NVIDIA Cosmos VSS 构建智慧交通（ITS）违章检测系统",
      "topic": "Video Analytics",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81823/"
    },
    {
      "description": "聚焦 LongCat-Flash 560B MoE 模型的系统级协同设计实践，展示如何实现面向 Agent 场景的极低延时与高性价比推理。在多层次并行调度与动态算子融合的支撑下，LongCat-Flash 在单用户 100 TPS 的高并发场景中，以每百万输出 token 仅 0.7 美元 的成本，达成吞吐与成本的最优平衡。 该架构通过结构相关的算子编排策略，充分调度 NVLink 高速互联、NIC 网络接口卡、GPU 计算单元及内存带宽 等异构资源，构建高吞吐、低延时的推理系统。依托 NVIDIA Dynamo router 等关键组件，系统在生产环境中实现高可用性与动态容错能力，稳态维持推理服务的 SLA 一致性，为智能体时代的大规模推理服务提供可落地的通用范式。",
      "format": "Virtual",
      "industry": "Consumer Internet",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "本文以 LongCat-Flash 为案例，深入探讨如何实现模型吞吐与推理效率的协同平衡——在 560B 亿参数的 MoE 模型上，以每百万输出 token 0.7 美元的成本达成 100 TPS 的单用户处理时延",
        "我们的目标是构建高效推理系统，通过结构相关的算子编排策略，实现 NVLink 高速互联、网络接口卡（NIC）、计算单元与内存带宽等异构硬件资源的高效运转，在降低延时的情况下提升系统整体吞吐",
        "借助 Dynamo 等技术栈，在生产环境的严苛要求下实现系统高可用性，并通过实时容错机制确保服务 SLA"
      ],
      "nvidia_technology": "CUDA, TensorRT, Infiniband Networking, Hopper, cuBLAS, NCCL, NSight Comute, NSight Systems, NVLink / NVSwitch, Blackwell, Dynamo",
      "session_id": "S81943",
      "session_type": "Talk",
      "speakers": [],
      "technical_level": "Technical - Intermediate",
      "title": "利用 NVIDIA Dynamo Router 的 LongCat-Flash 高效推理案例研究：面向智能体时代的模型-系统协同设计",
      "topic": "Text Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81943/"
    },
    {
      "description": "我们将展示探索如何利用图神经网络 (GNN) 和 NVIDIA PhysicsNeMo 将触觉传感器建模为具有实时、高保真物理特性的可变形体，从而加速柔性仿真的计算。",
      "format": "Virtual",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "大多数机器人目前仍仅执行粗略的抓取 操作；而精细任务则需要精确且高性能的物理仿真，尤其是涉及可变形接触的场景",
        "利用图神经网络 (GNN) 对触觉传感器表面的恒定拓扑结构进行建模，通过数据驱动的方式让网络学习弹性体的应变信息",
        "与传统求解器相比，利用 PhysicsNeMo 中的 GNN 求解器具有高精度和显著的效率优势"
      ],
      "nvidia_technology": "Cloud / Data Center GPU, RTX GPU, CUDA, Modulus, CUDA-X",
      "session_id": "S81969",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Juana Du",
          "title": "机器人解决方案架构师"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "利用 PhysicsNeMo 加速机器人中的柔性触觉传感器仿真",
      "topic": "Robot Manipulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81969/"
    },
    {
      "description": "NVIDIA 初创加速计划深耕中国十年，陪伴上千家创业企业从技术探索走向商业落地。2025 年，我们与生态伙伴持续携手，为中国创业公司提供了一系列产品、技术、市场、资本与业务对接的全链路支持，同时迎来了会员规模突破 3000 家的重要里程碑。本视频将回顾 2025 年该项目如何赋能中国创业者加速成长，并展望 2026 年在生态协同与创业扶持上的新机遇。同时，3 家 NVIDIA 初创加速计划会员企业也将分享他们如何借助 NVIDIA 的创新技术快速实现业务跃迁，为行业与社会创造更大价值。",
      "format": "Virtual",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "NVIDIA 初创加速计划如何助力会员企业加速成长",
        "创业企业如何通过 NVIDIA 创业生态获得多元化的支持",
        "会员企业成功案例分享：AI 创业企业如何推动行业变革，为社会创造更大价值"
      ],
      "nvidia_technology": "Jetson, CUDA, TensorRT, Clara Parabricks, cuDDN",
      "session_id": "S81981",
      "session_type": "Panel",
      "speakers": [],
      "technical_level": "Business / Executive",
      "title": "十载相伴，NVIDIA 赋能创业公司在 AI 时代加速前行",
      "topic": "Medical Imaging",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81981/"
    },
    {
      "description": "我们持续优化并维护新一代的 DeepGEMM × FlashMLA × DeepEP 技术栈，为 Blackwell 平台上的训练和推理提供强大支持。本次报告将重点介绍围绕 DeepSeek 模型在 FlashMLA 和 DeepGEMM 上所做的一系列优化工作，其中包括提升训练与推理核函数性能的 MLA 形状相关流水线优化、针对稀疏注意力内核的优化、面向特殊 batch 分布的优化，以及在特定场景和组件上的 GEMM 相关改进，例如 MoE GEMM、反向传播和小 batch 推理等；同时，我们还将在 DeepEP 中介绍 FP8/FP4、节点内通信支持及其优化。",
      "format": "Virtual",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "了解 DeepGEMM 2.0 如何在 Blackwell 平台上为 DeepSeek 模型提供优化的 GEMM、分组 GEMM 和 DSA 索引器，并学习我们在常见部署场景中采用的关键优化策略",
        "研究 FlashMLA 中 DSA 的稀疏性，并提出多种优化策略，以提升 Blackwell 平台上稀疏注意力核函数的性能",
        "了解如何在 DeepEP 中利用 NVFP4 量化、优化的 token 分发以及单批次重叠策略，在 GB200 集群上实现业界领先的 LLM 服务性能"
      ],
      "nvidia_technology": "CUDA, Blackwell",
      "session_id": "S81938",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "NVIDIA",
          "name": "Zeyu WANG",
          "title": "DevTech Engineer"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "基于 DeepGEMM × FlashMLA × DeepEP 对 DeepSeek 模型在 Blackwell 的全栈深度优化",
      "topic": "Deep Learning Frameworks",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81938/"
    },
    {
      "description": "本讲座将介绍基于 Megatron-Core 的动态自适应并行训练方案，该方案通过运行时自适应技术解决长尾序列训练中计算与内存的严重失配问题。其直接应对当前大规模视频模型训练中极端样本带来的效率瓶颈，已在数千GPU集群上验证能显著提升训练速度与资源利用率，是推动下一代多模态基础模型高效训练的关键技术。",
      "format": "Virtual",
      "industry": "Consumer Internet",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "Transformer 架构的模型训练中，计算量与序列长度平方成正比，内存仅与长度线性相关。少量极端长样本会急剧放大二者差距，引发并行气泡与高通信开销。以上下文并行为例：长样本需高并行度，短样本则因此承受低计算强度与额外通信负担，静态方案难以兼顾",
        ". 本 talk 介绍基于计算强度与内存占用的自适应动态训练方案，支持运行时调整并行维度、微批次数量及重计算/卸载级别，实现异构序列的均衡高效训练",
        "方案严格保持全局批次分布不变，避免分桶、填充等带来的效果损失。经数千块NVIDIA GPU 集群验证，在 Kling 视频生成模型训练中端到端性能提升达 60%。核心特性已被 Megatron-Core 团队采纳并集成"
      ],
      "nvidia_technology": "CUDA",
      "session_id": "S82347",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "快手科技有限公司",
          "name": "Wu Guohao",
          "title": "高级深度学习工程师"
        },
        {
          "company": "NVIDIA",
          "name": "Kunlun Li",
          "title": "Devtech工程师"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "基于 Megatron-Core 的动态自适应并行训练方案",
      "topic": "Video Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82347/"
    },
    {
      "description": "在腾讯混元大模型支持的各项服务中，长文本请求消耗的 GPU 计算资源占比高达 60%。随着深度搜索（DeepSearch）与智能体（Agent）技术的兴起，长文本处理已成为大模型推理服务的主要成本负担与优化核心方向。为此，我们针对长文本场景落地实施了稀疏注意力机制、并行计算策略及键值缓存（KV Cache）优化等一系列技术方案，不仅取得了显著的性能提升，还有效降低了线上长文本推理的部署成本。",
      "format": "Virtual",
      "industry": "HPC / Scientific Computing",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "为何首 token 生成时延（TTFT）是长上下文（LongContext）技术的关键瓶颈？阐述长上下文场景下首 token 生成时延面临的核心挑战：GPU 内存瓶颈、键值缓存（KV Cache）的管理与传输问题、多节点部署难点",
        "腾讯混元长上下文大模型推理中，如何通过稀疏注意力机制实现首 token 生成时延 40% 的性能提升，及其他核心优化技术细节长文推理并行策略优化、参数分离（PD）解耦部署架构、分阶段键值缓存（KV Cache）卸载与复用机制",
        "面向长上下文场景的模型并行计算策略及配套的通信优化方案"
      ],
      "nvidia_technology": "CUDA, Hopper, NCCL, NSight Systems, NIXL",
      "session_id": "S81986",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "腾讯",
          "name": "Jeson Xiang",
          "title": "AI Inference Architect"
        }
      ],
      "technical_level": "Technical - Intermediate",
      "title": "基于 NVIDIA Hopper 架构的 LLM 长文本推理场景性能优化实践和探索",
      "topic": "AI Inference",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81986/"
    },
    {
      "description": "我们基于 NVIDIA GPU，针对快手内部的生成式推荐模型 OneRec 做了较为深入的软硬结合优化，高效利用 TensorCore 算力、HBM、高速 RDMA 网络在单机算力/多机互联两个维度加速生成式推荐训练，助力 OneRec 在全场景的部署上线。",
      "format": "Virtual",
      "industry": "Consumer Internet",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "我们基于 NVIDIA GPU，针对快手内部的生成式推荐模型 OneRec 做了较为深入的软硬结合优化，高效利用 TensorCore 算力、HBM、高速 RDMA 网络在单机算力/多机互联两个维度加速生成式推荐训练，助力 OneRec 在全场景的部署上线",
        "针对 OneRec 负载，基于 cuda/cudf 做了专门的 Sparse/Dense 计算优化，相比开源方案有数倍提升",
        "基于 NCCL 做了一套动态通讯算子协商机制，在多级流水下更高限制的打满 RDMA 带宽"
      ],
      "nvidia_technology": "CUDA",
      "session_id": "S81984",
      "session_type": "Talk",
      "speakers": [],
      "technical_level": "Technical - Advanced",
      "title": "基于 NVIDIA HPC 打造端到端生成式推荐 OneRec 训练：面向工业级推荐大模型的”超算引擎",
      "topic": "Recommenders / Personalization",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81984/"
    },
    {
      "description": "",
      "format": "Virtual",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [],
      "nvidia_technology": "Isaac, Omniverse, PhysX",
      "session_id": "S81971",
      "session_type": "Talk",
      "speakers": [],
      "technical_level": "Technical - Intermediate",
      "title": "基于 NVIDIA Isaac Sim 的 Genie Sim 3.0 仿真平台加速工业级作业机器人落地部署",
      "topic": "Robot Manipulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81971/"
    },
    {
      "description": "本演讲课程主要围绕 NVIDIA Jetson Thor 为核心，重点分享基于 Jetson Thor 的机器人视觉解决方案。并分析和讨论行业主流功能应用，并深入讨论全新的机器人视觉解决方案，行业应用和未来展望。",
      "format": "Virtual",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": ["NVIDIA Jetson Thor 及 HSB 赋能的新机器人视觉解决方案"],
      "nvidia_technology": "Jetson",
      "session_id": "S81962",
      "session_type": "Talk",
      "speakers": [],
      "technical_level": "Technical - Beginner",
      "title": "基于 NVIDIA Jetson Thor 的机器人视觉解决方案",
      "topic": "Robot Perception",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81962/"
    },
    {
      "description": "从帮助团队快速搭建和优化多智能体工作流的 NVIDIA NeMo Agent Toolkit，到围绕 Data Flywheel 与 NVIDIA Agentic Blueprint 构建可自我提升的代理式 AI 系统，再到面向物理 AI 的 NVIDIA Isaac GR00T 基础模型、辅助驾驶以及 NVIDIA Cosmos 世界模型等平台能力。NVIDIA 技术专家将系统讲解如何借助 NVIDIA 完整技术栈，更高效地设计、训练与部署新一代代理式 AI 与 物理 AI 应用。同时，3 家 NVIDIA 初创加速计划会员企业将通过生动的案例展示如何借助这些技术突破业务瓶颈，推动行业变革与发展。",
      "format": "Virtual",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "了解 NVIDIA 代理式 AI 和物理 AI 的全栈解决方案",
        "如何借助 NVIDIA Nemo Agent Toolkit 快速开发代理式 AI 应用",
        "如何借助 NVIDIA Cosmos 和 NVIDIA Isaac GR00T 平台加速物理式 AI 的应用开发"
      ],
      "nvidia_technology": "NeMo, Cosmos, Blueprint",
      "session_id": "S81974",
      "session_type": "Panel",
      "speakers": [],
      "technical_level": "Technical - Beginner",
      "title": "基于 NVIDIA 全栈技术打造代理式 AI 与物理 AI 的未来基石",
      "topic": "AI Platforms",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81974/"
    },
    {
      "description": "KV Cache 通过“以存储换计算”的方式显著提升了大语言模型（LLM）的推理效率。然而，受限于高带宽内存（HBM）容量有限且成本高昂，将 KV Cache 扩展至外部高性能存储系统已成为关键优化方向。 本次 Session 聚焦我们与 NVIDIA 联合开展的系统级协同优化工作，在满足服务等级目标（SLO）的前提下，针对 LLM 推理中动态变化的工作负载，提出一套端到端的全局 KV Cache 解决方案。该方案涵盖：推理引擎与远程存储的深度集成优化，全局 KV Cache 元数据的统一管理与配置策略，面向 KV Cache 访问模式定制的远程存储架构设计。在典型 LLM 推理场景中，该方案实现了显著性能与成本收益：Cache 命中率提升 39%，P99 延迟降低 78%，单位 token 的计算与存储综合成本降至原来的 24%（即降低 76%）。",
      "format": "Virtual",
      "industry": "Cloud Services",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "经济性建模与权衡分析：我们构建了 KV Cache 存储成本与推理计算开销的联合帕累托模型，可精准评估不同配置下在延迟、吞吐和总拥有成本 Cost-Efficiency Modeling: We develop a Pareto-aware economic model that jointly evaluates KV cache storage cost and inference compute, enabling accurate trade-off analysis across latency, throughput, and total cost of ownership.（TCO）之间的平衡点，为系统设计提供量化依据。",
        "标准化元数据管理与仿真平台：在全局 KV Cache 池化管理成为行业共识的基础上，我们提出一个存储无关的标准化元数据管理组件，支持异构存储层级（如 DRAM、SSD、远程内存等）。配合高保真 KV Cache 模拟器，可自动评估各类配置的经济效益，并推荐最优部署策略 Unified Metadata & Simulation Framework: We propose a standardized, storage-agnostic KV cache metadata manager that supports heterogeneous storage tiers. Coupled with a high-fidelity KV cache simulator, it enables automated configuration selection for optimal cost-performance outcomes.",
        "面向下一代注意力机制的演进支持：随着模型架构从 Full Attention 向 Hybrid Attention（如 Linear Attention、Sparse Attention）演进，KV Cache 的管理复杂度显著提升。我们在推理引擎内部实现了对 Hybrid Attention / Cache / Memory 的协同优化，并通过可扩展的元数据管理框架，为未来注意力机制的持续演进提供底层支持 Future-Proofing for Advanced Attention Mechanisms: As models evolve from full attention to hybrid architectures (e.g., linear or sparse attention), KV cache management must co-evolve. We demonstrate how our inference engine and metadata layer are designed to support emerging attention patterns—enabling seamless integration of Hybrid Attention, dynamic cache strategies, and memory-aware scheduling."
      ],
      "nvidia_technology": "CUDA, Hopper, NSight Systems",
      "session_id": "S82360",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "阿里云智能集团",
          "name": "Zhengheng Wang",
          "title": "阿里云智能集团高级技术专家"
        },
        {
          "company": "NVIDIA",
          "name": "Shunkang Zhang",
          "title": "DevTech AI Engineer"
        }
      ],
      "technical_level": "Technical - Advanced",
      "title": "基于全局 KV Cache 存储系统的高效 LLM 推理加速方案 Efficient LLM Inference Acceleration via a Global KV Cache Storage System",
      "topic": "AI Inference",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82360/"
    },
    {
      "description": "本报告将系统阐述基于可控扩散模型的新一代机器人学习框架。首先，我们提出一种用于全新的可控扩散模型的统一理论框架，揭示其如何为机器人决策提供一种兼具表达力与可控性的数学新范式。随后，将展示一种突破性的、原生运行于 GPU 的 on-policy 扩散强化学习算法，在效率上树立新的标杆。最后，我们将通过真实的机器人实验展示其在精细操作与运动控制等核心任务上达到的业界领先性能。 This session will systematically elaborate on a new generation robot learning framework based on controllable diffusion models. First, we propose a unified theoretical framework for a novel controllable diffusion model, revealing how it provides a new mathematical paradigm for robot decision-making that combines expressiveness and controllability. Subsequently, a groundbreaking on-policy diffusion-based reinforcement learning algorithm, natively running on GPUs, will be presented, setting a new benchmark in efficiency. Finally, we will demonstrate its industry-leading performance in core tasks such as dexterous manipulation and motion control through real-world robot experiments.",
      "format": "Virtual",
      "industry": "All Industries",
      "intended_audience": "Research: Academic",
      "key_takeaways": [
        "理论基础：提出了一个用于可控扩散模型的新理论框架，为机器人决策建立了兼具表达力与可控性的数学新范式 Unified Theoretical Framework: A novel theoretical framework for controllable diffusion models is introduced, establishing a new expressive and controllable mathematical paradigm for robot decision-making.",
        "强化学习突破：提出新一代扩散强化学习范式，通过大规模 GPU 并行，在模型探索性上显著超越传统方法 Reinforcement Learning Breakthrough: We introduce a next-generation diffusion reinforcement learning paradigm that dramatically outperforms traditional methods in sample efficiency through massive GPU parallelism.",
        "业界领先的机器人性能：真实的机器人实验验证了该框架在精细操作与运动控制等核心任务上达到了业界领先的性能水平 Leading Robot Performance Real-world experiments validate the framework's state-of-the-art performance in critical robotic tasks like dexterous manipulation and motion control.",
        "如何在 Isaac Lab 基准测试中使用 Nsight 工具进行性能分析 How to use Nsight profiling in Isaac Lab benchmarks."
      ],
      "nvidia_technology": "Isaac",
      "session_id": "S81979",
      "session_type": "Talk",
      "speakers": [],
      "technical_level": "Technical - Advanced",
      "title": "基于扩散模型的具身智能：从统一理论到 GPU 加速的机器人策略 Diffusion-Driven Embodied AI: From Unified Theory to GPU-Accelerated Robot Policies",
      "topic": "Humanoid Robots",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81979/"
    },
    {
      "description": "跨越 Sim2Real 鸿沟是通用机器人落地的核心挑战。本 Session 将分享一套基于 NVIDIA Omniverse 的端到端工程实践，展示如何利用高保真 USD 资产构建“训练-评估-优化”闭环。我们将结合工厂搬运与家庭精细操作的实战案例，探讨如何通过物理仿真解决数据稀缺与泛化性难题，实现从虚拟到现实的无缝迁移。",
      "format": "Virtual",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "构建高保真 USD 数字资产，支撑精细化仿真操作",
        "训练与推理闭环：仿真执行 VLA 的推理结果，并规模测试",
        "建立自动化测试与性能评估，沉淀成功轨迹用于再训练",
        "把控 Sim2Real 部署要点，提升仿真到真机一致性"
      ],
      "nvidia_technology": "Isaac",
      "session_id": "S81619",
      "session_type": "Talk",
      "speakers": [],
      "technical_level": "Technical - Intermediate",
      "title": "机器人仿真实战：从资产到真机部署的全流程讲解",
      "topic": "Robotics Simulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81619/"
    },
    {
      "description": "本セッションでは、GMO GPUクラウドに新しく導入したHGX B300とSpectrum-Xを組み合わせた次世代GPU基盤について、どれだけ性能が変わったのかをベンチマークを通じて分かりやすくお伝えします。 また、こうした性能向上が、学習時間の短縮やコストの抑制、安定したGPUの提供といった形で、実際にサービスをご利用いただく皆さまにどんなメリットを生むのか、という点にも触れたいと思います。 技術的な進化とサービス価値の両面から、次世代GPUクラウドの可能性を共有します。",
      "format": "Virtual",
      "industry": "Cloud Services",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "HGX B300とSpectrum-Xが前世代のGPUと比べてどれだけ性能向上したのか",
        "性能向上が学習時間やコスト、安定性にどう影響するか",
        "HGX B300が我々のビジネスにどう影響を与えるのか"
      ],
      "nvidia_technology": "HGX, Interconnect Networking, Blackwell",
      "session_id": "S81893",
      "session_type": "Talk",
      "speakers": [],
      "technical_level": "Technical - Intermediate",
      "title": "次世代GPU基盤の実力とサービス価値 NVIDIA HGX B300とNVIDIA Spectrum-Xがもたらす変化",
      "topic": "Infrastructure",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81893/"
    },
    {
      "description": "NVIDIA 将与创投联盟中的优秀投资人代表及典型被投企业代表共同探讨 2026 年中国 AI 市场的前景，分析市场的潜在爆发点、新兴创业企业以及新机遇。此外，他们还将就大模型、生成式 AI、AI 智能体、物理 AI 等热门行业的投资趋势，以及投资人和投资机构如何选择被投企业等话题进行深入探讨。",
      "format": "Virtual",
      "industry": "All Industries",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "聚焦 AI 智能体、物理 AI 等落地方向，启发投资与创业赛道新思路",
        "了解 AI 商业化及全球化实操落地路径，探索中国 AI 赢利点与全球化潜力",
        "洞察 2026 AI 年度趋势，捕捉行业变革中的潜在机遇"
      ],
      "nvidia_technology": "Isaac, NVIDIA NIM, Cosmos",
      "session_id": "S81846",
      "session_type": "Panel",
      "speakers": [],
      "technical_level": "Business / Executive",
      "title": "洞察 2026 中国 AI 市场 — AI 智能体和物理 AI 浪潮下的创业风口",
      "topic": "Image Generation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81846/"
    },
    {
      "description": "本発表では、川崎重工が取り組むロボット技術の社会実装について、製造業で培った作業分析とAI・遠隔操作技術を基盤に応用が拡大している点を示す。特に医療分野では、病院全体を対象とする手術室から病棟まで一連の業務を統合的に支援する「ワンストップソリューション」の実現を目指し、手術支援ロボット「hinotori」を中心に、Issac for HealthcareやMONAIなどを活用したポート配置最適化、AIガイド表示、画像強調臨床的価値を高める取り組みを紹介する。さらに将来への展開として、屋外ロボティクスとして未来型モビリティCORLEOを対象とした「SAFE ADVENTURE」構想も示し、AlpamayoやCOSMOSを活用しつつ、ロボットが産業・医療・生活領域で新たな価値を創出する可能性を説明する。",
      "format": "Virtual",
      "industry": "Healthcare & Life Sciences",
      "intended_audience": "Business Executive",
      "key_takeaways": [
        "外科手術自動化におけるリアルタイムAIコンピューティング",
        "NVIDIA Isaac for HealthcareとCosmosを活用した、外科手術ロボット向けの包括的なシミュレーションおよび検証環境の構築",
        "低レイテンシ推論とデジタルツインに基づくロボットワークフローの例"
      ],
      "nvidia_technology": "Isaac, Cosmos",
      "session_id": "S82107",
      "session_type": "Talk",
      "speakers": [],
      "technical_level": "Business / Executive",
      "title": "産業・医療・モビリティを横断するフィジカルAI　〜 AIによる統合戦略 〜",
      "topic": "Robot Navigation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s82107/"
    },
    {
      "description": "在城市NOA走向大规模量产的当下，整个智能驾驶行业正面临两大共性挑战：一是如何更有效地应对复杂的长尾场景，二是如何以可控成本实现规模化能力泛化与交付。而基座模型（Foundation Model）的出现，为解决这两大痛点提供了新的范式可能。 作为最早实践“视觉-语言-动作”（VLA）模型的人工智能企业之一，元戎启行正通过构建统一架构的 Foundation Model，打造具备更强认知推理能力的辅助驾驶解决方案。目前，元戎启行已实现超过 20 万辆具备城市 NOA 功能的辅助驾驶系统量产交付，并通过真实道路数据形成持续优化的研发飞轮，计划于 2026 年达成百万级辅助驾驶交付规模。 本次演讲将系统分享元戎启行如何以 Foundation Model 为智能辅助驾驶系统的“研发引擎”，结合 NVIDIA 平台协同，构建从训练、验证及部署的研发闭环，为下一代辅助驾驶系统打造智能进化的“底座能力”。",
      "format": "Virtual",
      "industry": "Automotive / Transportation",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "2026 年的核心挑战在于应对“高频长尾场景”，目标是从传统的被动响应，全面进化为具备“类人级别”的主动安全和防御性驾驶能力",
        "如何通过 Foundation Model，破解长尾场景与如何 scale up 等行业性难题",
        "如何结合真实道路数据与 NVIDIA 工具链，加速完成数据挖掘、自动标注与仿真验证等能力闭环"
      ],
      "nvidia_technology": "DRIVE",
      "session_id": "S81995",
      "session_type": "Talk",
      "speakers": [],
      "technical_level": "Technical - Advanced",
      "title": "用基座模型重塑智能边界 Redefining the Boundaries of Autonomous Driving with Foundation Model",
      "topic": "AI Platforms",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81995/"
    },
    {
      "description": "一般的な LLM では処理が難しい、製造業で用いられる技術文書などの複雑かつ機密性の高いドキュメントの読み取りに特化したソブリン AI / ドキュメント読解 LLM の開発について紹介します。また、ドメイン特化モデルのトレーニングにおいて直面した実際の課題と、それらを NVIDIA の先端技術を用いてどのように解決したかを解説します。 更に、信頼性・生産性・イノベーション向上のために、企業がこれらのソブリン AI をどのように導入しているかについて、先端事例を紹介します。",
      "format": "Virtual",
      "industry": "Public Sector and Sovereign AI",
      "intended_audience": "Research: Non-Academic",
      "key_takeaways": [
        "一般的な LLM では処理が難しい、機密性の高い複雑ドキュメント向けソブリン AI・特化型 LLM の必要性や構築方法を学ぶ",
        "Nemotron-Personas などのNVIDIAの先端技術で、ドメイン/言語特化型 LLM 構築における課題をどのように克服するかについて学ぶ",
        "信頼性・生産性・イノベーション向上のために、企業がどのようにソブリン AI を活用しているかの先端事例について知る"
      ],
      "nvidia_technology": "TensorRT, Hopper, NeMo, NVIDIA NIM",
      "session_id": "S81712",
      "session_type": "Talk",
      "speakers": [
        {
          "company": "Stockmark, Inc.",
          "name": "Kosuke Arima",
          "title": "Co-Founder and CTO"
        }
      ],
      "technical_level": "Technical - Beginner",
      "title": "複雑ドキュメント向けソブリンAI：日本の先端事例から学ぶ",
      "topic": "Retrieval-Augmented Generation (RAG)",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81712/"
    },
    {
      "description": "随着集成触觉感知的高自由度（High-DoF）灵巧手产品出现——例如 SharpaWave——机器人灵巧操作以及高质量数据采集迎来了新的发展机遇。 本次分享将介绍如何利用触觉信息来：1)训练模型，使机械手能够根据接触状态自适应调整行为，从而实现精细的操作动作，2)提升操作数据采集的效率与质量，3)缩小在接触丰富的任务中从仿真到现实（Sim-to-Real）的差距。演讲还将涵盖一系列实用方法与实现方案，包括：1) 在 NVIDIA Isaac Sim 中对触觉灵巧机械手进行建模与仿真，2)通过仿真中的强化学习生成训练数据，3)在真实硬件实验中进行模型训练与验证",
      "format": "Virtual",
      "industry": "All Industries",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "高自由度灵巧机械手如何实现多样化且复杂的操作任务",
        "将触觉信息与机器人学习相结合，以提升系统的鲁棒性与能力",
        "基于动态触觉信息的建模如何帮助消减接触丰富任务中的 Sim-to-Real 差距"
      ],
      "nvidia_technology": "Cloud / Data Center GPU, RTX GPU, CUDA, Isaac, Omniverse, PhysX",
      "session_id": "S81577",
      "session_type": "Talk",
      "speakers": [],
      "technical_level": "Technical - Intermediate",
      "title": "通过触觉 AI 解锁高自由度灵巧操作",
      "topic": "Robot Manipulation",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81577/"
    },
    {
      "description": "在本次演讲中，我们主要探讨高速公路数字化转型的商业背景及技术挑战。基于我们对智能交通领域的深入积累，我们采用了云边协同的架构，在控制成本的同时，显著提升了系统性能。通过紧耦合的软-硬件协同设计，我们在特殊交通场景下提升了系统的视觉感知与视频理解能力。此外，我们还针对特定交通场景，对 VLM 模型进行了 finetune，使其更好地融入现有系统，从而提高了系统的准确性与稳定性。",
      "format": "Virtual",
      "industry": "Smart Cities / Spaces",
      "intended_audience": "Developer / Engineer",
      "key_takeaways": [
        "基于对智能交通深入理解而构建的云边协同架构，在控制成本的同时，显著提升了系统性能",
        "通过紧耦合的软-硬件协同设计，提升特定交通场景下，系统系统的视频理解能力",
        "经过后训练微调的 VLM 模型专门用于处理特定的交通场景，该模型被集成到现有系统中，从而提升了系统对交通事件的识别能力"
      ],
      "nvidia_technology": "Hopper",
      "session_id": "S81961",
      "session_type": "Talk",
      "speakers": [],
      "technical_level": "Technical - Intermediate",
      "title": "高速公路数字化转型——基于云边协同的 VLM 大模型行业长尾场景落地",
      "topic": "Video Analytics",
      "url": "https://www.nvidia.com/gtc/session-catalog/sessions/gtc26-s81961/"
    }
  ]
}
